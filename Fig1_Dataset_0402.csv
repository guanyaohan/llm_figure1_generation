paper_title,arxiv_id,fig1_file_path,fig1_caption,abstract,introduction
GenTranslate: Large Language Models are Generative Multilingual Speech and Machine Translators,2402.06894v2,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2402.06894v2.pdf,"Illustration of (a) Typical seq2seq translation with beam search decoding and top-1 hypothesis selection, (b) our ""GenTranslate"" with LLM integration.","Recent advances in large language models (LLMs) have stepped forward the development of multilingual speech and machine translation by its reduced representation errors and incorporated external knowledge. However, both translation tasks typically utilize beam search decoding and top-1 hypothesis selection for inference. These techniques struggle to fully exploit the rich information in the diverse N-best hypotheses, making them less optimal for translation tasks that require a single, high-quality output sequence. In this paper, we propose a new generative paradigm for translation tasks, namely ""GenTranslate"", which builds upon LLMs to generate better results from the diverse translation versions in N-best list. Leveraging the rich linguistic knowledge and strong reasoning abilities of LLMs, our new paradigm can integrate the rich information in N-best candidates to generate a higher-quality translation result. Furthermore, to support LLM finetuning, we build and release a HypoTranslate dataset that contains over 592K hypotheses-translation pairs in 11 languages. Experiments on various speech and machine translation benchmarks (e.g., FLEURS, CoVoST-2, WMT) demonstrate that our GenTranslate significantly outperforms the state-of-the-art model.","Recent advances in large language models (LLMs) have attracted a surge of research interest due to their strong abilities in logical reasoning and language generation. These models have achieved surprisingly wide-ranging success across various natural language processing (NLP) tasks. 
In the realm of NLP, the translation tasks, which encompasses speech and machine translation (ST & MT), hold significant practical importance for global communication. Similar to other NLP tasks, translation tasks also gain a notable progress thanks to the recent advancement of LLMs. In the domain of speech translation, Whisper demonstrates superior performance by collecting 680K-hour data for web-scale model training. AudioPaLM2 integrates both textand speech-based language models into a unified architecture to process and generate text and speech, thereby augmenting speech translation performance to a great extent. On the other hand, LLMs also show remarkable ability in machine translation. NLLB is the first to extend LLMs' linguistic capability to over 200 languages. BigTranslate is finetuned on LLaMA with multilingual instruction tuning, which achieves comparable performance to ChatGPT and Google Translate. Most recent work proposes SeamlessM4T, a foundational multilingual and multitask model that can translate across speech and text, which achieves the state-of-the-art on both ST and MT tasks on various public datasets. 
Despite the superior performance, most existing translation models employ the typical beam search algorithm for inference and select the top-1 hypothesis as final output (see Fig. (a) ), following that in automatic speech recognition (ASR). However, this strategy discards the 2 to N-best hypotheses that could be advantageous to the generation of ground-truth translation. As illustrated in Fig. , the discarded 2 to N-best hypotheses contain abundant semantic information that is the key to composite the ground-truth utterance, while the 1-best hypothesis lacks this part of information. As a result, the typical top-1 hypothesis selection is sub-optimal to the translation tasks that require a single informative and high-quality output sequence. 
Inspired by the recent works on LLMs-enhanced ASR, we propose a new generative paradigm for translation tasks, namely GenTranslate (see Fig. (b) ). Leveraging the rich linguistic knowledge and strong reasoning ability of LLMs, our paradigm integrates the diverse translation versions in the N-best list from foundation model to generate a higher-quality translation result. Furthermore, in order to support LLM finetuning, we also build and release a HypoTranslate dataset that contains over 592K pairs of N-best hypotheses and ground-truth translation in 11 languages. Experimental evidence on various ST and MT benchmarks (e.g., FLEURS, CoVoST-2, WMT) demonstrate that our proposed GenTranslate significantly outperforms the state-of-the-art model with efficient LLM finetuning. 
Our contributions are summarized as follows: 
 
 * We propose GenTranslate, a new generative paradigm for translation tasks that leverages LLMs to generate higher-quality translation results from the diverse N-best hypotheses decoded from foundation translation model. 
 * We release a HypoTranslate dataset to support LLM finetuning, which contains over 592K pairs of N-best hypotheses and ground-truth translation in 11 languages. 
 * Experiments on various ST and MT benchmarks show that our GenTranslate significantly outperforms the state-of-the-art model."
A Unified Temporal Knowledge Graph Reasoning Model Towards Interpolation and Extrapolation,2405.18106v1,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2405.18106v1.png,An illustration of Bellman-Ford-based recursive encoding.,"Temporal knowledge graph (TKG) reasoning has two settings: interpolation reasoning and extrapolation reasoning. Both of them draw plenty of research interest and have great significance. Methods of the former de-emphasize the temporal correlations among facts sequences, while methods of the latter require strict chronological order of knowledge and ignore inferring clues provided by missing facts of the past. These limit the practicability of TKG applications as almost all of the existing TKG reasoning methods are designed specifically to address either one setting. To this end, this paper proposes an original Temporal PAth-based Reasoning (TPAR) model for both the interpolation and extrapolation reasoning. TPAR performs a neural-driven symbolic reasoning fashion that is robust to ambiguous and noisy temporal data and with fine interpretability as well. Comprehensive experiments show that TPAR outperforms SOTA methods on the link prediction task for both the interpolation and the extrapolation settings. A novel pipeline experimental setting is designed to evaluate the performances of SOTA combinations and the proposed TPAR towards interpolation and extrapolation reasoning. More diverse experiments are conducted to show the robustness and interpretability of TPAR.","Knowledge graph (KG) is a semantic network that represents real-world facts in a structured way using entities and relations. Typically, a fact is represented by a triple (s, r, o) in KG, consisting of a subject entity s, an object entity o, and a relation r between s and o. 
In the real world, many facts are closely associated with a particular time interval. For example, the fact ""Barack Obama is the president of USA"" is valid for the time period of 2009 January 20th - 2017 January 20th and the fact ""Donald Trump is the president of USA"" is only valid for the following four years. To represent such time-sensitive facts, Temporal Knowledge Graphs (TKGs) have recently gained significant attention from both academic and industrial communities. Specifically, TKGs extend static KGs by incorporating the temporal information t into fact triples, represented as a quadruple (s, r, o, t), which allows for modelling the temporal dependencies and evolution of knowledge over time, being crucial for reasoning time-evolving facts in applications such as financial forecasting, social networks, and healthcare. 
TKG reasoning infers new knowledge with time-sensitive facts in existing TKGs, which generally has two settings: the interpolation reasoning and the extrapolation reasoning. Given a temporal knowledge graph with facts from time t_0 to time t_T, the interpolation reasoning infers missing facts at any time in history (t_0 ≤ t ≤ t_T) and the extrapolation reasoning attempt to predict unknown facts that may occur in the future (t> t_T). 
Many approaches have been proposed to tackle the TKG reasoning problem, however, these two reasoning tasks are tackled in totally different and incompatible manners. On the one hand, interpolation methods de-emphasize the temporal correlations among fact sequences while training, thus it's difficult to cope with the challenges of invisible timestamps and invisible entities in extrapolation due to their poor inductive reasoning ability. On the other hand, most state-of-the-art (SOTA) extrapolation solutions require a strict chronological order of data during training. As a result, they can only predict unknown future facts, but they could hardly infer missing historical facts which are crucial for completing the overall knowledge landscape and providing more clues for predicting accurate future events. These limit the practicability of TKG applications as almost all of the existing TKG reasoning methods are designed specifically to address either one setting. Experiments with a novel pipeline setting intuitively reveal that even with the SOTA methods from both settings, the composed methods show a frustrating decrease in reasoning performance. More in-depth analysis can be found in Section and Appendix. Therefore, the motivation of this work is to propose a unified method that can accommodate two types of reasoning settings, enabling temporal knowledge graph reasoning to be conducted simultaneously for both the interpolation and the extrapolation. 
To this end, we take inspiration from recent neural and symbolic TKG reasoning approaches. Neural network approaches can perform effective reasoning but lack interpretation as they cannot provide explicit rules to explain the reasoning results, while symbolic reasoning approaches use logical symbols and rules to perform reasoning tasks but are not suitable for handling ambiguous and noisy data due to their strict matching and discrete logic operations used during rule searching. In this paper, we propose a Temporal PAth based Reasoning (TPAR) model with a neural-symbolic fashion applicable to both the interpolation and the extrapolation TKG Reasoning. Specifically, we utilize the Bellman-Ford Shortest Path Algorithm and introduce a recursive encoding method to score the destination entities of various temporal paths, and then our TPAR performs symbolic reasoning with the help of the obtained scores. It is noticeable that the neural-driven symbolic reasoning fashion we adopted is more robust to the uncertainty data compared to traditional pure symbolic reasoning methods, and comprehensible temporal paths with fine interpretability as well. We summarize our main contributions as follows: 
 
 * We propose an original unified Temporal PAth-based Reasoning (TPAR) model for both the interpolation and extrapolation reasoning settings. To the best of our knowledge, this is the first work to achieve the best of both worlds. 
 * We develop a novel neural-driven symbolic reasoning fashion on various temporal paths to enhance both the robustness and interpretability of temporal knowledge reasoning. 
 * Comprehensive experiments show that TPAR outperforms SOTA methods on the link prediction task for both the interpolation and the extrapolation settings with decent interpretability and robustness. An intriguing pipeline experiment is meticulously designed to demonstrate the strengths of TPAR in addressing the unified prediction through both settings."
Unsupervised Information Refinement Training of Large Language Models for Retrieval-Augmented Generation,2402.18150v2,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2402.18150v2.pdf,"We consider the role of LLMs in RAG as ""Information Refiner"" that can generate more concise, accurate, and complete texts than the input retrieved texts. In this way, LLM can consistently make RAG system produce positive information gain.","Retrieval-augmented generation (RAG) enhances large language models (LLMs) by incorporating additional information from retrieval. However, studies have shown that LLMs still face challenges in effectively using the retrieved information, even ignoring it or being misled by it. The key reason is that the training of LLMs does not clearly make LLMs learn how to utilize input retrieved texts with varied quality. In this paper, we propose a novel perspective that considers the role of LLMs in RAG as ""Information Refiner"", which means that regardless of correctness, completeness, or usefulness of retrieved texts, LLMs can consistently integrate knowledge within the retrieved texts and model parameters to generate the texts that are more concise, accurate, and complete than the retrieved texts. To this end, we propose an information refinement training method named InFO-RAG that optimizes LLMs for RAG in an unsupervised manner. InFO-RAG is low-cost and general across various tasks. Extensive experiments on zero-shot prediction of 11 datasets in diverse tasks including Question Answering, Slot-Filling, Language Modeling, Dialogue, and Code Generation show that InFO-RAG improves the performance of LLaMA2 by an average of 9.39%relative points. InFO-RAG also shows advantages in in-context learning and robustness of RAG.","Retrieval-augmented generation (RAG) is a popular framework in modern NLP systems that equips neural with retrieved information for text generation like open-domain question answering, dialogue etc. Recently, RAG has been applied to large language models (LLMs) to provide additional knowledge and mitigate issues such as hallucination. 
Despite the improved performance of retrieval models, the internet continues to be inundated with fake news, rumors, and fragmented, noisy information, posing challenges for retrieval models to reliably identify and shield against such content. Consequently, not all retrieved texts are beneficial, necessitating that LLMs determine how to judiciously utilize them. However, pre-training tasks do not explicitly enable LLMs to learn how to utilize the retrieved texts with varied quality for generation. For a question and its retrieved texts as input sequence, RAG aims to minimize the negative log-likelihood (NLL) of sub-sequence (question and generated answer) by referring to the retrieved texts. However, mainstream pre-training for LLMs with decoder-only architecture is language modeling based on the prefix, the training objective aims to minimize the negative log-likelihood (NLL) of the entire input sequence (retrieved texts, question, and generated answer). This gap causes LLMs to only regard the input retrieved texts as a part of the prefix for language modeling rather than additional reference, which leads to the following problems. Firstly, for the long and complex retrieved texts, LLMs struggle to extract the correct answers accurately. Secondly, in situations where the retrieved texts cannot address the task, LLMs lack the capability to integrate the knowledge within model parameters with the retrieved texts to generate improved texts. Thirdly, LLMs are susceptible to incorrect and noisy information in retrieved texts, posing a risk of being misled. 
To solve above problems, some previous methods explore strategies for how or when to perform retrieval for LLMs by prompt techniques. However, prompt cannot materially change the ability of LLMs to utilize retrieved texts because model parameters are not updated for this ability. Some methods fine-tune LLMs on the constructed RAG data for a specific task such as QA. However, under the trend that LLMs are regarded as foundation models for various tasks in zero-shot setting, fine-tuning LLMs only on a few tasks make LLMs limited to the RAG of training tasks and lose their generalizability. Because catastrophic forgetting still exists in supervised fine-tuning of LLMs. Although constructing data for a large number of tasks can alleviate this, it is hard to design the data in various RAG tasks and requires high data annotation costs. Our paper aims to fundamentally improve the ability of LLMs to utilize retrieved texts while preserving the generalizability of LLMs for various RAG tasks in zero-shot setting, which is orthogonal to prompt techniques and can be combined with them to get better performance. 
In this paper, considering that LLMs have a certain ability to use their own knowledge to examine information, we introduce a novel perspective to reassess the role of LLMs in RAG. Specifically, we propose considering LLMs as ""Information Refiner"". The key idea behind this is to continue training the pre-trained LLMs with an Information Refinement objective that regardless of the correctness, completeness, or usefulness of the input retrieved texts, LLMs can consistently integrate knowledge within the retrieved texts and model parameters to generate the texts that are more concise, accurate, and complete than the retrieved texts (Figure). We term this process ""Positive Information Gain"". This enables LLMs to extract correct information from complex texts as well as resist and rectify retrieved erroneous information and noise, thereby improving the information bottleneck of the RAG and allowing the knowledge capacity of RAG to approximate the combined knowledge of IR and LLMs. 
We make the information refinement training work in a completely unsupervised manner, such that it is easy to obtain large-scale training data and maintain the generalizability of the trained LLMs that can be used in various RAG tasks in zero-shot setting. Specifically, we propose an unsupervised training method named InFO-RAG. InFO-RAG classifies the retrieved texts into three scenarios (shown in Figure) and proposes the unsupervised training task for each scenario. For the first scenario that all knowledge for the question is already in the retrieved texts, LLMs need to accurately extract relevant knowledge from complex retrieved texts and generate more concise texts. For the second scenario that retrieved texts are incomplete or incorrect for the question, LLMs need to combine the knowledge within model parameters to verify the retrieved texts, correct the wrong knowledge, and complete the missing knowledge. For the third scenario that retrieved texts are relevant but do not have any answer, LLMs need to find the knowledge within model parameters based on relevant context to generate correct answers. We mix the above three tasks to train InFO-RAG unsupervisedly. 
Main contributions of this paper are as follows: (1) We introduce a novel perspective to reassess the role of LLMs in the RAG system that considers LLMs as ""Information Refiner"" that can produce positive information gain in RAG scenarios. (2) We propose an unsupervised training method named InFO-RAG that enables LLMs to perform information refinement in RAG. InFO-RAG is low-cost and general for various RAG tasks. (3) Extensive experiments show InFO-RAG enhances the zero-shot RAG of LLaMA2 across Question Answering, Slot-Filling, Language Modeling, Dialog, and Code Generation. InFO-RAG also shows advantages in in-context learning and robustness of RAG. Code is released at <https: //github. com/xsc1234/INFO-RAG/>."
CSCD-NS: a Chinese Spelling Check Dataset for Native Speakers,2211.08788v3,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2211.08788v3.jpg,"An error from SIGHAN: misspelling ""错误"" as ""错勿"". Despite having the same pronunciation, it's hard to reproduce this error in the given context through a Chinese IME, no matter what input form is used.","In this paper, we present CSCD-NS, the first Chinese spelling check (CSC) dataset designed for native speakers, containing 40,000 samples from a Chinese social platform. Compared with existing CSC datasets aimed at Chinese learners, CSCD-NS is ten times larger in scale and exhibits a distinct error distribution, with a significantly higher proportion of word-level errors. To further enhance the data resource, we propose a novel method that simulates the input process through an input method, generating large-scale and high-quality pseudo data that closely resembles the actual error distribution and outperforms existing methods. Moreover, we investigate the performance of various models in this scenario, including large language models (LLMs), such as ChatGPT. The result indicates that generative models underperform BERT-like classification models due to strict length and pronunciation constraints. The high prevalence of word-level errors also makes CSC for native speakers challenging enough, leaving substantial room for improvement.","Chinese spelling check (CSC) is a task to detect and correct spelling errors in Chinese texts. There are two primary user groups for CSC: (1) Chinese learners, including teenage students and individuals who use Chinese as a second language, and (2) Chinese native speakers. It is obvious that the latter user group has a larger population and more diverse applications, therefore, this paper concentrates on CSC for native speakers. 
 UTF8gbsn 
 
However, there is still no CSC dataset specifically designed for native speakers. Existing CSC datasets, such as SIGHAN13,14, and 15, are all sourced from Chinese learners. Spelling errors made by Chinese learners differ greatly from those made by native speakers. This is because Chinese input relies on Chinese input methods (IME), and modern Chinese IMEs always have powerful language models, making it difficult to recommend candidates that clearly do not fit the context. As shown in Figure, native speakers using Chinese IMEs are unlikely to make such an unusual error. 
Furthermore, the size of existing datasets is limited. As shown in Table, for three SIGHAN datasets, the training set contains an average of merely 2158 samples, while the test set comprises an average of only 1054 samples, and no development set is provided. When using such small-scale datasets, it is difficult for models to be trained sufficiently and for evaluation results to be reliable. 
To address the aforementioned issues, we introduce CSCD-NS, a Chinese spelling check dataset designed for native speakers. The dataset is sourced from real Weibo (a Chinese social media platform) posts, which contain genuine spelling errors made by native speakers during their input process. Moreover, the dataset comprises 40,000 samples, which is ten times larger than previous datasets and this is also the largest dataset for the CSC task. To conduct an in-depth investigation into the distribution of spelling errors, we develop a tagging system that operates at phonetic and semantic levels. The analysis indicates that native speakers make a higher proportion of homophonic and word-level errors compared to Chinese learners, with the proportion of word-level errors doubling. 
 UTF8gbsn 
 
Due to the lack of labeled data, previous studies always build additional pseudo data to improve the performance of models. However, these methods, which rely on confusion sets or ASR transcriptions, do not align with the real-world input scenario. Therefore, we propose a novel method that directly simulates the input process through the Chinese IME and adds sampled noises to construct high-quality pseudo data. Experimental results show that our method can better fit the real error distribution and bring greater improvements. 
We conduct comprehensive experiments on CSCD-NS, with different model sizes (from 0.1B to 13B parameters), architectures (encoder-only, encoder-decoder, and decoder-only), and learning approaches (fine-tuning and in-context learning). We also evaluate the performance of ChatGPT and GPT4. The results demonstrate that BERT-like classification models outperform generative models, as the latter struggle with the simultaneous constraints of text length and pronunciation. Concurrently, the CSC task for native speakers is challenging due to the high proportion of word-level errors, leaving substantial room for improvement. 
In summary, our contributions are as follows: 
 
 * We introduce the first Chinese spelling check dataset for native speakers which is also the largest dataset for the CSC task. Through quantitative analyses, we further unveil the specific error distribution for this scenario. 
 * We propose a novel method for constructing high-quality and large-scale pseudo data through a Chinese IME. Experimental results show that our method can bring greater improvements than existing methods. 
 * We explore the performance of different types of models in this scenario and analyze the challenges. To the best of our knowledge, we are the first to investigate the effectiveness and limitations of large language models (LLMs), such as ChatGPT, in addressing the CSC task."
How Abilities in Large Language Models are Affected by Supervised Fine-tuning Data Composition,2310.05492v4,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2310.05492v4.pdf,The illustration of four different training strategies in this paper.,"Large language models (LLMs) with enormous pre-training tokens and parameters emerge diverse abilities, including math reasoning, code generation, and instruction following. These abilities are further enhanced by supervised fine-tuning (SFT). While the open-source community has explored ad-hoc SFT for enhancing individual capabilities, proprietary LLMs exhibit versatility across various skills. Therefore, understanding the facilitation of multiple abilities via SFT is paramount. In this study, we specificially focuses on the interplay of data composition between mathematical reasoning, code generation, and general human-aligning abilities during SFT. We propose four intriguing research questions to explore the association between model performance and various factors including data amount, composition ratio, model size and SFT strategies. Our experiments reveal that distinct capabilities scale differently and larger models generally show superior performance with same amount of data. Mathematical reasoning and code generation consistently improve with increasing data amount, whereas general abilities plateau after roughly a thousand samples. Moreover, we observe data composition appears to enhance various abilities under limited data conditions, yet can lead to performance conflicts when data is plentiful. Our findings also suggest the amount of composition data influences performance more than the composition ratio. In analysis of SFT strategies, we find that sequentially learning multiple skills risks catastrophic forgetting. Our proposed Dual-stage Mixed Fine-tuning (DMT) strategy offers a promising solution to learn multiple abilities with different scaling patterns.","Recent research has demonstrated the remarkable and versatile proficiency of large language models (LLMs) in dealing with a variety of real-world tasks expressed in natural languages, especially Information Extraction (IE), Information Retrieval (IR) and Spoken Language Understanding (SLU). Among the tasks, LLMs especially emerge with three outstanding abilities in reasoning, coding, and aligning general human intentions, which have drawn much attention from the LLM research community. In order to further incentivize such abilities, it necessitates supervised fine-tuning (SFT) stages on annotated task data. 
However, existing research has mostly conducted separate SFT investigations on each of the three tasks, where reasoning and coding abilities require SFT on in-domain human-annotated or augmented data while diverse and complex human instructions are applauded for aligning human intentions. As shown by the strong performance of proprietary LLMs such as GPT-4 and Claude, LLMs have the potential to master all the tasks in one model. Therefore, it is of paramount importance to investigate the versatile performance of SFT with composite task data, and understanding and addressing the challenges posed by the data composition problem in the SFT stage is crucial for further enhancing the capabilities of LLMs in a comprehensive manner. 
In essence, the tasks of reasoning, coding, and aligning human intentions are of different characteristics. Reasoning and coding tasks require ad-hoc abilities of complex and detailed logic in decomposing task instructions and dealing with non-linguistic and symbolic features, whereas aligning human intentions requires versatility and understanding obscure intentions expressed in human instructions. Given the fundamental difference among the tasks, multi-task learning with composite data fine-tuning for small-scaled pre-trained language models is prone to catastrophic forgetting, hindering the fine-tuned performance of one model on separate tasks. Many efforts have been made to compensate for the phenomenon. There has also been research discovering that scaling up the pre-trained language model scale and the fine-tuning data scale are beneficial for zero-shot out-of-domain generalization on various linguistic tasks while leaving out the assessment of in-domain performance. Given the increased capacity of LLMs, the multi-task performance by SFT on composite data of essentially different downstream tasks is less studied. Understanding the SFT performance with composite data and corresponding scaling patterns is of great utility in practice. 
In this study, we focus on the data composition problem among mathematical reasoning, code generation, and general human-aligning abilities in SFT. We aim to comprehensively investigate the relationship between model performance and different factors including data amount, data composition ratio, model scales, and SFT training strategies. We also investigate how the relationship varies under different scales. Specifically, we focus on the following four research questions: 
1. How do math reasoning, coding, and general abilities scale with SFT data amounts? 
2. Are there performance conflicts when combining these three abilities in SFT? 
3. What are the key factors that induce the performance conflicts? 
4. What are the impacts of different SFT strategies for composite data? 
 To answer these questions, we conduct experiments on three benchmarks, which are GSM8K for mathematical reasoning, HumanEval for coding, and MT-Bench for general human alignment. We fine-tune LLMs on the related training data to activate these abilities. Furthermore, we conduct extensive analysis regarding model parameter scales ranging from LLaMA 7B to 33B and explore four different SFT strategies shown in Figure: multi-task learning, sequential training, mixed sequential training, and dual-stage mixing fine-tuning (DMT), providing empirical guidance for learning a versatile LLM with composite SFT. The key findings of this paper can be summarized as follows: 
 
 * Different SFT abilities exhibit distinct scaling patterns, while larger models show better performances with the same data amount generally. 
 * Compared to single ability learning, multi-task learning multiple abilities exhibits improvement in low-resource and decline in high-resource. Additionally, as the model size increases, there is a greater performance gain in low-resource settings for math and general abilities. 
 * Data amounts directly influence each ability, while the data ratio is insignificant. 
 * Multi-task learning lead to conflicts, while sequential training results in catastrophic forgetting. Our proposed DMT effectively alleviates both performance conflicts and catastrophic forgetting in the SFT phrase, achieving a balance between general and specialized abilities."
Inference to the Best Explanation in Large Language Models,2402.10767v2,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2402.10767v2.pdf,"IBE-Eval qualifies LLM-generated explanations with a set of logical and linguistic selection criteria to identify the most plausible hypothesis. The corresponding explanation for each hypothesis is evaluated across the IBE criteria of logical consistency, parsimony, internal coherence, and linguistic uncertainty. A final plausibility score is computed across those features and the hypothesis with highest score is identified as the best explanation.","While Large Language Models (LLMs) have found success in real-world applications, their underlying explanatory process is still poorly understood. This paper proposes IBE-Eval, a framework inspired by philosophical accounts on Inference to the Best Explanation (IBE) to advance the interpretation and evaluation of LLM explanations. IBE-Eval estimates the plausibility of natural language explanations through a combination of explicit logical and linguistic features including: consistency, parsimony, coherence, and uncertainty. Extensive experiments are conducted on Causal Question Answering (CQA), where IBE-Eval is tasked to select the most plausible causal explanation amongst competing ones generated by the LLM (e.g.,GPT 3.5 or LLaMA 2). The experiments reveal that IBE-Eval can successfully identify the best explanation with up to 77%accuracy (≈ 27%above random), improving upon a GPT 3.5-as-a-judge baseline (≈ +17%) while being intrinsically more efficient and interpretable. Additional analysis suggests that, despite LLM-specific variances, generated explanations tend to conform to IBE criteria and that IBE-Eval is significantly correlated with human judgment, opening up opportunities for future development of automated explanation verification tools.","Large Language Models (LLMs) such as OpenAI 's GPT and LLaMA have been highly effective across a diverse range of language understanding and reasoning tasks. While LLM performances have been thoroughly investigated across various benchmarks, the principles and properties behind their step-wise reasoning process are still poorly understood. LLMs are notoriously black-box and can be difficult to interpret. Moreover, the commercialization of LLMs has led to strategic secrecy around model architectures and training details. Finally, LLMs are susceptible to hallucinations and adversarial perturbations, often producing plausible but factually incorrect answers. As the size and complexity of LLM architectures increase, the systematic study of generated explanations becomes crucial to better interpret and validate the LLM' s internal inference and reasoning processes. 
The automatic evaluation of natural language explanations presents several challenges. Without resource-intensive annotation, explanation quality methods tend to rely on either weak supervision, where the identification of the correct answer is taken as evidence of explanation quality, or require the injection of domain-specific knowledge. In this paper, we seek to better understand the LLM explanatory process through the investigation of explicit linguistic and logical properties. While explanations are hard to formalize due to their open-ended nature, we hypothesize that they can be analyzed as linguistic objects, with measurable features that can serve to define criteria for assessing their quality. 
Specifically, this paper investigates the following overarching research question: ""Can the linguistic and logical properties associated with LLM-generated explanations be used to qualify the models 'reasoning process? "". To this end, we propose an interpretable framework inspired by philosophical accounts of abductive inference, also known as Inference to the Best Explanation (IBE) - i.e.,the process of selecting among competing explanatory theories. In particular, we aim to measure the extent to which LLM-generated explanations conform to IBE expectations when attempting to identify the most plausible explanation. To this end, we present IBE-Eval, a framework designed to estimate the plausibility of natural language explanations through a set of explicit logical and linguistic features, namely: logical consistency, parsimony, coherence, and linguistic uncertainty. 
To evaluate the efficacy of IBE-Eval, we conduct extensive experiments in the multiple-choice Causal Question Answering (CQA) setting. The overall results and contributions of the paper can be summarized as follows: 
 
 * To the best of our knowledge, we are the first to propose an interpretable framework inspired by philosophical accounts on Inference to the Best Explanation (IBE) to automatically assess the quality of natural language explanations. 
 * We propose IBE-Eval, a framework that can be instantiated with external tools for the automatic evaluation of LLM-generated explanations and the identification of the best explanation in a multiple-choice CQA setting. 
 * We provide empirical evidence that LLM-generated explanations tend to conform to IBE expectations with varying levels of statistical significance correlated to the LLM' s size. 
 * We additionally find that uncertainty, parsimony, and coherence are the best predictors of plausibility and explanation quality across all LLMs. However, we also find that the LLMs tend to be strong rationalizers and can produce logically consistent explanations even for less plausible candidates, making the consistency metric less effective in practice. 
 * IBE-Eval can successfully identify the best explanation supporting the correct answers with up to 77%accuracy (+≈ 27%above random and +≈ 17%over GPT 3.5-as-a-Judge baselines) * IBE-Eval is significantly correlated with human judgment, outperforming a GPT3.5-as-a-Judge baseline in terms of alignment with human preferences. 
For reproducibility, our code is made available on Github to encourage future research in the field."
MinPrompt: Graph-based Minimal Prompt Data Augmentation for Few-shot Question Answering,2310.05007v3,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2310.05007v3.pdf,Framework overview for MinPrompt.,"Recent advances in few-shot question answering (QA) mostly rely on the power of pre-trained large language models (LLMs) and fine-tuning in specific settings. Although the pre-training stage has already equipped LLMs with powerful reasoning capabilities, LLMs still need to be fine-tuned to adapt to specific domains to achieve the best results. In this paper, we propose to select the most informative data for fine-tuning, thereby improving the efficiency of the fine-tuning process with comparative or even better accuracy on the open-domain QA task. We present MinPrompt, a minimal data augmentation framework for open-domain QA based on an approximate graph algorithm and unsupervised question generation. We transform the raw text into a graph structure to build connections between different factual sentences, then apply graph algorithms to identify the minimal set of sentences needed to cover the most information in the raw text. We then generate QA pairs based on the identified sentence subset and train the model on the selected sentences to obtain the final model. Empirical results on several benchmark datasets and theoretical analysis show that MinPromptis able to achieve comparable or better results than baselines with a high degree of efficiency, bringing consistent improvements in F-1 scores.","Question answering (QA) provides accurate responses to a series of questions based on given narrative contexts. Its diverse applications extend to areas such as chatbots, dialogue systems, and instant information retrieval, making it a key pursuit in the field of natural language processing (NLP). Supervised learning has traditionally been the approach for developing efficient QA systems that deliver commendable results. However, this method is intrinsically restricted by its reliance on a large set of annotated QA training examples, which becomes problematic due to the substantial cost associated with acquiring expert-level annotations. 
Our research focuses on the few-shot QA task, an effort to address the QA challenge with the presence of only a limited number of training examples. The prevalent approaches under the few-shot setting either introduce a new task and pre-train an extensive language model from scratch, or they fine-tune an already pre-trained model on the given training examples. The fine-tuning stage is crucial in the sense that it stimulates the power of the LLMs obtained during the pre-training stage and makes the model align with the input/output distribution of a certain domain or dataset. However, with an increasing data size for fine-tuning, the training duration increases accordingly, which is undesirable, especially when the model size is also large. As such, the importance of minimal data augmentation cannot be understated. The fine-tuning data, often a limited resource in our consideration (up to 128 shots), is directly used to adjust the parameters of a pre-trained model to enhance performance on the downstream task. The data is usually labeled by domain experts and thus could be time-consuming to obtain in large quantities. On the other hand, augmented data represents a broader dataset, generated in an unsupervised manner by converting statements into question-answer pairs. In QA tasks, it is vital for a model to be exposed to a diverse range of questions, answers, and contexts to develop a robust understanding of the language and the task at hand. However, not all parts of the training data hold equal relevance or significance for the model 's learning process. Some parts may contain more valuable information or more complex language structures that the model needs to understand to improve its performance. Consequently, identifying and augmenting these critical portions of the training data could substantially enhance the model' s capacity to answer questions accurately and comprehensively. 
To address the above challenges, we present MinPrompt, which consists of the following three modules: (1) A sentence graph construction module that leverages sentence graph representation to structurize the raw text. Each node in the graph symbolizes a sentence, while edges illustrate the shared entities between sentences. This sentence graph effectively encapsulates the complex interconnections between various textual elements; (2) A data selection module that features an approximate minimal dominating set algorithm. The algorithm is applied to the sentence graph to identify the smallest set of sentences to cover all shared entities. This module ensures efficient use of computational resources, reduces the risk of overfitting, and enhances the model's generalization ability, resulting in an overall improvement in QA performance; and (3) A question generation module that transforms the selected plain factual sentences into QA pairs. The synthesized QA pairs are further turned into prompts, providing a condensed, yet comprehensive representation of the text. The generated prompts serve as high-quality, information-rich training instances for the QA model. This model trained on the compact and meaningful prompts is then capable of generating accurate answers to the posed questions, all without requiring any additional explicit supervision. 
In summary, our contributions are as follows: 
 
 * We propose to study minimal data augmentation for effective and efficient few-shot QA 
 * We introduce MinPrompt, a minimal data augmentation framework that uses a graph-based algorithm and unsupervised question generation to synthesize the most informative QA training samples out of the raw text. 
 * We conduct extensive experiments on publicly accessible benchmarks to validate the effectiveness of MinPrompt, and observe a solid improvement over competitive compared methods. Beyond that, we also study the necessity of different parts of the model."
SportsMetrics: Blending Text and Numerical Data to Understand Information Fusion in LLMs,2402.10979v2,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2402.10979v2.pdf,"Play-by-plays of an NBA game. We include timestamps, player actions, team affiliations and a game recap. Total points for both teams are indicated in dotted circles and are withheld from LLMs.","Large language models hold significant potential for integrating various data types, such as text documents and database records, for advanced analytics. However, blending text and numerical data presents substantial challenges. LLMs need to process and cross-reference entities and numbers, handle data inconsistencies and redundancies, and develop planning capabilities such as building a working memory for managing complex data queries. In this paper, we introduce four novel tasks centered around sports data analytics to evaluate the numerical reasoning and information fusion capabilities of LLMs. These tasks involve providing LLMs with detailed, play-by-play sports game descriptions, then challenging them with adversarial scenarios such as new game rules, longer durations, scrambled narratives, and analyzing key statistics in game summaries. We conduct extensive experiments on NBA and NFL games to assess the performance of LLMs on these tasks. Our benchmark, SportsMetrics, introduces a new mechanism for assessing LLMs' numerical reasoning and fusion skills.","Large language models (LLMs) are more powerful than ever. OpenAI 's GPT-4 Turbo gpt4-turbo features a 128k context window, allowing it to process over 300 pages of text in a single prompt. Claude v2.1 claude-2.1 steps it up with a 200k token window, equivalent to roughly 150,000 words or more than 500 pages. Mistral AI mixtral-8x7b has created a sparse mixture of experts model capable of processing up to 32k tokens. The developments suggest language models can now engage with vast amounts of text content and data, opening doors to exciting new applications in various domains. 
One of the most promising uses of LLMs is in handling a combination of unstructured texts and structured data. For example, determining if a patient can be discharged from the hospital may involve reviewing doctor notes, radiology and pathology reports, lab results, and other records that blend text and structured data; LLM Assistants for online shopping need to process product catalogs, sales transactions, and customer queries. Yet, summarizing key details from a mix of unstructured and structured sources remains a considerable challenge. An LLM must navigate text descriptions, link entities, aggregate numbers, handle discrepancies, and beyond. 
Information fusion focuses on synthesizing information from multiple textual sources to derive meaningful conclusions. Current approaches involve summarizing multiple text documents, providing concise answers to user queries, and integrating summarization with natural language inference to deduce information. The output is often a short text summary, the quality of which is difficult to evaluate. In contrast, our approach emphasizes the numerical aspect of information fusion. We enable the LLM to navigate through lengthy texts, gather crucial statistics, and develop a working memory to manage complex data queries. 
We introduce SportsMetrics, a benchmark designed to assess LLMs' abilities in numerical reasoning and data fusion. This benchmark provides LLMs with detailed, play-by-play descriptions of sports games, including timestamps, player actions, and team affiliations, as illustrated in Figure. We focus on four novel tasks to evaluate LLMs in adversarial scenarios: (a) adapting to new game rules, (b) handling lengthy game descriptions, (c) managing scrambled game narratives, and (d) analyzing critical statistics in game summaries. E.g., an LLM might be asked to complete a basketball game recap by inserting missing key statistics, which requires the development of a working memory for game stats and reasoning skills. 
Our SportsMetrics benchmark presents three main benefits. First, it leverages sports data, including team-player affiliations and play-by-play details; they are dynamic narratives that LLMs cannot easily memorize. Second, it allows us to evaluate LLMs' ability to track key statistics such as team points, assists, blocks, steals, and more, while also offering an overall game efficiency score for direct LLM comparison. Lastly, its use of widely understood sports terminology makes it more accessible to researchers than specialized medical language, making it an ideal benchmarking tool. While our current focus is on English, SportsMetrics also holds promise for multilingual applications."
Expedited Training of Visual Conditioned Language Generation via Redundancy Reduction,2310.03291v3,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2310.03291v3.png,"Overview of our EVL_Gen. EVL_Gen employs a streamlined, single-stage training mechanism with a unified loss. Here, visual tokens (in grey) are progressively aggregated based on their inherent similarities at each layer of the TomeFormer architecture. The final set of merged tokens (in orange) serves as semantically rich but computationally efficient soft prompts, guiding the LLM to generate a corresponding caption for the input image.","In this paper, we introduce EVL_Gen, a streamlined framework designed for the pre-training of visually conditioned language generation models with high computational demands, utilizing frozen pre-trained large language models (LLMs). The conventional approach in vision-language pre-training (VLP) typically involves a two-stage optimization process: an initial resource-intensive phase dedicated to general-purpose vision-language representation learning, focused on extracting and consolidating relevant visual features. This is followed by a subsequent phase that emphasizes end-to-end alignment between visual and linguistic modalities. Our novel one-stage, single-loss framework bypasses the computationally demanding first training stage by gradually merging similar visual tokens during training, while avoiding model collapse caused by single-stage training of BLIP-2 type models. The gradual merging process effectively condenses visual information while preserving semantic richness, resulting in rapid convergence without compromising performance. Our experimental findings demonstrate that our approach accelerates the training of vision-language models by a factor of 5 without a noticeable impact on overall performance. Furthermore, we illustrate that our models significantly narrow the performance gap to current vision-language models using only 1/10 of the data. Finally, we showcase how our image-text models can seamlessly adapt to video-conditioned language generation tasks through novel soft attentive temporal token contextualizing modules. Code is available at <https: //github. com/yiren-jian/EVLGen>.","The landscape of vision-language modeling has undergone significant transformations in recent years, with CLIP serving as a landmark development. It distinguished itself through unparalleled zero-shot classification capabilities and efficiency in image-text retrieval tasks. Successive models like ALBEF, X-VLM, and VLMo further broadened the scope, addressing a myriad of tasks such as retrieval, visual entailment, and closed-set Visual Question Answering (VQA), among others. 
Recently, the field has been enriched by the advent of generative models designed for complex image-to-language tasks. Notable contributions include CoCa, SimVLM, Frozen, and Flamingo, targeting tasks like image and video captioning and open-set VQA. These models all rely on billion-scale datasets for training from scratch to bridge the substantial modality gap between vision and language. 
As a result, the resource-intensive requirements (i.e., thousands of TPUs) of these training-from-scratch Vision-Language Models (VLMs) led to the conceptualization of BLIP-2: this model alleviates computational costs (e.g., only requiring 16× fewer GPUs) by integrating existing well-pretrained vision encoders (ViT) with language decoders (LLM), and then tuning their joint operation. A central innovation in aligning vision and language modules in BLIP-2 is Q-former, a multimodal connector equipped with learnable queries for enhancing cross-attention mechanisms. This architectural choice, however, prevents the full model from end-to-end training and therefore still demands an additional pre-training regimen for Q-former, referred to as BLIP-2 's Stage 1. The stage involves three learning objectives—image-text contrastive, image-text matching, and language generation—and necessitates multiple forward passes for facilitating the Q-former' s optimization. 
Despite its efficiency gains over CoCa, BLIP-2 's training still imposes considerable computational costs. This poses challenges for research environments with limited computational resources, such as university labs. Our experiments indicate that the Stage-1 training of BLIP-2 took approximately eight days on eight A100-80G GPUs (See Appendix for training configurations). This computational burden has consequently restricted research to using the pre-trained Q-former, hindering the exploration of alternative ViTs in VLMs. This limitation is evident in subsequent works such as InstructBLIP, VideoChat, Video-LLaMA, X-LLM. 
The prospect of reducing BLIP-2' s computational cost through end-to-end, single-stage training is compelling. Such an approach would remove the complexities associated with resource allocation and hyper-parameter tuning inherent in multi-stage training. Yet, direct end-to-end training with BLIP-2 poses substantial challenges, corroborated by both original findings from BLIP-2 and our own empirical analyses. We hypothesize that these challenges emanate from the intrinsic design of the Q-former. Specifically, the inclusion of randomly initialized learnable queries and cross-attention mechanisms complicates the optimization landscape, especially when the aim is to minimize the representational disparity between visual and linguistic modalities. 
In this paper, we propose a token merging Transformer (TomeFormer) as an efficient vision-language connector. TomeFormer employs a systematic token-merging strategy that is both intuitive and effective. By connecting a pre-trained ViT as the visual encoder and a frozen LLM as the language decoder, we introduce a new VLM ""Expedited Visual Language Generation model"" (EVL_Gen), facilitates a streamlined, single-stage training process. It requires only a singular learning objective and a single forward pass per optimization step. This stands in contrast to BLIP-2 's multi-stage training, laden with multiple objectives and several forward passes. 
Further, we introduce a soft attentive temporal contextualization mechanism within the ViT for effective video-language modeling. This uncovers more shared semantic features across temporal frames, thereby improving the efficiency of the spatial token merging process. It eliminates the need for modality realignment, contrasting approaches such as the temporal Q-former, or the addition of new learnable temporal queries. Our strategy simplifies the optimization challenges tied to working with relatively smaller video-text datasets, compared to their image-text counterparts. Remarkably, we demonstrate that even without video pre-training, our temporal token contextualize approach can effectively train robust video-language models. This differs from recent work in video-language models that depend on pre-training models using vast million-scale video-text datasets. In summary, our contributions are: 
 
 * For reducing vision redundancy within the vision language connector, we adopt Token Merging, initially designed to enhance ViT inference speed without training. Concurrently, we present a novel temporal token contextualization scheme for video modeling. 
 * Our proposed VLM featuring TomeFormer competes effectively with BLIP-2, while requiring just a fraction of the computational resources. Given the reliance on BLIP-2' s pre-trained model in contemporary studies, our approach widens the exploratory scope for various ViTs. 
 * We introduce a straightforward spatial attentive temporal modeling technique that allows for the seamless adaptation of pre-trained image-text models to video tasks. This approach eliminates the need for complex modality re-alignment, a common requirement in alternative methods."
Picturing Ambiguity: A Visual Twist on the Winograd Schema Challenge,2405.16277v3,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2405.16277v3.PNG,A representative output from Stable Diffusion 2.0 on a WinoVis instance. The Diffusion Attentive Attribution Maps (DAAM) clarify the model 's focus for different terms and the correctness of its interpretation: correctly identifying' bee 'and' flower 'but erroneously associating' it' with the bee instead of the flower.,"Large Language Models (LLMs) have demonstrated remarkable success in tasks like the Winograd Schema Challenge (WSC), showcasing advanced textual common-sense reasoning. However, applying this reasoning to multimodal domains, where understanding text and images together is essential, remains a substantial challenge. To address this, we introduce WinoVis, a novel dataset specifically designed to probe text-to-image models on pronoun disambiguation within multimodal contexts. Utilizing GPT-4 for prompt generation and Diffusion Attentive Attribution Maps (DAAM) for heatmap analysis, we propose a novel evaluation framework that isolates the models' ability in pronoun disambiguation from other visual processing challenges. Evaluation of successive model versions reveals that, despite incremental advancements, Stable Diffusion 2.0 achieves a precision of 56.7%on WinoVis, showing minimal improvement from past iterations and only marginally surpassing random guessing. Further error analysis identifies important areas for future research aimed at advancing text-to-image models in their ability to interpret and interact with the complex visual world.","The interpretation of ambiguous constructs in language is crucial for assessing common-sense reasoning, with the Winograd Schema Challenge (WSC) significantly influencing the evaluation of natural language understanding models. Advances in transformer-based architectures have led Large Language Models (LLMs) to achieve impressive results on WSC-based tasks, approaching near-human performance. 
Extending common-sense reasoning into multimodal domains, especially disambiguation tasks, is a persisting challenge. Despite the ability of models like Google 's Imagen, OpenAI' s DALL-E 2, and Stability AI 's recently open-sourced Stable Diffusion to create visually compelling images from text, their interpretability—essential for deciphering the models' reasoning processes—is notably limited. This gap restricts the development of tools for visuals that match complex texts, reducing model effectiveness when deployed in areas like education and digital media, where text-image integration is essential. 
Our response to this challenge is WinoVis, a dataset aimed at probing text-to-image models 'common-sense reasoning capabilities through pronoun disambiguation within multimodal scenarios. WinoVis not only tests models' ability to distinguish entities within the generated images, but also examines how these models associate pronouns with the correct referents, a nuanced aspect of common-sense reasoning that has been overlooked. As depicted in the WinoVis example in Figure, while newer Stable Diffusion models can accurately separate entities within an image, they fail to correctly associate the pronoun 'it' with the intended referent, revealing the subtleties and potential gaps in multimodal common-sense reasoning. 
The development of WinoVis leveraged the generative power of GPT-4, using a methodical approach to create and refine prompts that elicit common-sense reasoning visually. This process included a complete manual review to ensure each scenario 's clarity and relevance for the disambiguation task. Moreover, we introduce a novel evaluation framework that distinguishes between models' pronoun disambiguation proficiency from their handling of visual processing challenges, such as susceptibility to typographic attacks and semantic entanglement. 
Our contributions are summarized as follows: 
 
 * WSC-Adapted Multimodal Dataset (WinoVis): A dataset of 500 scenarios for benchmarking text-to-image models 'pronoun disambiguation abilities within a visual context. 
 * Novel Evaluation Framework for Multimodal Disambiguation: Metrics and methods designed to isolate pronoun resolution from other visual processing challenges, advancing the understanding of models' common-sense reasoning. 
 * Insight into Stable Diffusion's Common-Sense Reasoning: A critical analysis revealing that even state-of-the-art models like Stable Diffusion 2.0 fall significantly short of human-level performance."
Subtle Biases Need Subtler Measures: Dual Metrics for Evaluating Representative and Affinity Bias in Large Language Models,2405.14555v4,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2405.14555v4.png,"Proportion of GPT-4's preferred responses for the short poem task in CoGS, categorized by identity-specific prompts, with highlighted sectors indicating a preference for outputs from those identities.","Research on Large Language Models (LLMs) has often neglected subtle biases that, although less apparent, can significantly influence the models 'outputs toward particular social narratives. This study addresses two such biases within LLMs: representative bias, which denotes a tendency of LLMs to generate outputs that mirror the experiences of certain identity groups, and affinity bias, reflecting the models' evaluative preferences for specific narratives or viewpoints. We introduce two novel metrics to measure these biases: the Representative Bias Score (RBS) and the Affinity Bias Score (ABS), and present the Creativity-Oriented Generation Suite (CoGS), a collection of open-ended tasks such as short story writing and poetry composition, designed with customized rubrics to detect these subtle biases. Our analysis uncovers marked representative biases in prominent LLMs, with a preference for identities associated with being white, straight, and men. Furthermore, our investigation of affinity bias reveals distinctive evaluative patterns within each model, akin to 'bias fingerprints'. This trend is also seen in human evaluators, highlighting a complex interplay between human and machine bias perceptions.","In recent years, the landscape of natural language processing has been transformed by the advent of Large Language Models (LLMs) such as GPT-4, PaLM, LLaMA-2, and Mixtral. These LLMs have expanded the boundaries of natural language generation and understanding beyond theoretical research, embedding themselves into critical decision-making processes with significant real-world implications, such as hiring practices, automated essay evaluations, and even judicial decision-making. 
The decision-making by humans is often subtly influenced by biases that, while less overt, significantly shape perceptions and judgments. Such subtle biases, although difficult to detect, can have far-reaching consequences. Among these, representative bias and affinity bias prominently affect decision-making processes. 
Representative bias stems from an unconscious presumption that dominant characteristics within a person 's environment are universally normative, thus skewing what is considered' normal. 'This bias is commonly seen in media representation, where prevalent cultural narratives disproportionately influence societal norms. Affinity bias is the unconscious preference for those who share similarities with oneself, such as cultural backgrounds, personal experiences, or gender identities. This type of bias is evident in scenarios like literary awards, where judges might favor narratives that resonate with their own experiences. 
As LLMs increasingly assume roles traditionally filled by humans, such as in creative writing and content moderation, they not only showcase their ability to replicate complex human tasks but also raise questions about their potential to perpetuate human biases. This study probes the extent to which LLMs exhibit representative and affinity biases, particularly in areas where they supplant human-generated content and its evaluation. 
We propose a comprehensive approach to quantify and analyze these biases in LLMs. Our methodology includes the' Creativity-Oriented Generation Suite ' (CoGS), a novel benchmark suite designed to scrutinize subtle biases through a series of structured yet open-ended tasks. Figure offers a snapshot of our findings, depicting GPT-4' s evaluation tendencies across different identity axes within the short poem task. 
Our contributions are threefold: 
 
 * Creation of the 'Creativity-Oriented Generation Suite, ' comprising 12 diverse open-ended tasks for content creation, ranging from short stories to haikus, complete with customized evaluation rubrics and a variety of themes for comprehensive analysis. 
 * Development of two novel metrics, the Representative Bias Score (RBS) and the Affinity Bias Score (ABS), tailored to measure biases in content generation and evaluation. 
 * Extensive testing of recent LLMs, such as LLaMA-2, GPT-4, and Mixtral, demonstrating prevalent representative biases towards identities typically associated with being white, straight, and men, and uncovering distinct patterns of affinity bias, with Mixtral displaying notably lowest ABS scores. ."
An Information-Theoretic Approach to Analyze NLP Classification Tasks,2402.00978v1,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2402.00978v1.png,Data generation for multiple-choice reading comprehension for the context (blue) and question (purple) respectively.,"Understanding the importance of the inputs on the output is useful across many tasks. This work provides an information-theoretic framework to analyse the influence of inputs for text classification tasks. Natural language processing (NLP) tasks take either a single element input or multiple element inputs to predict an output variable, where an element is a block of text. Each text element has two components: an associated semantic meaning and a linguistic realization. Multiple-choice reading comprehension (MCRC) and sentiment classification (SC) are selected to showcase the framework. For MCRC, it is found that the context influence on the output compared to the question influence reduces on more challenging datasets. In particular, more challenging contexts allow a greater variation in complexity of questions. Hence, test creators need to carefully consider the choice of the context when designing multiple-choice questions for assessment. For SC, it is found the semantic meaning of the input text dominates (above 80%for all datasets considered) compared to its linguistic realisation when determining the sentiment. The framework is made available at: <https: //github. com/WangLuran/nlp-element-influence>.","Natural Language Processing (NLP) requires machines to understand human language to perform a specific task. NLP tasks typically take a single (such as summarization, sentiment classification, machine translation) or multiple (such as reading comprehension, question generation) text elements at the input and return a specific output. Each input text element can further be partitioned into its semantic content and the linguistic realization. The semantic meaning is the inherent meaning of the input element while the linguistic realization is the specific word choice to present the meaning in human language. Typically, there are several possible linguistic realizations for any semantic content. Therefore, for all NLP tasks, the output variable has contributions from at least two components: the semantic meaning of the element and the specific linguistic realization. Here, element refers to a specific input text in an NLP task that is formed of exactly two components. 
In this work, we analyze the relative sensitivity of the output variable to each of the input elements as well as in terms of the breakdown between the elemental semantic content and its corresponding linguistic realization. A theoretical information-theoretic approach is applied to find the shared information content between each input component and the output variable. Here, the information-theoretic approach is framed for NLP classification tasks where the set of input components influence the output probability distribution over a discrete set of classes. We select multiple-choice reading comprehension (MCRC) and sentiment classification as case studies for the analysis. 
MCRC requires the correct answer option to be selected based on several inputs element: the context paragraph, the question and the set of answer options. Multiple-choice (MC) assessments are a widely employed method for evaluating the competencies of candidates across diverse settings and tasks on a global scale. Given their consequential impact on real-world decisions, the selection of appropriate MC questions tailored to specific scenarios is important for content creators. Consequently, there is a need to comprehend the underlying factors that contribute to the complexity of these assessments. 
Complexity of an MC question is best modelled by the distribution over the answer options by human test takers. Therefore, by understanding the influence of each input element on the output distribution, content creators can be better informed to what extent the complexity of an MC question can be controlled from changing each of the input elements. Moreover, analyzing the contribution of the semantic content vs the linguistic realization on the output human distribution informs the impact of the specific word choice in the element on the question complexity. However, it is not scalable to measure the variation in the output human distribution with variation in each of the input elements. demonstrated that the output distribution of machine reading comprehension systems is aligned (with minimal re-shaping parameters) to the human distribution. Therefore, the information-theoretic framework is applied to understand the influence of each input element as well as the semantic and linguistic components on the output probability distribution by an automated comprehension system. 
Sentiment classification (SC) is a common NLP classification task where the dominant sentiment class must be selected from a discrete set of sentiments based on a block of input text. This is an example of a single input text element NLP task. The information-theoretic approach is applied here to understand the role of the semantic content and the linguistic realization on the output distribution over the sentiment classes for common sentiment classification datasets. It is interesting to analyze SC as ideally the sentiment of a text block should be based on only its semantic meaning. Here, we determine whether this ideal is held in practice for popular SC corpuses. 
This work makes the following contributions: 
 
 * Propose an information-theoretic framework for determining the contribution of each text element and further each elemental component on the output distribution for NLP classification tasks. 
 * Detailed analysis of the element and component breakdown according to the proposed framework for multiple-choice reading comprehension and sentiment classification datasets. 
 Despite the framework being applied to NLP classification tasks, it can be adapted to regression, sequence output and even vision tasks."
OPEx: A Component-Wise Analysis of LLM-Centric Agents in Embodied Instruction Following,2403.03017v1,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2403.03017v1.pdf,Overview of our OPEx framework. We will open-source the code after acceptance.,"Embodied Instruction Following (EIF) is a crucial task in embodied learning, requiring agents to interact with their environment through egocentric observations to fulfill natural language instructions. Recent advancements have seen a surge in employing large language models (LLMs) within a framework-centric approach to enhance performance in embodied learning tasks, including EIF. Despite these efforts, there exists a lack of a unified understanding regarding the impact of various components—ranging from visual perception to action execution—on task performance. To address this gap, we introduce OPEx, a comprehensive framework that delineates the core components essential for solving embodied learning tasks: Observer, Planner, and Executor. Through extensive evaluations, we provide a deep analysis of how each component influences EIF task performance. Furthermore, we innovate within this space by deploying a multi-agent dialogue strategy on a TextWorld counterpart, further enhancing task performance. Our findings reveal that LLM-centric design markedly improves EIF outcomes, identify visual perception and low-level action execution as critical bottlenecks, and demonstrate that augmenting LLMs with a multi-agent framework further elevates performance.","Embodied learning, particularly through tasks like Embodied Instruction Following (EIF), stands at the forefront of artificial intelligence research. EIF, where agents must interpret natural language instructions to navigate and act within their environment using egocentric observations, epitomizes the challenge of integrating cognitive understanding with physical action. This intersection is crucial for developing autonomous agents capable of nuanced interaction with complex, real-world environments, marking a significant stride towards more advanced and versatile AI systems. As the research community harnesses advancements in deep learning, we edge closer to this ambition. 
Traditional approaches to Embodied Instruction Following (EIF) often rely on expert-generated annotations, a process that can be both expensive and challenging to scale for real-world applications. In contrast, Large Language Models (LLMs), such as those cited in recent studies, have emerged as a potent alternative, showcasing exceptional capabilities in natural language understanding and generation. These models, enriched by extensive textual datasets, demonstrate significant common-sense reasoning abilities. As a result, there 's a growing trend towards leveraging LLM-centric architectures for embodied learning tasks including EIF, which promise to simplify planning and execution tasks through a few-shot learning paradigm. However, despite their potential, the implementations of EIF systems introduce a variety of designs and components across different studies. There remains a notable gap in systematically understanding how these disparate elements influence overall task performance, underscoring the need for a thorough analysis of LLM-centric methods within the context of EIF. 
In addressing the complexities of Embodied Instruction Following (EIF), we introduce OPEx, a novel framework designed to systematically outline the essential components for mastering embodied learning tasks. OPEx is segmented into three core parts: Observer, Planner, and Executor. The Observer component is tasked with processing and interpreting sensory inputs, primarily visual, to construct an actionable understanding of the agent' s immediate environment. The Planner dynamically devises strategic plans as subtasks to complete the tasks based on perceptual inputs, effectively bridging the gap between perception and action. Lastly, the Executor is responsible for implementing these plans with a skill library, which translates several re-useable skills into precise, context-aware actions within the environment, ensuring the agent 's interactions are both relevant and goal-oriented. This tripartite structure provides a clear delineation of roles within the system, facilitating a granular analysis of how each contributes to the overarching performance of EIF tasks. 
To understand the impact of each OPEx component on performance in EIF tasks, we conducted an in-depth analysis. By experimenting with different versions of the Observer, Planner, and Executor components, we assessed how each contributes to and influences overall success. This approach allowed us to identify the key attributes and design choices that enhance the system' s ability to tackle complex embodied tasks, providing clear insights into optimizing embodied learning agents. 
To further unlock the potential of LLMs in embodied learning, we eliminate the influence of visual perception and low-level action execution of the system utilizing a pure-text counterpart environment and further adopt a multi-agent dialogue strategy, splitting the instruction-following challenge into distinct reasoning and grounding roles handled by a reasoner agent and an actor agent, respectively. This dialogue-driven approach simplifies the task into decision-making processes, where both agents utilize world knowledge obtained from an explorer. This explorer gathers insights either through direct interaction with the environment or from human contributions, thereby enriching the collaborative problem-solving capabilities of the reasoner and actor with more grounded and informed decision-making. 
Our experimental evaluation was conducted using the ALFRED and ALFWorld benchmarks, providing a comprehensive testing ground for our extensive evaluation. The core analysis of our experiments underscores significant advancements: the LLM-centric approach notably enhances performance in EIF tasks. We pinpoint visual perception and low-level action execution as pivotal bottlenecks. Moreover, our results affirm that incorporating a multi-agent dialogue strategy into an LLM-centric task solver significantly boosts overall task performance on AFLWorld, showcasing the effectiveness of our proposed methodology in addressing the complexities of embodied learning tasks."
ABEX: Data Augmentation for Low-Resource NLU via Expanding Abstract Descriptions,2406.04286v1,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2406.04286v1.pdf,"Illustration of our proposed augmentation methodology. Top: Learning to Expand Abstract Descriptions. (1) We synthesize a large-scale synthetic dataset 𝒟_ab with abstract-document pairs by prompting LLMs with unlabeled documents from 𝒟_ab. (2) We pre-train BART on this dataset with abstract as input and document as the target for learning to expand abstract descriptions. Bottom: Data Augmentation. (1) We convert the document into its AMR graph representation 𝒢_i using a Text-to-AMR Parser. (2) 𝒢_i then goes through multiple steps of deletion to obtain 𝒢̂_i (3) We optionally retrieve a semantically similar document from 𝒟_down, obtain its AMR graph 𝒢_k, and replace subtrees in 𝒢̂_i with similar subtrees in 𝒢̂_i. (4) 𝒢̂_i is then converted back to text (which is now an abstract description) using an AMR-to-Text generator. (5) This abstract description is then passed to the fine-tuned BART for generating augmentations. (6) We optionally fine-tune the fine-tuned BART (from the 1st step) on abstract-document pairs from 𝒟_down.","We present ABEX, a novel and effective generative data augmentation methodology for low-resource Natural Language Understanding (NLU) tasks. ABEX is based on ABstract-and-EXpand, a novel paradigm for generating diverse forms of an input document – we first convert a document into its concise, abstract description and then generate new documents based on expanding the resultant abstraction. To learn the task of expanding abstract descriptions, we first train BART on a large-scale synthetic dataset with abstract-document pairs. Next, to generate abstract descriptions for a document, we propose a simple, controllable, and training-free method based on editing AMR graphs. ABEX brings the best of both worlds: by expanding from abstract representations, it preserves the original semantic properties of the documents, like style and meaning, thereby maintaining alignment with the original label and data distribution. At the same time, the fundamental process of elaborating on abstract descriptions facilitates diverse generations. We demonstrate the effectiveness of ABEX on 4 NLU tasks spanning 12 datasets and 4 low-resource settings. ABEX outperforms all our baselines qualitatively with improvements of 0.04%- 38.8%. Qualitatively, ABEX outperforms all prior methods from literature in terms of context and length diversity. 
footnote-1","Improving the performance of deep learning models on downstream Natural Language Understanding (NLU) tasks requires sufficient good-quality training data. However, data annotation is an expensive, time-consuming, and noisy task. Data augmentation has proven to be an effective approach for overcoming the data scarcity issue in low-resource NLU tasks with limited training samples. The two major categories of study in data augmentation include online data augmentation by interpolation in the latent space and offline data augmentation that expands an existing small-scale dataset by generating additional synthetic data. Owing to advancements in generative models that facilitate the creation of high-quality synthetic data, the latter is gaining traction. 
However, generative data augmentation faces two major challenges: diversity in generated augmentations and consistency with the underlying data distribution. It is crucial to strike a balance between these two aspects, as overemphasizing one at the expense of the other can lead to poor downstream performance. Current augmentation methods based on text-infilling, where the primary task is to generate a new sentence constrained with keywords, are prone to replicate biases and overfit specific linguistic patterns in the low-resource training data, thereby hurting diversity. Additionally, we show that keyword-constrained free-form generation is unable to maintain the core semantic properties of the document, like style, which proves to be critical for specific tasks (e.g., question style document for intent classification. See example in Table). Diversity also proves to be an issue with token-level editing methods that rarely introduce novel entities or contexts and often randomly edits important tokens. Finally, prompt-based methods that employ Large Language Models (LLMs) require well-curated attributes selected from the data to control the distribution of the generated data. 
 
Main Contributions. In this paper, we propose ABEX, a novel data augmentation methodology based on a novel paradigm - Abstract-and-Expand. We first convert an input document into a concise, abstract description of itself and then generate augmentations by expanding the resultant abstraction. The task emulates human language perception and processing: the abstraction phase mirrors how humans distill core ideas from text, focusing on essential meanings, while the expansion phase reflects human creativity in generating varied narratives from a single abstract concept, akin to human extrapolation of ideas into diverse discussions. Our proposed Abstract-and-Expand task, which differs from all tasks proposed in prior art, generates augmentations that are both more consistent and diverse. To learn the task of expanding abstract descriptions, we first synthesize a large-scale synthetic dataset by prompting LLMs and then train an Encoder-Decoder Pre-trained Language Model (BART) on the dataset. Next, we propose a simple and controllable algorithm to generate abstract descriptions for training instances in any given downstream low-resource dataset. Our proposed algorithm leverages AMR-to-Text and Text-to-AMR and generates abstract descriptions by editing Abstract Meaning Representation (AMR) graphs. Inspired by the success of mixup in data augmentation, we also optionally mix AMR graphs of two sentences to boost the diversity of abstract descriptions. Finally, we synthesize diverse augmentations using the fine-tuned model and synthesized abstract descriptions. To summarize, our main contributions are: 
 
 * We propose ABEX, a novel and effective generative data augmentation methodology for low-resource NLP. We employ a novel Abstract-and-Expand task and fine-tune an Enc-Dec PLM to learn the task. ABEX differs from all prior work in its motivation and methodology and closely mimics the human perception and processing of language. 
 * We propose a simple, controllable, and training-free method for generating abstract descriptions of source documents from downstream NLU datasets. Our proposed methodology provides explicit control in the document-to-abstract generation process and overcomes the contained generation issue that LLMs face in abstract generation. 
 * To evaluate the efficacy of ABEX augmentations, we experiment on 12 datasets across 4 NLU tasks under 4 low-resource settings and show that ABEX outperforms most prior works quantitatively by 0.04%- 38.8%. Additionally, generations by ABEX are superior to prior work in terms of context, token (including entity), and length diversity. 
 * We also contribute the large-scale synthetic dataset with ≈0.2 million abstract-expansion pairs to promote further research in this space."
Token-wise Influential Training Data Retrieval for Large Language Models,2405.11724v2,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2405.11724v2.pdf,Influence estimation for a given generation.,"Given a Large Language Model (LLM) generation, how can we identify which training data led to this generation? In this paper, we proposed RapidIn, a scalable framework adapting to LLMs for estimating the influence of each training data. The proposed framework consists of two stages: caching and retrieval. First, we compress the gradient vectors by over 200,000x, allowing them to be cached on disk or in GPU/CPU memory. Then, given a generation, RapidIn efficiently traverses the cached gradients to estimate the influence within minutes, achieving over a 6,326x speedup. Moreover, RapidIn supports multi-GPU parallelization to substantially accelerate caching and retrieval. Our empirical result confirms the efficiency and effectiveness of RapidIn.","Large language models (LLMs) have been widely used in various applications across different industries, such as text generation, translation, summarization, and scientific applications, due to their unprecedented scale and the impressive capabilities derived from the massive training dataset. E.g., llama-2 has up to 70 billion parameters and is trained on 2 trillion tokens of online data. 
Given a model generation, can we determine which training data have the most influence for this generation? Understanding how training data influence the content they generate is particularly crucial. For example, when a risky generation is identified, tracing it back to the most influential training data can help developers filter out risky data and retrain the model. In addition, knowing the influence of training data for a target generation is highly valuable for machine unlearning, explainablity, detoxification, data cleansing and poisoning, privacy and security preserving. However, estimating influence of training data on LLMs of this unprecedented scale, trained on massive data containing over trillions of tokens, remains a challenge. 
Influence Estimation estimates the influence and traces generation back to training data (Figure). Although many studies explored influence estimation on deep learning, these methods cannot be scaled up to LLMs due to lacking of scalability and efficiency: e.g., proposed influence function using Hessian-vector products, but computing second-order gradients is prohibitively expensive for LLMs. To reduce computation, presented TracIn which only requires first-order gradient. However, even first-order gradients scale poorly—the gradients of a full-precision llama-2 7b model is ∼26GB in size; and ∼260GB for llama-2 70b. The massive gradient storage and processing make them impractical for LLMs. 
Although these studies have shown remarkable performance on influence estimation, they primarily focus on general deep learning models, and require first or second-order gradients. The extreme memory and computation of calculating full gradients presents substantial challenges in applying them to LLMs, particularly in the context of token-wise cases. 
Challenges. (1) Compared to general models, LLMs like llama-2, which has up to 70 billion parameters, present exceptional scalability challenges for influence estimation methods due to their vast number of parameters. (2) In addition to the scalability issues of model size, LLMs are trained on massive datasets (e.g., 2 trillion tokens for llama-2). Estimating the influence of each training data from such massive datasets presents another substantial challenge. (3) Almost all studies of influence function are based on the classification task and assign influence scores to each training sample. However, in LLM datasets, a single data sample consists of numerous tokens, and it is very challenging to assign an influence score to each token. 
In this paper, we propose RapidIn, a rapid influence estimating framework for LLMs, to estimate the influence of each training data for a given generation. RapidIn is designed to efficiently scale to large models and massive datasets. The framework includes two stages: caching and retrieval. Caching: RapidIn compresses the gradient vector of each training data into a low-dimensional representation called RapidGrad, reducing the size to MBs or even KBs. These compact RapidGrad representations are then cached to disk or memory. Subsequently, in retrieval, RapidIn can estimate the influence using the cached RapidGrad for the entire training data in minutes for any generation. 
Contributions. Our main contributions are: 
 
 * We present RapidIn that estimates the influence of each training data for a given LLM generation. 
 * We apply a collection of techniques to cache the gradients of LLMs by compressing gradient vectors by over 200,000x in the caching stage, and achieve a 6,326x speedup in the retrieval stage, enabling estimating the influence of the entire dataset for any test generation within minutes. 
 * We utilize multi-GPU parallelization to substantially accelerate the caching and retrieval. 
 * We release an open-source and easy-to-run implementation of RapidIn in PyTorch."
AbsInstruct: Eliciting Abstraction Ability from LLMs through Explanation Tuning with Plausibility Estimation,2402.10646v2,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2402.10646v2.pdf,An illustration of our AbsInstructframework. We collect explanation traces for each example and design a plausibility estimator to select data that match the knowledge of an LLM to be aligned.,"Abstraction ability is crucial in human intelligence, which can also benefit various tasks in NLP study. Existing work shows that LLMs are deficient in abstract ability, and how to improve it remains unexplored. In this work, we design the framework AbsInstructto enhance LLMs 'abstraction ability through instruction tuning. The framework builds instructions with in-depth explanations to assist LLMs in capturing the underlying rationale of abstraction. Meanwhile, we introduce a plausibility estimator to select instructions that are more consistent with the abstraction knowledge of LLMs to be aligned. Then, our framework combines abstraction instructions with general-purpose ones to build a hybrid dataset. Extensive experiments and analyses demonstrate that our framework can considerably enhance LLMs' abstraction ability with strong generalization performance while maintaining their general instruction-following abilities.","Abstraction ability is central to human cognition, which is identifying shared traits among items to build a broader concept, like deriving the concept of ""beverage"" from ""coffee"" and ""tea. "" With this ability, we can derive general rules and principles from past experiences, which enables us to adeptly navigate new situations in our daily life. In NLP, building abstraction resources has long been a vital challenge to which the community has devoted many efforts. 
Among them, built the first comprehensive benchmark, AbsPyramid, of abstract concepts for nouns, verbs, and events. In this benchmark, models are asked to detect the validity of an abstract concept, as shown in. Their evaluations on the benchmark reveal that abstraction remains challenging even for state-of-the-art LLMs. For example, ChatGPT only modestly exceeds majority voting and substantially trails behind fine-tuned smaller models. While prior works have explored ways for general-domain LLM alignment, how to elicit the abstraction knowledge of LLMs remains unexplored. 
Nonetheless, enhancing LLMs 'abstraction ability is a non-trivial task. We only observe slight improvements when gathering vanilla instructions from randomly sampled data for detecting abstract concepts. First, the responses of vanilla instructions only express the validity of abstract concepts as ""Yes/No. "" As a result, LLMs might only grasp the surface-level styles but miss underlying rationales in deciding the validity of abstract concepts. Moreover, existing studies show that LLMs acquire most of the knowledge and abilities during pre-training. Thus, instructions from randomly sampled data might not be consistent with the abstraction knowledge of pre-trained models for better elicitation. 
To tackle those issues, we propose the framework AbsInstructto build instructions with detailed explanation traces and well-crafted data selection, as shown in. The framework forms explanation traces by collecting meanings of each given instance and abstract concept. These traces can help LLMs better comprehend the underlying reasoning process of detecting abstract concepts. Moreover, we introduce a plausibility estimator to select instruction data consistent with the abstraction knowledge of a pre-trained model to be aligned. The estimator assesses the plausibility score of each example based on the probability computed by the pre-trained model. Then, we only retain examples with higher plausibility scores, which align better with the model' s knowledge. We also introduce a collection of filters based on lexical overlap, keywords, and predicted labels to ensure diversity and quality further. Ultimately, a hybrid dataset is constructed by combining instructions for abstraction detection with those in the general domain. 
For evaluation, the framework first builds instructions for abstraction detection based on AbsPyramid and combines them with instructions from Alpaca. Next, we conduct extensive experiments and analyses of several popular LLMs instruction-tuned with our framework. The evaluation results show that applying AbsInstructcan effectively unlock LLMs 'abstraction ability, with the performance surpassing existing alignment methods by a large margin of 6-10%. Also, thorough ablation studies corroborate the efficacy of explanation traces, the plausibility estimator, and various filters. Meanwhile, we conduct detailed analyses to show the robustness of our framework and the generalization ability of LLMs trained with our framework. Last but not least, the automatic and human evaluations on two general-domain instruction datasets, SuperNI and Self-Instruct, manifest that our framework can enhance abstraction ability without compromising LLMs' performance of following general instructions."
RORA: Robust Free-Text Rationale Evaluation,2402.18678v3,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2402.18678v3.pdf,"RoRa framework for evaluating rationales R_1^True, R_2^True, R_3^True. Existing baselines are highly sensitive to rationales that simply restate the label or paraphrase the given question and label, leading to inflated scores compared to the human-annotated rationale. In contrast, RoRa provides an informativeness score that better characterizes rationale quality. It is achieved by 1 detecting potential leakage tokens in the rationale () and 2 generate additional training data with counterfactual editing for data augmentation (), followed by 3 training an evaluation model invariant to label leakage ().","Free-text rationales play a pivotal role in explainable NLP, bridging the knowledge and reasoning gaps behind a model's decision-making. However, due to the diversity of potential reasoning paths and a corresponding lack of definitive ground truth, their evaluation remains a challenge. Existing evaluation metrics rely on the degree to which a rationale supports a target label, but we find these fall short in evaluating rationales that inadvertently leak the labels. To address this problem, we propose RoRa, a RObust free-text RAtionale evaluation against label leakage. RoRa quantifies the new information supplied by a rationale to justify the label. This is achieved by assessing the conditional 𝒱-information with a predictive family robust against leaky features that can be exploited by a small model. RoRa consistently outperforms existing approaches in evaluating human-written, synthetic, or model-generated rationales, particularly demonstrating robustness against label leakage. We also show that RoRa aligns well with human judgment, providing a more reliable and accurate measurement across diverse free-text rationales.","The ability of large language models (LLMs) to generate free-text rationales that elaborate on their decision-making processes holds promise for explainable NLP, either in the form of a reasoning chain or post-hoc explanations. Previous works have also collected human-written rationales to enhance model reasoning and the generation of free-text rationales. 
However, evaluating these rationales remains an open problem because of the diversity of reasoning paths and the lack of definitive ground truth. As a result, existing metrics rely on measuring how much the rationale supports a given label. This is usually achieved by comparing predictions of models trained with and without rationales. For example, Leakage-Adjusted Simulatability (LAS) and Rationale Quality (RQ) measure rationale quality through the difference in accuracy. Alternatively, Rationale Evaluation with conditional-𝒱-information (REV) evaluates the reduction in model predictive uncertainty upon conditioning on the rationale. 
Yet all these methods are vulnerable to label leakage: the rationale inadvertently paraphrasing or restating labels, creating a spurious shortcut for the evaluation model to infer the label. The critical issue stems from the mismatch in objectives: existing methods evaluate how easy it is to utilize information in the rationale, but rationales are explanations, whose quality does not always come with simplicity. The best explanation has to support the answer through some sense of mechanisms, such as methodically considering a set of axioms and running through a deductive chain, without which they are mere ""effects"". shows an example where existing evaluation methods are highly sensitive to label leakages in paraphrased or restated rationales, while in fact, these label leakages merely increase the predictive probability without providing any meaningful explanations. 
With this objective in mind, we introduce RoRa, a novel approach to evaluate rationales robust to label leakage. RoRa's construction consists of three stages as illustrated in. First, we fit a small model and identify label-leaking tokens via its gradient-based attributions (). After that, we generate additional training data with counterfactual editing (). Finally, we force the evaluation model to ignore these label-leaking tokens through invariant learning (). Our approach aligns with the human perception that explanations should apparently increase the understanding of a given phenomenon by helping to create knowledge and to develop better theories. On the contrary, label leakage tends to be repetitive and tautological, dominating the insightful parts of the explanation. 
We compare RoRa with baseline metrics in evaluating various synthetic and human-annotated rationales, with or without label leakage, on three QA datasets. RoRa consistently outperforms baseline metrics by providing robust evaluations against label leakages. We also compare RoRa against model-generated rationale evaluation and demonstrate its better agreement with human evaluation."
ConSiDERS-The-Human Evaluation Framework: Rethinking Human Evaluation for Generative Large Language Models,2405.18638v2,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2405.18638v2.png,"Yearly publications of papers with keywords ""human"" and ""eval"" in title/abstract","In this position paper, we argue that human evaluation of generative large language models (LLMs) should be a multidisciplinary undertaking that draws upon insights from disciplines such as user experience research and human behavioral psychology to ensure that the experimental design and results are reliable. The conclusions from these evaluations, thus, must consider factors such as usability, aesthetics, and cognitive biases. We highlight how cognitive biases can conflate fluent information and truthfulness, and how cognitive uncertainty affects the reliability of rating scores such as Likert. Furthermore, the evaluation should differentiate the capabilities and weaknesses of increasingly powerful large language models – which requires effective test sets. The scalability of human evaluation is also crucial to wider adoption. Hence, to design an effective human evaluation system in the age of generative NLP, we propose the ConSiDERS-The-Human evaluation framework consisting of 6 pillars – Consistency, Scoring Critera, Differentiating, User Experience, Responsible, and Scalability.","Generative tasks in natural language processing (NLP) have to rely on human evaluation, as the current set of automated metrics does not correlate well with human judgment. Human evaluation tends to be expensive and difficult to repeat or reproduce. Even more importantly, an all-too-common scenario tends to be that the evaluation method is fundamentally misaligned with the problem statement. In the age of generative large language models (LLMs) with increasing capabilities that can generate fluent content to even fool humans, ensuring that the human evaluation is set up appropriately to measure the right aspects and reach the right conclusions is crucial. 
In this position paper, first, we argue that to design and interpret the results of human evaluation accurately, the evaluation pipeline needs to be human-centric in the age of generative AI, accounting for human evaluators and their cognitive biases. The field of user experience (UX) takes into account the emotional states of a user, a. k. a. how a user feels. It is a well-known fact in UX that users tend to be heavily influenced by aesthetic aspects, while actual function or usability aspects take a second place when users perceive a system as useful, leading to the notion ""what is beautiful is useful"". Aesthetics also extends to language. Factors such as fluency can affect the evaluation, outweighing the actual content or substance. Studies in human-computer interface (HCI), cognitive, and social psychology have demonstrated that processing fluency – the ease with which information is perceived and processed in the human mind – has a positive effect on evaluation. Current state-of-the-art (SOTA) LLMs tend to be quite fluent and produce content that is easy to read and understand, and as a result, users can conflate fluency and usefulness. Therefore, we need to closely examine our human evaluation before reaching conclusions such as – The LLM can perform function <x> similar to or better than a trained professional. NLP evaluation procedures, therefore, at the very least must delineate style vs. substance. 
Secondly, the effectiveness of the test set in measuring the capabilities of a model is critical, as ineffective test sets cannot adequately evaluate these models, a common theme that has surfaced in many leader-boards and public data sets. 
Hence, in this position paper, we make the following contributions: 
1) We propose a framework, a structure for organizing and contextualizing human evaluation, that can be customized and adapted to specific contexts. Our proposed framework – the ConSiDERS-The-Human evaluation framework – has 6 pillars: 
 The 6 pillars of ConSiDERS-The-Human Evaluation Framework: (See Checklist in Appendix to follow. ) -3mm 
 * Consistency of human evaluation: The findings of human evaluation must be reliable and generalizable. 
 * Scoring Criteria: The scoring criteria must include both general purpose criteria such as readability, as well as be tailored to fit the goal of the target tasks or domains. 
 * Differentiating: The evaluation test sets must be able to differentiate the various capabilities as well as the weaknesses of generative LLMs. 
 * User Experience: The evaluation must take into account user experience, including their emotions & cognitive biases, when designing experiments and interpreting results. 
 * Responsible: The evaluation needs to account for responsible AI including aspects such as bias, safety, robustness, and privacy capabilities of the model. 
 * Scalability: Human evaluation must be scalable for pragmatic widespread adoption. 
 
2) We make the case for why UX and the psychology of cognitive biases should be at the forefront of human evaluation. In the last 20 years, less than 7%of the papers (only 16 papers) with ""human"" and ""eval"" in their title available in ACL Anthology mention user experience-related keywords in either the title or the abstract (see query in Appendix). 
3) We highlight how neglecting the role of cognitive biases in human evaluation can lead to incorrect inclusions from the study. We, therefore, provide specific recommendations to mitigate the effects of common cognitive biases. We also provide tips to troubleshoot and improve consistency issues in human evaluation. 
In the rest of this paper, we introduce the necessary background concepts in Section and explore each of the 6 pillars in detail in Section."
Linguistically Conditioned Semantic Textual Similarity,2406.03673v1,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2406.03673v1.png,A problematic example from the C-STS dataset. The binarity of the condition cannot be mapped to a 5-point similarity scale. The label can be subjective depending on how much inference is made from the context. No guideline on the scenario when the information regarding the condition is missing.,"sts is a fundamental NLP task that measures the semantic similarity between a pair of sentences. In order to reduce the inherent ambiguity posed from the sentences, a recent work called csts has been proposed to measure the sentences 'similarity conditioned on a certain aspect. Despite the popularity of csts, we find that the current csts dataset suffers from various issues that could impede proper evaluation on this task. In this paper, we reannotate the csts validation set and observe an annotator discrepancy on 55%of the instances resulting from the annotation errors in the original label, ill-defined conditions, and the lack of clarity in the task definition. After a thorough dataset analysis, we improve the csts task by leveraging the models' capability to understand the conditions under a QA task setting. With the generated answers, we present an automatic error identification pipeline that is able to identify annotation errors from the csts data with over 80%F1 score. We also propose a new method that largely improves the performance over baselines on the csts data by training the models with the answers. Finally we discuss the conditionality annotation based on the tfs of entity types. We show in examples that the tfs is able to provide a linguistic foundation for constructing csts data with new conditions.","sts is an essential NLP task that measures the semantic similarity between two sentences. It is also a popular benchmark for developing tasks such as text embedding learning and language understanding. While the sts datasets have been developed and improved over the past years, the task itself still suffers from sentence ambiguity and subjectivity to judgment. 
A new task called csts has been proposed to resolve those issues. It is designed to disambiguate the similarity between two sentences by measuring the similarity on a given condition. An accompanying dataset was also proposed to test models on the csts task. Despite the popularity of csts, we observe certain limitations in the csts dataset that could hinder the understanding and proper evaluation of models on this task. As illustrated in Figure, these limitations primarily revolve around annotation errors, ill-defined conditions, and a general lack of clarity in task definition. 
Taking into account the significance of these issues, we intend to improve the csts dataset by addressing the existing problems that we observed. We start by reannotating the csts validation set. By identifying an apparent annotation error rate of 55%in their validation set, we analyze the provenance of the errors and discrepancies between the original and relabeled datasets. 
To further explore the utility of the condition and how it is understood by language models, we treat it as a qa task and leverage llms to generate the answer to the question that is constructed from the condition. We find that the LLM-generated answers can better capture the similarity between two sentences and fit closely to our reannotated labels by having a higher Spearman's Correlation. Based on this finding, we propose an approach to identify potential annotation errors from the csts dataset utilizing the LLM-generated answers, achieving over 80%F1 score on the validation set. We also propose a new method to improve the csts task by training the models with the answers. We show that both supervised and generative models can efficiently and effectively learn the condition information encoded in the answers, improving the performance over baselines by a large margin. 
Finally, we discuss a new annotation specification of the conditionality that aims to improve the formulation of the conditions with a more concrete semantic base. We use the entity type identified from the sentence pair as the surface condition text that is described by its underlying tfs. We exemplify that tfs-based conditions can be successfully adopted to sentence pairs from the current csts dataset. 
We summarize the main contributions of this paper as threefold. We reannotate the csts validation set and propose an error identification pipeline that can be applied to the whole dataset to identify potential annotation errors and ambiguities; we propose a qa-facilitated method that largely improves the model performance on the csts task; we discuss using tfs as a new annotation specification to improve the conditionality in csts dataset with a more concrete semantic base. We make the source code and dataset publicly available."
Rewriting the Code: A Simple Method for Large Language Model Augmented Code Search,2401.04514v2,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2401.04514v2.pdf,"Comparison of GAR between passage retrieval and code search. In passage retrieval, the truth (yellow) is included in the generated content. In code search, despite the generated exemplar code satisfies the description of the query, it exhibits noticeable dissimilarity to the true code.","In code search, the Generation-Augmented Retrieval (GAR) framework, which generates exemplar code snippets to augment queries, has emerged as a promising strategy to address the principal challenge of modality misalignment between code snippets and natural language queries, particularly with the demonstrated code generation capabilities of Large Language Models (LLMs). Nevertheless, our preliminary investigations indicate that the improvements conferred by such an LLM-augmented framework are somewhat constrained. This limitation could potentially be ascribed to the fact that the generated codes, albeit functionally accurate, frequently display a pronounced stylistic deviation from the ground truth code in the codebase. In this paper, we extend the foundational GAR framework and propose a simple yet effective method that additionally Rewrites the Code (ReCo) within the codebase for style normalization. Experimental results demonstrate that ReCo significantly boosts retrieval accuracy across sparse (up to 35.7%), zero-shot dense (up to 27.6%), and fine-tuned dense (up to 23.6%) retrieval settings in diverse search scenarios. To further elucidate the advantages of ReCo and stimulate research in code style normalization, we introduce Code Style Similarity, the first metric tailored to quantify stylistic similarities in code. Notably, our empirical findings reveal the inadequacy of existing metrics in capturing stylistic nuances. The source code and data are available at <https: //github. com/Alex-HaochenLi/ReCo>.","Code search, aimed at retrieving the most semantically relevant code snippets from a codebase according to a specified natural language query, is a common activity that plays an important role in software development. Retrieving and reusing analogous code fragments from large-scale codebases like GitHub can enhance productivity significantly. 
Despite both being sequences of words, matching code queries and natural language queries is challenging as they share few grammatical rules, causing them to fall into two distinct modalities. This grammatical distinction results in limited word overlap, significantly hampering the application of sparse retrieval systems in code search. On the other hand, in dense retrieval systems, the alignment of query and code representations during the training phase assists in alleviating the challenge. As a result, these systems are capable of encapsulating potential semantic correlations between terminologies employed in programming languages and those in natural languages. However, this potential association becomes challenging to capture if two terminologies rarely manifest together within a query-code pair. 
To bridge this gap, one possible solution is to transform the data from one modality to the other. This could involve either generating exemplar codes based on the query or summarizing the functionality of codes in the codebase. Given that natural language queries in code search are often short and ambiguous, our research concentrates on the former solution, referred as Generation-Augmented Retrieval (GAR). GAR has demonstrated competitive performance in question answering and passage retrieval. In these NLP tasks, a language model is adopted to generate references based on the query to augment it. Similarly, we could use a language model to generate exemplar code snippets that realize the functionalities described in the query. Then the query and exemplar codes are combined to be fed into the retrieval system. With many LLMs demonstrating great intelligence in precisely writing codes, performing GAR with LLMs becomes a promising approach for code search. 
However, from our preliminary studies, the improvement in performance brought by GAR using LLMs is limited, especially with the high computational cost of LLMs. We argue that answer format influences the performance of GAR on question answering and code search. In question answering, the correct answer to the question is often unique and can be expressed in limited forms. The generated contents from LLMs, if correct, are usually in the exact same form as the answer. As highlighted in Fig. , the matching word ""depressive"" appears in the reference. On the other hand, code snippets with the same functionality can have diverse formulations, which lowers the chance of matching the code in the codebase, and thus leads to minor improvement of GAR in code search. As shown in Fig. , the true code uses Python built-in function Counter to count the number of elements in a list, while the exemplar code snippet does it manually. 
To address the mismatch of the generated and ground truth code snippets, we build upon GAR and propose a simple yet effective framework that additionally Rewrites and the Code (ReCo) in the codebase. As shown in Fig. , after rewriting, the style of codes in the codebase are normalized by LLMs to align with the exemplar code, thereby facilitating the retrieval. We evaluate ReCo on several code search models across various search scenarios, including coding challenge competence, online programming community, and general programming problems in Python and Java. Experimental results show that ReCo could significantly boost the performance of sparse retrieval systems (up to 35.7%) and dense retrieval systems in both zero-shot (up to 27.6%) and fine-tuning (up to 23.6%) settings. 
Furthermore, we propose a novel evaluation metric, dubbed Code Style Similarity, to quantitatively measure the disparity in code style. Our metric validates ReCo's capability in aligning the style of code within the codebase with that of code generated by LLMs. Conventional metrics like BLEU and CodeBLEU are deemed less appropriate as they calculate similarity based on exact-matched tokens of the given two code snippets. In contrast, Code Style Similarity evaluates style from three distinct perspectives: variable naming, API invocation, and code structure, based on edit distance. Our experiments show that Code Style Similarity exhibits superior explanatory power than existing metrics in measuring the style deviation of code from the dataset and that generated from LLM."
A Multidimensional Framework for Evaluating Lexical Semantic Change with Social Science Applications,2406.06052v1,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2406.06052v1.png,Three Major Dimensions of Semantic Change.,"Historical linguists have identified multiple forms of lexical semantic change. We present a three-dimensional framework for integrating these forms and a unified computational methodology for evaluating them concurrently. The dimensions represent increases or decreases in semantic 1) sentiment (valence of a target word's collocates), 2) breadth (diversity of contexts in which the target word appears), and 3) intensity (emotional arousal of collocates or the frequency of intensifiers). These dimensions can be complemented by the evaluation of shifts in the frequency of the target words and the thematic content of its collocates. This framework enables lexical semantic change to be mapped economically and systematically and has applications in computational social science. We present an illustrative analysis of semantic shifts in mental health and mental illness in two corpora, demonstrating patterns of semantic change that illuminate contemporary concerns about pathologization, stigma, and concept creep.","Lexical semantic change is defined by historical linguists as innovations that alter the meaning, but not the grammatical function, of a form. For instance, ""awesome"" once denoted the capacity to inspire awe, but its meaning has since been bleached to a general expression of approval. Computational linguists have made strides in developing distributional semantic methods to detect semantic change and its laws as distinct from cultural shifts. 
Advances in deep learning since 2018 afford new ways to model semantic change processes. These innovations have facilitated the development of language models with sophisticated word embeddings or vector representations. As a result, word embeddings have evolved from count-based models, where words are represented by their co-occurrence frequency with other words, to prediction-based representations, where word vectors are iteratively learned as part of a language modelling task objective. The granularity of these representations shifted from type-level, where each word has a single vector despite its usages, to token-based, or contextualized representations, where each word instance (token) has a vector, dynamically capturing shifts in meaning based on context. Lexical semantic relations can be detected by typeand token-level embeddings. 
Other work has started addressing the challenge of formalizing and understanding kinds of semantic change. Processes such as broadening, metaphorization, and pejoration/amelioration have been modelled. Researchers have created methods to automatically disambiguate a word's pejorative usage from its non-pejorative use. Attempts have also been made to evaluate understudied classes of semantic change. Sentence representations from neural language models were used for hyperbole detection. Exaggerated language can be generated and detected, alongside metaphor. Researchers have also evaluated semantic bleaching, whereby words lose elements of their meaning, and found it to be triggered in contexts where an adverb premodifies a semantically similar adjective (e.g., ""insanely jealous""). Nevertheless, there are a dearth of diachronic methods for evaluating lexical semantic change. 
Despite advances in detecting and modelling lexical semantic change, there is a need for a unifying framework to integrate multiple dimensions of change. The present study addresses this gap by proposing a framework which synthesizes the theoretical insights of historical linguists about the many distinct forms of diachronic lexical semantic change and aligns them with the methodological sophistication of natural language processing. The comprehensive computational framework for evaluating lexical semantic change that emerges should be valuable for computational social scientists seeking to understand and model social and cultural change."
Mitigating Catastrophic Forgetting in Large Language Models with Self-Synthesized Rehearsal,2403.01244v2,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2403.01244v2.pdf,Comparison of standard rehearsal and our proposed Self-Synthesized Rehearsal (SSR).,"Large language models (LLMs) suffer from catastrophic forgetting during continual learning. Conventional rehearsal-based methods rely on previous training data to retain the model's ability, which may not be feasible in real-world applications. When conducting continual learning based on a publicly-released LLM checkpoint, the availability of the original training data may be non-existent. To address this challenge, we propose a framework called Self-Synthesized Rehearsal (SSR) that uses the LLM to generate synthetic instances for rehearsal. Concretely, we first employ the base LLM for in-context learning to generate synthetic instances. Subsequently, we utilize the latest LLM to refine the instance outputs based on the synthetic inputs, preserving its acquired ability. Finally, we select diverse high-quality synthetic instances for rehearsal in future stages. Experimental results demonstrate that SSR achieves superior or comparable performance compared to conventional rehearsal-based approaches while being more data-efficient. Besides, SSR effectively preserves the generalization capabilities of LLMs in general domains.","Large language models (LLMs) have demonstrated remarkable performance across various natural language processing (NLP) tasks. In real-world applications, LLMs are often updated in a continual learning (CL) manner, where new instruction tuning data is incrementally introduced over time. However, a significant issue that limits the effectiveness of LLMs is catastrophic forgetting, which refers to the LLM 's tendency to forget previously acquired knowledge when learning new instances. 
To mitigate catastrophic forgetting, a line of work focuses on rehearsing previous training instances. These rehearsal-based methods maintain the model' s ability by training on real data from previous training stages. However, the real data may not always be desirable in practical applications. For instance, when conducting continual learning based on a publicly-released LLM checkpoint (e.g.,Llama-2-chat), the availability of the original training data may be non-existent. This raises an interesting research question: Can we maintain the LLM 's ability during continual learning without using real data in previous training stages? 
We propose the Self-Synthesized Rehearsal (SSR) framework to mitigate catastrophic forgetting in continual learning. As shown in Figure, unlike standard rehearsal-based continual learning that samples training instances from previous stages as rehearsal data, SSR framework uses the LLM to generate synthetic instances for rehearsal. Specifically, we first use the base LLM to generate synthetic instances, conducting in-context learning (ICL) with few-shot demonstrations. These demonstrations can be collected from the previous data or human-constructed containing similar knowledge to the previous data. Then, the latest LLM is used to refine the outputs of synthetic instances to retain the latest LLM' s ability. Finally, we select diverse high-quality synthetic instances for rehearsal in the future stages. 
Extensive experiments on the task sequences derived from the SuperNI dataset demonstrate that SSR has superior or comparable performance compared to the conventional rehearsal-based approaches, with higher data utilization efficiency. Besides, experiments on AlpacaEval and MMLU show that SSR can also effectively preserve the generalization capabilities of LLMs in general domains. We release our code and data at <https: //github. com/DeepLearnXMU/SSR>."
Dependency Transformer Grammars: Integrating Dependency Structures into Transformer Language Models,2407.17406v1,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2407.17406v1.pdf,An example sentence with its dependency tree and transition sequence. Numbers in blue and red are indices of tokens and arcs respectively.,"Syntactic Transformer language models aim to achieve better generalization through simultaneously modeling syntax trees and sentences. While prior work has been focusing on adding constituency-based structures to Transformers, we introduce Dependency Transformer Grammars (DTGs), a new class of Transformer language model with explicit dependency-based inductive bias. DTGs simulate dependency transition systems with constrained attention patterns by modifying attention masks, incorporate the stack information through relative positional encoding, and augment dependency arc representation with a combination of token embeddings and operation embeddings. When trained on a dataset of sentences annotated with dependency trees, DTGs achieve better generalization while maintaining comparable perplexity with Transformer language model baselines. DTGs also outperform recent constituency-based models, showing that dependency can better guide Transformer language models. Our code is released at <https: //github. com/zhaoyd1/Dep_Transformer_Grammars>.","Transformer language models have shown strong performance on language modeling tasks and a broad spectrum of downstream tasks. Despite the great power of the Transformer architecture, it lacks the inductive biases of syntactic structures, which has been hypothesized to improve generalization. A straightforward way to incorporate such biases into Transformers is explicit modeling of syntactic structures. 
Inspired by earlier work of generative parsing as language modeling that integrates syntactic structures into RNNs, recent studies have focused on adapting this method to Transformer architectures. The models proposed by these studies are categorized as syntactic language models because they jointly model the distribution of surface strings and their corresponding syntactic trees. Experiments show that these models achieve competitive perplexity in language modeling and gain better syntactic generalization, supporting the above hypothesis on the benefits of introducing inductive bias of syntactic structures. However, the structural supervision that has been used in all these models is based on constituency trees and it is unclear of the performance of dependency-based Transformer syntactic language models. Different from constituency structures, which model recursive syntactic compositions, dependency structures focus more on the relationship between tokens, which is similar to the self-attention mechanism in Transformer, hinting at potential synergy between the two. 
In this paper, we propose Dependency Transformer Grammars (DTGs), dependency-based syntactic language models that learn joint distributions of sentences and dependency trees. DTGs introduce an inductive bias of dependency structures to Transformers by (i) modeling transition sequences of transition-based dependency parsers instead of sentences, (ii) simulating the stack operations in transition-based dependency parsers through modification of attention masks, (iii) incorporating the stack information of transition-based systems through relative positional encoding of stack depth, and (iv) representing head-dependent relations through a combination of head token embeddings and transition operation embeddings. Following a line of previous work in generative dependency parsing, the generative formulation of our model is based on the arc-standard system, which builds a dependency tree in a bottom-up manner. We also explore models using other dependency transition systems for comparison. 
Our experiments show that DTGs achieve comparable perplexity in language modeling and improved syntactic generalization on both the BLiMP benchmark and the SG test suites over Transformer language model baselines. Furthermore, DTGs outperform constituency-based syntactic language models in both language modeling and syntactic generalization. 
In summary, our contributions are as follows. 
 
 * We propose dependency-based syntactic language models, DTGs, to incorporate dependency inductive bias into Transformers. 
 * We primarily build DTGs using the arc-standard transition system, while we also study the usage of other dependency transition systems. 
 * Experimental results on two syntactic generalization benchmarks show the benefits of introducing inductive bias of dependency structures."
Multimodal Prompt Learning with Missing Modalities for Sentiment Analysis and Emotion Recognition,2407.05374v1,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2407.05374v1.pdf,The overall architecture of our proposed method. A batch of data that contains different missing modality cases is fed to the Missing Modality Generation Module (see Section) to obtain generated features. They are then passed to the pre-trained backbone with missing-signal prompts and missing-type prompts (see Section).,"The development of multimodal models has significantly advanced multimodal sentiment analysis and emotion recognition. However, in real-world applications, the presence of various missing modality cases often leads to a degradation in the model's performance. In this work, we propose a novel multimodal Transformer framework using prompt learning to address the issue of missing modalities. Our method introduces three types of prompts: generative prompts, missing-signal prompts, and missing-type prompts. These prompts enable the generation of missing modality features and facilitate the learning of intraand inter-modality information. Through prompt learning, we achieve a substantial reduction in the number of trainable parameters. Our proposed method outperforms other methods significantly across all evaluation metrics. Extensive experiments and ablation studies are conducted to demonstrate the effectiveness and robustness of our method, showcasing its ability to effectively handle missing modalities. Codes are available at <https: //github. com/zrguo/MPLMM>.","Humans perceive the world in a multimodal way, such as sight, sound, touch and language. These multimodal features can provide comprehensive information to help us understand and explore the world. Thus, modeling and mining multimodal data is of great importance and has much potential. Recently, multimodal sentiment analysis has attracted much attention. However, there are two main challenges in many existing methods: 1) Different from common multimodal tasks which only have two modalities (image and text), multimodal sentiment analysis task often has more modalities (video, audio, text, etc. ). Therefore, in real-world scenarios, missing modality conditions always occur due to equipment failure, data corruption, privacy issues and the like, especially in low-resource domains, which could lead to a degradation in the model 's performance. Current multimodal models trained on complete data usually fail when tested on incomplete data. 2) With the success of large-scale multimodal models, lots of researchers tend to finetune these large pre-trained models to downstream tasks. However, this kind of finetuning is infeasible for many researchers because it requires large computational resources. Besides, finetuning such a pre-trained model on small datasets could lead to instability. 
Recently, prompt learning is proposed, which freezes all the parameters of a pre-trained model while only finetuning several prompts and it has achieved great success. Motivated by prompt learning, in this paper, we intend to exploit a high-resource dataset that contains relatively more complete modality data for pre-training and then leverage several trainable prompts to transfer the knowledge from high-resource domains to low-resource domains where missing modality cases often occur. 
Previous works mainly focus on introducing sophisticated architecture to address the issue of missing modalities. These methods do not use pre-trained models and usually require a lot of computational resources. However, our method is based on prompt learning, which only finetunes a few parameters of prompts. is a recent work which is similar to ours. However, its proposed missing-aware prompts increase exponentially with the number of modalities. In contrast, our proposed prompts increase linearly with the number of modalities which is more parameter-efficient. Specifically, we propose three types of prompts: generative prompts, missing-signal prompts, and missing-type prompts which can learn the representations of the missing modalities, cross-modal and fine-grained features. These three types of prompts play a combined role in improving the model' s performance. 
We conduct extensive experiments on four datasets: CMU-MOSEI, CMU-MOSI, IEMOCAP and CH-SIMS. The proposed method outperforms the baselines significantly across all metrics on all datasets. We further study the roles of three types of prompts, the effect of missing rate of training data, and the effect of prompt length. We find that: 1) missing-signal prompts are modality-specific while missing-type prompts are modality-shared which represent intra-modality and inter-modality information respectively. 2) with short prompts, our model can achieve very good results which demonstrates our proposed method is parameter-efficient. 3) the missing rate is important for the performance of the model, with 70%being the optimal value. 
Our contributions can be summarized as follows: 
 
 * We present a novel framework via prompt learning for sentiment analysis and emotion recognition which is not only computationally efficient but also capable of handling missing modalities during both the training and testing stages. 
 * The number of parameters of our proposed prompts is linearly related to the number of modalities, which significantly reduces computational resources. 
 * We propose three types of prompts to address the issue of missing modalities. These three types of prompts can generate missing information, and learn intraand inter-modality information respectively. 
 * Our proposed method outperforms all the baselines across all metrics significantly. Furthermore, we discover that applying modality dropout with a rate of 70%during training yields the best enhancement in the model's performance."
Generative Pre-trained Speech Language Model with Efficient Hierarchical Transformer,2406.00976v2,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2406.00976v2.pdf,The comparison of frameworks for generative speech pre-training. (a) AudioLM is a three-stage model. (b) VALL-E is a two-stage model. (c) GPST is a one-stage model.,"While recent advancements in speech language models have achieved significant progress, they face remarkable challenges in modeling the long acoustic sequences of neural audio codecs. In this paper, we introduce Generative Pre-trained Speech Transformer (GPST), a hierarchical transformer designed for efficient speech language modeling. GPST quantizes audio waveforms into two distinct types of discrete speech representations and integrates them within a hierarchical transformer architecture, allowing for a unified one-stage generation process and enhancing Hi-Res audio generation capabilities. By training on large corpora of speeches in an end-to-end unsupervised manner, GPST can generate syntactically consistent speech with diverse speaker identities. Given a brief 3-second prompt, GPST can produce natural and coherent personalized speech, demonstrating in-context learning abilities. Moreover, our approach can be easily extended to spoken cross-lingual speech generation by incorporating multi-lingual semantic tokens and universal acoustic tokens. Experimental results indicate that GPST significantly outperforms the existing speech language models in terms of word error rate, speech quality, and speaker similarity. The code is available at <https: //github. com/youngsheen/GPST>.","Speech quantization has emerged as a crucial technique for speech language models to generate controllable, high-quality speech waveforms. Specifically, a speech waveform can be quantized into two distinct types of discrete representations: semantic tokens and acoustic tokens. Semantic tokens are typically obtained by applying the K-means clustering algorithm to the continuous activation space of self-supervised speech models. Notably, GSLM finds that auto-regressive models trained on semantic tokens can capture high-level linguistic content, supporting language modeling and resynthesis. However, semantic tokens fail to retain acoustic details such as speaker identity, resulting in suboptimal reconstruction. In contrast, acoustic tokens generated by neural codec models effectively compress speech at low bitrates while capturing the nuances of speech waveforms. Consequently, a speech language model can maintain long-term consistency with semantic tokens and produce high-quality synthesis with acoustic tokens. 
However, neural codec models require an excessive number of codes for high-quality speech synthesis. For example, EnCodec generates codec embeddings at 75 Hz for audio waveforms at 24 kHz. Subsequently, these codec embeddings are modeled using residual vector quantization (RVQ), wherein high-quality synthesis typically requires eight or more hierarchical quantizers with 1024 entries. Therefore, a mere 10-second waveform results in at least 75 × 8 × 10 = 6000 codes, which constitutes an excessively long sequence for language models due to the quadratic complexity with respect to the sequence length for calculating self-attention. Consequently, addressing the trade-off between the perceptual quality and computational complexity remains a core challenge for speech language models. 
Recently, some methods have been proposed to address the issue of lengthy acoustic sequences. Acoustic tokens inherently possess a hierarchical structure because of residual vector quantization: tokens from the preceding quantizers restore acoustic properties such as speaker identity, while the subsequent quantizers capture finer acoustic details. Each quantizer is trained to model the residuals from the previous quantizers. Recent approaches treat the acoustic token generation process as a multi-stage framework to avoid learning excessively long sequences simultaneously. 
In this work, we present Generative Pre-trained Speech Transformer (GPST), a model that facilitates controllable, high-quality speech generation in single stage. Our approach combines speech quantization with the architecture of a hierarchical transformer. GPST initially models the semantic sequence with a next token prediction task, followed by modeling the acoustic sequence with the task of predicting the next D stack codes. The semantic sequence serves as a prompt for the acoustic token as a condition. We design a specialized hierarchical architecture to model the underlying hierarchical structure of the acoustic sequence, which comprises of a large global transformer and a small local transformer. The global transformer learns the high-level relationships between the semantic tokens and the stacked acoustic tokens, while the local transformer models the hierarchical details in the stacked acoustic codes. By incorporating semantic and acoustic tokens within one hierarchical transformer, GPST can significantly reduce computational costs and effortlessly learn the long-term interactions of semantic tokens and local dependencies among residual codes. Furthermore, we propose a training technique called ""local-drop"" to further improve the training efficiency of Hi-Res speech generation, which is typically impractical in current speech language models because of a large number of residual quantizers. Consequently, our model can generate high-quality and semantically coherent speeches in one stage efficiently. 
Our main contributions are summarized as follows. 
 
 * We propose a novel generative pre-trained speech language model GPST that enables controllable, high-quality speech generation in a single stage. By integrating semantic tokens and acoustic tokens within a hierarchical transformer, GPST significantly reduces computational costs while efficiently learning the long-term interactions of semantic tokens and local dependencies among residual codes simultaneously. 
 * We demonstrate GPST 's capacity not only to generate coherent speech unconditionally but also to generate speech while preserving the speaker' s identity with only a 3-second short prompt. Experimental results reveal its superiority over existing speech language models with only 33%parameters. 
 * To the best of our knowledge, GPST is the first work that supports spoken multilingual speech generation and Hi-Res speech synthesis."
UniCoder: Scaling Code Large Language Model via Universal Code,2406.16441v1,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2406.16441v1.pdf,"An example of UniCoder. The Code LLM solves the code generation question by ""translating"" the pseudocode description (Universal Code) into executable code of the target programming language.","Intermediate reasoning or acting steps have successfully improved large language models (LLMs) for handling various downstream natural language processing (NLP) tasks. When applying LLMs for code generation, recent works mainly focus on directing the models to articulate intermediate natural-language reasoning steps, as in chain-of-thought (CoT) prompting, and then output code with the natural language or other structured intermediate steps. However, such output is not suitable for code translation or generation tasks since the standard CoT has different logical structures and forms of expression with the code. In this work, we introduce the universal code (UniCode) as the intermediate representation. It is a description of algorithm steps using a mix of conventions of programming languages, such as assignment operator, conditional operator, and loop. Hence, we collect an instruction dataset UniCoder-Instruct to train our model UniCoder on multi-task learning objectives. UniCoder-Instruct comprises natural-language questions, code solutions, and the corresponding universal code. The alignment between the intermediate universal code representation and the final code solution significantly improves the quality of the generated code. The experimental results demonstrate that UniCoder with the universal code significantly outperforms the previous prompting methods by a large margin, showcasing the effectiveness of the structural clues in pseudo-code.","The field of code translation and generation has advanced significantly with the advent of code-specific large language models (LLMs). Code LLMs, such as StarCoder and Code-Llama, are capable of generating executable code by analyzing natural language prompts. Chain-of-thought (CoT) prompting has emerged as the leading technique in enhancing LLMs, where the intermediate steps provide a structured pathway from the problem statement to the solution, effectively mirroring the human problem-solving process. 
Considering the low accuracy of CoT in coder generation, structure CoT (SCoT) is proposed to minimize the gap between the intermediate steps and the generated code. More intuitively, using a universal code as the intermediate representation to handle multiple programming languages (PL) is promising. Here, universal code is a blueprint for implementing an algorithm, which helps to make the design of algorithms logically clear and readily comprehensible. Moreover, it is universal across different programming languages (PL-agnostic) since it typically does not follow specific syntax and omits execution details. Yet, how the universal code is used for code translation and generation in multilingual scenarios remains underexplored. 
In this work, we scale up the code LLMs to support multiple programming languages via the universal code (UniCode), which is used as an efficient and language-independent intermediate representation of the key algorithm principles. Specifically, we first define UniCode by specifying grammar rules and providing paradigms, followed by prompting GPT-4 to create an instruction dataset UniCoder-Instruct comprising natural-language questions, code solutions, and the corresponding universal code, as shown in Figure. Then, the UniCoder model is built by performing instruction tuning on multi-task learning objectives, including zero-shot question-answer generation (question→code), question-universal-code generation (question→UniCode→code), universal-code-solution translation (UniCode→code), and Universal-code-of-Thought (UoT) objectives. In UoT, the model is required to generate the universal code before the executable code. 
UniCoder is evaluated on the Python benchmark (Humaneval and MBPP) and the extended multilingual benchmark MultiPL-E. The results demonstrate that UniCoder consistently achieves state-of-the-art performance across all languages, notably surpassing the previous baselines. Furthermore, the ablation study verifies the efficacy of the proposed method, and extra discussions provide insights into the effect of our method. The contributions are summarized as follows: 
 
 * We introduce the universal code UniCode, which is agnostic to programming languages, allowing LLMs to grasp the essence of algorithms step by step. In addition, the instruction dataset UniCoder-Instruct is collected and provided for follow-up research. 
 * We propose UniCoder, a code generation method that uses multi-task learning objectives to fine-tune the code LLMs with the help of UniCode. The objectives include question-answer generation (QA), question-universal-code generation (QP), universal-code-answer translation (PA), and Universal-code-of-Thought (UoT). 
 * As extensive experiments show, our method UniCoder consistently outperforms the previous baselines on different benchmarks, including HumanEval, MBPP, and MultiPL-E. To further verify the effectiveness of the universal code, we propose UniCoder-Bench to test the capabilities of code LLMs."
Does DetectGPT Fully Utilize Perturbation? Bridging Selective Perturbation to Fine-tuned Contrastive Learning Detector would be Better,2402.00263v4,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2402.00263v4.pdf,"Example of the selective strategy perturbation of Pecola, which prevent modifying important tokens (in green). Orange tokens are the perturbed texts.","The burgeoning generative capabilities of large language models (LLMs) have raised growing concerns about abuse, demanding automatic machine-generated text detectors. DetectGPT, a zero-shot metric-based detector, first introduces perturbation and shows great performance improvement. However, in DetectGPT, the random perturbation strategy could introduce noise, and logit regression depends on the threshold, harming the generalizability and applicability of individual or small-batch inputs. Hence, we propose a novel fine-tuned detector, Pecola, bridging metric-based and fine-tuned methods by contrastive learning on selective perturbation. Selective strategy retains important tokens during perturbation and weights for multi-pair contrastive learning. The experiments show that Pecola outperforms the state-of-the-art (SOTA) by 1.20%in accuracy on average on four public datasets. And we further analyze the effectiveness, robustness, and generalization of the method.","Machine-generated text (MGT) detection is to discriminate MGT from human-written texts (HWT), preventing abuse of large language models (LLMs), including academic misconduct, spam synthesis, untrustworthy news, etc. Currently, existing MGT detection methods can be mainly classified into two categories, i.e., fine-tuned methods and zero-shot metric-based methods. In general terms, fine-tuned detector methods can achieve better accuracy than zero-shot metric-based methods, especially generalizable to black-box generators, but are more costly during data collection, fine-tuning, and running, in most cases. On the other hand, zero-shot metric-based methods show better interpretability than fine-tuned ones. 
DetectGPT, as an unsupervised zero-shot metric-based method, first introduces perturbation in MGT detection. Specifically, it applies random masking to the original input sample and uses T5 to fill in. It posits that minor perturbations of MGT tend to have lower log probability under the base model than the original sample. The introduction of perturbation in DetectGPT surpasses the vanilla log-probability-based method in white-box settings. 
However, DetectGPT still has three significant defects: (i) DetectGPT 's reliance on the logit regression module' s threshold compromises its generalization in zero-shot settings and limited to large batch input, failing on individual inputs. (ii) DetectGPT does not fully utilize the perturbation. As a metrics-based method, it only considers the probability difference caused by perturbation, which is overly simplified and slightly indistinguishable. Perturbation should indeed be a stronger augment that carries implicit language pattern information. (iii) DetectGPT perturbs the original sample randomly and unrestricted, which could introduce more noise and negatively impact the performance. For example, find entity-relationship plays a role in the detection, which might be destroyed in random perturbation of DetectGPT. 
In this paper, we thus propose a Perturbation-based Contrastive Learning model, Pecola, for MGT detection, toward the defects via two stages, i.e., Selective Strategy Perturbation (Sec. ) and Token-Level Weighted Multi-Pairwise Contrastive Learning (Sec. ). Firstly, Selective Strategy Perturbation is a token-level rewriting method with restrictions on modifying important texts to reduce noise. The motivation is to simulate the human behavior of modification. The perturbation strategy consists of token removal and substitution, as shown in Fig. . The experiments show that the Selective Strategy Perturbation method can improve the performance of both metrics-based (i.e., DetectGPT) and model-based methods. Secondly, we propose a Multi-Pairwise Contrastive Learning model to process the perturbed texts. Different from the logit regression module in DetectGPT, the trained model is generalizable without any threshold setting, and it can deal with individual inputs. Moreover, by utilizing multi-pairwise contrastive learning, the model could better utilize perturbation to focus on the language pattern gap between HWT and MGT. The importance weight from the perturbation stage is also reused as contrastive learning weight. Notably, by using contrastive learning, Pecola is a strong few-shot fine-tuning method, which effectively bridges and integrates metric-based and fine-tuned detector categories. Finally, extensive experiments show Pecola is significantly superior to baseline and SOTA methods on four datasets, Pecola improves by 1.20%to SOTA on average under few-shot settings, surpassing the latest methods by 3.84%among metric-based detectors and by 1.62%among fine-tuned detectors. Further experiments show that Pecola is also better at generalization, robustness, and effectiveness. 
Our contributions are summarized as follows: 
 
 * Selective Perturbation: Based on our analysis of various selective perturbation strategies, we propose a novel method considering token importance, which reduces the noise and benefits to both supervised and unsupervised approaches. 
 * Bridge Metric and Model-based Detectors: We utilize a novel fine-tuned contrastive learning module to replace the logit regression of DetectGPT (metric-based), which frees the detector from setting the threshold, enables it to deal with individual input, and can be generalizable and effective on the few-shot setting by contrasting perturbed texts with origin ones. 
 * Outperformance: Our detector Pecola outperforms all eight compared models on four public datasets. And Pecola is more robust to the choice of base model and filling model. Furthermore, we prove its generalization ability across domains and generators of data."
AFaCTA: Assisting the Annotation of Factual Claim Detection with Reliable LLM Annotators,2402.11073v3,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2402.11073v3.pdf,"AFaCTA Pipeline. All steps that need LLM prompting are annotated with the brain icon. Besides the target statement, a short context (if available) is also provided to help the model understand the statement.","With the rise of generative AI, automated fact-checking methods to combat misinformation are becoming more and more important. However, factual claim detection, the first step in a fact-checking pipeline, suffers from two key issues that limit its scalability and generalizability: (1) inconsistency in definitions of the task and what a claim is, and (2) the high cost of manual annotation. To address (1), we review the definitions in related work and propose a unifying definition of factual claims that focuses on verifiability. To address (2), we introduce AFaCTA (Automatic Factual Claim deTection Annotator), a novel framework that assists in the annotation of factual claims with the help of large language models (LLMs). AFaCTA calibrates its annotation confidence with consistency along three predefined reasoning paths. Extensive evaluation and experiments in the domain of political speech reveal that AFaCTA can efficiently assist experts in annotating factual claims and training high-quality classifiers, and can work with or without expert supervision. Our analyses also result in PoliClaim, a comprehensive claim detection dataset spanning diverse political topics.","The explosion of misand disinformation is a growing public concern, with misinformation being widely shared. Manual fact-checking is an important counter-measure to misinformation. However, fact-checking is a time-consuming and expensive endeavor, and computational remedies are required. 
A first step to identify misand disinformation consists of factual claim detection, which filters out the claims with factual assertions that need checking. Considering the sheer amount of daily online content and LLMs 'generative capability, we argue that a valid factual claim detection system should be efficient and easily deployable to monitor misinformation consistently. Therefore, we need a way to produce high-quality resources to build transparent, accurate and fair models to automatically detect such claims. However, there are two major challenges in the data collection process. 
 3.6135ptDiscrepancies in task and claim definitions. By now, arguably, several different claim definitions exist, which confuse practitioners. What is a claim is unclear, leading to various claim detection tasks, e.g., in automated fact-checking and argument mining. For example, dismiss all opinions from factual claims, but includes ""opinions with social impact"" as factual claims. Many studies aim at detecting ""check-worthy"" claims while argues the definition of ""check-worthiness"" is highly subjective and political. Such variances reflect a lack of clarity in conceptualizing critical distinctions, such as the overlap between opinions and verifiable facts (refer to row 1), and the separate nature of verifiability and check-worthiness in the context of factual claim detection (see rows 2 and 3). To address these inconsistencies, we propose a definition of factual claims based on verifiability: factual claims present verifiable facts; a fact is verifiable only if it provides enough specificity to guide evidence retrieval and fact-checking. We focus on verifiability to maximize the definition' s objectivity and clearly delineate facts from opinions. 
 3.6135ptManual annotations are expensive. All existing datasets are manually annotated, which is time-consuming and expensive. Thus, most existing resources are inevitably restricted to certain topics for which it is feasible to annotate claims manually. Such examples include presidential debates, COVID-19 tweets, biomedical and environmental claims. This potentially limits models 'ability to generalize to future topics. However, manually annotating datasets with new topics is too expensive. In light of this, we propose AFaCTA, a multi-step reasoning framework that leverages LLMs to assist in claim annotation, making annotation more scalable and generalizable while rigorously following our factual claim definition. 
In fact-checking, it is essential to have high annotation accuracy. However, LLM annotators are far from perfect. Thus, to ensure the reliability of LLM annotations, AFaCTA calibrates the correctness of the annotations based on the consistency of different paths. Our evaluation shows that AFaCTA outperforms experts by a large margin when all reasoning paths achieve perfect consistency but fails to achieve expert-level performance on inconsistent samples. Nevertheless, we argue that AFaCTA can be an efficient tool in assisting factual claim annotation: perfectly consistent samples can be labeled automatically by the tool, which roughly saves 50%of expert time (see GPT-4-AFaCTA' s perfect consistency rate in). However, inconsistent ones may need expert supervision. 
Using AFaCTA, we annotate PoliClaim, a high-quality claim detection dataset covering U. S. political speeches across 25 years, spanning various political topics. We split the 2022 speeches as the test set and the 1998 to 2021 speeches as the training set to imitate the real-world use case where a model learns from the past and predicts future claims. We evaluate hundreds of classifiers trained on various data combinations, finding that AFaCTA's annotated data with perfect consistency can be a strong substitute for data annotated by human experts. In summary, our contributions include: 
 * We review the regular misconceptions and confounders in claim definition, proposing a claim definition for fact-checking focusing on verifiability. 
 * We propose AFaCTA, an LLM-based framework that assists factual claim annotation and ensures its reliability by calibrating annotation quality with consistency along different reasoning paths. 
 * We annotate PoliClaim, a high-quality factual claim detection dataset covering political speeches of 25 years and various topics."
Towards Faithful and Robust LLM Specialists for Evidence-Based Question-Answering,2402.08277v5,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2402.08277v5.png,Synthetic data generation pipeline and Evaluation for Evidence-Based QA.,"Advances towards more faithful and traceable answers of Large Language Models (LLMs) are crucial for various research and practical endeavors. One avenue in reaching this goal is basing the answers on reliable sources. However, this Evidence-Based QA has proven to work insufficiently with LLMs in terms of citing the correct sources (source quality) and truthfully representing the information within sources (answer attributability). In this work, we systematically investigate how to robustly fine-tune LLMs for better source quality and answer attributability. Specifically, we introduce a data generation pipeline with automated data quality filters, which can synthesize diversified high-quality training and testing data at scale. We further introduce four test sets to benchmark the robustness of fine-tuned specialist models. Extensive evaluation shows that fine-tuning on synthetic data improves performance on both inand out-of-distribution. Furthermore, we show that data quality, which can be drastically improved by proposed quality filters, matters more than quantity in improving Evidence-Based QA.","Large Language Models (LLMs) have become the center of many cutting-edge applications due to their generalisability and information processing abilities. A typical application of LLMs is in Evidence-Based Question Answering (QA), where LLMs are expected to answer questions based on provided sources and cite the sources accurately. By providing these additional sources, multiple shortcomings of standalone LLMs, such as hallucination and limited knowledge capacity, can be addressed, thereby enhancing answer traceability. 
 However, the performance of existing LLMs on Evidence-Based QA is far from perfect. The SOTA close-sourced LLMs and generative search engines have an unignorable rate of hallucinated answers and false citation. Unfortunately, open-sourced LLMs are even less faithful than the already quality-lacking close-sourced LLMs in Evidence-Based QA (; ; also see our evaluation in), although they achieve competitive results on general instruction-following benchmarks. We argue that this may prevent practitioners from building Evidence-Based QA (or other RAG) applications in a robust way. Therefore, efficient data creation and fine-tuning methods are urgently needed to improve LLMs 'Evidence-Based QA performance in target applications. 
To address this research gap, we first formulate quality dimensions for Evidence-Based QA. Specifically, (1) LLMs need to always cite the right evidence at the end of each generated sentence to enable answer traceability, and (2) the answers need to be factually supported by the cited evidence. 
Fine-tuning LLMs using Evidence-Based QA data that follow these quality dimensions seems straightforward. However, we identify two major challenges of fine-tuning LLMs into faithful evidence-based question answerers. 
 3.6135ptC1. Fine-Tuning Data Scalability: Manual annotation for instruction tuning is costly and LLM-synthesized data can be a strong alternative. However, the potentially lower quality of synthesized data may lead to suboptimal fine-tuning performance, given the SOTA LLMs' hallucination rate on Evidence-Based QA. 
 3.6135ptC2. Generalisability after Fine-tuning: Previous work shows that diversified instruction tuning improves LLMs' generalisability. Hence, an intuitive worry is that fine-tuning LLMs (generalists) on Evidence-Based QA data (especially synthetic data) might turn LLMs into specialists that lack generalisability and, thus, struggle with out-of-distribution (OOD) questions and evidence. 
To address C1, we propose a data generation pipeline that synthesizes SynSciQA (Synthetic Scientific Question Answering), a well-diversified synthetic dataset for Evidence-Based QA, following prior work on data distillation for instruction tuning. We further extend the pipeline with two novel quality filters to sift out low-quality synthetic data points, leading to SynSciQA+ and SynSciQA++ (see the left half of). To address C2, we first collect an in-domain test set SynSciQA_test with the data generation pipeline, which shares the data distribution with the training data (i.e., SynSciQA) but covers different topics. We further collect three test sets with different distances to the training data distribution to study the OOD performance (see the right half of). 
Extensive experiments on all proposed train and test settings show that (1) data quality is more important than quantity in Evidence-Based QA fine-tuning; (2) fine-tuning on generated data improves the performance on both inand out-of-distribution test sets; and (3) performance scores on in-domain test set substantially indicate the OOD performance, suggesting that the synthetic data can be used for validation to estimate the OOD performance. All evaluation metrics are based on golden heuristics and best-performed models from previous work, which we further verified with human and GPT-4 evaluation. In summary, our contributions include: 
 * We propose a data generation pipeline to obtain fine-tuning data for Evidence-Based QA in a salable way, which ensures data diversity and quality. 
 * We propose four test sets to benchmark the inand out-of-distribution performance of fine-tuned Evidence-Based QA specialists. 
 * We conduct an extensive evaluation to show that our data-synthesizing strategy leads to effective training and development set for Evidence-Based QA, and quality-filtering significantly improves fine-tuning performance."
Handling Ambiguity in Emotion: From Out-of-Domain Detection to Distribution Estimation,2402.12862v1,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2402.12862v1.pdf,"The bar chart shows the number of labels assigned by annotators to the emotion class ""angry"" (Ang), ""frustrated"" (Fru), and ""neutral"" (Neu) in an example. In utterance (a), eight annotators interpret the emotion as angry while one interprets it as frustrated.","The subjective perception of emotion leads to inconsistent labels from human annotators. Typically, utterances lacking majority-agreed labels are excluded when training an emotion classifier, which cause problems when encountering ambiguous emotional expressions during testing. This paper investigates three methods to handle ambiguous emotion. First, we show that incorporating utterances without majority-agreed labels as an additional class in the classifier reduces the classification performance of the other emotion classes. Then, we propose detecting utterances with ambiguous emotions as out-of-domain samples by quantifying the uncertainty in emotion classification using evidential deep learning. This approach retains the classification accuracy while effectively detects ambiguous emotion expressions. Furthermore, to obtain fine-grained distinctions among ambiguous emotions, we propose representing emotion as a distribution instead of a single class label. The task is thus re-framed from classification to distribution estimation where every individual annotation is taken into account, not just the majority opinion. The evidential uncertainty measure is extended to quantify the uncertainty in emotion distribution estimation. Experimental results on the IEMOCAP and CREMA-D datasets demonstrate the superior capability of the proposed method in terms of majority class prediction, emotion distribution estimation, and uncertainty estimation.","The inherent subjectivity of human emotion perception introduces complexity in annotating emotion datasets. Multiple annotators are often involved in labelling each utterance and the majority-agreed (MA) class is usually used as the ground truth. Utterances that have no majority-agreed (NMA) labels (i.e., with tied votes) are typically excluded during emotion classifier training, which may cause issues when the system encounters such utterances in practical applications. 
This paper investigates three approaches to handling ambiguous emotion data. First, a naive method is tested which aggregates NMA utterances into an additional class when training an emotion classifier. This approach proves problematic as NMA utterances contain a blend of emotions, thereby confusing the classifier and undermining the classification performance. 
Then we explore if an emotion classifier can appropriately respond with ""I don 't know"" for ambiguous emotion data that does not fit into any predefined emotion class. This is realised by quantifying the uncertainty in emotion classification using evidential deep learning (EDL). When a classifier trained on MA data encounters an NMA utterance during the test, the model should identify it as an out-of-domain (OOD) sample by providing a high uncertainty score, indicating its uncertainty regarding the specific emotion class to which the NMA utterance belongs. 
Moreover, to obtain fine-grained distinctions between ambiguous emotional data, we re-frame the task from classification to distribution estimation. Consider the example shown in Figure with the annotations assigned to three utterances. Since the majority emotion classes are ""angry"" for both utterances (a) and (b), they will be assigned the same ground-truth label ""angry"" in the aforementioned classification system, which implies that they convey the same emotion content and is evidently unsuitable. On the contrary, utterance (c), though being an NMA utterance, is more likely to share similar emotional content with utterance (b). Therefore, in order to obtain more comprehensive representations of emotion content, we further propose representing emotion as a distribution rather than a single class label and re-framing emotion recognition as a distribution estimation problem rather than a classification problem. A novel algorithm is proposed which extends EDL to estimate the underlying emotion distribution given observed human annotations and quantify the uncertainty in emotion distribution estimation. The proposed approach considers all human annotations rather than relying solely on the majority vote class. Multiple evaluation metrics are adopted to evaluate the performance in terms of majority class prediction, uncertainty measure, and distribution estimation. Rather than simply saying ""I don' t know"", the proposed system demonstrates the ability to estimate the emotion distributions of the NMA utterances and also offer a reliable uncertainty measure for the distribution estimation. 
Our contributions are summarised as follows. (i) To the best of our knowledge, this paper is the first work that treats ambiguous emotion as OOD and detects it by uncertainty estimation; (ii) This is the first work that applies EDL to quantify uncertainty in emotion classification; (iii) Imposing a single ground truth through majority voting leads to under-representation of minority views. We instead estimate the distribution over emotion classes which provides a more comprehensive representation of emotion content as well as a more inclusive representation of human opinions; (iv) A novel algorithm is proposed that extends EDL to quantify uncertainty in emotion distribution estimation."
MoPS: Modular Story Premise Synthesis for Open-Ended Automatic Story Generation,2406.05690v1,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2406.05690v1.pdf,"Overview of MoPS. We divide the premise into four ordered modules: graytheme, redbackground, greenpersona, and blueplot, with each module further divided into submodules. From the top down, arrows indicate the dependency relationships within and between modules.","A story premise succinctly defines a story's main idea, foundation, and trajectory. It serves as the initial trigger in automatic story generation. Existing sources of story premises are limited by a lack of diversity, uneven quality, and high costs that make them difficult to scale. In response, we introduce Modular Story Premise Synthesis (MoPS) which breaks down story premises into modules like background and persona for automated design and generation. MoPS consists of three phases: (1) Pre-collect a consistent set of candidates for each module to form a nested dictionary. (2) Extract a key path from the nested dictionary as the premise design. (3) Instruct an LLM to integrate the design into a coherent premise sentence. Thorough evaluations demonstrate that our synthesized premises excel in diversity, fascination, completeness, and originality compared to those induced from large language models and captured from public story datasets. Similarly, the extended novels and scripts generated from our premises also exhibit higher quality. In supplementary materials, we provide the MoPS code suite, along with 7.6k generated premises and 1k extended stories. Code: <https: //github. com/GAIR-NLP/MoPS>."," ""If a story is going to fail, 
 it will do so first at the premise level. "" – Anatomy of a Premise Line 
Premise is what your story is about. A story premise is a concise line that captures the story 's main idea, conflict, and characters, outlining its foundation and direction. Writers use the premise to guide story development, offering strategic insight into characters, plot, theme, and resolution. In Automatic Story Generation (ASG), substantial research has explored various systems. These systems need input to trigger and guide story creation. A premise serves as such an input, offering a starting point for complex narrative development. However, crafting a story premise challenges artistic and technical skills, requiring the capture of core elements and appeal in minimalistic text. 
A strong dramatic premise is fundamental to most successful stories. In Tab. , we illustrate the significance of a fascinating story premise in creating engaging narratives. If we can automate the design and creation of diverse and high-quality premises, it would be a major boost to the field of story generation. Most future ASG frameworks could benefit from using these generated premises to thoroughly and comprehensively evaluate the effectiveness of their frameworks. 
Existing work primarily obtains story premises through the following three methods: (1) Dataset Premise Extraction: randomly extracting ready-made story premises from public datasets, such as WritingPrompts (WP). However, it suffers from inconsistent quality, including nonsensical premises, and offers limited customization. (2) LLM Premise Induction: utilize models' extensive knowledge to generate numerous story premises. Its drawback lies in an over-reliance on the model 's knowledge base, potentially curtailing the diversity and innovation of the generated premises. (3) Human-Curated Premise: depend on premises provided or predefined by humans. The significant flaw here is the time-consuming and labor-intensive nature of manually writing premises, especially when generating stories in bulk. Overall, current research area lacks a reliable automated method for generating premises. In this paper, we still adopt the approach of inducing from LLMs with extensive world knowledge via prompts. However, we focus on inducing fine-grained modules. Our novelty lies in creative combinations of modules to generate a large number of diverse, fascinating, complete, and original story premises. Based on this, we introduce Modular Story Premise Synthesis (MoPS). It deconstructs a complete premise into modules, gathers module candidates into a hierarchical structure, outlines a premise design from selected elements, and finally has LLM synthesize these into a cohesive story premise sentence (). Our evaluations () show that premises we' ve created stand out on various quality and diversity criterion (), surpassing those generated by LLMs or sourced from public story datasets. Generated premises, when integrated with state-of-the-art story generation pipelines, not only yield tailored narratives but enhance the overall quality of resulting stories (). 
This paper pioneers the modular synthesis of story premises. Our work aims to contribute to the field of ASG in the following ways: (1) Highlighting the critical role of premises in story generation, and encouraging a deeper focus on the design and creation of story premises. (2) Introducing MoPS, a method for automated design and creation of premises, along with two metrics for premise diversity and three for quality, conducting a thorough evaluation of our premises. (3) Grafting two story generation pipelines for our premises and offering three version datasets: curated (100 premise-story pairs), moderate (1k premise-story pairs), and complete (7.6k premises)."
ToolSword: Unveiling Safety Issues of Large Language Models in Tool Learning Across Three Stages,2402.10753v2,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2402.10753v2.pdf,"Responses of LLMs to unsafe queries between standard dialogue and tool learning Contexts. Tool learning may disrupt the safe alignment mechanism of LLMs, leading to responses to unsafe queries through tool invocation.","Tool learning is widely acknowledged as a foundational approach for deploying large language models (LLMs) in real-world scenarios. While current research primarily emphasizes leveraging tools to augment LLMs, it frequently neglects emerging safety considerations tied to their application. To fill this gap, we present ToolSword, a comprehensive framework dedicated to meticulously investigating safety issues linked to LLMs in tool learning. Specifically, ToolSword delineates six safety scenarios for LLMs in tool learning, encompassing malicious queries and jailbreak attacks in the input stage, noisy misdirection and risky cues in the execution stage, and harmful feedback and error conflicts in the output stage. Experiments conducted on 11 open-source and closed-source LLMs reveal enduring safety challenges in tool learning, such as handling harmful queries, employing risky tools, and delivering detrimental feedback, which even GPT-4 is susceptible to. Moreover, we conduct further studies with the aim of fostering research on tool learning safety. The data is released in <https: //github. com/Junjie-Ye/ToolSword>.","Recently, tool learning has garnered significant attention as a potent approach for seamlessly integrating large language models (LLMs) into real-world applications. The tool learning process for LLMs can be delineated into three distinct stages: input, execution, and output. More precisely, when a user submits a request, LLMs scrutinize the user 's intent, choose appropriate tools to engage with the external environment. Upon receiving feedback from the environment, LLMs structure the pertinent information to provide a response to the user' s initial query. 
Existing research primarily concentrates on enhancing LLMs capabilities through tool utilization. One proposed approach involves fine-tuning the base model by generating numerous tool usage trajectories for a specific set of tools. This approach aids LLMs in swiftly grasping the functionality of various tools and mastering their utilization for problem-solving. Another strategy aims to bolster the model's generalization skills by devising prompts that instruct LLMs to read tool descriptions and employ external tools as necessary. 
However, these studies overlook the fact that tool learning also introduces new safety concerns. As illustrated in Figure, in standard dialogues, LLMs can recognize and refuse to provide assistance when users enter unsafe queries. In contrast, in the context of tool learning, the safety alignment mechanism may be compromised. Consequently, LLMs may provide corresponding answers to unsafe queries by utilizing relevant tools. Furthermore, the selection of tools by LLMs may be influenced by malicious noise. Therefore, there is an urgent need for a comprehensive analysis of the current safety challenges faced by LLMs in the realm of tool learning to facilitate research aimed at their development. 
To fill this gap, we introduce ToolSword, a comprehensive framework crafted for unveiling the safety issues of LLMs throughout the tool learning process. ToolSword encompasses six safety scenarios that LLMs encounter in tool learning, encompassing malicious queries and jailbreak attacks in the input stage, noisy misdirection and risky cues in the execution stage, as well as harmful feedback and error conflicts in the output stage. Through an analysis of LLMs performance within these safety scenarios, we can gain insight into how they manage various safety challenges in tool learning at a granular level. 
Leveraging ToolSword, we analyze 11 open-source and closed-source LLMs equipped with robust tool learning capabilities. Our findings reveal that current LLMs frequently encounter safety issues across all stages of tool learning, leading to significant safety risks such as responding to harmful queries, invoking risky tools, and providing detrimental feedback, despite these issues being easily discernible by humans. Even the most advanced LLMs, such as GPT-4, are not immune to these challenges. Moreover, our futher studies indicate that LLMs can demonstrate performance comparable to humans in tool learning environments devoid of safety concerns. Hence, enhancing safety measures is essential to drive the practical application of LLMs. We hope that our findings will contribute to advancing research in the domain of tool learning safety. 
The main contributions of our work are summarized as follows: 
 
 * We introduce ToolSword, a comprehensive framework designed to unveil the complete spectrum of safety issues associated with LLMs in tool learning. ToolSword conducts a thorough examination of LLMs across three distinct stages, thereby encompassing the entirety of the tool learning process. 
 * We develop two distinct types of safety scenarios for each stage, specifically tailored to address the real-world safety challenges encountered by LLMs. These scenarios enable us to meticulously evaluate the safety performance of LLMs when confronted with various challenges at a granular level. 
 * We conduct experiments involving 11 open-source and closed-source LLMs, and identify notable safety issues across each stage of tool learning. These findings emphasize the urgent requirement for enhancing the safety of LLMs in tool learning."
Benchmarking Knowledge Boundary for Large Language Models: A Different Perspective on Model Evaluation,2402.11493v2,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2402.11493v2.pdf,"Illustration of three classes of knowledge based on the model's mastery of knowledge in different textual forms. Existing evaluation methods suffer from sensitivity to input prompt. Therefore, the knowledge ability depicted by these methods is irregularly shaped. We propose to evaluate the knowledge capacity with a knowledge boundary containing both Prompt-Agnostic Knowledge and Prompt-Sensitive Knowledge.","In recent years, substantial advancements have been made in the development of large language models, achieving remarkable performance across diverse tasks. To evaluate the knowledge ability of language models, previous studies have proposed lots of benchmarks based on question-answering pairs. We argue that it is not reliable and comprehensive to evaluate language models with a fixed question or limited paraphrases as the query, since language models are sensitive to prompt. Therefore, we introduce a novel concept named knowledge boundary to encompass both prompt-agnostic and prompt-sensitive knowledge within language models. Knowledge boundary avoids prompt sensitivity in language model evaluations, rendering them more dependable and robust. To explore the knowledge boundary for a given model, we propose a projected gradient descent method with semantic constraints, a new algorithm designed to identify the optimal prompt for each piece of knowledge. Experiments demonstrate a superior performance of our algorithm in computing the knowledge boundary compared to existing methods. Furthermore, we evaluate the ability of multiple language models in several domains with knowledge boundary.","Recently, large language models (LLMs) have made significant advancements in a variety of tasks. In order to gain deeper insights into the knowledge capabilities of different LLMs to help select appropriate LLM in practice, numerous studies have proposed various benchmarks for LLM evaluation. The majority of previous research on model evaluation constructs a test dataset sourced from standardized examinations, such as college entrance exams and law school admission tests. Subsequently, the questions are fed to LLMs as prompts, eliciting responses that are then scored for evaluation. 
However, each piece of knowledge embodies abstract concept that can be expressed in a nearly infinite number of textual forms. When evaluating a specific piece of knowledge, existing work only evaluated LLMs with one or several textual forms randomly sampled from the semantic space of the knowledge. However, existing LLMs are notorious for being sensitive to prompt, thereby undermining the reliability of such evaluations. Consequently, current studies on model evaluation are reasonably considered to be insufficiently robust. 
As shown in Figure, from the perspective of the model 's mastery of the textual form of knowledge, knowledge can be divided into three classes: 1) Prompt-Agnostic Knowledge that can be correctly answered for any textual form; 2) Prompt-Sensitive Knowledge that is sensitive to the form of the prompt fed into the model; 3) Unanswerable Knowledge that is unable to be answered by the model, regardless of the prompt employed. The majority of previous research on model evaluation ignored the presence of Prompt-Sensitive Knowledge, resorting to oversimplified binary evaluations, classifying the model' s knowledge mastery merely as true or false. attempts to assess LLM through diverse paraphrases, yet these evaluations remain confined to limited textual forms of knowledge. We give strict definitions of three types of knowledge in Section. 
In this paper, we aim to reduce the contingency when evaluating LLMs. Different from previous paradigms of LLM evaluation, we attempt to explore the Unanswerable Knowledge of the model to be evaluated, thereby illuminating the knowledge boundaries of LLMs. How can we find Unanswerable knowledge for the model? It is obvious that trying all prompts for the knowledge to query the model is too resource-intensive. Therefore, we choose to make efforts to search for the optimal prompt. We formalize optimal prompt searching as a discrete optimization problem: given some question paraphrases, we search for a prompt to maximize the probability of generating the correct answer. We propose the Projected Gradient Descent method with Constraints (PGDC), a new algorithm that updates prompt with gradient descent and implements proximal projection to search discrete prompts. To ensure that the optimized prompt has the same semantics as the original prompt, we introduce semantic loss, which is a measure of the distance between the semantic representations of the optimized prompt and the original prompt. 
Experimental results demonstrate that our proposed PGDC can outperform baselines in depicting knowledge boundaries. In addition, results on counterfactual datasets demonstrate that our approach is reasonable and robust. Human evaluation also reveals that our optimized prompts generally have the same semantics as the original questions. Moreover, we delineate models 'knowledge boundaries in different domains using PGDC to evaluate LLMs. The size of the model' s domain knowledge boundaries is strongly associated with the performance of downstream tasks in the domain. The optimal prompts also have some patterns that can give some inspiration for designing prompts when using corresponding LLMs. 
In summary, our contributions are: (1) We propose a new evaluation paradigm for benchmarking knowledge boundaries to compare models' capabilities, which can reduce the randomness in current evaluations. (2) We design PGDC, a projected gradient descent method with constraints, to optimize prompts and obtain knowledge boundaries of LLMs which achieves the best results on four datasets. (3) We evaluate five models using knowledge boundaries and obtain some valuable findings. 
Our code and data are released to facilitate future research."
Exploring the Potential of Large Language Models in Computational Argumentation,2311.09022v3,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2311.09022v3.pdf,Explored tasks and datasets in this work.,"Computational argumentation has become an essential tool in various domains, including law, public policy, and artificial intelligence. It is an emerging research field in natural language processing that attracts increasing attention. Research on computational argumentation mainly involves two types of tasks: argument mining and argument generation. As large language models (LLMs) have demonstrated impressive capabilities in understanding context and generating natural language, it is worthwhile to evaluate the performance of LLMs on diverse computational argumentation tasks. This work aims to embark on an assessment of LLMs, such as ChatGPT, Flan models, and LLaMA2 models, in both zero-shot and few-shot settings. We organize existing tasks into six main categories and standardize the format of fourteen openly available datasets. In addition, we present a new benchmark dataset on counter speech generation that aims to holistically evaluate the end-to-end performance of LLMs on argument mining and argument generation. Extensive experiments show that LLMs exhibit commendable performance across most of the datasets, demonstrating their capabilities in the field of argumentation. Our analysis offers valuable suggestions for evaluating computational argumentation and its integration with LLMs in future research endeavors.","Argumentation is a powerful and indispensable tool in various domains such as legality, debating, and education. It plays a vital role in facilitating understanding between individuals by providing insights into different perspectives and their underlying reasons. Additionally, argumentation serves as a means of communicating convincing opinions, enhancing the acceptability of positions among readers. As computational argumentation becomes a growing research field in natural language processing (NLP), researchers have dedicated considerable efforts to two distinct directions. The first direction, argument mining, focuses on understanding unstructured texts and automatically extracting various argumentative elements. The other direction is argument generation, which aims to generate argumentative texts based on external knowledge or summarize key argument points. . 
Unlike classical structure prediction NLP tasks like named entity recognition that typically take a single sentence as the input and extract token-level information, computational argumentation tasks require discourse-level comprehension. This requirement makes it challenging and laborious to gather a large volume of labeled data for training, hindering the progress of research in this field. Fortunately, recent studies have shown that large language models (LLMs) have demonstrated impressive performance on a wide variety of NLP tasks in both zero-shot and few-shot settings. Given their strong capability in understanding long contexts and generating natural language, it is exciting yet still questionable how well LLMs can perform computational argumentation tasks without any supervised training. 
In light of this, our objective is to investigate the performance of LLMs on diverse computational argumentation tasks. There are two main issues we aim to address in our study. Firstly, although there are existing surveys about argument mining, the systematic study of the broader definition of computational argumentation including argument mining and argument generation is under-explored. To bridge this gap, we categorize current computational argumentation tasks into two primary classes, comprising six distinct categories. In addition, we establish a standardized format and evaluation metrics for fourteen openly available datasets. Secondly, existing tasks and datasets either focus on argument mining or argument generation. To take a holistic approach, we propose a new task that integrates both argument mining and generation. This task is designed to generate counter speeches in response to debate speeches, which typically advocate a particular stance. We name them counter speech and supporting speech respectively in the remainder of our paper. This task requires the model to understand the argumentative structures in the supporting speech, meanwhile to generate the counter speech against the proposition. To facilitate the study, we construct a new document-to-document counterargument generation benchmark based on a debate database. 
To evaluate the performance of LLMs on computational argumentation tasks, we choose from both open-source and proprietary LLMs to conduct our main experiments, in zero-shot and few-shot settings. Our results reveal that LLMs exhibit promising performance in both argument mining and argument generation tasks. While LLMs might fail to achieve exceptionally high scores on specific metrics such as Rouge, we hypothesize that the strict nature of these metrics could potentially underestimate the true potential of LLMs, which are inherently generative in nature. Human evaluation shows that LLMs are able to comprehend the core meaning of arguments and convey them effectively, even if the exact wording might not match. Collectively, these findings highlight the strengths of LLMs in grasping and effectively conveying the essence of arguments, showcasing their potential beyond what traditional metrics may suggest. 
To summarize, our contributions include: 
∙ 
 * We organize the existing computational argumentation tasks including argument mining and argument generation, and standardize the format of related datasets. 
 * We introduce a new task targeted at evaluating both argument mining and argument generation capabilities as a whole. 
 * To the best of our knowledge, we for the first time systematically evaluate the performance of multiple computational argumentation tasks using LLMs in zero-shot and few-shot settings. 
 * Extensive experimental results and analysis demonstrate the potential of LLMs in the computational argumentation research field and also suggest limitations in existing evaluation."
TaxoLLaMA: WordNet-based Model for Solving Multiple Lexical Semantic Tasks,2403.09207v2,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2403.09207v2.pdf,Training procedure of TaxoLLaMA: hypernym relations from the WordNet are linearized and fed into an LLM model. The model aims at generating the correct hypernym (s) as output.,"In this paper, we explore the capabilities of LLMs in capturing lexical-semantic knowledge from WordNet on the example of the model and test it on multiple lexical semantic tasks. As the outcome of our experiments, we present TaxoLLaMA, the ""all-in-one"" model for taxonomy-related tasks, lightweight due to 4-bit quantization and LoRA. TaxoLLaMA achieves 11 SOTA results, and 4 top-2 results out of 16 tasks on the Taxonomy Enrichment, Hypernym Discovery, Taxonomy Construction, and Lexical Entailment tasks. Moreover, it demonstrates a very strong zero-shot performance on Lexical Entailment and Taxonomy Construction with no fine-tuning. We also explore its hidden multilingual and domain adaptation capabilities with a little tuning or few-shot learning. All datasets, code, and pre-trained models are available online.","Recent studies in Natural Language Processing widely utilize Large Language Models (LLMs) for their capability to store extensive knowledge and to adapt quickly to different tasks via in-context learning without backpropagation. However, the application of LLMs to the classical lexical semantic tasks still remains understudied: for instance, no recent experiments with LLMs have been performed for the Hypernym Discovery task for different domains and languages. In Taxonomy Enrichment, LLMs are mostly used to extract vector representations which are further processed with a complex pipeline. 
Our work aims to investigate the capabilities of LLMs in addressing four tasks requiring taxonomic knowledge: Hypernym Discovery, Taxonomy Enrichment, Lexical Entailment, and Taxonomy Construction. We hypothesize that the model finetuned with hypernym (IS-A relationships) would be useful for solving taxonomy-related tasks. To verify this hypothesis, we develop a method inspired by to compile a taxonomy-focused instruction tuning dataset, sourced from English WordNet, to bring the implicit word knowledge of an LLM to the forefront when addressing lexical semantic tasks. 
Having trained our model in this specialized setting, we are releasing the TaxoLLaMA — the finetuned version of the LLaMA-2-7b model — that is capable of solving tasks requiring taxonomic knowledge. Figure presents the main idea of the model finetuning process. TaxoLLaMA operates effectively in a zero-shot setting, surpassing SOTA results in Lexical Entailment and Taxonomy Construction. With additional tuning, it also achieves SOTA performance in the Hypernym Discovery task across several languages and in half of the Taxonomy Enrichment tasks. Furthermore, we have optimized TaxoLLaMA to be lightweight through 4-bit quantization and the application of LoRA, making it feasible to run on GPU devices with only 4.8Gb of GPU for forward pass and 5.5Gb for fine-tuning, ensuring its accessibility for widespread use, e.g.,using Colab. 
The contributions of the paper are as follows: 
 
 * We introduce the use of LLMs across various lexical semantic tasks via hypernym prediction and propose an appropriate taxonomy instruction tuning method that exploits for dataset sampling. 
 * We present TaxoLLaMA – a unified model designed to address a spectrum of lexical-sematic tasks achieving state-of-the-art (SOTA) results in 11 out of 16 tasks and securing the second rank in 4 tasks. 
 * We present an instructive dataset based on English WordNet-3.0 only for training a taxonomy-based LLM and collected definitions for input words in the Taxonomy Enrichment datasets and the Lexical Entailment datasets using Wikidata and ChatGPT. 
 * We perform a detailed error analysis for all tasks using both manual and automatic approaches: e.g.,we evaluate error patterns and model quality using ChatGPT."
TransliCo: A Contrastive Learning Framework to Address the Script Barrier in Multilingual Pretrained Language Models,2401.06620v2,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2401.06620v2.pdf,An illustration of applying TransliCoto a single batch of data during fine-tuning. The training data is used by the two training objectives in TransliCo: Masked Language Modeling (MLM) and Transliteration Contrastive Modeling (TCM). MLM is applied to both the original sentences and their Latin transliterations. TCM is used to learn better-aligned cross-script representations by contrasting the positive pairs (paired data connected with red lines) against the negative pairs (the remaining samples connected with blue lines).,"The world's more than 7000 languages are written in at least 293 scripts. Due to various reasons, many closely related languages use different scripts, which poses a difficulty for multilingual pretrained language models (mPLMs) in learning crosslingual knowledge through lexical overlap. As a consequence, mPLMs are faced with a script barrier: representations from different scripts are located in different subspaces, which can result in crosslingual transfer involving languages of different scripts performing suboptimally. To address this problem, we propose TransliCo, a framework that optimizes the Transliteration Contrastive Modeling (TCM) objective to fine-tune an mPLM by contrasting sentences in its training data and their transliterations in a unified script (in our case Latin), which enhances uniformity in the representation space for different scripts. Using Glot500-m, an mPLM pretrained on over 500 languages, as our source model, we fine-tune it on a small portion (5%) of its training data, and refer to the resulting model as Furina. We show that Furina not only better aligns representations from distinct scripts but also outperforms the original Glot500-m on various zero-shot crosslingual transfer tasks. Additionally, we achieve consistent improvement in a case study on the Indic group where the languages exhibit areal features but use different scripts. We make our code and models publicly available.","In recent years, mPLMs have made impressive progress in various crosslingual transfer tasks. Such achievement is mainly due to the availability of monolingual corpora of many languages, the amelioration of model architectures suitable for scaling up, as well as the advancement of self-supervised learning objectives. Despite the fact that mPLMs present attractive performance in high-resource languages, those models often gain unsatisfactory results for low-resource languages, especially when the writing systems or scripts are different from the transfer source languages. 
This undesired behavior is related to the script barrier in the representation space, where different scripts are located in different subspaces. To tackle this problem, transliteration or romanization is leveraged in some recent work: all languages from different scripts are converted into one common script and the language model is pretrained or adapted with transliterated data. For testing and inference, the queries also need to be transliterated, as the model only supports one script, the pretraining or adaptation script of the model. 
However, this line of approaches presents two limitations. First, it does not break the script barrier, rather, it circumvents it. The representations from different scripts are still not aligned. Second, for some tasks, e.g., question answering, it is necessary to transliterate the response back to the original script because we cannot assume that end users know the common script. Unfortunately, transliteration and transliterating back to the original script is not immune to information loss. The romanized words in many languages, e.g., Chinese, Japanese, and Korean, can be converted to different words in their original scripts, which unfortunately leads to ambiguity. 
In this paper, we present TransliCo, a contrastive learning framework to address the script barrier in the representation space of mPLMs in a way that overcomes the limitations of prior work. To start with, a small portion of the data from the pretraining corpus of an mPLM is used to generate Latin transliteration, using Uroman. Then we create paired data using sentences in their original script and their transliterations. The data is subsequently used by the two objectives: Masked Language Modeling (MLM) and Transliteration Contrastive Modeling (TCM). MLM is applied to both the original sentences and their transliterations; we use TCM to learn better-aligned representations by contrasting the positive pairs (paired data) against negative pairs (the remaining in-batch samples) as shown in Figure. 
Using Glot500-m as our source model, we evaluate TransliCoboth ""globally"" and ""locally"". Specifically, we fine-tune Glot500-m on 5%of its pretraining data of all languages and refer to the resulting model as Furina. We show that Furinaaligns representations from different scripts better and it generally outperforms the baselines on sentence retrieval, sequence labeling, and text classification tasks for different script groups. Our ablation study indicates MLM and TCM in TransliCoare both important for achieving good crosslingual performance. We additionally conduct a case study on Indic languages, a group of languages that show areal features and use different scripts. Furina_Indicfine-tuned by TransliCousing the data from Indic languages shows consistent improvement over the baseline. 
The main contributions of this work are summarized as follows: (i) We present TransliCo, a simple but effective framework, to address the script barrier in the representation space of mPLMs. (ii) We conduct extensive and controlled experiments on a variety of crosslingual tasks and show TransliCoboosts performance. (iii) We show the framework encourages the representations from different scripts to be better aligned. (iv) In a case study on Indic languages, we demonstrate that TransliCoalso works for areal languages that have shared vocabulary but use distinct scripts."
Time is Encoded in the Weights of Finetuned Language Models,2312.13401v2,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2312.13401v2.png,"We present time vectors, a simple tool to customize language models to new time periods. Time vectors (τ _i) specify a direction in weight space that improves performance on text from a time period i. They are computed by subtracting the pretrained weights (θ _pre; left panel) from those finetuned to a target time period (θ _i). We can customize model behavior to new time periods (e.g., intervening months or years) by interpolating between time vectors and adding the result to the pretrained model (middle panel). We can also generalize to a future time period j with analogy arithmetic (right panel). This involves combining a task-specific time vector with analogous time vectors derived from finetuned language models (τ ^LM_j).","We present time vectors, a simple tool to customize language models to new time periods. Time vectors are created by finetuning a language model on data from a single time (e.g., a year or month), and then subtracting the weights of the original pretrained model. This vector specifies a direction in weight space that, as our experiments show, improves performance on text from that time period. Time vectors specialized to adjacent time periods appear to be positioned closer together in a manifold. Using this structure, we interpolate between time vectors to induce new models that perform better on intervening and future time periods, without any additional training. We demonstrate the consistency of our findings across different tasks, domains, model sizes, and time scales. Our results suggest that time is encoded in the weight space of finetuned models.","Temporal variation is a fundamental characteristic of language. As we show in, it manifests in language model development as temporal misalignment, where deviations in train and test data lead to large performance degradation across different time periods. This necessitates adaptation techniques for customizing models to specific time periods as needed. Designing such techniques is difficult, however, due to the multitude of time scales and the possibility that data from a target time period might be unavailable. 
Recent work has shown that the behavior of neural networks can be edited through closed-form interpolation between parameters of finetuned models. In this work, we demonstrate that weight-space interpolation can also be used to cheaply edit language model behavior over time. To this end, we introduce time vectors (), an extension of task vectors. We finetune a pretrained language model on text from a single time period, and then subtract the pretrained weights. This vector represents a direction of movement in weight space that improves performance on text from the target time period. 
We analyze the structure of time vectors with temporally organized datasets for language modeling, classification, and summarization (). Our results consistently suggest that time vectors are intuitively organized on a manifold; years or months that are closer together in time yield time vectors that are also closer together in weight space. Similarly, we show that temporal degradation in yearly and monthly settings is strongly correlated with the angles between time vectors (). 
We use this structure of time vectors to induce models that generalize better to data from new time periods. By interpolating between two time vectors, we discover vectors that, when applied to the pretrained model, improve performance on intervening months or years (). The structure can also be used to generalize task-specific models across time periods with analogous time vectors specialized to unlabeled data (). 
Our results show that temporal variation is to some extent encoded in the weight space of finetuned models, and that weight interpolation can help customize language models to new time periods. We publicly release our code, data, and over 500 models finetuned on specific time periods."
ItD: Large Language Models Can Teach Themselves Induction through Deduction,2403.05789v1,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2403.05789v1.pdf,"Task of Induction. The tested model observes a batch of input-output (x, y) pairs and needs to predict the latent transformation f shared by these (x, y) pairs.","Although Large Language Models (LLMs) are showing impressive performance on a wide range of Natural Language Processing tasks, researchers have found that they still have limited ability to conduct induction. Recent works mainly adopt ""post processes"" paradigms to improve the performance of LLMs on induction (e.g., the hypothesis search & refinement methods), but their performance is still constrained by the inherent inductive capability of the LLMs. In this paper, we propose a novel framework, Induction through Deduction (ItD), to enable the LLMs to teach themselves induction through deduction. The ItD framework is composed of two main components: a Deductive Data Generation module to generate induction data and a Naive Bayesian Induction module to optimize the fine-tuning and decoding of LLMs. Our empirical results showcase the effectiveness of ItD on two induction benchmarks, achieving relative performance improvement of 36%and 10%compared with previous state-of-the-art, respectively. Our ablation study verifies the effectiveness of two key modules of ItD. We also verify the effectiveness of ItD across different LLMs and deductors. The data and code of this paper can be found at https: //anonymous.4open. science/r/ItD-E844.","Induction can take we humans from the observed to the unobserved. The task of Induction aims to discover consistent transformations from a set of input-output pairs, where the transformations map the inputs to the outputs well. As shown in Figure, given the input-output pairs {x_i, y_i} _i=1^n, the model needs to predict the latent transformation f. For a detailed example, given the input [1,2] with the output [1] and other input-output pairs, the tested model is supposed to figure out the transformation output the first element of the input list. The Induction task is an important task in Natural Language Processing (NLP) and the mastery of the induction ability is an important sign of intelligence. 
Currently, humans have already mastered the capability of induction and have found thousands of laws from the physical world and human society. However, machine intelligence still struggles to induce basic logic rules in structure data like knowledge graphs. Recently, with the rapid development of Large Language Models (LLMs), many works have begun to adopt the LLMs to induce the transformations given the input-output observations of various tasks and express the induced transformations as rules, guidelines, instructions, and codes. These methods take advantage of the interpretability and generalization ability of LLMs in solving the Induction task. 
However, recent research have revealed that LLMs have inherently limited ability in induction. To tackle such a limitation, work like Hypothesis Search proposes to select the generated hypotheses from LLMs by evaluating them on the observations, while another following work Iterative Hypothesis Refinement proposes to further refine them through LLMs based on the evaluating results on the observations. Nevertheless, as shown in Figure (a), these hypothesis search & refinement methods are essentially ""post processes"" to the directly induced hypotheses of LLMs. They still heavily rely on the inherent induction ability of LLMs which are Weak Inductors. 
Even though LLMs are limited in induction, recent work finds out that they possess much better capability in deduction. Different from induction, deduction aims to infer the correct output given the transformation and the input. Despite the distinction that induction associates multiple (x, y) pairs with the latent transformation f, whereas deduction links x and f to the resultant y, both approaches fundamentally share the commonality of reasoning within the framework of input, output, and transformation (x, y, f). Therefore, it motivates us to propose a novel framework ItD (Induction through Deduction), to enable the LLMs to teach themselves induction through deduction. Different from previous methods, ItD fine-tunes the LLMs on their deduced data to make them Strong Inductors, as shown in Figure (b). For a given induction task, ItD first proposes Deductive Data Generation to leverage the deductive capability of the LLMs to generate a set of task data (x, y, f), which is simple yet effective and does not rely on human annotations or any larger LLMs' assistance. The data will then be used to fine-tune the LLMs to obtain better inductive capability. 
However, it is non-trivial to utilize the deduced data. We find out that directly fine-tuning the LLMs using the IO prompt used in the previous methods cannot effectively leverage the observed samples (as shown in Figure). Thus, ItD further proposes Naive Bayesian Induction as a strategy to optimize the use of each sample. Moreover, we also observe performance gains with the increase in the number of samples using our approach. Specifically, this novel technique fine-tunes the LLM to predict f conditioned on single pair x, y (p (f|x, y) ) instead of n pairs (p (f| {x_i, y_i} _i=1^n) ). During the decoding phase, it utilizes the Naive Bayesian approach to equivalently infer the probability distribution of f under all n (x, y) conditions (p (f| {x_i, y_i} _i=1^n) ) with the probability distribution of f under a single (x, y) condition (p (f|x, y) ). 
We conduct experiments on two different types of induction tasks for evaluation: Instruction Induction and List Function. Compared with previous methods, The experiment results show that ItD is superior to the existing methods in assisting LLMs in induction, and both the Deductive Data Generation and the Naive Bayesian Induction components effectively contribute to ItD. We also make discussions to show that ItD can be effectively applied to different LLMs, and a more powerful deductor, e.g.,ChatGPT, will further improve the performances of ItD. In summary, the major contributions of this paper are as follows: 
 
 * We propose a novel framework ItD to enable the LLMs to teach themselves induction through deduction. 
 * We propose Deductive Data Generation to effectively leverage the deductive capability of LLMs to generate task data. which is fully self-supervised and needs no human annotations or any larger LLMs to assist. 
 * We propose Naive Bayesian Induction to allow LLMs to optimize the use of each observed sample and be able to take advantage of the increase in the number of observed samples."
Enhancing In-Context Learning via Implicit Demonstration Augmentation,2407.00100v1,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2407.00100v1.pdf,Illustration for demonstration augmentation using semantic directions (vectors) sampled from the deep feature distribution of demonstration examples.,"The emergence of in-context learning (ICL) enables large pre-trained language models (PLMs) to make predictions for unseen inputs without updating parameters. Despite its potential, ICL's effectiveness heavily relies on the quality, quantity, and permutation of demonstrations, commonly leading to suboptimal and unstable performance. In this paper, we tackle this challenge for the first time from the perspective of demonstration augmentation. Specifically, we start with enriching representations of demonstrations by leveraging their deep feature distribution. We then theoretically reveal that when the number of augmented copies approaches infinity, the augmentation is approximately equal to a novel logit calibration mechanism integrated with specific statistical properties. This insight results in a simple yet highly efficient method that significantly improves the average and worst-case accuracy across diverse PLMs and tasks. Moreover, our method effectively reduces performance variance among varying demonstrations, permutations, and templates, and displays the capability to address imbalanced class distributions.","Large pre-trained language models (PLMs) have showcased exceptional abilities in in-context learning (ICL), which assists the model in discerning the underlying patterns within demonstrations and make more accurate predictions. As a new paradigm, ICL offers compelling advantages, allowing for natural language interaction with PLMs, as well as reduced computational costs. 
While promising, ICL's performance is highly dependent on provided demonstrations and templates, resulting in subpar and unstable performance. This promotes research aimed at improving the quality, quantity, and permutations of demonstrations. Other research avenues include prediction adjustment and learning process design (e.g., channel models and meta-training frameworks). Despite ongoing efforts, ICL still struggles with efficiently and reliably capturing sufficient knowledge from context, leaving performance stability as a persistent bottleneck. 
In this study, we propose enriching contextual knowledge for PLMs by augmenting demonstrations. We first attempt to enhance the representation of demonstrations by transforming them along semantic directions sampled from the deep feature space of demonstration examples, as depicted in Figure. This operation stems from the observation that the deep features in a network are usually linearized, implying the existence of numerous semantic directions within the deep feature space, hence potentially enabling us to incorporate richer contextual knowledge without extending input length. From this novel perspective, we theoretically prove that when the number of augmented pieces approaches infinity, its effect approximately equals a logit adjustment operation. Specifically, we derive a refined Softmax function that integrates the statistical properties of demonstrations. Consequently, rather than explicitly executing the augmentation procedure, we can efficiently conduct implicit demonstration augmentation using the derived prediction function, obtaining an improved ICL method with theoretical guidance. 
We conduct extensive experiments across seven PLMs and various classification tasks. The empirical results demonstrate that our approach remarkably enhances prediction accuracy and reduces performance variability across different demonstrations, permutations, and templates. Notably, our method is straightforward, effective, and generalizable, enabling seamless integration with other ICL methods to enhance their performance. 
Our contributions can be summarized as follows: 
 
 * We introduce Implicit Demonstration Augmentation-based ICL (IDAICL), a pioneering work that incorporates demonstration augmentation into ICL. Instead of solely enhancing demonstration quality, quantity, or order, our method explores context augmentation within the deep feature space, offering a new perspective to enrich demonstrations bypassing input length limitations. 
 * We theoretically establish that as the number of augmented pieces approaches infinity, our augmentation strategy approximates a logit-adjusted prediction function that integrates statistical properties derived from the input data distribution. Equipped with this function, IDAICL provides a straightforward yet theory-guided solution to enhance ICL. 
 * Extensive experiments conducted across diverse tasks and PLMs conclusively illustrate that IDAICL considerably improves average and worst-case accuracy compared to existing ICL methods. Moreover, it effectively enhances performance stability."
Hypergraph based Understanding for Document Semantic Entity Recognition,2407.06904v1,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2407.06904v1.pdf,Difference in Document Task.,"Semantic entity recognition is an important task in the field of visually-rich document understanding. It distinguishes the semantic types of text by analyzing the position relationship between text nodes and the relation between text content. The existing document understanding models mainly focus on entity categories while ignoring the extraction of entity boundaries. We build a novel hypergraph attention document semantic entity recognition framework, HGA, which uses hypergraph attention to focus on entity boundaries and entity categories at the same time. It can conduct a more detailed analysis of the document text representation analyzed by the upstream model and achieves a better performance of semantic information. We apply this method on the basis of GraphLayoutLM to construct a new semantic entity recognition model HGALayoutLM. Our experiment results on FUNSD, CORD, XFUND and SROIE show that our method can effectively improve the performance of semantic entity recognition tasks based on the original model. The results of HGALayoutLM on FUNSD and XFUND reach the new state-of-the-art results.","With the development of information technology, documents have become a main information carrier nowadays, which contains kinds of information type, such as text, table and image. Manual recognition of these documents often requires plenty of manpower. OCR tools can only help us to identify the text, layout and other simple information in the document. To further understand documents, Visually-rich Document Understanding (VRDU) is proposed to make use of visual, textual and other information for more in-depth analysis. 
Semantic Entity Recognition (SER) is an important task in the field of VRDU. Its purpose is to extract and classify the text with special semantic information in documents. Different from text sequences in traditional natural language processing tasks, the information in documents is not one-dimensional, single-modal and continuous, but two-dimensional, multimodal and discrete. It is necessary to analyze not only text information, but also other modal information such as layout and vision in the document. Figure shows the difference between the traditional named entity recognition (NER) task on a single modal text and the semantic entity recognition task on a document. Firstly, the text form of a single modal text task is a fixed text sequence, while the discrete text in a document is composed of text nodes in different locations. Secondly, the named entity recognition task of a single modal text only needs to consider the semantic relationship between the tokens in the text sequence. However, the semantic entity recognition task on the document needs to consider not only the semantic relationship between nodes, but also the position relationship between nodes. Finally, the span range of entity tags of NER task is flexible, while the range of task tags of semantic entity recognition task on document is affected by nodes. Texts of the same node in the document share the same label in most cases. 
With the development of pre-training technology, document pre-training model has become popular. LayoutLM is the first multi-modal pre-trained model to associate text with layout and vision, achieving leading results on multiple downstream document understanding tasks including semantic entity recognition. Subsequently, more multi-mode pretraining models, such as LayoutLMv2, BROS, ERNIE-Layout and LayoutLMv3 have been proposed successively. By integrating text, layout and visual information, they realize the understanding and information extraction of documents. So far, GraphLayoutLM and GeoLayoutLM have the best performance in semantic entity recognition tasks. GraphLayoutLM achieves the best F1 score of 94.39 and 93.56 on the FUNSD and XFUND datasets. GeoLayoutLM achieves the best F1 score of 97.97 on the CORD datasets. However, these existing methods focus on the upstream document understanding part and pay little attention to the downstream task. GeoLayoutLM has studied the novel relational extraction head and achieves great improvement in the relational extraction task. But it has not done more research on the semantic entity recognition task. We study the problem of ignoring the downstream head and classification method in the semantic entity recognition task in the existing document intelligence work and propose a novel improvement scheme. 
 
Traditional Semantic Entity Recognition. The traditional document semantic entity recognition task process is shown in (a) of the Figure. In document understanding process, text nodes are spliced into text sequences and become text token sequences of documents after tokenization. These text nodes will be transformed to the high-dimensional feature representations after the analysis of the document understanding model. To extract semantic information from document token features, linear layer or multilayer perceptron (MLP) will be used to convert high-dimensional features into label probabilities and the training objective is cross entropy loss. Although this method can distinguish the node categories in the document, it ignores the characteristics of the document structure and it is difficult to make the classification layer pay attention to the node span."
An Iterative Associative Memory Model for Empathetic Response Generation,2402.17959v2,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2402.17959v2.pdf,An example of iterative association. Words with the same color are associated. The memory stores the associated words.,"Empathetic response generation aims to comprehend the cognitive and emotional states in dialogue utterances and generate proper responses. Psychological theories posit that comprehending emotional and cognitive states necessitates iteratively capturing and understanding associated words across dialogue utterances. However, existing approaches regard dialogue utterances as either a long sequence or independent utterances for comprehension, which are prone to overlook the associated words between them. To address this issue, we propose an Iterative Associative Memory Model (IAMM) for empathetic response generation. Specifically, we employ a novel second-order interaction attention mechanism to iteratively capture vital associated words between dialogue utterances and situations, dialogue history, and a memory module (for storing associated words), thereby accurately and nuancedly comprehending the utterances. We conduct experiments on the Empathetic-Dialogue dataset. Both automatic and human evaluations validate the efficacy of the model. Variant experiments on LLMs also demonstrate that attending to associated words improves empathetic comprehension and expression.","As an important task for improving dialogue quality, empathetic response generation aims to comprehend the emotional and cognitive states of the user in dialogue utterances and provide appropriate responses. 
The majority of methods treat the dialogue utterances as a long sequence to comprehend the user states. These approaches ignore the discrepancies in meanings among individual utterances, leading to inaccurate understanding of emotional and cognitive states. To address this issue, some methods comprehend more delicate emotional and cognitive states within a set of independent utterances by distinguishing self-other awareness or emphasizing emotion-intent transitions. 
However, the situation model, as an important theory for understanding empathy, posits that comprehending emotional and cognitive states in detail necessitates not only understanding independent utterances, but also iteratively associating pivotal associated words within those utterances. As shown in Figure, the speaker 's independent utterances imply the emotion of ""anger, "" yet the overall expression conveys ""furious. "" If the utterances are understood independently, the listener is likely to misinterpret the speaker' s emotion as ""anger. "" In contrast, the iterative association integrates subtle associated words, allowing for an accurate understanding of the dialogue. Specifically, when faced with the first utterance, the listener combines this utterance with related words from the situation and stores it in its memory to form an initial understanding. When encountering the speaker 's second utterance, the listener meticulously compares and reasons this utterance with the dialogue history, situation, and related words in memory, to deepen its understanding of the utterance. For instance, associating ""jerks"" and ""guy"" reveals an intensification of the emotion of anger, i.e., ""furious"". Additionally, reasoning that ""cut me off"" and ""caused an accident"" makes it easier to realize the speaker' s furious due to the life-threatening event. Overall, by associating explicit and implicit information, the listener attains a more nuanced understanding of the utterances. While this comprehension process proves effective, simulating it to achieve meticulous understanding of dialogues remains an open challenge. 
In this paper, we propose an iterative associative memory model (IAMM) that iteratively employs an information association module to identify and learn subtle connections within both explicit and implicit information. We first treat the dialogue content, including both the dialogue utterances and situations, as explicit information, and treat the reasoning knowledge about the dialogue content generated by COMET as implicit information. Subsequently, we iteratively utilize the information association module to identify and learn associated words between utterances and situations, dialogue history, and memory (initialized as an empty set) in the explicit/implicit information, and store them in the memory for a thorough understanding of the utterances. Specifically, the information association module, inspired by the idea that ""pages (nodes) linked by important pages (nodes) are also more important"", effectively identifies associated words in the to-be-associated sentences through a second-order interaction attention mechanism. 
To validate our model, we construct IAMM and IAMM_large (LLMs-based model). Experiments are conducted on the Empathetic-Dialogue dataset. Both automatic evaluation and human evaluation demonstrate that compared with the state-of-the-art baselines, our models possess stronger understanding while expressing more informative responses. 
Overall, our contributions are as follows: 
 
 * We introduce an iterative association framework for empathetic response generation, which simulates the human iterative process of understanding emotions and cognition. 
 * We propose an iterative associative memory model (IAMM), which iteratively employs a second-order interaction attention mechanism to capture subtle associations in dialogues. 
 * Experiments on the Empathetic-Dialogue dataset validate the efficacy of our models."
Dr.Academy: A Benchmark for Evaluating Questioning Capability in Education for Large Language Models,2408.10947v1,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2408.10947v1.pdf,Comparison between general and educational questions.,"Teachers are important to imparting knowledge and guiding learners, and the role of large language models (LLMs) as potential educators is emerging as an important area of study. Recognizing LLMs 'capability to generate educational content can lead to advances in automated and personalized learning. While LLMs have been tested for their comprehension and problem-solving skills, their capability in teaching remains largely unexplored. In teaching, questioning is a key skill that guides students to analyze, evaluate, and synthesize core concepts and principles. Therefore, our research introduces a benchmark to evaluate the questioning capability in education as a teacher of LLMs through evaluating their generated educational questions, utilizing Anderson and Krathwohl' s taxonomy across general, monodisciplinary, and interdisciplinary domains. We shift the focus from LLMs as learners to LLMs as educators, assessing their teaching capability through guiding them to generate questions. We apply four metrics, including relevance, coverage, representativeness, and consistency, to evaluate the educational quality of LLMs' outputs. Our results indicate that GPT-4 demonstrates significant potential in teaching general, humanities, and science courses; Claude2 appears more apt as an interdisciplinary teacher. Furthermore, the automatic scores align with human perspectives.","Large language models (LLMs) have demonstrated great performance in various natural language processing (NLP) tasks, including question answering, information retrieval, reasoning, and generation, etc. Beyond these general NLP applications, LLMs are also widely used in other domains, such as education. In the educational field, LLMs can now be used as substitutes for teachers. They can help automated teaching or assisted learning applications, thereby alleviating the pressure on human teachers. Additionally, LLMs can recommend appropriate elective courses based on a student 's knowledge state, learning style, and interests, automatically generating practice problems of corresponding difficulty levels, and identifying areas where a student is struggling to provide targeted improvement. 
However, the capability of questioning is a crucial aspect in the educational field. As LLMs take on the role of teachers, can they pose high-quality questions like human educators? Therefore, evaluating what constitutes a high-quality question in education becomes necessary. According to Anderson and Krathwohl' s educational taxonomy, we consider that high-quality questioning in the educational field must meet the following characteristics: i) achieve a higher level across the six domains including memory, understanding, application, analysis, evaluation, and creation; ii) be relevant to the given context; iii) comprehensively cover the content of the context, and iv) also reflect the important knowledge of this context. We consider that questions meeting these characteristics can effectively assess students 'knowledge levels, and LLMs capable of posing such questions can assume the role of competent human educators. The first characteristic is the most basic requirement for LLMs to act as human teachers, while the following three characteristics measure the excellence of LLMs in their role as a teacher. 
Evaluating and enhancing the capability of LLMs to generate questions of high quality standards in the educational domain requires a benchmark. However, previous studies have mainly viewed LLMs from a student' s perspective, focusing on tasks like reading comprehension and exam evaluations. However, these tasks focus on adopting contexts to passively answer questions or make reasoning, and these tests treat LLMs as students, assessing their abilities by how they answer questions, while the LLM 's questioning capability through generating educational questions is under-studied. Current education-related research is far from adequate to determine LLMs' question raising capability as a teacher, and there isn 't a benchmark that studies the overall teaching abilities of LLMs, seeing them as teachers. Although some role-playing tasks mimic professional dialogues but don' t truly assess the LLMs 'teaching capabilities. Therefore, if we want LLMs to assist in teaching effectively, we need to evaluate and enhance their teaching abilities, as possessing knowledge and guiding others to learn are distinct skills. 
Therefore, in this paper, we have developed a benchmark for assessing whether LLMs generate high-quality questions in the field of education, guided by professional educational theories. Unlike general questioning, as shown in Fig. (a), our benchmark requires that the generated questions not only be fluent and readable but also meet the fundamental characteristics proposed earlier (i.e.,the first characteristic), as shown in Fig. (b). Specifically, we draw on Anderson and Krathwohl' s educational taxonomy to prompt LLMs to generate questions at six levels for each context. We select tasks from three domains, including general, single-discipline, and interdisciplinary domains, to more comprehensively assess the strengths of LLMs as teachers in various fields. Based on the four characteristics proposed earlier, we have also designed four evaluation metrics: consistency, relevance, coverage, and representativeness, to assess the value of questions posed by LLMs in the educational domain, thereby comprehensively evaluating the questioning capability of LLMs as teachers in education through evaluating their generated educational questions. Our experiments reveal that LLMs like GPT-4, Claude2, and GPT-3.5 demonstrate good questioning capability across domains as teachers in education through evaluating their generated educational questions. In summary, our contributions are threefolds: 
 
 * We introduce the problem of evaluating questioning capability in education as a teacher for LLMs through evaluating their generated educational questions, building a framework based on educational theory that includes six cognitive levels and tasks from three different domains. 
 * We establish four evaluation metrics to assess the questioning capability in education as a teacher of LLMs through evaluating their generated educational questions. 
 * We conduct experimental evaluations of 11 LLMs, providing quantitative standards and subject orientations for each LLM's questioning capability as a teacher."
UniBridge: A Unified Approach to Cross-Lingual Transfer Learning for Low-Resource Languages,2406.09717v3,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2406.09717v3.png,"Some languages/scripts are not covered in the pre-trained corpora. Hence, the pre-trained tokenizer will eventually produce many unknown tokens which corrupts the sentence's meaning and results in poor performance.","In this paper, we introduce UniBridge (Cross-Lingual Transfer Learning with Optimized Embeddings and Vocabulary), a comprehensive approach developed to improve the effectiveness of Cross-Lingual Transfer Learning, particularly in languages with limited resources. Our approach tackles two essential elements of a language model: the initialization of embeddings and the optimal vocabulary size. Specifically, we propose a novel embedding initialization method that leverages both lexical and semantic alignment for a language. In addition, we present a method for systematically searching for the optimal vocabulary size, ensuring a balance between model complexity and linguistic coverage. Our experiments across multilingual datasets show that our approach greatly improves the F1-Score in several languages. UniBridge is a robust and adaptable solution for cross-lingual systems in various languages, highlighting the significance of initializing embeddings and choosing the right vocabulary size in cross-lingual environments.","Recently, multilingual pre-trained language models (LMs) have significantly advanced natural language processing (NLP) tasks, narrowing the performance gap between English and various other languages. Multilingual pre-trained models such as XLM-R and mBERT are currently strong models for effectively cross-lingual transfer. However, these models pose a limitation that they are pre-trained on a limited set of approximately 100 languages, leaving a substantial void for the vast array of the world's nearly 7000 languages. The resultant disparity disproportionately affects low-resource languages that are not covered in their pre-trained corpora, impeding their performance compared to their high-resource counterparts. 
 
 
Recent efforts propose the use of adapters to mitigate the knowledge gap in low-resource languages prior to transferring knowledge for specific tasks. These methods adapt the pre-trained LMs to a new language by utilizing monolingual data, enabling the model to acquire a robust representation of the target language before receiving knowledge from the source language. Despite enhanced performance in languages not included in the pre-trained corpora, these approaches still exhibit poor performance in languages with unseen scripts (i.e., the scripts that are not presented in the pre-training corpora; see Figure). To address the issue of unseen scripts, existing studies propose acquiring a new vocabulary embedding for newly discovered languages. However, these methods heavily rely on manually configuring the vocabulary size and initializing the embedding matrix. 
Furthermore, recent Cross-Lingual Transfer Learning studies focus on English due to its abundant pre-trained data and impressive task performance, our experiments reveal that high performance in English tasks does not necessarily guarantee successful transfer to other languages, particularly low-resource languages. Therefore, we suggest an automated method utilizing the LMs to identify the most suitable set of source languages for knowledge aggregation, leading to notable performance improvements over single-source language transfer. 
Our research empirically tested the effectiveness of newly random initialized embeddings and fixed vocabulary size. We then introduce an efficient technique for determining the optimal vocabulary size for new languages, utilizing the syntactic and semantic insights from the pre-trained LMs. In addition, we present an innovative method for transferring knowledge from multiple sources, which allows the model to choose the best combination of source languages to improve the overall performance. Our results contribute to the ongoing discussion about managing linguistic diversity in NLP, particularly for languages with limited resources, emphasizing the importance of a detailed and inclusive strategy in creating multilingual pre-trained LMs. 
We evaluate our approach on sequence tagging tasks (e.g.,NER, POS) and classification (e.g.,NLI) with two strong baselines, mBERT and XLM-R, and observe a significant increase in the F1 and accuracy score. In summary, our contributions are: 
 
 * We propose a novel approach to automatic search for a suitable vocabulary size to adapt to a new language. 
 
 * We propose a new strategy to initialize the embedding that leverages the syntactic and semantic knowledge encoded in the pre-trained LMs to address the missing tokens when adapting to low-resource languages. 
 
 * We propose a method to aggregate multi-source transfer learning to enhance the performance on cross-lingual transfer tasks. We show that multi-source can outperform effective multi-language learning."
T2S-GPT: Dynamic Vector Quantization for Autoregressive Sign Language Production from Text,2406.07119v1,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2406.07119v1.pdf,Comparison of fixed-length encoding and variable-length encoding.,"In this work, we propose a two-stage sign language production (SLP) paradigm that first encodes sign language sequences into discrete codes and then autoregressively generates sign language from text based on the learned codebook. However, existing vector quantization (VQ) methods are fixed-length encodings, overlooking the uneven information density in sign language, which leads to under-encoding of important regions and over-encoding of unimportant regions. To address this issue, we propose a novel dynamic vector quantization (DVA-VAE) model that can dynamically adjust the encoding length based on the information density in sign language to achieve accurate and compact encoding. Then, a GPT-like model learns to generate code sequences and their corresponding durations from spoken language text. Extensive experiments conducted on the PHOENIX14T dataset demonstrate the effectiveness of our proposed method. To promote sign language research, we propose a new large German sign language dataset, PHOENIX-News, which contains 486 hours of sign language videos, audio, and transcription texts. Experimental analysis on PHOENIX-News shows that the performance of our model can be further improved by increasing the size of the training data. Our project homepage is <https: //t2sgpt-demo. yinaoxiong. cn>.","Sign language is a visual language with complex grammatical structures and is the primary means of communication for nearly 70 million deaf people worldwide. Research on sign language production and sign language translation has attracted widespread attention. Sign language production (SLP) is a challenging problem that aims to automatically translate spoken language descriptions into corresponding continuous sign sequences. SLP can help deaf people better access information and communicate with others, thereby facilitating their lives, which has important social significance. 
SLP models are expected to learn precise mapping from the spoken language space to the sign language space. Early work used 2D or 3D skeleton poses to represent sign language, while recent work has suggested using 3D human models, such as SMPL-x, to represent sign language, as it introduces human priors and can better animate. To learn the mapping between these two different modal spaces, some work uses autoregressive models, non-autoregressive models, or diffusion models to learn the direct mapping from spoken language text to sign language skeleton poses. proposed to learn the discrete representation of sign language through VQ-VAE and then learn the mapping from text to discrete representation through a discrete diffusion model. However, we found that existing sign language discrete representation methods are fixed-length encodings, as shown in, which overlooks the uneven information density in sign language. In addition, many existing works rely on expert-annotated intermediate representations, i.e.,glosses, which limit the scalability of the model. 
In this work, we are inspired by recent advances from learning the discrete representation for generation. Specifically, we investigate a two-stage framework based on Dynamic Vector Quantized Variational Autoencoders (DVQ-VAE) and Generative Pre-trained Transformer (GPT) for text-to-sign language production. In the first stage, as shown in, DVQ-VAE will learn the weights of each frame and the boundaries of the basic semantic units. Then, the weighted latent vectors are mapped to discrete code indices. Further quantitative analysis of the uneven information density in sign language is provided in. To encourage models to perform variable-length encoding and compress sequence lengths, we propose a novel budget loss. Additionally, to preserve the semantic information of the reconstructed sign language sequences, we also introduce a translation auxiliary loss. In the second stage, a GPT-like model is learned to to generate code index sequences from spoken language text. Furthermore, since the duration of quantized code in a sequence can also vary dynamically, we further propose a duration transformer to predict the duration of the next code based on the previous code's duration and the current code. 
The experimental results on the widely used SLP dataset PHOENIX14T demonstrate that our proposed method achieves superior back translation performance compared to previous approaches. Furthermore, throughout the entire development process of image generation and text generation, the scale of the dataset has played a crucial role. A large amount of high-quality corpus is also very important for SLP tasks. In this paper, we present the largest known German Sign Language dataset, PHOENIX-News, which consists of 486 hours of sign language videos, audio, and transcription texts. The native expression, clear hand details, and extensive coverage of our large-scale dataset make it suitable for a variety of sign language research tasks, such as sign language translation and sign language production. Based on this dataset, we further explore the impact of training data size on SLP tasks. Empirical analysis shows that the performance of our model can be further improved by increasing the size of the training data. 
Our main contributions are summarized as follows: 
 
 * We analyse the uneven information density in sign language. Additionally, we propose for the first time an information density based variable length coding method suitable for sign language. 
 * We propose a two-stage SLP framework consisting of two components: 1) DVQ-VAE to dynamically assign variable-length codes to sequences based on their different information densities through a novel adaptive downsampling module and budget loss. 2) A novel T2M-GPT model to predict variable-length codes and their corresponding durations. 
 * Extensive experiments on the challenging PHOENIX14T dataset show the effectiveness of our proposed method. 
 * We propose the largest known German sign language dataset, PHOENIX-News, which can be used for a variety of sign language research tasks."
OceanGPT: A Large Language Model for Ocean Science Tasks,2310.02031v8,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2310.02031v8.pdf,Capabilities of OceanGPT. Our proposed model not only shows a higher level of knowledge expertise for oceans science tasks but also gains preliminary embodied intelligence capabilities in ocean technology.,"Ocean science, which delves into the oceans that are reservoirs of life and biodiversity, is of great significance given that oceans cover over 70%of our planet's surface. Recently, advances in Large Language Models (LLMs) have transformed the paradigm in science. Despite the success in other domains, current LLMs often fall short in catering to the needs of domain experts like oceanographers, and the potential of LLMs for ocean science is under-explored. The intrinsic reasons are the immense and intricate nature of ocean data as well as the necessity for higher granularity and richness in knowledge. To alleviate these issues, we introduce OceanGPT, the first-ever large language model in the ocean domain, which is expert in various ocean science tasks. We also propose DoInstruct, a novel framework to automatically obtain a large volume of ocean domain instruction data, which generates instructions based on multi-agent collaboration. Additionally, we construct the first oceanography benchmark, OceanBench, to evaluate the capabilities of LLMs in the ocean domain. Though comprehensive experiments, OceanGPT not only shows a higher level of knowledge expertise for oceans science tasks but also gains preliminary embodied intelligence capabilities in ocean technology.","Ocean science, which delves into the intricacies of oceans that cover over 70%of our planet's surface, is essential not only for understanding the rich reservoirs of life and biodiversity but also for recognizing their pivotal role in regulating the global climate and supporting economies. Recently, advances in Large Language Models (LLMs) have transformed the paradigm in science domains such as medical science, molecular science, protein science and geoscience. However, the potential for the large language model in ocean science is under-explored. 
Despite remarkable success in general domain, current LLMs still do not fully meet the specific demand of oceanographers. This inadequacy is primarily due to: (1) The immense volume and intricate nature of ocean data. As ocean science research progresses, acquiring data becomes increasingly challenging, which makes enhancing the oceanic understanding both a golden opportunity and a significant hurdle. (2) The necessity for higher granularity and richness in knowledge. Note that the data requirements faced by researchers are becoming increasingly intricate and diverse. Ocean science encompasses various domains and subjects, each with its distinct data attributes and patterns. 
To alleviate these issues, we introduce OceanGPT, the first-ever LLM in the ocean domain, which is expert in various ocean science tasks. Specifically, we propose DoInstruct, an efficient ocean science instruction generation framework that capitalizes on multi-agent collaboration. Each agent in our designed framework is considered as an expert in a specific domain (science and research, resources and development, ecology and environment etc. ) and is responsible for generating the corresponding data. For the advancement of ocean science research using LLMs, we also create a benchmark called OceanBench to evaluate the capabilities in ocean science tasks. 
Through extensive experiments, OceanGPT shows superiority for diverse ocean science tasks. Note that our benchmark data is based on criteria manually evaluated by ocean experts, and can accurately reflect the capabilities that LLMs possess in the field of ocean science. As depicted in Figure, our model can comprehensively answer questions according to the instructions of oceanographers, which demonstrates its expertise in oceanography. We further explore the potential of OceanGPT from the perspectives of ocean engineering. Specifically, we integrate ocean robotics instructions into the training data and evaluate its ability via code or console commands. OceanGPT not only demonstrates a higher level of knowledge expertise but also gains preliminary embodied intelligence capabilities in ocean technology. 
Our contributions can be summarized as follows: 
 
 * We introduce OceanGPT, the first ocean LLM, which shows superiority for various ocean science tasks. It can answer oceanographic questions according to the instructions of oceanographers, demonstrating expertise in oceanography. 
 * We propose DoInstruct, an automated domain instruction evolving framework that constructs the ocean instruction dataset by multi-agent collaboration. Our framework effectively alleviates the difficulty of obtaining ocean domain data. 
 * Extensive experiments demonstrate the superiority of OceanGPT in the OceanBench. OceanGPT not only demonstrates a higher level of knowledge expertise for oceans science tasks but also gains preliminary embodied intelligence capabilities."
What Does the Bot Say? Opportunities and Risks of Large Language Models in Social Media Bot Detection,2402.00371v2,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2402.00371v2.pdf,Overview of the opportunities of LLM-based bot detectors and risks of LLM-based evasive bots.,"Social media bot detection has always been an arms race between advancements in machine learning bot detectors and adversarial bot strategies to evade detection. In this work, we bring the arms race to the next level by investigating the opportunities and risks of state-of-the-art large language models (LLMs) in social bot detection. To investigate the opportunities, we design novel LLM-based bot detectors by proposing a mixture-of-heterogeneous-experts framework to divide and conquer diverse user information modalities. To illuminate the risks, we explore the possibility of LLM-guided manipulation of user textual and structured information to evade detection. Extensive experiments with three LLMs on two datasets demonstrate that instruction tuning on merely 1,000 annotated examples produces specialized LLMs that outperform state-of-the-art bot detection baselines by up to 9.1%on both datasets. On the other hand, LLM-guided manipulation strategies could significantly bring down the performance of existing bot detectors by up to 29.6%and harm the calibration and reliability of bot detection systems. Ultimately, this works identifies LLMs as the new frontier of social bot detection research.","Social media bot accounts are behind many online perils such as misinformation, election interference, extremist campaigns, and conspiracy theories. Research on detecting social media bots has always been an arms race: early methods focus on analyzing user metadata with machine learning classifiers, while bot operators manipulate user features to evade detection; later approaches employed word embeddings and encoder-based language models to characterize user texts, while bot operators re-post genuine content to dilute malicious content and appear innocuous; recent models tap into the network information of user interactions with graph neural networks, while advanced bots strategically follow and unfollow users to appear out-of-distribution. 
Recent advances brought us large language models (LLMs) that excel in academic tasks and benchmarks, capable of following instructions, but they also come with risks and biases that could cause real-world harms. In this work, we ask: What are the opportunities and risks of large language models in social bot detection? As the arms race escalates, we focus on how state-of-the-art large language models could aid robust bot detection systems and how LLMs might be maliciously employed to design more evasive bots. 
For opportunities, we propose a mixture-of-heterogeneous-experts framework, employing LLMs to divide and conquer various user information modalities such as metadata, text, and user interaction networks. For user metadata, we verbalize categorical and numerical user features in natural language sequences and employ in-context learning for bot detection. For user-generated texts, we retrieve similar posts from an annotated training set as in-context learning examples. For the network information, guided by previous works about LLMs 'graph reasoning capabilities, we include the user' s following information, in either random or similarity-based order, as part of the prompt context to aid detection. These modality-specific LLMs are then used through in-context learning prompting or instruction tuning, and modality-specific results are ensembled through majority voting. 
For risks, we investigate the possibility of LLM-guided bot design to evade detection by tampering with the textual and structural information of bot accounts. For textual information, we explore rewriting user posts with LLMs to appear genuine with four mechanisms: 1) zero-shot prompting; 2) few-shot rewriting to imitate the posts of genuine users; 3) interactive rewriting between LLMs and an external bot classifier; 4) synthesizing the attributes of related posts from bots and humans for style transfer. For structural information, we employ LLMs to suggest new users to follow or existing users to unfollow, editing the neighborhood of bot accounts. LLM-guided manipulation of textual and structural features is then merged to produce LLM-guided social media bots. 
We conduct extensive experiments with three LLMs on two standard bot detection datasets to evaluate the proposed detectors and manipulation strategies. We find that on the opportunities side, LLMs are liable to become state-of-the-art detectors: while in-context learning struggles to capture the nuances of bot accounts, instruction tuning outperforms baselines by up to 9.1%on both datasets. With respect to threat and risk modeling, LLM-guided manipulations on both textual and structural information reduce the performance of existing detectors by up to 29.6%, and LLM-based detectors are more robust towards bots designed by LLMs. Our work opens up new research avenues in the ever-lasting arms race between researchers and bot operators, focusing on LLMs as the new frontier of social bot detection research."
StreamAtt: Direct Streaming Speech-to-Text Translation with Attention-based Audio History Selection,2406.06097v1,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2406.06097v1.png,"Decision steps of the StreamST policy. The order followed by our StreamAtt policy (step 1.3keycap: 1, step 1.3keycap: 21.3scroll, and step 1.3keycap: 21.3speaker high volume) is indicated from 1 (first) to 3 (last).","Streaming speech-to-text translation (StreamST) is the task of automatically translating speech while incrementally receiving an audio stream. Unlike simultaneous ST (SimulST), which deals with pre-segmented speech, StreamST faces the challenges of handling continuous and unbounded audio streams. This requires additional decisions about what to retain of the previous history, which is impractical to keep entirely due to latency and computational constraints. Despite the real-world demand for real-time ST, research on streaming translation remains limited, with existing works solely focusing on SimulST. To fill this gap, we introduce StreamAtt, the first StreamST policy, and propose StreamLAAL, the first StreamST latency metric designed to be comparable with existing metrics for SimulST. Extensive experiments across all 8 languages of MuST-C v1.0 show the effectiveness of StreamAtt compared to a naive streaming baseline and the related state-of-the-art SimulST policy, providing a first step in StreamST research.","Streaming speech-to-text translation (StreamST) is the task of automatically translating spoken content from the source language into the target language in real-time, while continuously receiving an input audio stream. By processing longer, unsegmented audio, StreamST adds another layer of complexity to the difficulties of simultaneous ST (SimulST) which, instead, operates on – often manually – pre-segmented speech segments. 
In SimulST, the primary objective revolves around finding a balance between producing high-quality translations and minimizing latency. This balance is managed by a simultaneous policy, which is the strategy for determining, at each time step, whether to emit a partial translation hypothesis or to wait for additional audio input. This hypothesis, together with the processed audio, is temporarily stored in memory to provide context for subsequent generations and is automatically removed from memory at the end of each audio segment. However, when the input is a continuous, unbounded stream, the memory retained as useful context can indefinitely grow, rendering the direct application of conventional SimulST approaches to StreamST impractical due to latency and computational constraints. 
Despite representing the real-world scenario for providing real-time ST in many applications, such as interpreting and lectures, and garnering increasing market interests, research on streaming translation remains limited, with existing works solely focusing on text-to-text machine translation (MT). Moreover, as these works focus on (unbounded) text streams as input, there is currently no metric in the literature suitable to evaluate the StreamST task, where the input is an audio stream. 
To fill these gaps, in this paper we delve into the unexplored domain of StreamST and its associated challenges. First, we define the concept of streaming policy for ST by dividing the decision-making process into two steps: 1.3keycap: 1 hypothesis selection, to determine which part of the translation hypothesis should be emitted (akin to the simultaneous policy), and 1.3keycap: 2 history selection, to identify which part of past audio and generated partial translations should be retained in memory. Then, motivated by the success of direct ST models in overcoming the high latency of cascade architectures in the related field of SimulST, we propose StreamAtt (Section), the first StreamST policy designed for direct ST systems. To enable the evaluation of our StreamST solution, we also introduce StreamLAAL (Section), the first latency metric for StreamST. StreamLAAL is designed to facilitate a direct comparison with SimulST solutions, which provide upper-bound results as they operate on pre-segmented audio. Lastly, we demonstrate the effectiveness of StreamAtt through extensive experiments across all 8 languages of MuST-C v1.0. We show that our policy significantly outperforms a naive streaming baseline (Section) that relies on a fixed number of past words and audio frames as memory, and is even competitive with the related state-of-the-art SimulST policy at low latency (Section), providing a first promising step in StreamST research."
FLEUR: An Explainable Reference-Free Evaluation Metric for Image Captioning Using a Large Multimodal Model,2406.06004v1,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2406.06004v1.pdf,"Top: Comparison between other non-explainable metrics and our explainable metric, FLEUR. FLEUR provides the explanation for the assigned score as well. Bottom: Existing explainable metric cannot consider the image. The information highlighted in red in the candidate caption is not present in the reference caption set, causing confusion for that metric.","Most existing image captioning evaluation metrics focus on assigning a single numerical score to a caption by comparing it with reference captions. However, these methods do not provide an explanation for the assigned score. Moreover, reference captions are expensive to acquire. In this paper, we propose FLEUR, an explainable reference-free metric to introduce explainability into image captioning evaluation metrics. By leveraging a large multimodal model, FLEUR can evaluate the caption against the image without the need for reference captions, and provide the explanation for the assigned score. We introduce score smoothing to align as closely as possible with human judgment and to be robust to user-defined grading criteria. FLEUR achieves high correlations with human judgment across various image captioning evaluation benchmarks and reaches state-of-the-art results on Flickr8k-CF, COMPOSITE, and Pascal-50S within the domain of reference-free evaluation metrics. Our source code and results are publicly available at: <https: //github. com/Yebin46/FLEUR>.","Evaluating image captions is essential as it provides a significant indicator of the model 's ability to understand visual and language information effectively (; ). However, there are two primary challenges with existing image captioning evaluation metrics. Existing methods 1) require reference captions to evaluate candidate captions and 2) lack explainability. 
First, traditional image captioning evaluation metrics (; ; ) have the drawback of requiring reference captions. These metrics assign scores to candidate captions by comparing them to reference captions. However, in practice, obtaining reference captions is challenging because it requires human annotators to create reference captions. Furthermore, evaluating captions only based on text without direct image comparison cannot yield accurate scores. Therefore, new methods (; ; ) have emerged that evaluate captions without the need for reference captions by incorporating images. 
Second, existing evaluation metrics still lack explainability. Throughout this paper, we clarify the meaning of an explainable metric. As defined in, we embrace the broad concept of explainability for metrics. The explainability contains the ability to provide an explanation for the score obtained from the metric. Existing metrics cannot provide intuitive explanations in sentence form. This makes it difficult to discern whether the score is accurate or not. Hence, we categorize metrics incapable of providing descriptive explanations as non-explainable metrics (see the top of Figure). 
To overcome these two limitations, we propose a reference-Free expLainable EvalUation metRic (FLEUR) for image captioning. FLEUR can evaluate captions even in the absence of reference captions and provide explanations for the scores by using a large multimodal model (LMM). We introduce score smoothing to calibrate the scores from the LMM more finely and make FLEUR robust to prompts. Additionally, we propose a prompt including grading criteria for caption evaluation to align the scores more closely with human judgment. It is noteworthy that FLEUR is the only caption evaluation metric both explainable and reference-free. 
FLEUR achieves state-of-the-art results across multiple benchmark datasets among the reference-free evaluation metrics, calculated through correlations with human judgment. Furthermore, we demonstrate FLEUR' s explainability by comparing its explanations with those of CLAIR, a reference-based and explainable evaluation metric. We hypothesize that directly viewing the image enables a more accurate and comprehensive evaluation of a candidate caption as shown at the bottom of Figure. 
Our contributions are as follows: 
 
 * We propose FLEUR, an explainable reference-free image captioning evaluation metric. FLEUR achieves the highest correlations with human judgment across various benchmark datasets. 
 * To the best of our knowledge, our work is a pioneering work of using an LMM to evaluate image captions. We improve the rating performance of an LMM by introducing score smoothing and grading criteria. 
 * Through a comparison with the reference-based metric CLAIR, we show that FLEUR generates better explanations because it can consider images."
Identifying while Learning for Document Event Causality Identification,2405.20608v1,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2405.20608v1.pdf,An example of the event causality graph and event structures in the EventStoryLine corpus.,"Event Causality Identification (ECI) aims to detect whether there exists a causal relation between two events in a document. Existing studies adopt a kind of identifying after learning paradigm, where events 'representations are first learned and then used for the identification. Furthermore, they mainly focus on the causality existence, but ignore causal direction. In this paper, we take care of the causal direction and propose a new identifying while learning mode for the ECI task. We argue that a few causal relations can be easily identified with high confidence, and the directionality and structure of these identified causalities can be utilized to update events' representations for boosting next round of causality identification. To this end, this paper designs an iterative learning and identifying framework: In each iteration, we construct an event causality graph, on which events' causal structure representations are updated for boosting causal identification. Experiments on two public datasets show that our approach outperforms the state-of-the-art algorithms in both evaluations for causality existence identification and direction identification.","Event Causality Identification (ECI) is the task of identifying whether there exists a causal relation between two events. ECI can facilitate a wide range of practical applications, including knowledge graph construction, question answering, and information extraction. The ECI task can be divided into the sentence-level ECI (two events are in the same sentence) and document-level ECI (two events may be in different sentences). 
In this paper, we focus on the document-level ECI task, which faces greater challenges due to the requirement of comprehending long texts for cross-sentence reasoning. The traditional feature-based methods utilize Integer Linear Programming (ILP) to model the document causal structure. In order to better capture the interactions among events, recent methods usually construct document-level undirected graphs to facilitate cross-sentence causal reasoning. Other methods use sparse attention to address the issue of long-distance dependencies and distinguish between intraand inter-sentential reasoning. 
Modeling the interactions among events has been proven effective for the document-level ECI task, however, almost all existing methods focus on only identifying the existence of causal relation between the event e_i and e_j, yet without considering the causality direction being from e_i to e_j (or from e_j to e_i). In this paper, e_i → e_j indicates that ""event e_i causes e_j"". This may lead to the learning of events 'representations towards capturing events' correlations, but correlations may not be directly mapped into causalities. Furthermore, undirected connections may also lead to incorrect causality identifications, as some properties of causal structures cannot be respected without directionality. 
There are three basic causal structures, namely, the chain, fork, and collider. Causality identification without directionality cannot well exploit causal structures. As shown in Figure, ""Shootinge1""  ""killinge4""  ""arrestede5"" is a chain causal structure. For a model considering causality direction, if the two directional causal relations, i.e., e_1 → e_4 and e_4 → e_5, can be first identified with high confidence, then this can help to identify the causal relation between e_1 and e_5 due to the causal transmission in the chain structure. We argue that events 'causal relation should be with directionality, and considering causal directions could further boost event causality identification. 
Besides ignoring directionality, existing solutions for the ECI task adopt a kind of identifying after learning paradigm. That is, learning events' representations first via some advanced neural networks, and then identifying causal relations for all event pairs at only one pass. However, it could happen that some causal relations can be easily identified with high confidence. As reported by, identifying intra-sentence events 'causality (two events in a same sentence) is often easier and with better accuracy than identifying inter-sentence events' causality (two events in different sentences). This motivates us to propose a new identifying while learning mode for the ECI task. That is, identifying some events 'causal relations with high confidence, and then utilizing the directionality and structure of such identified causalities to update events' representations for boosting next round of causality identification. 
Motivated from the aforementioned considerations, this paper proposes an iterative Learning and Identifying Framework (iLIF) for the document-level event causality identification. For an event e_i, we not only encode its contextual text representation 𝐡_i, but also update its causal structure representation 𝐳_i in each iteration. Causality identification is modeled as a classification issue based on the representation 𝐡_i and 𝐳_i of an event pair. Initially, we employ a pretrained language model to encode 𝐡_i. In each iteration, we first construct a directed event causality graph (ECG) based on the identified causalities, and propose a causal graph encoder to next update 𝐳_i on the ECG. After the termination, we output the directed ECG as the final causality identification results. In order to differentiate the importance of iterations, we design a novel iteration discounted loss function to mitigate the error propagation issue. 
We conduct experiments on two public datasets: The EventStoryLine (v0.9) dataset and MAVEN-ERE dataset and consider both direction and existence settings for causal relations. We preprocess the EventStoryLine dataset to ensure that each ground truth ECG is a directed acyclic graph. Experiment results validate that our iLIF outperforms the state-of-the-art competitors for the document-level ECI task in evaluations for both causality existence identification and direction identification."
OlympiadBench: A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems,2402.14008v2,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2402.14008v2.pdf,An example of IMO in OlympiadBench. Solving this example requires AI systems to span different mathematical domains and conduct advanced reasoning.,"Recent advancements have seen Large Language Models (LLMs) and Large Multimodal Models (LMMs) surpassing general human capabilities in various tasks, approaching the proficiency level of human experts across multiple domains. With traditional benchmarks becoming less challenging for these models, new rigorous challenges are essential to gauge their advanced abilities. In this work, we present OlympiadBench, an Olympiad-level bilingual multimodal scientific benchmark, featuring 8,476 problems from Olympiad-level mathematics and physics competitions, including the Chinese college entrance exam. Each problem is detailed with expert-level annotations for step-by-step reasoning. Evaluating top-tier models on OlympiadBench, we implement a comprehensive assessment methodology to accurately evaluate model responses. Notably, the best-performing model, GPT-4V, attains an average score of 17.97%on OlympiadBench, with a mere 10.74%in physics, highlighting the benchmark rigor and the intricacy of physical reasoning. Our analysis orienting GPT-4V points out prevalent issues with hallucinations, knowledge omissions, and logical fallacies. We hope that our challenging benchmark can serve as a valuable resource for helping future AGI research endeavors. The data and evaluation code are available at <https: //github. com/OpenBMB/OlympiadBench> ","Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks such as text generation, code generation and mathematical reasoning, garnering significant attention from both academia and industry. The most powerful models such as GPT-4 and Gemini Ultra have even surpassed oridinary human level on a wide variety of benchmarks such as MMLU, MMMU, and even surpassing human expert in many area. These results show a promising future that LLMs can serve as proficient assistants for human scientists. Among the array of expert-level skills exhibited by LLMs, scientific reasoning consistently emerges as one of the most brilliant, showcasing some of the most distinguished intellectual properties that experts possess. Therefore, this paper primarily focuses on mathematical and physical reasoning. 
In recent years, several benchmarks related to mathematics have been proposed, such as the dataset GSM8K as well as the dataset MATH. However, these benchmarks, are primarily developed before the advent of highly capable LLMs, and now lack sufficient challenge for the latest models. For instance, GPT-4 with prompting techniques has achieved a 97.0%success rate on GSM8K and 84.3%on MATH. The rapid evolution of LLMs may soon lead to saturated results on these benchmarks. Concurrently, LLMs are not yet fully equipped to assist mathematicians in solving complex problems, nor are they capable of performing expert-level mathematical reasoning independently. This discrepancy underscores the need for more challenging datasets to benchmark future advancements of LLMs in this domain. Similarly, physics presents comparable challenges for AI to those found in mathematics. Nevertheless, existing benchmarks related to physics are characterized by their relatively low difficulty and limited scope. There is also a significant lack of a rigorous and challenging benchmark in physics. 
In addition to the issue regarding the benchmark difficulty, it is important to note that these benchmarks predominantly focus on text. This presents a significant limitation, as a wide range of scientific reasoning contexts require multimodal reasoning abilities. For example, grasping geometry reasoning in mathematics or understanding experiments designs in physics are scenarios where multimodal reasoning capabilities are crucial. Notably, various large multimodal models (LMMs) have been developed and demonstrate proficiency on a variety of tasks, offering the potential for multimodal scientific reasoning. Nevertheless, there is still a lack of sufficient benchmarks to prove whether these LMMs are capable of handling scientific problems. Consequently, a challenging multimodal benchmark is essential for advancing scientific reasoning tasks. 
To address the aforementioned inadequacies, we introduce OlympiadBench, an Olympiad-level bilingual multimodal scientific benchmark. This collection comprises 8,476 math and physics problems sourced from International Olympiads, Chinese Olympiads, and the most challenging segments of the Chinese College Entrance Exam (GaoKao). We download PDF data from official websites and utilize Mathpix for OCR parsing. We meticulously inspect, clean, and revise the data, and further adopt LLMs for deduplication. Finally, we annotate the data with crucial information such as answer types and subfields, yielding a dataset that is clean, accurate, and detailed. As shown in Figure, OlympiadBenchfeatures numerous distinct characteristics such as difficulty, free-form generation, expert-level solution annotation, detailed labeling of difficulty, wide-coverage of modality and language, etc. These features are summarized more clearly from Table. 
We conduct an evaluation of current state-of-the-art LLMs and LMMs on the OlympiadBench. The best-performing model, GPT-4V, is a multimodal version of GPT-4 developed by OpenAI that can understand images. Despite its advanced capabilities, GPT-4V achieves a score of only 17.97%on OlympiadBench, with individual scores of 21.70%in mathematics and 10.74%in physics. 
Importantly, the experiment results show that LMMs still struggle in computational error, incorrect reasoning or induction. For the process involved in the correct responses, the process occasionally includes hallucinated reasoning, or choosing a more complex solution when a simpler solution exists. All these results highlight the substantial challenge OlympiadBench presents to contemporary large models and point the direction of future efforts. 
OlympiadBenchis inspired by the significant advances made by DeepMind AlphaGeometry, which nearly matches the proficiency of International Mathematical Olympiad (IMO) gold medalists in geometry proofs. It is clear that OlympiadBench, along with other challenging datasets like the AI-MO challenge, will witness and benchmark the swift progress towards expert-level AI assistants for solving scientific problems."
Insert or Attach: Taxonomy Completion via Box Embedding,2305.11004v4,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2305.11004v4.pdf,Example of taxonomy completion with our TaxBox framework.,"Taxonomy completion, enriching existing taxonomies by inserting new concepts as parents or attaching them as children, has gained significant interest. Previous approaches embed concepts as vectors in Euclidean space, which makes it difficult to model asymmetric relations in taxonomy. In addition, they introduce pseudo-leaves to convert attachment cases into insertion cases, leading to an incorrect bias in network learning dominated by numerous pseudo-leaves. Addressing these, our framework, TaxBox, leverages box containment and center closeness to design two specialized geometric scorers within the box embedding space. These scorers are tailored for insertion and attachment operations and can effectively capture intrinsic relationships between concepts by optimizing on a granular box constraint loss. We employ a dynamic ranking loss mechanism to balance the scores from these scorers, allowing adaptive adjustments of insertion and attachment scores. Experiments on four real-world datasets show that TaxBox significantly outperforms previous methods, yielding substantial improvements over prior methods in real-world datasets, with average performance boosts of 6.7%, 34.9%, and 51.4%in MRR, Hit@1, and Prec@1, respectively.","Taxonomy, a critical knowledge graph with an ""is-a"" relationship, plays a vital role in information retrieval, recommendation systems, and question answering. However, manual taxonomy enrichment is inefficient and costly due to the constant emergence of new concepts. To address the challenge of incorporating new concepts, taxonomy completion has been introduced, with new concepts either inserted as both parents and children or attached only as children. This task goes beyond taxonomy expansion, which primarily treats new concepts as leaf nodes and tends to have limitations in downstream applications. 
Taxonomy completion entails a more comprehensive incorporation of new concepts with two operations: insertion and attachment. For instance, in Figure, new query concepts such as cat and insect are added to the existing animal taxonomy. The process requires enumerating all possible candidate positions within the original taxonomy, including existing edges like <Animal, Vertebrate> and implicit edges from each node to its descendants such as <Animal, Tiger>. Each candidate position is then paired with the query concept, and a confidence score is calculated. Finally, insect is attached as a child of animal and cat is inserted as a parent of Siamese cat and children of Domestic Animal and Vertebrate according to their confidences. 
Recent research on taxonomy enrichment has examined various practical methods. Nevertheless, all of these approaches embed concepts as vectors in Euclidean space, which makes them less capable of modeling the asymmetric relationship (""is-a"") in taxonomy. BoxTAXO tried to employ box embedding, a representation method that can capture more prosperous and asymmetric relationships like inclusion, disjoint, and proximity among concepts through its geometric properties. However, this method is limited in real-world applications for its reliance only on the volume property, rendering it suitable only for the taxonomy expansion and even incapable of discerning optimal ancestor concepts and handling multiple parents during inference. Moreover, methods for taxonomy completion suffer from using a ""pseudo-leaf"" as a child node in attachment cases, leading to confusion in the matching. It is attributed that attachment cases often predominate due to leaf nodes 'prevalence in real taxonomies. Therefore, learning too much about the pseudo-leaf in the attachment cases may reduce the network' s perceptual ability for child nodes in the insertion cases. 
To overcome these limitations, we present a novel framework for taxonomy completion called TaxBox, which is the first to apply box embedding to taxonomy completion. This approach adopts a structurally enhanced box decoder, representing concepts as box embeddings encompassing the information of children, furnishing richer semantics. Most importantly, TaxBox combines two probabilistic scorers to unify the process of insertion and attachment in the box embedding space and incorporates both the volume and center closeness properties of box embedding. Such a design effectively exploits the fine-grained geometric attributes of box embeddings, circumventing the need for a pseudo-leaf and yielding optimal, feasible results during the ranking process. Additionally, we propose two novel training objectives, optimizing both box volume and position, and rectifying scorer numerical imbalances. 
The specific contributions of this paper are outlined as follows: 
 
 * We introduce TaxBox, the first framework using box embedding for taxonomy completion with a structurally enhanced box decoder. 
 * We establish insertion and attachment scorers, obviating the need for pseudo-leaves and ensuring the determination of optimal results. 
 * We design box constraint loss, focusing on both volume and center closeness, and dynamic ranking loss, rectifying scorer numerical imbalance. 
 * Experimental outcomes from four datasets demonstrate our model's efficacy, achieving 6.7%MRR, 34.9%Hit@1, and 51.4%Prec@1 improvements over the previous methods."
Towards Real-world Scenario: Imbalanced New Intent Discovery,2406.03127v1,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2406.03127v1.pdf,Illustration of proposed i-NID task: (a) i-NID unifies open-world and long-tail learning paradigms; (b) i-NID uses labeled and unlabeled data following a long-tail distribution to identify and categorize user intents.,"New Intent Discovery (NID) aims at detecting known and previously undefined categories of user intent by utilizing limited labeled and massive unlabeled data. Most prior works often operate under the unrealistic assumption that the distribution of both familiar and new intent classes is uniform, overlooking the skewed and long-tailed distributions frequently encountered in real-world scenarios. To bridge the gap, our work introduces the imbalanced new intent discovery (i-NID) task, which seeks to identify familiar and novel intent categories within long-tailed distributions. A new benchmark (ImbaNID-Bench) comprised of three datasets is created to simulate the real-world long-tail distributions. ImbaNID-Bench ranges from broad cross-domain to specific single-domain intent categories, providing a thorough representation of practical use cases. Besides, a robust baseline model ImbaNID is proposed to achieve cluster-friendly intent representations. It includes three stages: model pre-training, generation of reliable pseudo-labels, and robust representation learning that strengthens the model performance to handle the intricacies of real-world data distributions. Our extensive experiments on previous benchmarks and the newly established benchmark demonstrate the superior performance of ImbaNID in addressing the i-NID task, highlighting its potential as a powerful baseline for uncovering and categorizing user intents in imbalanced and long-tailed distributions.","New intent discovery (NID) has captured increasing attention due to its adaptability to the evolving user needs in open-world scenarios. NID methods generally follow a two-stage training process, including a knowledge transfer and a discovery stage. The prior knowledge is injected into the model via pre-training and then the discriminative representation is learned for known and novel intent categories. 
Despite the considerable advancements in NID, there remain two salient challenges impeding adoption in practical scenarios. In Fig. , most NID approaches predominantly address the issue of intent discovery within the framework of balanced datasets. But the distribution of intents often follows a long-tailed pattern, particularly in dialogue systems, wherein a small number of intents are highly represented and a wide variety of intents (unknown intents) are sparsely exemplified. Secondly, NID methods suffer from severe clustering degradation, where lack of improved methods for unbalanced data distributions and leading to poor performance in unbalanced scenarios. Therefore, we explore the new methods under the Imbalanced New Intent Discovery (i-NID) task to bridge the gap between the NID and real-world applications. 
To break out the aforementioned limitations, we propose a novel framework ImbaNID, which includes three key components: model pre-training, reliable pseudo-labeling (RPL), and robust representation learning (RRL). Specifically, the multi-task pre-training incorporates the generalized prior knowledge into the mode for establishing a robust representational foundation conducive to clustering known and novel intents. The RPL component formulates the pseudo-label generation as a relaxed optimal transport problem, applying adaptive constraints to recalibrate the class distribution for enhanced uniformity. The model bias issues can be mitigated in long-tail settings while furnishing reliable supervisory cues for downstream representation learning. Then, a novel distribution-aware and quality-aware noise regularization technique is introduced in RRL to effectively distinguish between clean and noisy samples. A contrastive loss function is subsequently used to facilitate the formation of distinct and well-separated clusters of representations for known and novel intent categories. The collaborative synergy between RPL and RRL fosters an iterative training process to create a symbiotic relationship. This iterative approach cultivates intent representations conducive to clustering, significantly aiding the i-NID task. For better evaluation of unbalanced distribution, we introduce a comprehensive benchmark ImbaNID-Bench for i-NID evaluation. 
Extensive experiments of ImbaNID are evaluated on the previous common benchmarks and our proposed benchmark ImbaNID-Bench. The results demonstrate that ImbaNID consistently achieves state-of-the-art performance across all clusters, notably surpassing standard NID models by an average margin of 2.7%in long-tailed scenarios. The contributions are summarized as follows: 
 
 * We introduce the imbalanced new intent discovery (i-NID) task, which first encapsulates the challenges of clustering known and novel intent classes within long-tailed distributions. Different model performances under unbalanced distribution are sufficiently explored. 
 * We construct three comprehensive i-NID datasets to facilitate further advancements in i-NID research. Our extensive experiments on these datasets validate the superiority of the proposed method ImbaNID. 
 * For i-NID, we develop a novel ImbaNID approach that iteratively enhances pseudo-label generation and representation learning to ensure cluster-adaptive intent representations."
"Rule or Story, Which is a Better Commonsense Expression for Talking with Large Language Models?",2402.14355v2,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2402.14355v2.pdf,"Comparison between rules and a story written by ChatGPT. The rules only provide useful knowledge until the 4^th rule and also include an incorrect answer option, ""classroom"". The story presents a detailed scenario where an adult uses glue sticks in an office.","Building machines with commonsense has been a longstanding challenge in NLP due to the reporting bias of commonsense rules and the exposure bias of rule-based commonsense reasoning. In contrast, humans convey and pass down commonsense implicitly through stories. This paper investigates the inherent commonsense ability of large language models (LLMs) expressed through storytelling. We systematically investigate and compare stories and rules for retrieving and leveraging commonsense in LLMs. Experimental results on 28 commonsense QA datasets show that stories outperform rules as the expression for retrieving commonsense from LLMs, exhibiting higher generation confidence and commonsense accuracy. Moreover, stories are the more effective commonsense expression for answering questions regarding daily events, while rules are more effective for scientific questions. This aligns with the reporting bias of commonsense in text corpora. We further show that the correctness and relevance of commonsense stories can be further improved via iterative self-supervised fine-tuning. These findings emphasize the importance of using appropriate language to express, retrieve, and leverage commonsense for LLMs, highlighting a promising direction for better exploiting their commonsense abilities.","Building machines with commonsense has been a longstanding goal in AI and NLP. Despite advancements in large language models (LLMs), incorporating commonsense knowledge in these models remains a significant challenge, due to the reporting bias of commonsense knowledge and the exposure bias of commonsense reasoning. The reporting bias arises because many aspects of commonsense are rarely stated explicitly in language. For example, ""A person is late"" may appear more frequently than ""A person arrives on time"" in text corpora. Furthermore, commonsense rules are often left implicit and omitted in human language reasoning, leading to exposure bias. For example, the commonsense rule ""humans need air to breathe"" is usually ignored in cases like ""The room was getting too stuffy, and I opened the windows"" as it is commonly known. 
To enhance the commonsense ability of NLP models, current studies usually express commonsense as rules. For instance, commonsense rules structured as knowledge graphs of concepts and events are incorporated to support rule-based logical reasoning. Recently, as studies reveal that LLMs like GPT-3 and ChatGPT have already learned abundant commonsense, there is a current trend to extract commonsense knowledge from the models' memory, also expressed as rules like in Figure, and enhance LLMs by reintegrating this knowledge into the models. 
However, commonsense is more than just rules. Humans acquire commonsense by recognizing prototypical patterns, extracting memories of similar past experiences, and contrasting them with the current novel situation to make decisions, as supported by psychological studies. Our commonsense is often conveyed and passed down through stories such as myths and fairy tales, with only a limited portion expressed in rules. Renowned AI theorist and cognitive psychologist Roger Schank argues in his book ""Tell Me a Story: Narrative and Intelligence"" that ""knowledge is stories"". He emphasizes that humans struggle to learn and remember abstract rules derived from past experiences but can more easily remember a good story, because ""stories give life to past experience"". 
As a result, human-written text corpora mainly convey commonsense through stories, with limited instances of explicit rules and logical reasoning. In this way, models trained on these corpora acquire commonsense and reasoning abilities implicitly. Studies show that LLMs exhibit a strong storytelling ability, generating narratives that adhere to real-world logic. However, these models may not effectively learn commonsense rules and explicit reasoning through mimicking human behaviors, as shown by recent studies. 
These observations lead to a critical question: Which is the better commonsense expression for talking with LLMs—rule or story? Specifically, this paper aims to answer the following two questions: (1) Which expression is more effective for retrieving commonsense from the memory of LLMs? (2) Which expression is more suitable for LLMs to leverage commonsense in solving problems? 
To answer the questions, we systematically compare stories and rules as commonsense expressions for talking with LLMs. We use a total of 28 commonsense QA datasets for experiments. For the first question, we instruct LLMs to generate stories and rules based on commonsense questions, as shown in Figure. We compare the confidence and the accuracy of commonsense generation using stories and rules, showing that LLMs are more confident and more accurate at retrieving commonsense as stories than as rules. For the second question, we compare the confidence of generating the correct answers with stories or rules as contexts, showing that LLMs can more confidently leverage stories than rules for reasoning. The QA accuracy results further demonstrate that the story is a more effective commonsense expression for answering questions regarding daily events, while the rule is more effective for scientific commonsense QA. This phenomenon aligns with the reporting bias of commonsense in the text corpora. Moreover, stories and rules can complement each other, i.e., combining them can further enhance the answer accuracy. 
In-depth analyses reveal two main issues in generating commonsense stories: commonsense hallucination and semantic drifting. To address these problems, we propose an iterative self-supervised fine-tuning (self-SFT) method. We ask the model to generate stories given the training set of 8 datasets and design a scoring method to rank the stories based on their consistency with commonsense and similarity with the question. We filter the stories based on the scores and use them to fine-tune the model. The tuned model is then used to generate stories in the next iteration. Experimental results show that the self-SFT method leads to further accuracy improvements, highlighting the potential for LLMs to self-improve their commonsense abilities. 
The main contributions of this paper are: 
1. We systematically investigate and compare the effects of using stories and rules as commonsense expressions for retrieving and leveraging commonsense in LLMs. To our best knowledge, this is the first study to investigate the effects of specific commonsense expressions in LLMs. 
2. We show that the story is a more effective expression for retrieving commonsense from LLMs and for leveraging commonsense in answering questions regarding daily events. 
3. We identify two main issues that hinder commonsense story generation: commonsense hallucination and semantic drifting, and propose an iterative self-SFT method to improve the accuracy and relevance of stories generated by LLMs."
DAPR: A Benchmark on Document-Aware Passage Retrieval,2305.13915v4,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2305.13915v4.pdf,"An example instance from DAPR. To find the relevant passage to the query, the retriever needs to utilize the document context, which in this case means coreference resolution for the noun the venue. See other categories of the document context and examples in.","The work of neural retrieval so far focuses on ranking short texts and is challenged with long documents. There are many cases where the users want to find a relevant passage within a long document from a huge corpus, e.g.,Wikipedia articles, research papers, etc. We propose and name this task Document-Aware Passage Retrieval (DAPR). While analyzing the errors of the State-of-The-Art (SoTA) passage retrievers, we find the major errors (53.5%) are due to missing document context. This drives us to build a benchmark for this task including multiple datasets from heterogeneous domains. In the experiments, we extend the SoTA passage retrievers with document context via (1) hybrid retrieval with BM25 and (2) contextualized passage representations, which inform the passage representation with document context. We find despite that hybrid retrieval performs the strongest on the mixture of the easy and the hard queries, it completely fails on the hard queries that require document-context understanding. On the other hand, contextualized passage representations (e.g.,prepending document titles) achieve good improvement on these hard queries, but overall they also perform rather poorly. Our created benchmark enables future research on developing and comparing retrieval systems for the new task. The code and the data are available.","Information Retrieval (IR) helps efficiently locate relevant information from a vast resource collection, acting as a central component of many natural language applications. Traditional approaches like BM25 compute simple statistics such as the frequency of the matched terms. Recent approaches apply neural networks to represent queries and passages into vector representations, extending the task modeling from simple term matching to complex semantic matching achieving better effectiveness. 
Despite their success, these neural approaches are usually limited to short passage inputs, e.g.,512 tokens due to expensive operations such as self-attention in their architectures. Such short-passage retrieval faces severe challenges in real-world scenarios, where long documents such as Wikipedia articles, scientific papers, etc. can easily go beyond this length limit. Recent work proposes new memory-efficient architectures to encode much longer document inputs and fulfill document-retrieval tasks. However, returning a long document is still inefficient for a user to locate useful information. For example, collects user queries from Google Search logs and annotates the relevant passage in Wikipedia pages. We find for 35.8%of the queries, their relevant passage is located at the 7.6th paragraph on average (standard deviation 12.7), indicating a large further-search range. This adds tremendous extra effort for the users for information seeking. 
To understand the ability of the retrieval systems for filling this gap, we propose the Document-Aware Passage Retrieval (DAPR) task, where the retriever is required to consider the associated document context for returning relevant passages. An example is shown in. In this case, the user asks for musicians that have played at a specific venue. However, the relevant passage does not mention the venue name but only the noun reference and the retriever needs to understand such document context for finding the correct passage. To gain insight into the challenges, we first carry out an error analysis for the SoTA passage retrievers (DRAGON+, SPLADEv2, ColBERRTv2) and BM25. We find the major errors (53.5%) are due to missing document context, where the correct passage misses coreference resolution, the information of the underlying main topic, etc. In these cases, understanding the document context is necessary for relating the query to the correct passage. This motivates us to create a benchmark for this task including 5 datasets from heterogeneous domains that provide such annotations of the relevant passage within its associated document. 
In experiments, we test the approaches that extend the SoTA neural passage retrievers by introducing the document context to them in two types of approaches: (1) hybrid retrieval with BM25 and (2) contextualized passage representations, which inform the passage representations with the document context. We find while the hybrid-retrieval systems achieve the strongest performance on the mixture of the easy queries and the hard queries, they fail to process the latter case where understanding the document context is necessary. Contextualized passage representations, on the other hand, can achieve good improvement on these hard queries, but overall perform rather poorly. This presents new exciting research opportunities for developing improved retrieval methods that understand the context of documents during passage retrieval. The benchmark we developed enables the research community to develop, evaluate, and compare the retrieval systems on the new task."
Uncovering the Full Potential of Visual Grounding Methods in VQA,2401.07803v2,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2401.07803v2.png,Example of Flawed VG that VG-methods in VQA teach based on the unverified assumption of presence of relevant visual information (left). Correct content cues are a prerequisite for teaching True VG (right).,"Visual Grounding (VG) methods in Visual Question Answering (VQA) attempt to improve VQA performance by strengthening a model's reliance on question-relevant visual information. The presence of such relevant information in the visual input is typically assumed in training and testing. This assumption, however, is inherently flawed when dealing with imperfect image representations common in large-scale VQA, where the information carried by visual features frequently deviates from expected ground-truth contents. As a result, training and testing of VG-methods is performed with largely inaccurate data, which obstructs proper assessment of their potential benefits. 
In this study, we demonstrate that current evaluation schemes for VG-methods are problematic due to the flawed assumption of availability of relevant visual information. Our experiments show that these methods can be much more effective when evaluation conditions are corrected. Code is provided on GitHub.","Visual Grounding (VG) in VQA has garnered interest not only as a key aspect to furthering understanding and rationalization of a VQA model 's inference procedure, but also as a way to improve Out-of-Distribution (OOD) performance by preventing certain dataset biases to form. Various works have reported evidence of problematic tendencies in VQA models that point to a disregard of relevant image regions during answer inference and the manifestation of Q/A distribution biases in the model. A lack of VG in VQA models has been shown to negatively impact OOD performance and have been tied to a general unpredictability of answering behavior. To alleviate these issues, methods have been developed that seek to strengthen a model' s reliance on question-relevant visual features. These VG-methods either modify the training procedure of existing models (e.g., HINT, SCR, VisFIS), or are integrated directly into specialized model architectures such as MMN, PVR and VLR. 
On a technical level, the goal of VG-methods in VQA is to align a model 's internal valuation of visual input feature importance (FI) with human-based FI, which is given as guidance in training. These Human-based FI scores can be inferred from a question' s visual relevance annotations, which may be given as highlighted regions in the raw image (e.g., spatial heat maps as in VQA-HAT), or explicit pointers to ground-truth objects (as in GQA). Notably, relevance annotations are not given in input feature space directly, and therefore a mapping function is required to identify corresponding visual features and determine FI scores. The predominant approaches for such a mapping between image and feature space rely exclusively on spatial matching: Visual input features receive their FI scores depending on spatial overlap between the region they represent and annotated, question-relevant locations in the raw image (cf. ). High-scoring features can then be identified as relevant cue objects. In this approach, the actual visual content carried by the cue objects is simply assumed to be appropriate without further semantic verification and therefore does not influence their score. In this work, we report evidence that such incomplete verification can result in grossly mismatched cues, thereby leading to inadequate guidance in VG-method training, as illustrated in Fig. , left. Similarly, tests performed under such unchecked conditions fail to accurately evaluate the originally intended use case that VG-methods were designed for, as question-relevant content is often missing in the input and proper VG impossible. While work such as and investigate the underlying effects of VG-method application in detail, we are unaware of any study that also considers the impact of these problematic conditions in their analysis. 
In this study, we seek to develop a better understanding of the benefits of VG-methods in VQA when training and testing conditions properly support their intended use-case. We identify two flaws and their causes in current evaluation practices for VG-methods and outline an approach to fix them. Finally, we investigate their impact on VG-method training and testing in a series of experiments. The used methodology establishes a framework for evaluating VG-methods more thoroughly. 
 
Contributions. Summarized as follows: 
 
 * An analysis of the flawed assumptions of cue object availability in current practices used for training and testing VG-methods in VQA. 
 * Comprehensive investigations of the impact of inadequate guidance for VG-method training. 
 * A methodology for training and testing VG-methods under corrected, proper conditions (code is provided)."
"Small Models, Big Insights: Leveraging Slim Proxy Models To Decide When and What to Retrieve for LLMs",2402.12052v3,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2402.12052v3.png,"A display of the main process of SlimPLM. Solid lines with arrows represent the flow of data, while dashed lines with arrows signify control signals from the retrieval necessity judgment model. Step 1 and step 2 are mandatory in the pipeline, but step 3 involves choosing between direct generation and RAG.","The integration of large language models (LLMs) and search engines represents a significant evolution in knowledge acquisition methodologies. However, determining the knowledge that an LLM already possesses and the knowledge that requires the help of a search engine remains an unresolved issue. Most existing methods solve this problem through the results of preliminary answers or reasoning done by the LLM itself, but this incurs excessively high computational costs. This paper introduces a novel collaborative approach, namely SlimPLM, that detects missing knowledge in LLMs with a slim proxy model, to enhance the LLM's knowledge acquisition process. We employ a proxy model which has far fewer parameters, and take its answers as heuristic answers. Heuristic answers are then utilized to predict the knowledge required to answer the user question, as well as the known and unknown knowledge within the LLM. We only conduct retrieval for the missing knowledge in questions that the LLM does not know. Extensive experimental results on five datasets with two LLMs demonstrate a notable improvement in the end-to-end performance of LLMs in question-answering tasks, achieving or surpassing current state-of-the-art models with lower LLM inference costs.","Large language models (LLMs) have demonstrated significant prowess in various natural language processing (NLP) tasks, attributed to their advanced language comprehension and generation capabilities. Despite being trained on extensive text corpora, these models occasionally produce hallucinated content. To tackle this problem, the integration of retrieval systems with LLMs has been proposed, enabling access to external knowledge bases for more accurate and reliable text generation. 
Retrieval-augmented generation (RAG) involves using a retrieval system to supplement LLMs with relevant external information, thereby improving text generation quality. Yet, recent studies have suggested that retrieval may not always be beneficial. In cases where LLMs can adequately respond without external knowledge, retrieval may introduce irrelevant information, potentially degrading performance. Therefore, it is critical to determine when retrieval is necessary for user questions. The challenge lies in identifying questions that exceed the LLMs 'intrinsic knowledge and require external retrieval, due to the prevalence of content hallucination. Efforts to address this challenge can be categorized into two groups: (1) The first group of methods involves fine-tuning LLMs for RAG scenarios, allowing them to autonomously signal the need for external knowledge. This method, while effective, demands substantial computational resources and risks diminishing the LLMs' general capabilities due to potential catastrophic forgetting. (2) The second category avoids direct tuning of LLMs, assessing the necessity for retrieval based on the quality of the generated content or specific indicators within it. However, this approach still has its drawbacks, as it requires multiple inferences, thereby increasing both the inference costs and the latency of responses to user questions. 
In light of this, we put forward a question: Is it feasible to employ a proxy model with a relatively smaller parameter size to facilitate effective retrieval results for an LLM? Theoretically, existing decoder-only language models share similar Transformer structures, and they are pre-trained on some common text corpora, such as Common Crawl web pages, books, and Wikipedia pages. Therefore, it is possible for them to reach a consensus on relative mastery over different knowledge and the necessity of retrieval. Our preliminary quantitative analysis, shown in Section, also supports this hypothesis. The experimental results show that on questions well understood by the LLM, the relatively smaller language model also has considerable knowledge. The gap between larger and smaller LLMs mainly manifests in questions they do not understand. This further validates the possibility of employing a proxy model to help determine the necessity of retrieval. 
Based on our analysis, in this paper, we introduce a novel approach, called SlimPLM (Slim Proxy Language Model), which leverages a relatively smaller language model as a ""proxy model"" to help determine when and how to perform retrieval for LLMs. Specifically, for a user question, SlimPLM first uses the proxy model to generate a preliminary ""heuristic answer"". This heuristic answer serves two purposes. First, it is evaluated by a lightweight model designed to assess the necessity for retrieval. If this evaluation shows that the heuristic answer is of high quality, it implies that the question may be addressed directly by LLMs without additional information retrieval. In contrast, a lower-quality answer triggers the retrieval process to identify and supplement missing knowledge. To facilitate this, SlimPLM utilizes the heuristic answer again to generate multiple queries, each reflecting a specific aspect of the initial response. These queries are then individually assessed for their need for retrieval, filtering out queries that do not require retrieval. By this means, the remaining queries can retrieve more relevant knowledge that is lacking in LLMs. The integration of SlimPLM into existing RAG frameworks offers a flexible and effective enhancement without notably increasing computational costs or response latency. Experimental results across five commonly used question-answering datasets validate SlimPLM 's effectiveness in determining the necessity for retrieval and improving retrieval results. 
Our contributions are threefold: (1) We propose a novel approach that leverages a small proxy model to generate heuristic answers, helping determine when and how to perform retrieval for LLMs. (2) We devise a retrieval necessity judgment model based on the heuristic answer. It is capable of accurately identifying which queries necessitate further information retrieval. (3) We formulate a query rewriting strategy that decomposes the heuristic answer into distinct claims. This is complemented by a claim-based filtering mechanism to enhance the relevance of the retrieval results for LLMs' text generation."
Interpretability of Language Models via Task Spaces,2406.06441v1,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2406.06441v1.pdf,The process of similarity probing to obtain a task space based on transfers: 1. Evaluate the untuned LM on all tasks (eval1); 2. Tune one LM for each task; 3. Re-evaluate the LMs on all tasks (eval2). Calculate all transfers (eval2 - eval1) and compare the resulting transfer task space to a hypothesized set of transfers (Hypothesis space).,"The usual way to interpret language models (LMs) is to test their performance on different benchmarks and subsequently infer their internal processes. In this paper, we present an alternative approach, concentrating on the quality of LM processing, with a focus on their language abilities. To this end, we construct 'linguistic task spaces' – representations of an LM 's language conceptualisation – that shed light on the connections LMs draw between language phenomena. Task spaces are based on the interactions of the learning signals from different linguistic phenomena, which we assess via a method we call' similarity probing '. To disentangle the learning signals of linguistic phenomena, we further introduce a method called' fine-tuning via gradient differentials' (FTGD). We apply our methods to language models of three different scales and find that larger models generalise better to overarching general concepts for linguistic tasks, making better use of their shared structure. Further, the distributedness of linguistic processing increases with pre-training through increased parameter sharing between related linguistic tasks. The overall generalisation patterns are mostly stable throughout training and not marked by incisive stages, potentially explaining the lack of successful curriculum strategies for LMs.","Recently, language models (LMs) have reached a level of sophistication in language production where their output is often indistinguishable from human-generated language. However, the complexity inherent in language production means that effective models are also inherently complex, making them challenging to interpret. 
Commonly, linguistic interpretability involves assessing an LM 's ability through simple evaluation tasks like grammatical acceptability judgments of various language constructions. While these methods inform us about a model' s performance, they do not provide insights into the quality of the model 's solutions. This is especially the case when error analysis is not possible due to high model performance. However, it is the quality of processing that is interesting from the viewpoint of the interpretability researcher, the cognitive scientist or linguist. Here, we introduce a method to interpret the language processing of LMs holistically. We show how linguistic knowledge in LMs interconnects. We build upon the framework of that proposes to consider linguistic phenomena as' tasks 'an LM has to optimise and allows us to analyse the interactions of those tasks, similar to ideas from multi-task learning (MTL). For example, consider the following sentences: . John did not see anything. . If John sees anything, he will be surprised. 
In both sentences, a downward-entailing environment (negation vs. conditional) allows for the negative polarity item (NPI) anything to be used. Understanding whether it is acceptable to produce an NPI in either sentence can be considered a different task. LMs can use different rules to solve these tasks: an LM might learn the co-occurrence statistics of certain trigger words (e.g.,' not 'vs. ' if ') with NPIs. On the other hand, it might generalise to a more abstract linguistical conceptualisation and understand that both – negation and conditionals – create downward-entailing environments permitting NPIs. With either rule, the model resolves acceptability judgements correctly, while the quality of both solutions is decisively different. Hence, assessing the generalisation of linguistic tasks reveals how LMs conceptualise language. 
Similar to' task spaces 'in MTL (more details in §), we can represent an LM' s generalisation behaviour in a linguistic task space, a multi-dimensional space relating linguistic tasks according to their similarity. To construct linguistic task spaces, we introduce similarity probing, a method to estimate linguistic similarity. This method involves selectively fine-tuning LMs on specific linguistic tasks and assessing the impact of the fine-tuning on other tasks (see Figure), as well as the alignment of tasks in gradient space. We extricate single linguistic tasks from their entanglement in natural language via a method we call fine-tuning via gradient differentials (FTGD). FTGD selectively updates a small, relevant subspace of parameters with 'gradient differentials'. 
The contributions of this paper can be summarised as follows: 
 * Propose linguistic task spaces as an interpretability method for deeper model understanding and as a tool for linguistic theory testing. 
 * Introduce FTGD, a technique to disentangle linguistic tasks from their language context and selectively fine-tune them in LMs. 
 * Introduce similarity probing, an efficient method for generating large linguistic task spaces. 
 * Analyze the development of language conceptualisation of LMs throughout pre-training by constructing language task spaces at various stages of LM pre-training."
One-Shot Learning as Instruction Data Prospector for Large Language Models,2312.10302v4,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2312.10302v4.pdf,"The comparison between our Nuggets and previous empirical methods. In contrast to empirical methods (blue area), Nuggets (orange area) can directly sample a gold subset, offering a more direct contribution to model fine-tuning.","Contemporary practices in instruction tuning often hinge on enlarging data scaling without a clear strategy for ensuring data quality, inadvertently introducing noise that may compromise model performance. To address this challenge, we introduce Nuggets, a novel and efficient methodology that leverages one-shot learning to discern and select high-quality instruction data from extensive datasets. Nuggets assesses the potential of individual instruction examples to act as effective one-shot learning instances, thereby identifying those that can significantly improve performance across diverse tasks. Nuggets utilizes a scoring system based on the impact of candidate examples on the perplexity of a diverse anchor set, facilitating the selection of the most advantageous data for instruction tuning. Through comprehensive evaluations on two benchmarks, including MT-Bench and Alpaca-Eval, we show that instruction tuning with the top 1%of examples curated by Nuggets substantially outperforms conventional methods employing the entire dataset.","Large language models (LLMs) have showcased remarkable capabilities across a wide range of language tasks by scaling the model size and training data. Despite their proficiency, it is imperative to further enhance their alignment with human instructions. This alignment process involves supervised fine-tuning (SFT) on input-output pairs, known as instruction tuning. Instruction tuning is a crucial step, serving not only to activate the valuable knowledge acquired by LLMs during pre-training but also to facilitate their interaction with humans in a manner that aligns with natural conversational dynamics. 
Considerable efforts in instruction tuning have been concentrated on collecting larger, more diverse, and intricate datasets. This is commonly achieved through human crowd-sourcing or extracting data from larger pre-existing models. Despite the growth in the size of datasets employed for instruction tuning, certain studies suggest that smaller yet valuable datasets tend to be more effective in harnessing the capabilities of LLMs. Blindly expanding the volume of instruction data without ensuring quality may introduce noise and lead to hallucination issues. However, there is a lack of standard criteria for selecting high-quality instruction data. As depicted in Figure, the common practice depends on empirical methods for data selection, introducing bias in determining data combinations and adjusting based on outcomes. This trial-and-error approach elevates alignment costs for models. We posit that optimal instruction combinations are present within the extensive data available, yet an efficient and cost-effective identification method remains underexplored. 
In this paper, we introduce Nuggets, a simple yet efficient method that harnesses LLMs as data explorers through one-shot (in-context) learning. This method facilitates extracting high-quality, valuable data from expansive instruction datasets. Intuitively, an instructional example holds value in training if it serves as an excellent one-shot demonstration for a specific task. If it can facilitate many tasks, it will be worth being treated as a prime data focus, i.e., ""gold instruction"". Another noteworthy perspective arises from the observation that in-context learning employs prompting to implicitly fine-tune the model, while instruction tuning operates through gradient descent. Leveraging the performance of in-context learning offers a promising avenue to predict the effects of instruction tuning. Concretely, we first select a set that spans multiple tasks, designated as the anchor set, and the dataset of instructions to be optimized is identified as the candidate set. One example is sequentially chosen from the candidate set to act as a one-shot example for in-context learning. Subsequently, it is scored based on its impact on the perplexity of each anchor example. This scoring mechanism enables the inference of dependencies between anchor and candidate examples, providing a reference standard for data selection. 
To evaluate the effectiveness of the proposed Nuggets, we conduct extensive evaluations on two widely recognized benchmarks, namely MT-Bench and Alpaca-Eval. We choose a popular and powerful LLM, LLaMA, as our base model. Experimental findings demonstrate that the Nuggets' data filtering strategy engenders a significant improvement in comparison to vanilla fine-tuning approaches. 
We summarize our main contributions as follows: 
 
 * We present Nuggets, a methodology designed to dynamically assess the quality of instructional examples by using LLMs themselves. Nuggets is expected to extract the most valuable data from a vast pool of instruction data for the purpose of fine-tuning. 
 * Fine-tuning LLMs with solely the top 1%of highest-scoring instructional examples yields superior results than using the entire instruction dataset. This observation underscores the significance of prioritizing the quality and strategic composition of the training data over sheer volume. 
 * The results of extensive experiments substantiate our hypotheses regarding ""golden instructions"", indicating that the effectiveness of an instructional example is measured by its impact on the task generalization capability of the model following the fine-tuning process. This observation holds considerable promise, potentially providing valuable insights for future endeavors in data quality screening."
A Chain-of-Thought Is as Strong as Its Weakest Link: A Benchmark for Verifiers of Reasoning Chains,2402.00559v4,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2402.00559v4.pdf,"We collect Reveal, an evaluation benchmark for the task of verifying reasoning chains in Chain-of-Thought format, which checks whether a reasoning chain is a correct justification to the final answer (importantly, the answer can be correct even if the reasoning is incorrect, as in the example above). The figure shows four verifiers (middle) verifying the correctness of a CoT (left). We use the dataset to benchmark multiple verifiers (right).","Prompting language models to provide step-by-step answers (e.g., ""Chain-of-Thought"") is the prominent approach for complex reasoning tasks, where more accurate reasoning chains typically improve downstream task performance. Recent literature discusses automatic methods to verify reasoning steps to evaluate and improve their correctness. However, no fine-grained step-level datasets are available to enable thorough evaluation of such verification methods, hindering progress in this direction. We introduce Reveal: Reasoning Verification Evaluation, a new dataset to benchmark automatic verifiers of complex Chain-of-Thought reasoning in open-domain question answering settings. Revealincludes comprehensive labels for the relevance, attribution to evidence passages, and logical correctness of each reasoning step in a language model's answer, across a wide variety of datasets and state-of-the-art language models. Available at <reveal-dataset. github. io>.","Complex reasoning tasks involve answering questions that require multiple steps of reasoning. Addressing these questions may require open-domain knowledge, mathematical reasoning, logic, and so on. Reasoning chains—breaking the task into multiple steps explicitly—is useful for improving performance in such tasks, with LMs demonstrating better performance when encouraged to generate the reasoning chain behind their answer, commonly implemented via Chain-of-Thought (CoT) prompting. 
Evaluation in such settings is traditionally limited to evaluating only whether the final answer is correct. However, correct reasoning chains have been shown to be correlated with better final answers, with recent literature proposing automatic methods for verifying the quality of the reasoning chains themselves along various axes such as informativeness, relevance, factuality and logical correctness. While such verification methods are a promising direction for improving reasoning in LLMs, it is not clear how to evaluate them due to the lack of high-quality, step-level annotated data, and collecting such data was shown to be difficult (in terms of reaching high inter-annotator agreement) and costly. 
We present Reveal (Reasoning Verification Evaluation), an evaluation benchmark for complex reasoning verifiers. Revealcovers a diverse set of reasoning skills, complexity levels, and knowledge domains. It contains 704 unique questions from 4 popular QA datasets, and 1,002 CoT answers generated by 3 language models, consisting of 3,360 CoT steps in total. 
Each step is first labeled for relevance with respect to the final answer, and then whether the step is an attribution step (introduces factual knowledge which can be attributed to a source), a logical step (introduces logical inference from previous steps) or both. For attribution steps, we collect labels for correctness to retrieved Wikipedia paragraphs given as evidence (with full support, partial support, contradiction, or no-support as labels). For logical steps, we label for logical correctness. Each label includes free-text justifications written by the annotators. An illustrative instance from the dataset is shown in. We split the dataset into Reveal-Eval, the main evaluation benchmark containing high inter-annotator-agreement labels, and Reveal-Open, a smaller set of interesting borderline cases with open labels due to low inter-annotator agreement. In we describe the dataset, and report fine-grained analyses of non-attributable steps in Reveal-Eval (i.e., evidence that supports or contradicts them was not found) and of disagreement categories in Reveal-Open. 
Revealsupports versatile evaluation settings, for example: (1) Attribution steps, along with their evidence, can serve as a high-quality Natural Language Inference benchmark in a setting of fact-checking LM outputs; (2) CoT verifiers can be evaluated at the level of individual steps, or (3) at the level of full CoT answers; (4) Each label in the data contains five free-text justifications (one per annotator), which can accommodate research around the generation of explanations and justifications, or be used to understand nuance in borderline cases. 
As we focus on the evaluation of step-level validation in complex reasoning, in we report the performance of multiple up-to-date verification baselines, leveraging NLI classifiers, GPT-3 and PaLM 2, showing much room for improvement in current state-of-the-art solutions. In particular, verifiers struggle at classifying whether a step conveys correct logical inference from previous steps. 
In summary, this work includes the following contributions: (I) A protocol for step-by-step verification of reasoning chains (); (II) An annotation schema to reliably execute the protocol with human annotators (); (III) A new benchmark dataset for evaluating automatic reasoning chain verifiers (, ); (IV) Detailed analyses of challenges in retrieving evidence to knowledge claims in reasoning and documentation of disagreements in the data (); (V) A study on the challenges for current verifiers (). These contributions advance the research on verification of reasoning chains and methods for correctly reasoning about complex questions."
Re3: A Holistic Framework and Dataset for Modeling Collaborative Document Revision,2406.00197v1,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2406.00197v1.pdf,"Re3 offers a holistic framework for studying the relationships between reviews (a), revisions (b-c) and responses (d) in text-based collaboration. It is instantiated in the Re3-Sci dataset that covers all edits in 314 full-length scientific publications manually labeled with edit action and intent (e) on different granularity levels, along with reviews that trigger edits and manually curated responses that summarize all edits made including self-initiated ones (f).","Collaborative review and revision of textual documents is the core of knowledge work and a promising target for empirical analysis and NLP assistance. Yet, a holistic framework that would allow modeling complex relationships between document revisions, reviews and author responses is lacking. To address this gap, we introduce Re3, a framework for joint analysis of collaborative document revision. We instantiate this framework in the scholarly domain, and present Re3-Sci, a large corpus of aligned scientific paper revisions manually labeled according to their action and intent, and supplemented with the respective peer reviews and human-written edit summaries. We use the new data to provide first empirical insights into collaborative document revision in the academic domain, and to assess the capabilities of state-of-the-art LLMs at automating edit analysis and facilitating text-based collaboration. We make our annotation environment and protocols, the resulting data and experimental code publicly available.","Textual documents are a key medium of information exchange in the modern world. These documents often result from a collaboration of multiple individuals. The typical process of collaborative text production involves iterations of drafting, getting feedback (reviews), executing revisions, and providing responses that outline the implemented changes, serving as a vital element in facilitating effective communication. 
 Despite the importance of collaborative text revision and its high potential for NLP applications, we are missing a framework that formally describes this review-revision-response procedure grounded in real-world data. While prior work in NLP has studied relationships between original and revised documents, reviews and original documents, reviews and revisions, and reviews and responses – no prior frameworks allow jointly modeling all three components of text-based collaboration. Yet, such joint modeling is important as it provides deeper insights into the processes involved in text work, and opens new opportunities for NLP applications. Important tasks that involve reviews, revisions and responses such as edit summarization thus remain underexplored. 
Comprehensive analysis of document-level revisions poses additional challenges. Contrary to sentence-level analysis, hierarchically structured documents bring distinct levels of granularity into editing. Individuals execute revisions at various granularity levels, with a range of actions and a spectrum of intents, reflecting the what, how, and why of the revisions (Figure and §). Realistic modeling of document revision in text-based collaboration thus requires datasets and annotations that encompass the entire document context, incorporating all edits made across various levels of granularity, and providing qualitative labels for both action and intent. We further term this kind of analysis as full-scope modeling of document revision. Prior research in NLP has primarily studied sentence-level edits while neglecting the broader document context, variations in granularity, and the underlying intent behind the edits. There is thus a gap in both methodologies and datasets for creating and analyzing full-scope annotations of document revisions, limiting our grasp of the intricate nature of the editing process. 
To close this gap and enable a comprehensive study of text-based collaboration in NLP, we introduce Re3: the first holistic framework for modeling review, revision and response in collaborative writing (§). We instantiate our framework in the scholarly domain and create Re3-Sci, the first large-scale human-annotated dataset that comprises 11.6k full-scope revision annotations for over 300 revised documents with substantial Inter-Annotator Agreement (IAA), as well as cross-document connections between reviews, revisions and responses (§). Our framework and dataset, for the first time, enable large-scale empirical investigation of collaborative document revision, including edit localization and clustering within documents, edit mechanisms and motivations inferred through action and intent labels, and the impact of review requests (§). Manually analyzing the complex relationships between reviews, revisions and responses is costly, and constitutes a promising NLP automation target. Facilitated by our data, we present a first exploration of the capability of large language models (LLMs) to address novel revision assistance tasks, such as review request extraction, revision alignment, edit intent classification and document edit summarization (§). Our work thus makes four key contributions: 
 
 * A holistic framework for studying document revisions and associated interactions in collaborative writing, including label taxonomy and robust annotation methodology; 
 * A high-quality large-scale dataset that instantiates the framework in the domain of academic writing and peer review; 
 * An in-depth analysis of human editing behavior in the scholarly domain; 
 * Extensive experiments in automation with LLMs on four NLP tasks: review request extraction, revision alignment, edit intent classification and document edit summarization. 
 Our work paves the path towards comprehensive study of NLP for text-based collaboration in the scholarly domain and beyond."
DolphCoder: Echo-Locating Code Large Language Models with Diverse and Multi-Objective Instruction Tuning,2402.09136v1,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2402.09136v1.pdf,"The overall architecture of our proposed diverse instruction tuning with self-evaluating for code generation, DolphCoder. Stage (a) denotes Diverse Instruction Tuning (DIT) and Stage (b) denotes Multi-Objective Instruction Tuning (MOT) for self-evaluating.","Code Large Language Models (Code LLMs) have demonstrated outstanding performance in code-related tasks. Several instruction tuning approaches have been proposed to boost the code generation performance of pre-trained Code LLMs. In this paper, we introduce a diverse instruction model (DolphCoder) with self-evaluating for code generation. It learns diverse instruction targets and combines a code evaluation objective to enhance its code generation ability. Our model achieves superior performance on the HumanEval and MBPP benchmarks, demonstrating new insights for future code instruction tuning work. Our key findings are: (1) Augmenting more diverse responses with distinct reasoning paths increases the code capability of LLMs. (2) Improving one's ability to evaluate the correctness of code solutions also enhances their ability to create it.","Code pre-trained models have achieved remarkable progress in the era of large language models (LLMs), such as Codex, AlphaCode, and PaLM-Coder. Code-related tasks are also the key factors in evaluating the capability of LLMs. Numerous code LLMs have been proposed, including closed-source models and open-source models. They perform expensive pre-training using substantial amounts of code data and display impressive performance. 
In contrast to these pre-trained code LLMs, another lightweight paradigm of enhancing code capability is instruction tuning using relatively small high-quality code-related data. For example, Code Alpaca employs a similar self-instruct method as Alpaca to generate code instructions via OpenAI 's ChatGPT. Further, WizardCoder introduces a more complicated Evol-Instruct method which evolves existing instruction data to generate more complex and diverse datasets. Instead, OctoPack and Magicoder construct code instructions by mining existing code corpus. All of these methods enhance the performance of the open-source Code LLMs. 
However, these methods have two weaknesses: (1) They take the only golden answer but ignore the diversity of answers in code generation. We find that augmenting more diverse responses using different system prompts increases the code capability of LLMs. (2) Current models generate plausible code snippets in terms of grammar and logic but are unable to identify subtle errors, such as corner cases and wrong input/output formats. It has no guarantee that temperature sampling will consistently produce accurate answers over time. We suppose that LLMs are capable of generating correct solutions while struggling to discriminate correct from incorrect ones. Improving one' s ability to evaluate the correctness of code also enhances their ability to create it. 
Inspired by the two insights, we introduce a diverse instruction model (DolphCoder) with self-evaluating for code generation. Specifically, we use Code Llama-python as our base model and obtain evolved instruction data following WizardCoder. Then motivated by rejection sampling and ORCA, we use different system prompts to generate diverse answers via ChatGPT. After removing low-quality and similar data using heuristic rules, we perform supervised fine-tuning on the remaining instruction data. Further, we explore whether improving one's ability to evaluate code helps generate it. We propose a self-evaluate multi-task learning framework by adding a code evaluation objective to the traditional instruction fine-tuning task. We find training the model for both code generation and code evaluation benefits the code capability. 
Our key contributions are summarized as follows: 
 
 * We introduce a diverse instruction model (DolphCoder) with self-evaluating for code generation. It learns diverse instruction targets and combines a code evaluation objective to enhance its code generation ability. 
 * DolphCoder outperforms strong open-source code LLMs by a large margin, including CODELLAMA-INSTRUCT, OctoCoder, and WizardCoder."
"Spectral Filters, Dark Signals, and Attention Sinks",2402.09221v1,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2402.09221v1.jpg,Spectral filters project signals exchanged between components onto selected subspaces as defined by the spectral decomposition of the vocabulary embedding and unembedding matrices of the model.,"Projecting intermediate representations onto the vocabulary is an increasingly popular interpretation tool for transformer-based LLMs, also known as the logit lens. We propose a quantitative extension to this approach and define spectral filters on intermediate representations based on partitioning the singular vectors of the vocabulary embedding and unembedding matrices into bands. We find that the signals exchanged in the tail end of the spectrum are responsible for attention sinking, of which we provide an explanation. We find that the loss of pretrained models can be kept low despite suppressing sizeable parts of the embedding spectrum in a layer-dependent way, as long as attention sinking is preserved. Finally, we discover that the representation of tokens that draw attention from many tokens have large projections on the tail end of the spectrum. 
 Nicola Cancedda at ncan@meta. com","Large foundation models dominate the state of the art in numerous AI tasks. While we understand how these models work in terms of elementary operations, and black-box evaluations help characterize observable behaviours, we lack a clear understanding of the connection between the two. 
There is a growing body of work providing insights into properties of model components, e.g., as well as identifying and explaining fundamental phenomena, often with the support of simple models. 
Most recent works assign a central role to the model's residual stream (RS) as the shared communication channel between model components. In this perspective, the probability distribution of a token is initialised from the projection of the embedding of the previous token through the unembedding matrix, and receives additive updates from attention heads and MLP components, each reading from the residual stream of the same or previous tokens. The role played by components is interpreted projecting their contribution on the probability distribution over vocabulary items, in what is referred to as the logit lens. We extend this approach and introduce logit spectroscopy, the spectral analysis of the content of the residual stream and of the parameter matrices interacting with it. Equipped with this tool, we look at the part of the residual stream spectrum that is most likely to be neglected by the logit lens: the linear subspace spanned by the right singular vectors of the unembedding matrix with the smallest singular values. Drawing an analogy with ""dark matter"" in astrophysics, that interacts with light only indirectly, we dub projections onto this subspace dark parameters, features, activations etc. 
We were motivated by the thought that LLMs could learn to use signals in the dark linear subspace to maintain global features responsible for long-range dependencies while minimizing their interference with the next token prediction. We discovered instead that dark signals are instrumental to implementing the recently described phenomenon of attention sinks, of which we provide a detailed account. We also show that the negative log likelihood of pretrained models can be kept low despite suppressing large swaths of the unembedding spectrum, as long as the dark signals required for attention sinking are untouched. Finally, we find a significant positive correlation between the average attention received by a token and the relative prevalence of dark signals in its residual stream."
Systematic Task Exploration with LLMs: A Study in Citation Text Generation,2407.04046v1,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2407.04046v1.pdf,"Citation text generation with LLMs. The task (1) is to generate a paragraph of related work from the citing paper (A) about a cited paper (B). The instruction combined with task inputs constitutes a prompt (2) that is communicated to the model. The model's response (3) is evaluated using a range of measurements, from word count to NLI-based factuality metrics (4).","Large language models (LLMs) bring unprecedented flexibility in defining and executing complex, creative natural language generation (NLG) tasks. Yet, this flexibility brings new challenges, as it introduces new degrees of freedom in formulating the task inputs and instructions and in evaluating model performance. To facilitate the exploration of creative NLG tasks, we propose a three-component research framework that consists of systematic input manipulation, reference data, and output measurement. We use this framework to explore citation text generation – a popular scholarly NLP task that lacks consensus on the task definition and evaluation metric and has not yet been tackled within the LLM paradigm. Our results highlight the importance of systematically investigating both task instruction and input configuration when prompting LLMs, and reveal non-trivial relationships between different evaluation metrics used for citation text generation. Additional human generation and human evaluation experiments provide new qualitative insights into the task to guide future research in citation text generation. We make our code and data publicly available.","Thanks to their instruction-following abilities, large language models (LLMs) allow specifying and executing NLP tasks with unprecedented flexibility and speed, while reducing the need for task-specific architecture design, data annotation, and model training. This has led to a surge of new, complex, creative natural language generation (NLG) tasks like peer review generation or story and poetry generation, that push the boundary of what was deemed feasible for NLP systems just a few years ago. 
The flexibility comes at a cost, as it introduces new degrees of freedom into the analysis. LLMs generate output in response to a prompt, which consists of a natural-language task instruction supplemented by additional bits of information about an instance, which we term input components (Figure). LLM-powered creative NLG tasks often feature a complex input component space, and the task instruction wording can affect model behavior in non-intuitive ways. The output space is varied as well, as there might exist infinitely many acceptable generations. This overall variability brings the risk of creative NLG tasks being defined and evaluated ad hoc, hindering systematic comparison of NLP systems and leading to anecdotal accounts of LLM capabilities. 
Although optimizing model instructions to maximize performance of LLMs is an active research area (Section), prompt engineering mostly targets the tasks where input and output spaces are well-defined (e.g., question answering). However, some creative NLG tasks need a step of exploration of what inputs are required and how the evaluation of outputs will be carried out before deeply exploring the best way to introduce the task to LLMs. 
Our work addresses task variability in citation text generation – a widely studied scholarly NLG task aiming to increase efficiency of scientific work. Citation text generation is a good example of creative NLG, as it features a complex input component space combined with multiple plausible outputs. Prior work on citation text generation lacks consensus on the required inputs, explores only a limited number of measurements to characterize the outputs, and does not investigate the use of instruction-tuned LLMs to tackle the task (Table). 
To address this gap, we design a framework to systematically explore the task of citation text generation with LLMs (Figure). We systematically manipulate the input components and instructions communicated to the model via a prompt, and study the effects of these manipulations on the model output using a wide range of measurements, supplemented by a novel reference dataset for citation text generation based on the ACL Anthology, and featuring novel use of free-form citation intents to guide generation (Section). Our experiments with two state-of-the-art LLMs – Llama 2-Chat and GPT 3.5 Turbo reveal that input components and task instructions both impact the generations, and their effects add up. Free-form citation intents, as illustrated in Figure, show promise as an alternative to categorical intents used in prior citation text generation work. Our results (Section) imply that the relative performance of alternative task input configurations can be estimated on a small set of instructions, while the best absolute performance needs experimentation with a wide array of instruction wordings. Through correlation analysis, we observe that the NLG metrics in our measurements are complementary, motivating the use of wide-spanning measurement sets for NLG tasks beyond citation text generation. Our human studies (Section) reveal both quantitative and qualitative insights about input components and task instructions from both generation and evaluation perspectives. 
In summary, this work contributes: 
 
 * A framework for exploring the task of citation text generation with LLMs; 
 * A new reference corpus of citation texts based on the ACL Anthology enriched with novel free-form citation intents; 
 * Experimental results on the impact of task inputs and instructions on citation text generation outputs, and an examination of the relationships between the measurements; 
 * Human evaluation and generation studies providing additional insights to shape future work in citation text generation and creative NLG. 
We stress that our work neither seeks nor claims state-of-the-art citation text generation, as the differences in pre-trained model capabilities would hinder a fair comparison and likely lead to confounding. Instead, the objective of our work is to explore prompting as a tool for systematic task manipulation in the LLM age. We believe our approach to be general and adaptable to other creative NLG tasks."
RelayAttention for Efficient Large Language Model Serving with Long System Prompts,2402.14808v3,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2402.14808v3.pdf,"Llama-30B attention inference latency w. r. tlet@token. system prompt length (A40 GPU, batch size 32). We set the length of (request-specific) contexts, which include user prompts and previously generated tokens, to 128.","A practical large language model (LLM) service may involve a long system prompt, which specifies the instructions, examples, and knowledge documents of the task and is reused across requests. However, the long system prompt causes throughput/latency bottlenecks as the cost of generating the next token grows w. r. tlet@token. the sequence length. This paper aims to improve the efficiency of LLM services that involve long system prompts. Our key observation is that handling these system prompts requires heavily redundant memory accesses in existing causal attention computation algorithms. Specifically, for batched requests, the cached hidden states (i. elet@token. , key-value pairs) of system prompts are transferred from off-chip DRAM to on-chip SRAM multiple times, each corresponding to an individual request. To eliminate such a redundancy, we propose RelayAttention, an attention algorithm that allows reading these hidden states from DRAM exactly once for a batch of input tokens. RelayAttentionis a free lunch: it maintains the generation quality while requiring no model retraining, as it is based on a mathematical reformulation of causal attention. We have observed significant performance improvements to a production-level system, vLLM, through integration with RelayAttention. The improvements are even more profound with longer system prompts.","After around one decade of rapid development, we have experienced a revolution of large language models (LLMs) over the past year. LLMs like GPT-4 and Gemini are so powerful that they can now serve as programming copilots, universal chatbots, computer assistants and other roles that penetrate our daily life. However, the high inference cost of these large models has become a substantial obstacle to serving more people. It is therefore important to improve the hardware utilization so that LLMs can have a higher throughput within a fixed hardware budget. 
LLM services commonly use an application-specific system prompt to specify the task's instructions. The system prompt is concatenated with the user prompt as the full input to the LLM for response generation and is shared by all requests to a service. The system prompt becomes long if the service provider wants to provide detailed guidelines and examples for better response quality or apply more restrictions/policies for ethical safety. As the sequence length that LLMs can process grows, some emerging professional applications, such as legal analysis, healthcare applications, and the shopping assistant example shown in, may include one or more knowledge documents to provide domain-specific knowledge, resulting in even longer system prompts. Although long system prompts are beneficial to improving the generation quality or enabling new applications, they also pose a challenge to the LLM service: the inference throughput and latency of the service can be heavily degraded, thus increasing the per-request cost. This is inherently caused by the causal attention, in which each new token is generated by ""looking at"" all precedent ones. 
In this paper, we propose a novel approach to mitigate the efficiency problem of using long system prompts in LLM services. Our key observation is that there are not only redundant memory footprint and computations corresponding to the system prompt, but also unnecessary memory accesses during causal attention computation. Specifically, while the system prompt is shared by all requests, its hidden states (i. elet@token. , key-value pairs) are read from DRAM multiple times by existing attention algorithms such as PagedAttention and FlashAttention, each for an individual request in the batch. This severely slows down LLM inferences, which are known to be memory-bound (). To eliminate such redundant memory access, we propose RelayAttention, an exact algorithm to compute causal attention based on a mathematical reformulation of it. The key idea of RelayAttentionis to group the matrix-vector multiplications corresponding to the system prompt into matrix-matrix multiplications, which allow loading the hidden states of the system prompt from DRAM exactly once for all request tokens in a batch (). As a result, the attention inference latency grows much slower than PagedAttention w. r. tlet@token. the length of system prompt, as shown in. We provide an in-depth analysis of the theoretic speedup of the standalone attention based on the IO redundancy reduction (). Our empirical results for end-to-end serving further verify the efficiency: integrating RelayAttentioninto vLLM, an already highly optimized production-level LLM serving system, we still observe up to 2.2× sustainable request rate and 2.0× throughput with the Llama2-7B model for a chatbot workload. Similar efficiency improvements are also observed for several other popular LLMs and are consistent on several data center GPUs. The efficiency gains continue growing with longer system prompts. 
Our key contributions can be summarized as: 
 
 * We have identified a LLM service bottleneck that has not been studied by existing works: there are highly redundant memory accesses caused by long system prompts. We anticipate that our analysis will inspire more works on deep architectures with IO-awareness. 
 * We propose RelayAttention, a novel approach to compute exact causal attention. It allows accessing cached hidden states of the system prompt exactly once for a batch of request tokens. We conduct an in-depth analysis of the theoretic speedup brought by RelayAttention. 
 * We empirically verify the end-to-end efficiency improvement by integrating RelayAttentioninto vLLM, a production level LLM serving system, and observe non-trivial efficiency gains on several popular LLMs with different GPUs."
Boosting Language Models Reasoning with Chain-of-Knowledge Prompting,2306.06427v3,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2306.06427v3.pdf,"Comparison of three prompting methods: (a) ICL, (b) Chain-of-Thought (CoT), and (c) Chain-of-Knowledge (CoK) solving a StrategyQA question.","Recently, Chain-of-Thought (CoT) prompting has delivered success on complex reasoning tasks, which aims at designing a simple prompt like ""Let's think step by step"" or multiple in-context exemplars with well-designed rationales to elicit Large Language Models (LLMs) to generate intermediate reasoning steps. However, the generated rationales often come with hallucinations, making unfactual and unfaithful reasoning chains. To mitigate this brittleness, we propose a novel Chain-of-Knowledge (CoK) prompting, where we aim at eliciting LLMs to generate explicit pieces of knowledge evidence in the form of structure triple. This is inspired by our human behaviors, i.e., we can draw a mind map or knowledge map as the reasoning evidence in the brain before answering a complex question. Benefiting from CoK, we additionally introduce a F2-Verification method to estimate the reliability of the reasoning chains in terms of factuality and faithfulness. For the unreliable response, the wrong evidence can be indicated to prompt the LLM to rethink. Extensive experiments demonstrate that our method can further improve the performance of commonsense, factual, symbolic, and arithmetic reasoning tasks.","Large Language Models (LLMs) have succeeded in advancing the state-of-the-arts for many Natural Language Processing (NLP) tasks, benefiting from the ultra-large-scale training corpora and computation resources. To unleash the LLMs 'power of adaptation on unseen tasks without any parameter updates, in-context learning (ICL) has become one of the flourishing research topics, aiming at generating the prediction by conditioning on a few labeled exemplars (Figure (a) ). 
A series of recent works have explored that LLMs can spontaneously decompose the complex multi-step problem into intermediate reasoning chains, elicited by a simple prompt like ""Let' s think step by step"" or well-designed demonstrations with human-annotated rationales, which are called chain-of-thought (CoT) prompting (Figure (b) ). This finding is intriguing and has been sensational because CoT may mainly specify an output space/format that regularizes the model generation to look step-by-step while being in order and relevant to the query. 
Despite impressive performances, current LLMs are susceptible to generating hallucination, along with providing unfactual or unfaithful reasoning chains that inevitably lead to a wrong conclusion. Take Figure as an example. Given a query ""Is the following sentence plausible 'Derrick White backhanded a shot. ' "" from StrategyQA, the standard ICL and CoT make a wrong answer. One of the reasoning steps ""Derrick White is most likely a hockey player"" is fake (In fact, Derrick White is a basketball player), making the unfactual inference towards the question. In addition, the response may be unfaithful when the LLM generates logically sound reasoning chains while still providing an incorrect answer. 
To address these concerns, we propose a novel Chain-of-Knowledge (CoK) prompting method to boost the LLM 's reasoning capability by a series of exemplars that combine explicit structure knowledge evidence with textual explanations. To elaborate, CoK prompting consists of two compositions (Figure (c) ), i.e., evidence triples (CoK-ET) and explanation hints (CoK-EH), where CoK-ET is a list of structure triples can reflect the overall reasoning evidence from the query towards the answer and CoK-EH is the explanation of this evidence. To construct in-context exemplars with the CoK prompt, we first sample K labeled examples and each of them can be concatenated with a simple hint ""Let' s think step by step"" to prompt the LLM to generate reasoning chains. Then, we retrieve some structure triples from the external knowledge base (KB) and judiciously manually annotate evidence triples to obtain a well-designed CoK prompt. Like standard ICL and CoT, the CoK prompt can be perceived as a rule that regularizes the output space/format and urges LLMs to generate explicit evidence instead of only attempting to generate vague textual reasoning chains. Furthermore, we also propose an F2-Verification strategy to estimate the reliability of the reasoning chains in terms of factuality and faithfulness, where factuality is the quantification of the matching degree between reasoning evidence and ground-truth knowledge, and faithfulness is the consistency degree between reasoning evidence and the textual explanation with the final answer. Particularly for the unreliable response, the wrong pieces of evidence can be indicated to prompt the LLM to rethink the problem. We design a rethinking algorithm to reach this goal. 
We have conducted empirical evaluations across various reasoning tasks (e.g., commonsense, factual, arithmetic, and symbolic), showing that CoK prompting with F2-Verification can significantly outperform standard ICL and CoT prompting. We also integrate CoK prompting with some prevailing strategies, such as self-consistency. The results indicate that such CoK can serve as a plug-and-play module to further improve reasoning ability."
Estimating Agreement by Chance for Sequence Annotation,2407.11371v1,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2407.11371v1.png,"The probability distributions for all possible locations of each random segment in a length=100 sequence annotated with four segments. The lengths of the four segments are 1,5,10,15, from left to right.","In the field of natural language processing, correction of performance assessment for chance agreement plays a crucial role in evaluating the reliability of annotations. However, there is a notable dearth of research focusing on chance correction for assessing the reliability of sequence annotation tasks, despite their widespread prevalence in the field. To address this gap, this paper introduces a novel model for generating random annotations, which serves as the foundation for estimating chance agreement in sequence annotation tasks. Utilizing the proposed randomization model and a related comparison approach, we successfully derive the analytical form of the distribution, enabling the computation of the probable location of each annotated text segment and subsequent chance agreement estimation. Through a combination simulation and corpus-based evaluation, we successfully assess its applicability and validate its accuracy and efficacy.","Reliable annotation is a cornerstone of NLP research, enabling both supervised learning methods and evaluation. Though not frequently employed for evaluation of model performance in the field of NLP, one of the most widely accepted metrics for evaluation of annotation reliability is Cohen's Kappa, which offers an assessment of inter-rater reliability that is adjusted in order to avoid offering credit for the portion of observed agreement that can be attributed to chance. Some NLP tasks, such as Named Entity Recognition or other span detection/labeling tasks, lack an appropriate chance corrected metric. This paper addresses this gap by proposing such a measure for these tasks, demonstrating its application in both simulation and CoNLL03 corpus experiments. 
Numerous studies caution against using non-chance-corrected agreement metrics. They can lead to unfair task or system comparisons due to biases introduced due to varying levels of chance agreement across tasks and systems. Furthermore, without correction for chance agreement, measurements tend to cluster within a narrow range, making it difficult to discern differences between approaches. Therefore, both estimating and correcting for chance agreement have become critical in annotation evaluation, except in cases where chance agreement is negligible. 
The main contributions of our work are summarized as follows: 
 
 * We propose a novel random annotation model that considers the specific characteristics of sequence annotation tasks as well as the annotation tendencies of different annotators. This model can be divided into sub-models, enabling us to separately address cases with or without annotation overlap. We also apply chance agreement to measure task difficulty. 
 * Due to the additive nature of many popular similarity measures, we simplify the modeling of dependent annotation segments within a text. We successfully derive analytical probability distributions for random annotations, presenting a streamlined formulation that avoids redundant calculations. 
 * We delve into the asymptotic properties of agreement by chance, highlighting scenarios where it can be disregarded.
 * We design and implement both simulation-based and naturalistic experiments, demonstrating that our proposed method is accurate, effective, and computationally efficient. 
 In the remainder of the paper, we provide a theoretical foundation for our work through a review of past literature. We then explain our methodology, and evaluate it first through a simulation study, and then through application to real-world corpora. Finally, we conclude with discussions of limitations, ethical considerations, and future research."
WaveCoder: Widespread And Versatile Enhancement For Code Large Language Models By Instruction Tuning,2312.14187v5,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2312.14187v5.pdf,The overview of the widespread and versatile enhancement for Code LLM. Part B and C indicates the LLM-based Generator and LLM-based Disciminator where the generator can leverage different examples in example database by in-context learning.,"Recent work demonstrates that, after instruction tuning, Code Large Language Models (Code LLMs) can obtain impressive capabilities to address a wide range of code-related tasks. However, current instruction tuning methods for Code LLMs mainly focus on the traditional code generation task, resulting in poor performance in complex multi-task scenarios. In this paper, we concentrate on multiple code-related tasks and present WaveCoder, a series of Code LLMs trained with Widespread And Versatile Enhanced instruction data. To enable the models to tackle complex code-related tasks, we propose a method to stably generate diverse, high-quality instruction data from open source code dataset in multi-task scenarios and obtain CodeSeaXDataset, a dataset comprising 19,915 instruction instances across 4 code-related tasks, which is aimed at improving the generalization ability of Code LLM. Our experiments demonstrate that WaveCoder models significantly outperform other open-source models in terms of the generalization ability across different code-related tasks. Moreover, WaveCoder-Ultra-6.7B presents the state-of-the-art generalization abilities on a wide range of code-related tasks.","Recently, Large Language Models (LLMs) such as ChatGPT, GPT-4, and Gemini have attained unprecedented performance levels in a broad array of NLP tasks. These models utilize a self-supervised pre-training process, and subsequent supervised fine-tuning to demonstrate exceptional zero/few-shot capabilities, effectively following human instructions across various tasks. 
For code-related tasks, several previous works, including Codex, StarCoder, CodeLLaMa and DeepseekCoder, have successfully demonstrated that pre-training on code corpus can significantly improve the model's capability to tackle code-related problems. After the process of pre-training, instruction tuning has shown its effectiveness in the aspect of improving the quality of LLM responses. To specifically enhance the performance of Code LLMs on code-related tasks through instruction tuning, many existing methods for instruction data generation have been designed. For example, Code Alpaca utilizes the method of self-instruct within the coding domain, leveraging the few-shot capabilities of teacher LLM to generate instruction data. Similarly, WizardCoder applies the evol-instruct approach based on Code Alpaca, demonstrating a novel and effective method for the generation of instruction data. These applications underscore the potential of utilizing teacher LLMs to produce instructional content effectively, thereby offering an avenue for the creation of instruction data in the code domain. However, the quality of the data they generate heavily relies on the performance of the teacher LLM and the limited initial seeds, which often produces a large amount of duplicate instruction instances and reduce the effectiveness of instruction tuning. To break away from dependence on teacher LLMs, Octopack constructs a code instruction dataset leveraging the natural structure of Git commits. Nonetheless, ensuring the quality of data in git messages presents a considerable challenge, and the comprehensive screening of data through artificial filtering rules is often a complex task. Additionally, these endeavors are predominantly centered on traditional code generation tasks and lack the capability to produce detailed, task-specific instructions in multi-task scenarios. 
In this paper, we primarily focus on multiple code-related tasks, aiming to generate high-quality and diverse instructional data tailored to specific task requirements. Addressing the aforementioned challenges, we refine the instruction data by classifying the instruction instances to four universal code-related tasks in CodeXGLUE: 1) Code Summarization, 2) Code Generation, 3) Code Translation, 4) Code Repair and propose a widespread and versatile enhanced instruction generation method that could make full use of open source code data and stably generate high quality and diverse instruction data in multi-task scenarios. By this generation strategy, we obtain a dataset of 19,915 instruction instances across four code-related tasks, termed CodeSeaXDataset. 
To validate our approach, we train StarCoder, CodeLLaMa, and DeepseekCoder with our initial CodeSeaXDataset dataset and get WaveCoder. Following a thorough assessment on HumanEval, MBPP, HumanEvalPack benchmarks, experimental results show that our WaveCoder exhibits outstanding generalization ability based on widespread and versatile enhanced instruction tuning. Moreover, to further explore the improvements brought by data quality, we use GPT-4 to regenerate response for the instruction in CodeSeaXDataset. Fine-tuned with the enhanced 20K CodeSeaXDataset dataset, we obtain WaveCoder-Pro-6.7B which achieve 72.0%pass@1 on HumanEval and surpass open source Code LLMs but still behind SoTA Code LLM. Combining enhanced CodeSeaXDataset with WaveCoder-evol-codealpaca, the decontaminated Magicoder-evol-codealpaca dataset, we present WaveCoder-Ultra-6.7B, with SoTA generalization capabilities on multiple code-related tasks."
Eliciting Better Multilingual Structured Reasoning from LLMs through Code,2403.02567v2,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2403.02567v2.pdf,"An overview of our methods to improve multilingual structured reasoning. First (top), we create the translated code comments (Tcc) dataset, and use it in a fine-tuning setup. Second (bottom), we use the resulting LLM for inference on reasoning tasks. We find the most success with a code prompt format that bridges the representations between training and inference.","The development of large language models (LLM) has shown progress on reasoning, though studies have largely considered either English or simple reasoning tasks. To address this, we introduce a multilingual structured reasoning and explanation dataset, termed xSTREET, that covers four tasks across six languages. xSTREET exposes a gap in base LLM performance between English and non-English reasoning tasks. 
We then propose two methods to remedy this gap, building on the insight that LLMs trained on code are better reasoners. First, at training time, we augment a code dataset with multilingual comments using machine translation while keeping program code as-is. Second, at inference time, we bridge the gap between training and inference by employing a prompt structure that incorporates step-by-step code primitives to derive new facts and find a solution. Our methods show improved multilingual performance on xSTREET, most notably on the scientific commonsense reasoning subtask. Furthermore, the models show no regression on non-reasoning tasks, thus demonstrating our techniques maintain general-purpose abilities.","The ability to perform complex reasoning tasks is fundamental to human intelligence, where multiple steps of thought are required. Complex reasoning remains an open-problem for large language models (LLMs), despite some recent progress. Prior works consider complex reasoning tasks specified only in English. Such an English-centric perspective provides a limited assessment of the underlying reasoning capabilities of LLMs, given any specific language is largely a surface-form representation. This motivates our first inquiry into the multilingual complex reasoning capabilities of LLMs. 
We introduce the xSTREET reasoning and explanation dataset (as shown in Figure). xSTREET covers 4 tasks, and extends the English STREET benchmark to 5 additional diverse languages, inheriting the source 's expert annotations and structured graphs for reasoning steps (7.8 average steps/answer). The tasks cover arithmetic, logic and science commonsense problems. We perform machine translation for the training and development data splits, and also perform human post-editing to the test sets, to ensure a high quality multilingual benchmark. We use xSTREET to evaluate several LLMs, identifying the multilingual setting as significantly challenging. 
To remedy the non-English reasoning gap, we turn to the widely accepted hypothesis that LLMs trained on code are better at reasoning than those trained only on text. This code and reasoning hypothesis has been empirically corroborated by several papers. Our work takes a further step in investigating the extent to which this hypothesis holds for non-English tasks. We proceed with the insight that code can be leveraged as a structured framework to represent the underlying reasoning steps, regardless of the surface-form language of the task. We thus propose two techniques to elicit better multilingual complex reasoning from LLMs (as shown in Figure): at training time through a lightweight fine-tuning recipe on code, and at inference time using a novel code prompt format. 
In the LLM literature, many capabilities have been characterized as' emergent 'with model scale. Recent work on complex reasoning has thus focused on huge (175B+) and closed-source models. In our work, we instead aim to boost performance on far smaller open-source LLMs (7B). To make our findings reproducible, we release our benchmark. Our contributions are: 
 * We collect and release the first dataset for multilingual structured reasoning, xSTREET, covering 6 diverse languages and 4 tasks (5.5K entries total). 
 * At train time: we enhance reasoning capabilities of off-the-shelf LLMs by further training on program code data where code is interleaved with non-English comments. To this end, we augment a source code corpus through translating code comments and apply low-rank parameter-efficient fine-tuning (LoRA) to BLOOMZ. Our method is effective yet lightweight, while preserving general-purpose LM capabilities. 
 * At inference time: we design a code-like prompting format that mimics the structure of the reasoning tasks by interweaving function calls and multilingual text. We show this format outperforms several other prompt formats used. 
 * We evaluate multiple LLMs (BLOOMZ, GPT-3, Falcon-40b-instruct) on our benchmark, and show improved performance — even for top-performing models — across structured reasoning tasks in different languages. As our inference and training-time techniques are orthogonal, we show that they can be used in tandem to achieve the best performance. 
 * We perform qualitative and quantitative analysis to understand the roles of both the program code, and the code comments. Our findings taken together suggest that code elicits better multilingual structured reasoning by improving LLM' s adherence to the reasoning format."
Marathon: A Race Through the Realm of Long Context with Large Language Models,2312.09542v2,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2312.09542v2.pdf,"The overall accuracy of different models on Marathon. The x-axis represents the model, and the y-axis represents the average accuracy across all tasks. The different colors represent different methods of optimization.","With the advancement of large language models (LLMs) and the expansion of their context windows, existing long-context benchmarks fall short in effectively evaluating the models 'comprehension and reasoning abilities in extended texts. Moreover, conventional benchmarks relying on F1 metrics often inaccurately score responses: they may undervalue correct answers that differ from the reference responses and overvalue incorrect ones that resemble the reference texts. In response to these limitations, we introduce Marathon, a novel evaluation benchmark that adopts a multiple-choice question format. It is specifically designed to overcome the constraints of previous benchmarks and provide a rapid, precise, and unbiased appraisal of the long-context comprehension skills of large language models. We conducted comprehensive evaluations on the Marathon benchmark with a range of state-of-the-art LLMs and assessed the effectiveness of various optimization strategies tailored for long-context generation. We anticipate that the Marathon benchmark and its associated leaderboard will enable a more precise and equitable evaluation of LLMs' capabilities in understanding and reasoning over extended contexts. Marathon is available at <https: //github. com/Hambaobao/Marathon>.","In the rapidly evolving landscape of artificial intelligence technologies, the emergence of large language models (LLMs), as exemplified by ChatGPT, showcases notable capabilities. The influence of these models extends beyond the well-established ChatGPT, gaining increasing prominence across diverse sectors. Existing LLMs are typically built upon Transformer architectures, which demand memory and computational resources that grow quadratically with sequence length. Consequently, Transformer language models have historically been trained with relatively modest predetermined context windows. For instance, LLaMA employs a context size of 2048 tokens, while Llama2 utilizes a context size of 4096 tokens. However, the pre-defined size imposes constraints on LLMs in various applications, such as summarizing extensive documents or addressing lengthy questions. 
Significant research efforts have been devoted to extending the context length of LLMs. Due to the prohibitive expense of training LLMs with extended context lengths from scratch, the predominant studies have endeavored to enhance the capabilities of LLMs to comprehend long contexts through fine-tuning. These methods encompass extending the context window, incorporating recurrent memory, employing sparse attention mechanisms, and augmenting with external memory. Concurrently, an increasing multitude of benchmarks have been introduced to assess the long-context understanding capabilities of LLMs. LongBench stands out as the first bilingual, multi-task benchmark specifically designed for the assessment of long-context understanding. This dataset continues to depend on the F1 score, which evaluates the responses of LLMs against a predefined set of possible answers. LooGLE encompasses intricate long dependency tasks, including event timeline reordering, comprehension/reasoning, and computation. Nevertheless, the diverse nature of model-generated content introduces a challenge, as these predefined answers may not encompass all valid responses, thereby diminishing the precision of assessing model performance. There is a growing demand for high-quality benchmarks characterized by significantly longer text lengths and more challenging tasks, ensuring comprehensive evaluations. 
In this study, we introduce a novel benchmark named Marathon, designed for long-context understanding and reasoning. In particular, this benchmark is constructed upon the foundations established by LooGLE and LongBench. The contextual lengths within this benchmark span from 2K to over 260K characters. For each extensive context provided, an associated question is paired with four meticulously crafted response options. These options have been carefully reviewed by humans and contain only one correct answer, with the remaining options designed to be highly misleading. This design makes the Marathon benchmark a particularly challenging one. The task for the large language model is to discern the accurate response option based on the extensive context provided. 
The main contributions of this work are threefold: 
 
 * We introduce a novel multiple-choice long context benchmark that comprehensively evaluates the long context understanding and reasoning capabilities across 10 leading open-source large language models, as well as ChatGPT and GPT-4, covering six diverse types of tasks. 
 * We compare two prevalent methods for long context optimization (Prompt Compression and Retrieval Augmented Generation) along with two leading embedding models, assessing their impact on enhancing the long context reasoning abilities of large language models. 
 * Our findings reveal a general tendency among current open-source large language models to generate lengthier responses, accompanied by a notable deficiency in following instructions accurately."
Improving Hateful Meme Detection through Retrieval-Guided Contrastive Learning,2311.08110v3,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2311.08110v3.jpg,"Illustrative examples from. The meme on the left is hateful, the middle one is a benign image confounder, and the right one is a benign text confounder. We show HateCLIPper's prediction below each meme. HateCLIPper misclassifies the hateful meme on the left as benign.","Hateful memes have emerged as a significant concern on the Internet. Detecting hateful memes requires the system to jointly understand the visual and textual modalities. Our investigation reveals that the embedding space of existing CLIP-based systems lacks sensitivity to subtle differences in memes that are vital for correct hatefulness classification. We propose constructing a hatefulness-aware embedding space through retrieval-guided contrastive training. Our approach achieves state-of-the-art performance on the HatefulMemes dataset with an AUROC of 87.0, outperforming much larger fine-tuned large multimodal models. We demonstrate a retrieval-based hateful memes detection system, which is capable of identifying hatefulness based on data unseen in training. This allows developers to update the hateful memes detection system by simply adding new examples without retraining — a desirable feature for real services in the constantly evolving landscape of hateful memes on the Internet.","The growth of social media has been accompanied by a surge in hateful content. Hateful memes, which consist of images accompanied by texts, are becoming a prominent form of online hate speech. This material can perpetuate stereotypes, incite discrimination, and even catalyse real-world violence. To provide users the option of not seeing it, hateful memes detection systems have garnered significant interest in the research community. 
Correctly detecting hateful memes remains difficult. Previous literature has identified a prominent challenge in classifying ""confounder memes"", in which subtle differences in either image or text may lead to a completely different meaning. As shown in Figure, the top left and top middle memes share the same caption. However, one of them is hateful and the other benign depending on the accompanying images. Confounder memes resemble real memes on the Internet, where the combined message of images and texts contribute to their hateful nature. Even state-of-the-art models, such as HateCLIPper, exhibit limited sensitivity to nuanced hateful memes. 
We find that a key factor contributing to misclassification is that confounder memes are located in close proximity in the embedding space due to the similarity of text or image content. For instance, HateCLIPper 's embedding of the confounder meme in Figure has a high cosine similarity score with the left anchor meme even though they have opposite meanings. This poses challenges for the classifier to distinguish harmful and benign memes. 
We propose ""Retrieval-Guided Contrastive Learning"" (RGCL) to learn hatefulness-aware vision and language joint representations. We align the embeddings of same-class examples that are semantically similar with pseudo-gold positive examples and separate the embeddings of opposite-class examples with hard negative examples. We dynamically retrieve these examples during training and train with a contrastive objective in addition to cross-entropy loss. RGCL achieves higher performance than state-of-the-art large multimodal systems on the HatefulMemes dataset with far fewer model parameters. We demonstrate that the RGCL embedding space enables the use of K-nearest-neighbor majority voting classifier. The encoder trained on HarMeme can be applied to HatefulMemes without additional training while maintaining high AUC and accuracy using the KNN majority voting classifier, even outperforming large multi-modal models under similar settings. This allows efficient transfer and update of hateful memes detection systems to handle the fast-evolving landscape of hateful memes in real-life applications. Our contributions are: 
 
 * We propose RGCL for hateful memes detection which learns a hatefulness-aware embedding space via an auxiliary contrastive objective with dynamically retrieved examples. We propose to leverage novel pseudo-gold positive examples to improve the quality of positive examples. 
 * Our proposed approach achieves state-of-the-art performance on HatefulMemes and the HarMeme. We show RGCL' s capability across various domains of meme classification tasks on MultiOFF, Harm-P and Memotion7K. 
 * Our retrieval-based KNN majority voting classifier facilitates straightforward updates and extensions of hateful meme detection systems across various domains without retraining. With RGCL training, the retrieval-based classifier demonstrates strong cross-dataset generalizability, making it suitable for real services in the dynamic environment of online hateful memes."
Agent-Pro: Learning to Evolve via Policy-Level Reflection and Optimization,2402.17574v3,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2402.17574v3.pdf,"For interactive tasks, e.g., imperfect-information games, we propose a versatile agent framework capable of self-learning and evolving. Firstly, our agent constructs beliefs about itself and the environment. Then it autonomously updates its prompts through policy-level reflection on past trajectories and beliefs, evolving a better behavioral strategy.","Large Language Models (LLMs) exhibit robust problem-solving capabilities for diverse tasks. However, most LLM-based agents are designed as specific task solvers with sophisticated prompt engineering, rather than agents capable of learning and evolving through interactions. These task solvers necessitate manually crafted prompts to inform task rules and regulate LLM behaviors, inherently incapacitating to address complex dynamic scenarios e.g., large interactive games. In light of this, we propose Agent-Pro: an LLM-based Agent with Policy-level Reflection and Optimization that can learn a wealth of expertise from interactive experiences and progressively elevate its behavioral policy. Specifically, it involves a dynamic belief generation and reflection process for policy evolution. Rather than action-level reflection, Agent-Pro iteratively reflects on past trajectories and beliefs, ""fine-tuning"" its irrational beliefs for a better policy. Moreover, a depth-first search is employed for policy optimization, ensuring continual enhancement in policy payoffs. Agent-Pro is evaluated across two games: Blackjack and Texas Hold'em, outperforming vanilla LLM and specialized models. Our results show Agent-Pro can learn and evolve in complex and dynamic scenes, which also benefits numerous LLM-based applications.","Designing a human-level agent with robust problem-solving abilities has long been a vision in the academic community. This necessitates the agent to possess learning and generalization capabilities across a diverse array of tasks. The advent of Large Language Models (LLMs) has shed light on this vision, especially they can be rapidly generalized across a wide range of tasks with only a few demonstrations. Benefiting from this, many systems built upon LLMs have showcased markedly enhanced performance such as question-answering, code generation, and real-world application. 
Despite these achievements, building a human-level agent remains a challenging endeavor. First, most LLM-based agents are designed for specific tasks through sophisticated prompts, including detailed task descriptions and behavioral specifications. However, numerous real-world tasks, e.g., business, company negotiations, and security, are more intricate with imperfect information, necessitating laborious efforts to design strategic behavior. 
Second, most LLM-based agents do not consider interacting with task scenarios, and more critically, cannot learn from past experiences and evolve their behavioral strategies during interactions. In contrast, humans often learn and adjust their behaviors through interaction, especially in novel scenarios. In light of these, a promising yet under-explored topic emerges: Can LLM-based agents learn and elevate behavioral strategies by interacting with the environment like humans? It should be an indispensable ability of a human-level agent. 
Recently, numerous studies undertake intriguing explorations, e.g., utilizing feedback for self-correction at the action-level. Besides, several efforts also explore deploying LLM in interactive games, including StarCraft, Minecraft, strategy-based gaming. 
Similarly, we first evaluate LLM-based agents with the self-correction strategy in dynamic interactive scenarios, such as multi-player Texas Hold 'em, which is a zero-sum game with imperfect information. However, we observe that it loses most of the rounds to its opponents, even the most advanced LLMs. Upon examining its reasoning thoughts and actions, we find that it often adopts irrational behaviors and is unable to deduce effective strategies from long action sequences. 
To answer the above question, the Theory of Mind (ToM) may provide some insight. In this framework, each human develops perceptions of himself (self-belief) and the external environment (social-belief) in the social context, and then grounds their decisions on these beliefs, or adjusts incorrect beliefs in response to external feedback. Inspired by this, we advocate Agent-Pro: a LLM-based Agent with Policy-level Reflection and Optimization. Agent-Pro is endowed with the capacity to learn and evolve within environments, i.e., autonomously reflect on past experiences, calibrate its beliefs about itself and the environment, and optimize its behavior policy without parameter tuning. 
Concretely, as shown in, an LLM-based agent involves an LLM as the foundational model and some instructions in the prompt to regulate its behavior (policy). Upon observing partial information from the scenarios, Agent-Pro first updates its self-belief and world-belief, then makes decisions based on these beliefs. After exploring tasks, Agent-Pro performs a policy-level reflection and optimization on past trajectories, beliefs, and results. It autonomously ""fine-tunes"" its beliefs, searches for useful prompt instructions, and consolidates them into a new behavior policy. 
The experiments in two zero-sum games, Blackjack and Texas Hold' em, demonstrate that Agent-Pro, after evolution, can defeat vanilla LLMs and specialized models, improving the game 's payoffs. It indicates that Agent-Pro enhances its capabilities through interaction and reflection without human guidance. As depicted in, the initial prompt is quite simple (Left Bottom), but after learning and evolution, the Agent-Pro generates many practical instructions (Right Bottom). For instance, Agent-Pro records estimations of each opponent' s style in Task Description and adds specific Goals, Strategies in Behavior Policy. 
Our Agent-Pro is different from previous strategies, like Reflexion. Firstly, Policy-level reflection is designed for policy updating in long-horizon tasks. It is aimed at long-horizon policy updating rather than immediate action correction. The input is a sequence of actions and delayed feedback, while the output is an optimized strategy, rather than a specific action. Therefore, policy-level reflection corrects irrational beliefs and optimizes the old policy into the new one. As introduced in, our policy-level reflection includes belief calibration, policy updates by refining behavioral guidelines and world modeling, and policy verification. 
Besides, we innovatively distill long-term memory into Behavioral Guidelines and World Models through prompt optimization. Most previous strategies store historical experience as verbal long-term memory and use it for text-based reasoning. In contrast, we further construct an optimizable policy from long-term interactions, i.e., Behavioral Guidelines and Environmental Models. This includes self-summarized game objectives and rules, effective strategies derived from reflection, and demonstrative trajectories. The contributions of our work are as follows: 
 
 * We introduce Agent-Pro, a framework capable of learning and evolving within interactive games, empowering LLM-based agents to efficiently adapt to more complex dynamic tasks. 
 * We devise a belief-aware decision-making process with self and world-belief, enhancing its capabilities for intricate tasks, i.e., generating more rational actions in interactive scenarios. 
 * We utilize policy-level reflection and optimization to iteratively update prompt instructions, which empower Agent-Pro to progressively evolve from a novice to a skilled veteran with many strategic behaviors. 
 * After learning, Agent-Pro is evaluated in multiplayer games and defeats specialized models, gaining notable progress. It develops strategic skills like humans, e.g., actively cutting losses, bluffing, or disguising to influence others. 
Not just in card games, similar scenarios abound in the real world as well. Through self-learning and evolution, Agent-Pro can enhance deployment effectiveness in those scenarios, expanding the capability boundaries of LLM-based agents notably."
Your Transformer is Secretly Linear,2405.12250v1,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2405.12250v1.png,Linearity profiles for different open source models. Normalized depth is the layer index divided by the total depth.,"This paper reveals a novel linear characteristic exclusive to transformer decoders, including models such as GPT, LLaMA, OPT, BLOOM and others. We analyze embedding transformations between sequential layers, uncovering a near-perfect linear relationship (Procrustes similarity score of 0.99). However, linearity decreases when the residual component is removed due to a consistently low output norm of the transformer layer. Our experiments show that removing or linearly approximating some of the most linear blocks of transformers does not affect significantly the loss or model performance. Moreover, in our pretraining experiments on smaller models we introduce a cosine-similarity-based regularization, aimed at reducing layer linearity. This regularization improves performance metrics on benchmarks like Tiny Stories and SuperGLUE and as well successfully decreases the linearity of the models. This study challenges the existing understanding of transformer architectures, suggesting that their operation may be more linear than previously assumed.","Transformers have revolutionized the field of natural language processing, offering unprecedented advances in a wide range of applications. However, despite their widespread adoption and success, the complex work of these models remains an area of active research. One aspect that has received less attention is the inherent linearity of intermediate embedding transformations within these architectures. In this study, we embark on an in-depth analysis of the linearity properties of transformers, specifically focusing on decoders, and explore its implications during the pretraining and fine-tuning phases. 
Our investigation reveals a surprising discovery: the embedding transformations between sequential layers in transformer decoders exhibit almost linear properties. This observation is quantified using Procrustes similarity analysis, demonstrating a near-perfect linearity score of 0.99. Such a discovery not only challenges the traditional understanding of transformer architectures but also opens new opportunities for model optimization and efficiency. 
Based on this insight, we introduce several new contributions to the field: 
 
 * Extensive analysis of the linearity properties of transformer decoders and its dynamics at the pretraining and fine-tuning stages. 
 * The development of new algorithms for depth pruning of transformer decoders, allowing to remove the most linear layers without a significant loss in performance. 
 * A novel distillation technique that involves pruning, replacing certain layers with linear approximations, and then distilling layer-wise embeddings to preserve model performance. 
 * Introducing a new regularization approach for pretraining based on the cosine similarity, designed to decrease the layer linearity. This method not only enhances the performance of transformer models on benchmark datasets such as SuperGLUE and TinyStories, but also improves the expressiveness of embeddings, as evidenced by linear probing tasks. 
With our findings, we are paving the way for more computationally efficient transformer architectures without sacrificing their effectiveness, thereby addressing one of the critical challenges in deploying these models."
CodeScope: An Execution-based Multilingual Multitask Multidimensional Benchmark for Evaluating LLMs on Code Understanding and Generation,2311.08588v3,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2311.08588v3.pdf,"Diagrams illustrating four code understanding tasks, including the input and expected output for each task.","Large Language Models (LLMs) have demonstrated remarkable performance on assisting humans in programming and facilitating programming automation. However, existing benchmarks for evaluating the code understanding and generation capacities of LLMs suffer from severe limitations. First, most benchmarks are insufficient as they focus on a narrow range of popular programming languages and specific tasks, whereas real-world software development scenarios show a critical need to implement systems with multilingual and multitask programming environments to satisfy diverse requirements. Second, most benchmarks fail to consider the actual executability and the consistency of execution results of the generated code. To bridge these gaps between existing benchmarks and expectations from practical applications, we introduce CodeScope, an execution-based, multilingual, multitask, multidimensional evaluation benchmark for comprehensively measuring LLM capabilities on coding tasks. CodeScope covers 43 programming languages and eight coding tasks. It evaluates the coding performance of LLMs from three dimensions (perspectives): length, difficulty, and efficiency. To facilitate execution-based evaluations of code generation, we develop MultiCodeEngine, an automated code execution engine that supports 14 programming languages. Finally, we systematically evaluate and analyze eight mainstream LLMs and demonstrate the superior breadth and challenges of CodeScope for evaluating LLMs on code understanding and generation tasks compared to other benchmarks. The CodeScope benchmark and code are publicly available at <https: //github. com/WeixiangYAN/CodeScope>.","Driven by advances in deep learning and NLP, LLMs have demonstrated outstanding proficiency in various generation and understanding tasks. However, existing benchmarks for evaluating LLMs mainly focus on NLP tasks, such as common sense reasoning, academic examination, and authenticity verification. Existing evaluation methods are significantly insufficient in terms of evaluating completeness and comprehensiveness for code understanding and generation capabilities of LLMs. Firstly, many code LLMs, such as CodeT5+, WizardCoder, and Code LLaMA, employ their own specific single-task evaluation datasets, making it infeasible to comprehensively compare the performance of various LLMs on code understanding and generation tasks on a unified standard. 
Secondly, existing datasets mostly evaluate LLMs on code tasks for a narrow range of popular programming languages, with a focus on Python and single program synthesis tasks. However, software development often involves multiple programming languages, each following different programming paradigms such as object-oriented, functional, and procedural. Evaluating LLMs within a multilingual framework can reveal their ability to generalize across various languages and paradigms. Moreover, the complementarity between multiple tasks facilitates a comprehensive evaluation of the overall performance of LLMs, ensuring that an LLM is not over-optimized for a specific task and can maintain strong performance across diverse tasks. Importantly, multitask settings more accurately simulate the various requirements and challenges faced in real-world software development practices and hence better test the generalizability of LLMs. 
Thirdly, most studies (e.g., widely used benchmarks CodeXGLUE and XLCoST) rely on matching-based evaluation metrics, such as BLEU or CodeBLEU, to measure the quality of generated code. However, these metrics may not reflect the practical applicability of the code, as they only compare the surface form similarity between the generated code and the reference code. The ultimate goal of code generation is to produce code that can execute correctly and accomplish specific tasks. Therefore, execution-based metrics, which evaluate the functionality and correctness of the generated code by running it on test cases or comparing its output with the expected output, are more reliable and informative. 
To address these limitations, we propose CodeScope, a benchmark that evaluates the coding proficiency of LLMs using execution-based metrics in a multilingual and multitask setting. CodeScope consists of eight tasks for code understanding and generation, covering 43 programming languages with an average of 13 languages per task. The task descriptions are summarized in Table. We also conduct comprehensive evaluations of LLMs across three dimensions (that is, multidimensional): Length, Difficulty, and Efficiency. Length measures the ability to process code of different lengths; Difficulty evaluates proficiency in solving increasingly complex programming challenges; and Efficiency examines the execution speed and resource consumption of the code generated by LLMs for a specific Code Optimization task. 
To support CodeScope, we develop a Multilingual Code Execution Engine, MultiCodeEngine, which extends the ExecEval engine to accommodate 14 programming languages for code generation tasks. We also establish eight strong baselines for each task to facilitate comprehensive comparisons of coding capabilities of LLMs. We expect these explorations will provide a deep understanding of the strengths and limitations of LLMs on code understanding and generation tasks and provide valuable guidance for future research directions. Our contributions can be summarized as follows: 
 
 * CodeScope benchmark: We built the first-ever comprehensive benchmark for evaluating LLMs on code understanding and generation tasks, CodeScope, which covers the largest number of programming languages (43 in total) and comprises the most comprehensive spectrum of diverse code understanding and generation tasks (eight tasks in total) to date. This benchmark evaluates the actual execution of the generated code, facilitated by MultiCodeEngine, a multilingual code execution engine supporting 14 programming languages. 
 * Multidimensional fine-grained evaluation: We comprehensively evaluate the performance of LLMs on eight tasks from three dimensions, namely, length (i.e., length of code required to solve the problem); difficulty (i.e., complexity of programming problems); and efficiency (i.e., execution efficiency of generated code). 
 * Comprehensive evaluations and in-depth analyses: We evaluate and compare the coding capabilities of eight mainstream LLMs and establish strong baselines for each task. We conduct comprehensive validations and analyses of the utility of the CodeScope benchmark."
Digital Socrates: Evaluating LLMs through Explanation Critiques,2311.09613v3,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2311.09613v3.pdf,"Given a multiple-choice question (together with the answer options and correct answer), as well as a model-generated reasoning chain and answer, our system Digital Socrates gives a critique of the model-generated explanation. In its critiques, Digital Socrates provides localized feedback on where and why reasoning chains are flawed (focusing on the main flaw, if any), accompanied by general and fine-grained suggestions to address the identified flaw, providing nuance and interpretability to the critiques.","While LLMs can provide reasoned explanations along with their answers, the nature and quality of those explanations are still poorly understood. In response, our goal is to define a detailed way of characterizing the explanation capabilities of modern models and to create a nuanced, interpretable explanation evaluation tool that can generate such characterizations automatically, without relying on expensive API calls or human annotations. Our approach is to (a) define the new task of explanation critiquing - identifying and categorizing any main flaw in an explanation and providing suggestions to address the flaw, (b) create a sizeable, human-verified dataset for this task, and (c) train an open-source, automatic critique model (called Digital Socrates) using this data. Through quantitative and qualitative analysis, we demonstrate how Digital Socrates is useful for revealing insights about student models by examining their reasoning chains, and how it can provide high-quality, nuanced, automatic evaluation of those model explanations for the first time. Digital Socrates thus fills an important gap in evaluation tools for understanding and improving the explanation behavior of models.","Large language models (LLMs) have demonstrated promising end-task performance on a range of tasks. These models, given their text-generation abilities, can also be prompted or trained to externalize their reasoning as a window into their reasoning capabilities. Despite promising end-task performance, examining LLMs 'reasoning chains reveals gaps in the correctness of their factual knowledge and the coherence of their reasoning. Such efforts delving deeper into the quality of model-generated intermediate reasoning chains enable us to advance our understanding of LLMs' strengths and weaknesses in different tasks beyond measuring their performance on standard benchmarks. 
In existing NLP works, however, judging the quality of LLMs 'intermediate reasoning chains is met with several challenges. Early practices for determining the quality of such intermediate generations include (1) reporting end-task accuracy and (2) sampling a subset to perform human annotations on e.g., . Using end-task performance as a proxy for the quality of intermediate reasoning can be problematic as LLMs' final answers can be unfaithful to the generated intermediate reasoning, whereas relying on human annotations is labor-intensive and expensive. 
To tackle such challenges, several automatic measurements have been proposed. One category focuses on evaluating model-generated text based on a given reference text y. Another proposes numerical metrics like ROSCOE and ReCEval scores as a summary of reasoning quality. Others build models that generate free-form critiques and refinement suggestions e.g., . Our work builds upon these previous efforts, introducing a way of automatically evaluating reasoning chains that (1) focuses on the intrinsic quality of the reasoning chains, moving away from the reliance on comparing to any reference reasoning chain; (2) localizes where the reasoning went wrong and provides interpretable feedback on why that part of the reasoning chain should be revised; and (3) uses a semi-structured format useful for gaining both quantitative and qualitative insights about the reasoning chain quality. 
To operationalize this, our approach and contributions are thus as follows: 
 
 * We define the task of explanation critiquing. 
 * We create DS Critique Bank, a sizeable, human-verified dataset for the task, both to train critique models and to compare against explanation capabilities of future models. 
 * We train and release a high-performing, open-source critique model, Digital Socrates, that does not rely on expensive API calls or human annotations. 
 * We demonstrate the usefulness of Digital Socrates critiques. 
 By providing high-quality, nuanced automatic evaluation of explanations, Digital Socrates fills an important gap in evaluation tools for the community. We make our dataset and model publicly available at <https: //allenai. org/data/digital-socrates>."
A Deep Dive into the Trade-Offs of Parameter-Efficient Preference Alignment Techniques,2406.04879v1,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2406.04879v1.pdf,"Performance comparison for helpful and harmless benchmarks when models are aligned using QLoRA over HH-RLHF (in red) and BeaverTails (in blue). We observe better performance when using a more informative and high-quality preference alignment dataset, albeit it is often overfitting for non-instruction tuned models when aligned using DPO (Section).","Large language models are first pre-trained on trillions of tokens and then instruction-tuned or aligned to specific preferences. While pre-training remains out of reach for most researchers due to the compute required, fine-tuning has become affordable thanks to parameter-efficient methods such as LoRA and QLoRA. Alignment is known to be sensitive to the many factors involved, including the quantity and quality of data, the alignment method, and the adapter rank. However, there has not yet been an extensive study of their effect on downstream performance. To address this gap, we conduct an in-depth investigation of the impact of popular choices for three crucial axes: (i) the alignment dataset (HH-RLHF and BeaverTails), (ii) the alignment technique (SFT and DPO), and (iii) the model (LLaMA-1, Vicuna-v1.3, Mistral-7b, and Mistral-7b-Instruct). Our extensive setup spanning over 300 experiments reveals consistent trends and unexpected findings. We observe how more informative data helps with preference alignment, cases where supervised fine-tuning outperforms preference optimization, and how aligning to a distinct preference boosts performance on downstream tasks. Through our in-depth analyses, we put forward key guidelines to help researchers perform more effective parameter-efficient LLM alignment.","Large Language Models (LLMs) have achieved human-like performance across various tasks such as summarization, commonsense reasoning, and open-ended generation. These LLMs have billions of parameters and are pre-trained on trillions of tokens scraped from the web. A lucrative utilization of LLMs is in the form of autonomous agents, to make them follow user instructions and adhere to specific preference requirements. However, the pre-trained models are often incapable of following instructions, and they need to be aligned using specially curated preference alignment datasets and methods for generalization. 
Alignment methods either involve fine-tuning the pre-trained model using auto-regressive language modeling over the ground truth completions (supervised fine-tuning or SFT) or using specialized alignment methods such as reinforcement learning from human feedback (RLHF), direct preference optimization (DPO), or prompt tuning. However, applying these methods to the full models is computationally expensive due to their large sizes. Parameter-efficient training (PEFT) methods such as Low-Rank Adaptation (LoRA) and QLoRA have achieved comparable performance to full fine-tuning of LLMs at a much lower cost. This has enabled researchers to experiment with preference alignment datasets, methods, and models on systems with a single GPU. However, alignment is sensitive to numerous factors and design choices involved in the training. 
The design choices to align LLMs fit into one of the following three broad, crucial axes: (i) the quality and quantity of the alignment dataset, (ii) the preference alignment method, and (iii) the nature of the base model. Given the increasing interest in preference alignment, an in-depth analysis of the effect of these axes on downstream performance is required. To the best of our knowledge, no extensive study has investigated them, especially in a PEFT setting. We fill this important gap by attempting to answer various key research questions across these three axes: 
Alignment Dataset How do the informativeness and quality, number of samples, and content of the preference dataset impact the downstream performance? 
Alignment Method How do different alignment methods affect pre-trained and instruction-tuned models over complementary preferences? 
Nature of the Base Model How does the downstream performance compare between pre-trained models, instruction-tuned models, and their merged variants? 
Though our study covers all three axes, given the rapidly growing number of options for each, we restrict the studied choices to the most popular ones. Specifically, we perform our experiments on two commonly used preferences in literature to study alignment trade-offs: harmlessness and helpfulness. We use the (i) two most popular preference alignment datasets with harmlessness and helpfulness annotations: HH-RLHF and BeaverTails, with (ii) the two most widely-used alignment methods: SFT and DPO, and (iii) two commonly used LLMs, LLaMA-1 and Mistral-7b along with their instruction-tuned versions, Vicuna-v1.3 and Mistral-7b-Instruct. For an in-depth study of PEFT methods, we conducted all experiments using both LoRA and QLoRA. Our extensive analysis across these core axes reveals certain consistent trends and unexpected findings, as shown in Table. We hope that consolidating the key findings into guidelines will benefit the community in conducting impactful research toward LLM alignment. Our contributions can be summarized as: 
 
 * We provide an in-depth study into the trade-offs of parameter-efficient preference alignment training, particularly when using LoRA and QLoRA. 
 * We conduct over 300 experiments across three core axes of preference alignment: the dataset, the alignment method, and the model. 
 * Through experiments on 5 evaluation benchmarks across harmlessness and helpfulness, we consolidate our key findings as guidelines for more effective preference alignment practices."
Zero-Shot Cross-Domain Dialogue State Tracking via Dual Low-Rank Adaptation,2407.21633v1,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2407.21633v1.pdf,Example of a multi-domain dialogue from MultiWOZ dataset.,"Zero-shot dialogue state tracking (DST) seeks to enable dialogue systems to transition to unfamiliar domains without manual annotation or extensive retraining. Prior research has approached this objective by embedding prompts into language models (LMs). Common methodologies include integrating prompts at the input layer or introducing learnable variables at each transformer layer. Nonetheless, each strategy exhibits inherent limitations. Prompts integrated at the input layer risk underutilization, with their impact potentially diminishing across successive transformer layers. Conversely, the addition of learnable variables to each layer can complicate the training process and increase inference latency. To tackle the issues mentioned above, this paper proposes Dual Low-Rank Adaptation (DualLoRA), a plug-and-play architecture designed for zero-shot DST. DualLoRA incorporates two distinct Low-Rank Adaptation (LoRA) components, targeting both dialogue context processing and prompt optimization, to ensure the comprehensive influence of prompts throughout the transformer model layers. This is achieved without incurring additional inference latency, showcasing an efficient integration into existing architectures. Through rigorous evaluation on the MultiWOZ and SGD datasets, DualLoRA demonstrates notable improvements across multiple domains, outperforming traditional baseline methods in zero-shot settings. Our code is accessible at: <https: //github. com/suntea233/DualLoRA>.","Task-oriented dialogue (TOD) systems, designed for specific tasks like restaurant reservations or travel bookings, rely on dialogue state tracking (DST) to interpret user intents as slot-value pairs across dialogue turns, essential for managing multi-domain conversations. Figure shows the change of dialogue states in a multi-turn dialogue crossing multiple domains. An ideal system seamlessly transitions to new domains with minimal training, overcoming the resource-intensive process of data collection and annotation for new domains. Zero-shot learning emerges as a solution, enabling TOD systems to adapt to unseen domains by leveraging existing knowledge, thus bypassing the need for extensive domain-specific data. 
The advancement in language models (LMs) has significantly enhanced the exploration of zero-shot learning approaches within the context of dialogue state tracking tasks. Adaptation to novel domains is frequently achieved through the incorporation of prompts into transformer-based models. The current method can be classified into two categories. 
The first type of methods incorporate prompts, e.g.,slot description, in the first transformer layer. A common practice within this approach involves the concatenation of prompts subsequent to the input, as shown in Fig. Despite the straightforward nature of their implementation, these strategies exhibit suboptimal utilization of prompts. This refers to the phenomenon where the influence of prompts on the model 's output progressively diminishes with each additional transformer layer. Figure shows an example, where deep transformer layers, compared with shallow transformer layers, exhibit a reduced allocation of attention to other tokens within the dialogue context or prompt, in the token encoding process. This issue is further exacerbated in the context of dialogue state tracking, where the dialogue' s progression causes a rapid escalation in the context length — often extending to 300-400 tokens or more, in stark contrast to the prompts 'modest length of 10-20 tokens. Such a diminishing effect of prompts can severely impair the system' s performance in zero-shot learning scenarios, where the ability to discern and leverage domain-specific knowledge becomes critically important. 
The second category of methodologies incorporates a learnable vector, initialized by prompts, into each layer of the transformer architecture, as shown in Fig. This technique facilitates the model 's capacity to consistently engage with the prompt information across all layers, ensuring the prompt' s considerations are integrated throughout the encoding process. However, these methods present certain limitations. Direct concatenation of prompts at each layer might introduce extraneous noise, adversely affecting the efficacy of model training. Furthermore, this method of concatenation can result in augmented inference latency of the model. Consequently, devising a strategy to effectively utilize prompts without impeding the standard input-output dynamics of the model and without exacerbating inference time emerges as a critical challenge in the field. 
To address the aforementioned challenges, we propose a novel plug-and-play framework designed for zero-shot dialogue state tracking, termed Dual Low-Rank Adaptation (DualLoRA). This architecture is characterized by the integration of two Low-Rank Adaptation (LoRA) components: one dedicated to processing dialogue context and the other to refining prompts, as shown in Fig. DualLoRA operates by receiving prompts and applying modifications within the attention layers of the transformer model, subsequently integrating this modified output with the transformer's original output. Through the implementation of this framework across each layer, DualLoRA ensures that the influence of the prompts is perpetuated throughout the entire depth of the model. Notably, this architecture does not incur additional latency during the inference phase, maintaining efficiency in model performance. Furthermore, DualLoRA requires few parameter adjustments and demonstrates exceptional transferability across different domains. 
Comprehensive experimental evaluations were performed utilizing the MultiWOZ and SGD datasets to assess the efficacy of the proposed methodology. Comparative analyses reveal that our approach yields enhanced Joint Goal Accuracy (JGA) metrics across various domains within both the MultiWOZ and SGD datasets, relative to established baseline methodologies. 
The structure of this paper is organized as follows: Section 2 offers an overview of the literature pertinent to this research area. Section 3 delineates a detailed exposition of our proposed model. Section 4 presents a detailed account of the experimental outcomes and the specifics of the implementation. Section 5 encapsulates the conclusions derived from this study."
SEER: Facilitating Structured Reasoning and Explanation via Reinforcement Learning,2401.13246v4,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2401.13246v4.pdf,"An example of structured explanation. Given a hypothesis h (a declarative sentence derived from a question-answer pair) and a set of facts (or corpus), the goal is to generate a structured explanation, which delineates the reasoning process from facts to the hypothesis.","Elucidating the reasoning process with structured explanations from question to answer is crucial, as it significantly enhances the interpretability, traceability, and trustworthiness of question-answering (QA) systems. However, structured explanations demand models to perform intricately structured reasoning, which poses great challenges. Most existing methods focus on single-step reasoning through supervised learning, ignoring logical dependencies between steps. Moreover, existing reinforcement learning (RL) based methods overlook the structured relationships, underutilizing the potential of RL in structured reasoning. In this paper, we propose Seer, a novel method that maximizes a structure-based return to facilitate structured reasoning and explanation. Our proposed structure-based return precisely describes the hierarchical and branching structure inherent in structured reasoning, effectively capturing the intricate relationships between different reasoning steps. In addition, we introduce a fine-grained reward function to meticulously delineate diverse reasoning steps. Extensive experiments show that Seersignificantly outperforms state-of-the-art methods, achieving an absolute improvement of 6.9%over RL-based methods on EntailmentBank, a 4.4%average improvement on STREET benchmark, and exhibiting outstanding efficiency and cross-dataset generalization performance. Our code is available at <https: //github. com/Chen-GX/SEER>.","Navigating machines to understand and articulate the thought process from posing a question to arriving at an answer has been a long-term pursuit in the AI community. Current QA explainable systems adeptly furnish brief supporting evidence. However, they often fail to clarify the reasoning process from prior knowledge to the derived answer. By elucidating the reasoning process of answers generation from the language models, we can greatly improve interpretability, trustworthiness, and debuggability. As illustrated in Figure, when generating answers for the question ""Which natural material is best for making a table? "", the reasoning process with structured explanations, such as entailment trees or reasoning graphs, explains why ""sturdy wood"" is the best answer. 
Deriving such complex structured explanations poses a great challenge. Previous methods consider structured explanations as linearized sequences and generate the entire reasoning process in one go. However, these methods lack controllability and may hallucinate unreliable reasoning steps. To address these concerns, recent studies decompose structured explanations and focus on single-step reasoning via supervised learning. Nevertheless, this kind of approach may not always yield optimal results as they fail to consider the interdependencies between different steps. FAME attempts to compensate for these shortcomings by leveraging Monte-Carlo planning, which significantly increases the running time and inadvertently explores numerous ineffective steps (as shown in Table). Furthermore, FAME still concentrates on isolated single-step reasoning, which lacks support for structured reasoning. As a general framework for solving sequential decision-making problems, reinforcement learning (RL) is employed in RLET to enhance multi-step reasoning. However, RLET defines the return (a. k. a. cumulative reward) using the standard chain structure, thus lacking the ability to represent the tree or graph logical structures inherent in structured reasoning. As a result, the potential of RL for structured reasoning is not fully exploited. 
To address the above issues, we propose Seer, a novel method that facilitates Structured rEasoning and Explanation via Reinforcement learning. In structured reasoning, we observe that the logical dependencies between different steps no longer follow a chained trajectory but instead adhere to the inherent tree or graph structure. Therefore, we propose the structure-based return to precisely describe a tree or graph logical structure, effectively capturing the complex interdependencies between different steps. Additionally, we refine the reward function to meticulously delineate diverse reasoning steps, specifically targeting redundant ones that do not contribute to the final structured explanations. Through experiments in Sec. , we find that redundant steps represent the exploration in the environment, and appropriate penalization contributes to improved reasoning performance. 
Our contributions are summarized as follows: 
∙ We propose Seer, a novel RL-based method that facilitates structured reasoning and explanation. To our knowledge, Seeris the first general framework that accommodates scenarios of chained, tree-based, and graph-based structured reasoning. 
∙ We propose the structure-based return to address the intricate interdependencies among different reasoning steps, effectively stimulating the potential of RL in structured reasoning. 
∙ We conduct extensive experiments to demonstrate the superiority of Seerover state-of-the-art methods. Our method facilitates the effectiveness and efficiency of structured reasoning and exhibits outstanding cross-dataset generalization performance."
Are AI-Generated Text Detectors Robust to Adversarial Perturbations?,2406.01179v2,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2406.01179v2.pdf,An example of adversarial perturbation to a RoBERTa-based AIGT detector.,"The widespread use of large language models (LLMs) has sparked concerns about the potential misuse of AI-generated text, as these models can produce content that closely resembles human-generated text. Current detectors for AI-generated text (AIGT) lack robustness against adversarial perturbations, with even minor changes in characters or words causing a reversal in distinguishing between human-created and AI-generated text. This paper investigates the robustness of existing AIGT detection methods and introduces a novel detector, the Siamese Calibrated Reconstruction Network (SCRN). The SCRN employs a reconstruction network to add and remove noise from text, extracting a semantic representation that is robust to local perturbations. We also propose a siamese calibration technique to train the model to make equally confident predictions under different noise, which improves the model's robustness against adversarial perturbations. Experiments on four publicly available datasets show that the SCRN outperforms all baseline methods, achieving 6.5%-18.25%absolute accuracy improvement over the best baseline method under adversarial attacks. Moreover, it exhibits superior generalizability in cross-domain, cross-genre, and mixed-source scenarios. The code is available at <https: //github. com/CarlanLark/Robust-AIGC-Detector>.","Large Language Models (LLMs) such as GPT-4 have shown great promise in producing text that closely mimics human language. However, concerns about the misuse of AI-generated text (AIGT) have arisen in various areas, including the spread of fake news, academic dishonesty, and gender bias. To tackle these issues, various AIGT detection methods have been developed, using statistical features from language models and text features from different model architectures and training approaches. 
Current AI-generated text (AIGT) detectors can effectively identify AI-generated text but struggle with minor adversarial perturbations, such as word substitutions or character swapping. Small changes that do not change the original text 's meaning can cause these detectors to fail. Figure shows a concrete example: a RoBERTa-based AIGT detector can be fooled into classifying AI-generated text as human-written by simply abbreviating ""California"" to ""Calif. "" This example underscores the limitations of relying solely on token-level features. Therefore, developing robust AIGT detection methods that rely on high-level features is crucial to counteract adversarial perturbation attacks. 
To address these challenges, we introduce the Siamese Calibrated Reconstruction Network (SCRN), which consists of an encoder, a reconstruction network, and a classification head. The model first converts input texts into token representations, then introduces random Gaussian noise to simulate a perturbation attack. The reconstruction network, acting as a denoising auto-encoder, aims to remove this noise and recover the original representations. The classification head processes these denoised features to produce the final result. During training, we optimize both classification and reconstruction losses, encouraging the model to learn representations that are resilient to random input perturbations. 
Empirically, we observe that a model trained for robustness against random perturbations may not necessarily be robust against adversarial perturbations. To address this issue, we introduce a training technique called siamese calibration. During training, the model generates two classification results using two independent sets of random noise. The training procedure aims to minimize the symmetric Kullback–Leibler (KL) divergence between the two output probability distributions. Since KL divergence is sensitive to changes in probabilities at all confidence levels, the model can incur a significant loss even when it makes consistently correct predictions but with varying confidence levels due to different noise. This stronger constraint forces the model to make equally confident predictions regardless of the noise. In experiments, we find that this approach encourages the model to rely more on high-level contextual features, thereby significantly enhancing its robustness against adversarial attacks. 
Our contributions are as follows: (1) We introduce a reconstruction network that enhances the model' s robustness by promoting the learning of resilient representations under token-level perturbations. (2) We propose a siamese calibration technique that trains the model to make predictions with consistent confidence levels for various random perturbations, which improves its robustness against adversarial attacks. (3) We establish a comprehensive benchmark for assessing the robustness of AIGT detection methods against a range of adversarial perturbations, including word-level and character-level substitution, deletion, insertion, and swapping. This benchmark encompasses a wide variety of detectors, such as metric-based and model-based detectors, trained using different methods. We evaluate these detectors on four publicly available datasets to test their robustness in in-domain, cross-domain, cross-genre, and mixed-source scenarios. (4) Our experiments on the benchmark show that SCRN significantly outperforms all baselines in terms of robustness, achieving higher accuracy under adversarial perturbation attacks. Specifically, our method improves over the best baseline method by 11.25,18.25,14.5, and 15.75 absolute points of accuracy under attack in in-domain, cross-domain, cross-genre, and mixed-source scenarios, respectively."
FinTextQA: A Dataset for Long-form Financial Question Answering,2405.09980v1,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2405.09980v1.pdf,An LFQA sample in FinTextQA. Models are expected to generate paragraph-length answers when given questions and documents.,"Accurate evaluation of financial question-answering (QA) systems necessitates a comprehensive dataset encompassing diverse question types and contexts. However, current financial QA datasets lack scope diversity and question complexity. This work introduces FinTextQA, a novel dataset for long-form question answering (LFQA) in finance. FinTextQA comprises 1,262 high-quality, source-attributed QA pairs extracted and selected from finance textbooks and government agency websites. Moreover, we developed a Retrieval-Augmented Generation (RAG) -based LFQA system, comprising an embedder, retriever, reranker, and generator. A multi-faceted evaluation approach, including human ranking, automatic metrics, and GPT-4 scoring, was employed to benchmark the performance of different LFQA system configurations under heightened noisy conditions. The results indicate that: (1) Among all compared generators, Baichuan2-7B competes closely with GPT-3.5-turbo in accuracy score; (2) The most effective system configuration on our dataset involved setting the embedder, retriever, reranker, and generator as Ada2, Automated Merged Retrieval, Bge-Reranker-Base, and Baichuan2-7B, respectively; (3) models are less susceptible to noise after the length of contexts reaching a specific threshold.","The growing demand for financial data analysis and management has led to the expansion of artificial intelligence (AI) -driven question-answering (QA) systems. These systems not only enhance customer service but also assist in risk management and personalized stock recommendations. The intricate nature of financial data, with its domain-specific terminologies, concepts, and the inherent uncertainty of the market and decision-making processes, demands a deep understanding of the financial domain to generate accurate and informative responses. In this context, long-form question answering (LFQA) scenarios become particularly relevant as they require models to demonstrate a broad spectrum of sophisticated skills, including information retrieval, summarization, data analysis, comprehension, and reasoning. 
In the general domain, there are several LFQA datasets available, including ELI5, WikiHowQA and WebCPM. However, it is important to note that there is currently no LFQA dataset specifically tailored for the finance domain. Existing financial QA benchmarks often fall short in addressing question complexity and variety by primarily on sentiment analysis and numerical calculation, as comprehensive paragraph-length responses and relevant document retrievals are often required to answer intricate, open-domain questions. To address these challenges, we introduce a new dataset, FinTextQA, which comprises LFQAs from finance-related textbooks and government agency websites to assess QA models on general finance and regulation or policy-related questions. FinTextQA consists of 1,262 high-quality, source-attributed question-answer pairs and associated document contexts. It contains six question types with an average text length of 19.7k words, curated from five rounds of human screening. This dataset is pioneering work in integrating financial regulations and policies into LFQA, challenging models with more demanding content. 
In addition to introducing the dataset, we conduct comprehensive benchmarking of state-of-the-art (sota) models on FinTextQA to provide baselines for future research. Current LFQA systems frequently solely rely on fine-tuning pre-trained language models such as GPT-3.5-turbo, LLaMA2, Baichuan2, etc. , which often fail to provide detailed explanations or effectively handling complicated finance questions. In response, we opt for the Retrieval-augmented generation (RAG) framework, as illustrated in Figure. By processing documents in multiple steps, RAG systems can pre-process and provide the most relevant information to LLMs, enhancing their performance and explanation capabilities. 
We believe this work, by introducing the first LFQA financial dataset and conducting comprehensive benchmark experiments on the dataset, marks a milestone in advancing the comprehension of financial concepts and enhancing assistance in this field: FinTextQA offers a rich and rigorous framework for building and assessing the capabilities of general finance LFQA systems. Our experimental analysis not only highlights the efficacy of various model configurations but also underscores the critical need for enhancing current methodologies to improve both the precision and explicability of financial question-answering systems."
On Measuring Faithfulness or Self-consistency of Natural Language Explanations,2311.07466v4,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2311.07466v4.png,CC-SHAP method on a toy example. Contribution values for illustration only. See for real samples.,"Large language models (LLMs) can explain their predictions through post-hoc or Chain-of-Thought (CoT) explanations. But an LLM could make up reasonably sounding explanations that are unfaithful to its underlying reasoning. Recent work has designed tests that aim to judge the faithfulness of post-hoc or CoT explanations. In this work we argue that these faithfulness tests do not measure faithfulness to the models 'inner workings – but rather their self-consistency at output level. Our contributions are three-fold: i) We clarify the status of faithfulness tests in view of model explainability, characterising them as self-consistency tests instead. This assessment we underline by ii) constructing a Comparative Consistency Bank for self-consistency tests that for the first time compares existing tests on a common suite of 11 open LLMs and 5 tasks – including iii) our new self-consistency measure CC-SHAP. CC-SHAP is a fine-grained measure (not a test) of LLM self-consistency. It compares how a model' s input contributes to the predicted answer and to generating the explanation. Our fine-grained CC-SHAP metric allows us iii) to compare LLM behaviour when making predictions and to analyse the effect of other consistency tests at a deeper level, which takes us one step further towards measuring faithfulness by bringing us closer to the internals of the model than strictly surface output-oriented tests. Our code is available at <https: //github. com/Heidelberg-NLP/CC-SHAP> ","Large language models (LLMs) generate answers in various tasks of increasing difficulty, acting as chatbots, as programming or scientific writing assistants. But often enough they behave unintuitively, showing undesirable behaviour: They can endorse a user 's misconceptions, or generate Chain-of-Thought (CoT) explanations that hide their sensitivity to biasing inputs; they can be insensitive to label correctness in in-context learning, and can produce correct predictions with irrelevant or misleading prompts. 
Especially in cases of unintuitive behaviour, explanations for their way of acting would be helpful. Even though LLMs can provide plausibly sounding explanations for their answers, recent work argues that model generated natural language explanations (NLEs) are often unfaithful. Obtaining faithful explanations that accurately reflect the reasoning process of a model is important for understanding the reasons behind an LLM' s answer, and is instrumental for a trustworthy AI. Being able to measure NLE faithfulness is most critical when models provide answers we are unable to judge – whether it is AI uncovering new scientific facts or ChatGPT helping with homework. 
Recent works aim to test the faithfulness of NLEs that LLMs produce about their own predictions (cf. §). But the studies are hard to compare, as they use both different models and data (Tab. ). They test for faithfulness by editing model inputs and measuring whether the prediction changes or stays consistent to the original answer. We argue that faithfulness of a NLE is more elusive than what existing tests (including ours) can measure, and that what current tests are measuring is self-consistency. We demonstrate this by comparing all tests (including ours) on the same models and data, showing that predictions differ widely. While existing tests compare output changes resulting from input edits on the surface, we propose a measure that does not need input edits and that more closely analyses how model outputs relate to how it processes the input. 
Overall, our paper contributes the following: 
 
 * We argue () that current tests that aim to measure NLE faithfulness, in reality measure the self-consistency of model outputs – without giving insight into a model 's inner reasoning processes. 
 * We introduce (§) CC-SHAP, a new fine-grained and explainable self-consistency measure gauging how well a model' s input contributions align, when it produces a prediction and explanation, and use it for post-hoc and CoT explanations. 
 * Since we cannot obtain ground truth for faithfulness by human judgement, we can only compare the predictions of existing tests (§). Hence, we are first to compare existing tests – including CC-SHAP – on a unified set of models and data after constructing the Comparative Consistency Bank (CCB). 
 In summary, our takeaways § are the following: 
 
 * We argue in § that existing tests measure self-consistency and not faithfulness. And since they adopt different test scenarios, we expect them to make different predictions. Indeed, they deliver different results for the same models and data (§), highlighting the heterogeneity of prior tests that target faithfulness. Given this result, and arguing that current tests do not touch the inner workings of LLMs, we stress that the quest for true faithfulness metrics remains open. 
 * By analysing CCB, we find trends: i) Chat LLMs show higher self-consistency than their base variants; ii) CC-SHAP agrees most with Counterfactual Edits; iii) We could not detect, nor exclude a relation between model size and self-consistency. 
 * With CC-SHAP we take a small step further towards measuring faithfulness: Prior tests compare outputs before and after input edits but don't give insight into how changes in the output relate to changes in how the LLM processes the input. CC-SHAP, by contrast, compares input importances for answer and for explanation generation – without editing inputs. Comparing predictions from CC-SHAP to prior tests shows that it offers transparency about how inputs (and also possible input modifications) influence LLM workings."
UNIMO-G: Unified Image Generation through Multimodal Conditional Diffusion,2401.13388v3,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2401.13388v3.png,"Examples of UNIMO-G for both text-driven and zero-shot subject-driven generation. UNIMO-G can perceive free-form interleaved visual-language inputs and faithfully generate images. Particularly, it can generate images from multi-modal prompts with multiple image entities.","Existing text-to-image diffusion models primarily generate images from text prompts. However, the inherent conciseness of textual descriptions poses challenges in faithfully synthesizing images with intricate details, such as specific entities or scenes. This paper presents UNIMO-G, a simple multimodal conditional diffusion framework that operates on multimodal prompts with interleaved textual and visual inputs, which demonstrates a unified ability for both text-driven and subject-driven image generation. UNIMO-G comprises two core components: a Multimodal Large Language Model (MLLM) for encoding multimodal prompts, and a conditional denoising diffusion network for generating images based on the encoded multimodal input. We leverage a two-stage training strategy to effectively train the framework: firstly pre-training on large-scale text-image pairs to develop conditional image generation capabilities, and then instruction tuning with multimodal prompts to achieve unified image generation proficiency. A well-designed data processing pipeline involving language grounding and image segmentation is employed to construct multi-modal prompts. UNIMO-G excels in both text-to-image generation and zero-shot subject-driven synthesis, and is notably effective in generating high-fidelity images from complex multimodal prompts involving multiple image entities.","Recent advancements in text-to-image (T2I) diffusion models have yielded impressive results in the generation of high-fidelity images from textual descriptions. Various methods, including DALL-Es, Imagen, Stable Diffusion, and MM-DiT, have been successful in producing photorealistic and contextually relevant images based on textual prompts. Nevertheless, a fundamental challenge persists due to the inherent brevity of textual descriptions, particularly when intricate details, specific entities, or nuanced scenes are involved. Thus, faithfully generating images from general vision-language (VL) inputs is essential to improve the controllability of image generation. 
Numerous studies have explored VL-to-image generation techniques. Methods such as DreamBooth, Imagic, SuTI and BLIP-Diffusion emphasize subject-driven generation, where they use both subject images and textual descriptions as inputs to recontextualize the subject in a newly described setting. They either fine-tune specific models for a given subject or employ pre-trained subject representations. However, their specific training design and input templates hinder their scalability, especially in complex scenarios with multiple entities. Additionally, studies like FastComposer and Subject-Diffusion focus on multiple-entity image generation, integrating image embeddings from image encoders with the standard text conditioning in pre-trained diffusion models. Nevertheless, these approaches lack the capacity to efficiently process generalized vision-language inputs that comprise a mix of textual and visual information in free forms. 
In this paper, we propose UNIMO-G, a simple multimodal conditional diffusion framework that operates on multimodal prompts comprising free-form interleaved vision-language inputs. Unlike traditional text-only prompts, multimodal prompts encompass various combinations of image entities and textual elements, as demonstrated in Figure. UNIMO-G is designed to faithfully reproduce all image entities, render textual content, and follow the instructions in multimodal prompts. Specifically, we leverage the perception capabilities of Multimodal Large Language Models (MLLMs) to encode multimodal prompts into a unified vision-language semantic space. Subsequently, a conditional diffusion network generates images from these encoded representations. 
To train UNIMO-G efficiently, we implement a two-phase strategy. Initially, the model undergoes pre-training on a large-scale dataset of text-image pairs, enhancing its proficiency in conditional image generation. This is followed by a phase of instruction tuning with multimodal prompts, learns to generate images that align with the detailed specifications provided in these prompts. A carefully designed data processing pipeline, incorporating language grounding and image segmentation, is employed to construct these multimodal prompts. This approach enables UNIMO-G to harness rich features from the MLLM encoder to generate images faithfully reproducing the contents across various contexts. 
UNIMO-G exhibits a comprehensive capability for controllable image generation, excelling not only in text-to-image synthesis but also in zero-shot subject-driven generation. It adeptly produces high-fidelity images from multimodal prompts, even those containing multiple image entities. To assess its performance, we conducted evaluations in both text-to-image and subject-driven generation contexts using the MS-COCO and DreamBench datasets, respectively. The results consistently highlight UNIMO-G 's superior performance in these scenarios. Additionally, recognizing DreamBench' s focus on single-subject generation, we introduce MultiBench, a new benchmark featuring images with multiple entities. The evaluation on MultiBench confirms UNIMO-G's effectiveness in zero-shot multi-entity subject-driven generation. 
In summary, our contributions in this work can be summarized as follows: 
 
 * We propose a simple multi-modal conditional diffusion framework that significantly enhances the controllability of image generation by supporting multimodal prompts with interleaved images and text input. 
 * We introduce an effective two-stage training strategy, empowering zero-shot multi-entity subject-driven generation through multi-modal instruction tuning. 
 * UNIMO-G outperforms existing VL-to-image models in both single and multi-entity subject-driven generation tasks, especially on the capabilities of multimodal instruction following."
The Fine-Tuning Paradox: Boosting Translation Quality Without Sacrificing LLM Abilities,2405.20089v2,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2405.20089v2.png,X→English (top) and English→X (bottom) COMET scores on WMT22 for models trained on human-written translations with different amounts of training data.,"Fine-tuning large language models (LLMs) for machine translation has shown improvements in overall translation quality. However, it is unclear what is the impact of fine-tuning on desirable LLM behaviors that are not present in neural machine translation models, such as steerability, inherent document-level translation abilities, and the ability to produce less literal translations. We perform an extensive translation evaluation on the LLaMA and Falcon family of models with model size ranging from 7 billion up to 65 billion parameters. Our results show that while fine-tuning improves the general translation quality of LLMs, several abilities degrade. In particular, we observe a decline in the ability to perform formality steering, to produce technical translations through few-shot examples, and to perform document-level translation. On the other hand, we observe that the model produces less literal translations after fine-tuning on parallel data. We show that by including monolingual data as part of the fine-tuning data we can maintain the abilities while simultaneously enhancing overall translation quality. Our findings emphasize the need for fine-tuning strategies that preserve the benefits of LLMs for machine translation.","Recent work has highlighted a range of qualitative advantages that large language models (LLMs) hold over Neural Machine Translation (NMT) models. One significant advantage is the controllability of style and language variety which can be achieved through prompting and in-context learning. LLMs also exhibit inherent document-level translation abilities. Another advantage is their ability to produce less literal translations. Finally, LLMs have been shown to have better performance in handling difficult linguistic phenomena such as idioms and ambiguous expressions. Taken together, LLMs are surpassing NMT models in terms of versatility. 
Recent studies have demonstrated that fine-tuning LLMs on parallel data further improves their translations as measured by metrics that reflect overall quality (such as COMET). However, relying on general translation quality metrics and generic test sets does not fully capture the nuanced abilities of LLMs in machine translation. This oversight raises questions about the retention of LLM-specific advantages — such as controllability, document-level translation proficiency, and the production of less literal translations — after fine-tuning on parallel data. While it is clear that general machine translation quality improves through fine-tuning, there is a risk that LLMs lose their unique strengths due to catastrophic forgetting. Determining the extent of this risk and comparing the effect of various fine-tuning strategies in preserving the qualitative benefits of LLMs remains an important yet unresolved question. 
We investigate how qualitative advantages of LLMs change when fine-tuning on parallel data. We consider LLaMA and Falcon models, with parameter counts ranging from 7 billion up to 65 billion. The LLM properties we investigate are general translation quality, formality steerability, non-literalness in idiom translations, performance on specialized domains, and performance on document-level input which requires contextualisation of ambiguous tokens. We compare two fine-tuning strategies for varying data sizes (89K up to 1.4M) in six translation directions. Our main findings and contributions are: 
 
 * We show that while fine-tuning LLMs on parallel data enhances overall translation quality as measured by COMET, it simultaneously leads to a decline in important attributes. Even when only using 18k fine-tuning samples we observe degradations in formality steering, technical translation through few-shot examples, and contextualization capabilities required for document-level translation. In general, we find that using larger data sets for fine-tuning data results in more severe degradations, and these trends are consistent across all tested model scales and architectures. The exception we observe is in the ability to produce less literal translations, which improves in fine-tuning. 
 * We show that incorporating a mix of monolingual and parallel data during fine-tuning can preserve abilities of LLMs. Overall translation quality is enhanced to a greater extent compared to fine-tuning on parallel data alone. 
 * We introduce a novel evaluation dataset, IdiomsInCtx-MT, to measure non-literalness performance. To our knowledge, it is the first dataset that consists of idiomatic expressions in context and their human-written translations. It covers 2 language pairs with 3 translation directions. 
 Our findings highlight the importance of creating fine-tuning approaches that enhance general translation quality while also preserving the distinctive capabilities of LLMs for machine translation."
Unveiling Linguistic Regions in Large Language Models,2402.14700v3,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2402.14700v3.pdf,"Three main findings of our experiments: (1) Identification of core language regions within the LLMs, where removals lead to linguistic competence loss; (2) Discovery of monolingual regions, where removals cause significant proficiency loss in specific languages; (3) Optimization of freezing core regions during further pre-training decelerates language forgetting.","Large Language Models (LLMs) have demonstrated considerable cross-lingual alignment and generalization ability. Current research primarily focuses on improving LLMs 'cross-lingual generalization capabilities. However, there is still a lack of research on the intrinsic mechanisms of how LLMs achieve cross-lingual alignment. From the perspective of region partitioning, this paper conducts several investigations on the linguistic competence of LLMs. We discover a core region in LLMs that corresponds to linguistic competence, accounting for approximately 1%of the total model parameters. Removing this core region by setting parameters to zero results in a significant performance decrease across 30 different languages. Furthermore, this core region exhibits significant dimensional dependence, perturbations to even a single parameter on specific dimensions leading to a loss of linguistic competence. Moreover, we discover that distinct monolingual regions exist for different languages, and disruption to these specific regions substantially reduces the LLMs' proficiency in those corresponding languages. Our research also indicates that freezing the core linguistic region during further pre-training can mitigate the issue of catastrophic forgetting (CF), a common phenomenon observed during further pre-training of LLMs. Overall, exploring the LLMs' functional regions provides insights into the foundation of their intelligence.","Over the years, the field of Natural Language Processing (NLP) has been at the forefront of understanding the core principles of intelligence. The emergence of Large Language Models (LLMs) such as GPT-4, PaLM 2 and LLaMA 2, showcase a significant breakthrough. Thanks to unparalleled scales of model architecture and the vastness of training data, these LLMs now exhibit exceptional linguistic competence and can execute complex tasks requiring abstract knowledge and reasoning. 
Previous research has revealed that LLMs naturally capture cross-linguistic similarities in their representation space, facilitating zero-shot cross-lingual transfer. The model is fine-tuned on one language, enabling the acquisition of comparable capabilities in another language, and exhibits the phenomenon of code-switching when generating context. Attempts to improve LLMs 'cross-lingual generalization abilities have been successful through parameter and information transfer learning, aligning languages compulsorily and utilizing in-context learning techniques. However, a detailed investigation into the internal mechanisms of how LLMs possess cross-linguistic alignment capability remains elusive. 
To delve deeper into the intrinsic mechanisms of LLMs' linguistic competence, this paper focuses on the LLMs 'parameter importance and investigate the linguistic regions of LLMs based on 30 distinct languages' performance, with the purpose of figuring out the following questions: 
Q1: Does a core linguistic region exist within LLMs that facilitates cross-lingual alignment and generalization? By conducting further pre-training across six languages and evaluating models 'parameter importance (Section), we discover a region in LLMs corresponding to the core linguistic competence, which accounts for approximately 1%of the model' s total parameters. As shown at the top of Figure, removing this region (setting parameters to zero) consistently leads to a significant decline in performance across 30 test languages (Section). 
Furthermore, by visualizing the core linguistic region (Figure), we observe that the linguistic core region of LLMs exhibits significant dimensional dependence. In certain dimensions, only perturbing a single parameter could lead to the model losing its linguistic competence (Section). Additionally, ablation study in shows that beyond outlier dimensions, other non-outlier dimensions in this region are also critical. 
Q2: Beyond the core linguistic region within LLMs, do distinct monolingual regions exist that specifically influence individual languages? While LLMs possess strong multilingual capabilities, we discover that each individual language (or language with similar compositional elements or grammatical structures) encompasses independent regions within the LLMs. As shown in the middle of Figure, the analysis of the Russian sentences identifies a particular linguistic region that likewise exerts influence both on the Russian and Ukrainian language, both of which belong to the Slavic group (Section). 
Q3: If and how core linguistic regions affect further pre-training, how to utilize it to optimize further pre-training? After pre-training, core linguistic parameter regions of the LLMs are established for multilingual alignment. Notable shifts in these regions potentially lead to a decline in model lingual capabilities. Our findings reveal that freezing this core region can mitigate the issue of catastrophic forgetting, a common phenomenon observed during further pre-training of LLMs. As shown at the bottom of Figure, we investigate the impact of selectively freezing 5%key parameters of all parameters during further pre-training, compared to the full-scale fine-tuning technique. Findings indicate that this method facilitates comparable learning of the target language while concurrently decelerating the rate of language attrition for previously learned languages (Section). Significantly, our methodology is compatible with the data-replay techniques, with no necessity for integrating extra components into the model. Unlike regularization methods, our approach restricts to a minimal core region in LLMs. 
The main contributions of our work are summarized as follows: 
 
 * We discover that LLMs possess a core linguistic region, and removing this region (setting parameters to zero) results in a significant loss of the model 's linguistic capabilities. Furthermore, perturbations to specific dimensions or even a single parameter can lead to a substantial decline in the model' s linguistic abilities. 
 * We observe that distinct monolingual regions exist in LLMs for different languages. Removing a specific monolingual region causes a significant deterioration in the linguistic capabilities within corresponding language. 
 * We perform further pre-training for specific languages within the core linguistic region of LLMs frozen, achieving comparable performance in the target language while mitigating catastrophic forgetting in non-target languages."
FastFiD: Improve Inference Efficiency of Open Domain Question Answering via Sentence Selection,2408.06333v1,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2408.06333v1.pdf,"Inference Time for FiD (base) and FastFiD (base) with varying numbers of retrieved passages. As the number of retrieved passages increases, FiD encounters increasingly severe efficiency issues. Our FastFiD significantly accelerates the process by greatly reducing decoding time.","Open Domain Question Answering (ODQA) has been advancing rapidly in recent times, driven by significant developments in dense passage retrieval and pretrained language models. Current models typically incorporate the FiD framework, which is composed by a neural retriever alongside an encoder-decoder neural reader. In the answer generation process, the retriever will retrieve numerous passages (around 100 for instance), each of which is then individually encoded by the encoder. Subsequently, the decoder makes predictions based on these encoded passages. Nevertheless, this framework can be relatively time-consuming, particularly due to the extensive length of the gathered passages. To address this, we introduce FastFiD in this paper, a novel approach that executes sentence selection on the encoded passages. This aids in retaining valuable sentences while reducing the context length required for generating answers. Experiments on three commonly used datasets (Natural Questions, TriviaQA and ASQA) demonstrate that our method can enhance the inference speed by 2.3X-5.7X, while simultaneously maintaining the model 's performance. Moreover, an in-depth analysis of the model' s attention reveals that the selected sentences indeed hold a substantial contribution towards the final answer. The codes are publicly available at <https: //github. com/thunlp/FastFiD>.","Open Domain Question Answering (ODQA) is a longstanding task in Natural Language Processing that involves generating an answer solely based on a given question. Recent advancements in this field have typically adopted the Retriever-Reader framework, which breaks down the task into two distinct stages. Initially, a retriever retrieves a set of relevant passages from a high-quality collection of open domain documents, such as Wikipedia. Subsequently, a reader model generates an answer by considering the question and the retrieved passages. Thanks to advancements in neural models, the retriever has transitioned from traditional search methods like TF-IDF to dense passage retrieval, resulting in improved retrieval performance. Furthermore, driven by the progress of Pretrained Language Models (PLMs), the reader has evolved from extracting answers from a single passage to generating answers from multiple passages. This approach enables the model to leverage information from various passages more effectively, thereby producing more accurate answers. 
A recently successful model is Fuse-in-Decoder (FiD), which utilizes Dense Passage Retrieval and a generative reader based on T5, an encoder-decoder model. FiD is capable of encoding each retrieved passage independently and subsequently concatenating these encoded passages to form an extensive context. The concatenated context is then used by the decoder to generate a response. Owing to its straightforward and extensible architecture, numerous subsequent works have introduced modifications based on this framework. However, as the decoder must generate a response based on all retrieved passages, it can be time-consuming to enhance performance through the retrieval of additional passages. Moreover, in real-world scenarios, the latency in generating an answer is a significant factor. As larger language models continue to be developed and demonstrate superior performance, this issue may become more pronounced. 
To address this issue, we introduce FastFiD, a novel approach that performs sentence selection post the encoder 's output and maintains only the essential sentences as references for the decoder, thereby significantly reducing the inference time for each query. 
To demonstrate the effectiveness of our approach, we first carry out experiments to ascertain that the multi-task training, which involves sentence selection and answer generation, does not conflict with one another during the model' s learning process. This is achieved by seamlessly incorporating a selection loss on the encoder outputs with a language modelling loss on answer generation, enabling the model to simultaneously handle both sentence selection and answer generation tasks. An in-depth analysis of the decoder 's cross-attention reveals that tokens from the chosen sentences yield a higher average attention score compared to those unchosen. This finding provides compelling evidence that the selected sentences significantly contribute more to the model' s predictions. Guided by this insight, we execute a secondary training phase, obliging the model to solely anchor to the selected encoder outputs when making the final prediction. 
The experimental results obtained from two widely used ODQA datasets, namely Natural Questions (NQ) and TriviaQA, along with a long-form QA dataset called ASQA, demonstrate that FastFiD can achieve performance metrics comparable to the original FiD. Notably, it can reduce the context length by up to 38X and accelerate the inference time by 2.3X-5.7X on different datasets. To validate the effectiveness of sentence selection, we also compare its performance with passage reranking after the encoder outputs. The results show that sentence selection yields better performance while maintaining a similar context length. This comparison indicates that sentence selection is a more effective strategy for compressing information across multiple passages. 
In summary, our contributions can be encapsulated within the following three key points: 
 
 * We implement a multi-task training approach, demonstrating that a singular reader model can concurrently perform sentence selection and answer generation. 
 * We introduce a novel technique to enhance the inference efficiency of FiD while preserving its question-answering capabilities. 
 * We carry out plenty of experiments to validate and analyze the effectiveness of our method."
An Open Multilingual System for Scoring Readability of Wikipedia,2406.01835v1,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2406.01835v1.png,Sketch of the readability scoring system for Wikipedia articles. Higher scores indicate more difficult-to-read text.,"With over 60M articles, Wikipedia has become the largest platform for open and freely accessible knowledge. While it has more than 15B monthly visits, its content is believed to be inaccessible to many readers due to the lack of readability of its text. However, previous investigations of the readability of Wikipedia have been restricted to English only, and there are currently no systems supporting the automatic readability assessment of the 300+ languages in Wikipedia. To bridge this gap, we develop a multilingual model to score the readability of Wikipedia articles. To train and evaluate this model, we create a novel multilingual dataset spanning 14 languages, by matching articles from Wikipedia to simplified Wikipedia and online children encyclopedias. We show that our model performs well in a zero-shot scenario, yielding a ranking accuracy of more than 80%across 14 languages and improving upon previous benchmarks. These results demonstrate the applicability of the model at scale for languages in which there is no ground-truth data available for model fine-tuning. Furthermore, we provide the first overview on the state of readability in Wikipedia beyond English.","To appear in ACL '24. The concept of readability aims to capture how easy it is to read a given text, usually defined as the sum of all factors that affect a reader' s understanding, reading speed, and level of interest. In practice, the goal is often to model and quantify the readability of a text on a pre-defined scale using linguistic features, known as Automatic Readability Assessment (ARA). Common approaches are based on readability formulas such as the Flesch-Kincaid score, with a recent shift towards more complex computational models leveraging progress on language models in NLP. These readability scores are used to better serve readers' information needs in educational contexts for choosing appropriate reading materials to support, e.g., language learners or readers with learning disabilities. Assessing the accessibility of content in terms of readability is also of relevance more broadly, as general textual information found on the web or in the news is often linguistically too complex for large fractions of the population. 
This use case for ARA is particularly relevant for Wikipedia, which has become the largest platform for open and freely accessible knowledge, read by millions of people worldwide with more than 60M articles across 300+ language versions. Unfortunately, this knowledge is believed to remain inaccessible to many readers because the text is written at a level above their reading ability – denoted as the readability gap. In fact, studies on English Wikipedia have concluded that ""overall readability is poor"". 
However, the state of readability in Wikipedia beyond English is unknown. Despite recent advances in ARA, there is no currently available system to systematically score articles across many languages due to several challenges (see also). There is a lack of availability of ready-to-use multilingual approaches, as existing web interfaces such as or, support only a limited number of languages. At the same time, there are no established readability formulas, such as the Flesch Reading Ease Formula, for most languages beyond English. Furthermore, models (or formulas) for ARA are often designed only for individual or pairs of languages, which makes it challenging to adapt existing models because (i) they are difficult to scale to hundreds of languages, and (ii) resulting scores cannot be easily compared across languages. Furthermore, there is a general lack of ground-truth data. While there are many resources for English, the datasets are often small in size and some of the most commonly-used ones are not available under an open license (such as Newsela or WeeBit), severely limiting their use in real-world applications. Beyond English, resources are scarce and scattered, such that there are no ready-to-use datasets in most languages. 
In this paper, we develop a multilingual system to score the readability of articles in Wikipedia (see Figure). Specifically, we make the following contributions. First, we compile a new multilingual dataset of pairs of encyclopedic articles with different readability levels covering 14 languages and make it publicly available under an open license. Second, we develop a single multilingual model for readability, demonstrating the effectiveness of the zero-shot cross-lingual transfer. Third, we apply the model to obtain the first systematic overview of the state of readability of Wikipedia articles beyond English and provide a public API endpoint of the model for use by readers, editors, and researchers."
Unlearning Traces the Influential Training Data of Language Models,2401.15241v2,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2401.15241v2.pdf,"Overview of leave-dataset-out vs. proposed methods, UnTrac and UnTrac-Inv.","Identifying the training datasets that influence a language model 's outputs is essential for minimizing the generation of harmful content and enhancing its performance. Ideally, we can measure the influence of each dataset by removing it from training; however, it is prohibitively expensive to retrain a model multiple times. This paper presents UnTrac: Unlearning Traces the influence of a training dataset on the model' s performance. UnTrac is extremely simple; each training dataset is unlearned by gradient ascent, and we evaluate how much the model's predictions change after unlearning. Furthermore, we propose a more scalable approach, UnTrac-Inv, which unlearns a test dataset and evaluates the unlearned model on training datasets. UnTrac-Inv resembles UnTrac, while being efficient for massive training datasets. In the experiments, we examine if our methods can assess the influence of pretraining datasets on generating toxic, biased, and untruthful content. Our methods estimate their influence much more accurately than existing methods while requiring neither excessive memory space nor multiple checkpoints.","Large language models (LLMs) have had a significant impact on our society. They exhibit remarkable abilities (e.g., chain-of-thought reasoning) without being explicitly trained for such tasks. At the same time, LLMs also pose potential risks, such as the amplification of discrimination through the propagation of toxic language. LLMs are trained on a vast number of corpora via pretraining or refined through finetuning on diverse tasks. Although some efforts have been made to unravel the black box of LLMs, it is still unclear which data sources cause their unprecedented abilities and potential harms. 
Ideally, we can answer this question by removing each dataset from the training datasets and assessing the change in the model 's performance after retraining (leave-dataset-out). However, since we need to retrain a model on each dataset, leave-dataset-out is prohibitively expensive. Training data attribution overcomes this problem by approximating the influence with Hessian-based influence functions or tracking changes in test loss during training. However, HIF requires a large memory space to approximate the inverse Hessian, while TracIn generally needs multiple model checkpoints. 
In this paper, we propose UnTrac, which traces the influence of a training dataset by unlearning it from a trained model (Figure). Leave-dataset-out removes each training dataset and measures its influence by assessing the trained model' s performance on a test dataset. Analogous to leave-dataset-out, UnTrac unlearns each training dataset and estimates its influence by assessing the unlearned model 's performance on a test dataset. Unlearning has been studied to eliminate sensitive data from a trained model and has recently been applied to LLMs. Following, we unlearn a training dataset using gradient ascent, in contrast to the gradient descent normally used in training. Interestingly, argued that influence functions can be regarded as an approximation of the effect of finetuning on a number of examples (e.g., unlearning mislabeled examples). With UnTrac, instead of using the approximations, we directly quantify the effect of unlearning. 
When many datasets are used for training, UnTrac is computationally costly because unlearning must be run for every individual training dataset. To overcome this drawback, we propose UnTrac-Inv as a scalable approach particularly effective for an increasing number of training datasets. UnTrac-Inv ""unlearns"" a test dataset instead of training datasets and evaluates the unlearned model on training datasets. UnTrac-Inv requires only a single run of unlearning, and, as we will show, can be considered as an efficient approximation of UnTrac. 
In our experiments, we first examine whether our methods can trace influential training tasks in the setting of finetuning. We created a dataset representing a mixture of synthetic tasks, designed to evaluate our method' s capability in assessing the influence of each task. In order to make this assessment more challenging, we have created task pairs that, while semantically distinct, require responses in the same format. Additionally, we include pairs that are nearly identical in content but demand responses in differing formats. By estimating the influence across these task pairs, we verify that our methods are not overly reliant on superficial similarities between tasks. In this controlled dataset, we show that our methods accurately assess the influence of training tasks, where we use the expensive leave-dataset-out method as the ground-truth, and are only slightly affected by the output format. 
Next, we assess whether our methods can identify the source of harmful content generated by a pretrained language model. Using smaller open pretrained transformers, the influence of eight pretraining datasets is estimated. We use three test datasets: Toxigen, WinoBias, and TruthfulQA, which contain toxic language, biased text, and false answers to various questions, respectively. We calculate the ground-truth influence of each training dataset and evaluate the correlation between the estimated influence and ground-truth influence. We demonstrate that our methods accurately estimate the influence of pretraining datasets, significantly outperforming other influence functions. 
Finally, we investigate how hyperparameters affect the performance of our methods. We found that UnTrac works robustly as long as we use preconditioned gradient methods with higher learning rates and a sufficient number of training iterations. In contrast, UnTrac-Inv works well for large batch sizes while being relatively sensitive to the learning rate and the number of training steps."
Self-Evolving GPT: A Lifelong Autonomous Experiential Learner,2407.08937v1,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2407.08937v1.pdf,An example of experience-enhanced LLMs inference.," [1] Corresponding Author 
To improve the performance of large language models (LLMs), researchers have explored providing LLMs with textual task-solving experience via prompts. However, they rely on manual efforts to acquire and apply such experience for each task, which is not feasible for the growing demand for LLMs and the variety of user questions. To address this issue, we design a lifelong autonomous experiential learning framework based on LLMs to explore whether LLMs can imitate human ability for learning and utilizing experience. It autonomously learns and accumulates experience through experience transfer and induction, categorizing the types of input questions to select which accumulated experience to employ for them. Experimental results on six widely used NLP datasets show that our framework performs reliably in each intermediate step and effectively improves the performance of GPT-3.5 and GPT-4. This validates the feasibility of using LLMs to mimic human experiential learning and application capabilities. Additionally, we provide a detailed analysis of the behavior of our framework at each step.","Recently, large language models (LLMs) like ChatGPT have achieved excellent performance in various NLP tasks. However, numerous NLP tasks still cannot be effectively addressed by them. This is mainly because they have not accumulated enough experience to handle these tasks during their training. 
To address these issues, previous studies have explored injecting task-solving experience into LLMs during the inference stage via prompts (as shown in Figure). Their experience is textual descriptions of the task-solving processes, guidelines, and other insights. Some studies manually craft such experience. Others attempt to summarize experience from manually annotated task datasets, and then during inference, they essentially need to manually select the experience to apply to each question. However, the demands of users on LLMs are ever-expanding, and the types of user questions continue to grow. These methods would lead to high and unbounded costs for human labor. 
In contrast, humans are capable of autonomous learning and utilizing experience. Humans categorize encountered problems into different task types and induce experience from multiple concrete task practices, which are reused when encountering new problems of the same task type. Besides, humans can transfer experience between similar tasks, thus gaining more experience without time-consuming practices. As lifelong autonomous experience accumulates, humans gradually achieve ability growth. Inspired by this, we want to explore whether LLMs can mimic the above process. This could avoid the substantial manual labor and provide a unique evolutionary path for artificial general intelligence. 
To facilitate this, we propose a lifelong autonomous experiential learning framework called Self-Evolving GPT (SE-GPT), which consists of a task-specific experience memory and five experience-centric modules based on ChatGPT. For any user question, SE-GPT automatically categorizes the target task type and responds to the question with the target task experience in the memory. For newly encountered task types, it learns experience through experience transfer and induction before responding. Firstly, it locates similar tasks in its memory and transfers their experience to the target task. Then, it autonomously references web information and the transferred experience to practice the target task multiple times, thereby inducing more experience from its successes and failures. Finally, the transferred and induced experience is added to the memory. For tasks encountered previously, it assesses the need for repeating experience transfer and induction before responding, taking into account its proficiency level with the task. 
To conduct experiments, we provide a basic implementation of our framework. We mainly focus on the overall framework and aim to analyze its effectiveness and behavior. Experiments show that our framework is practically feasible. It effectively improves the average performance of GPT-3.5 and GPT-4 on six widely used datasets by 3.8%and 5.3%, respectively. Our framework reliably executes each intermediate module, achieving consistent performance improvements. Besides, we provide a detailed analysis of the behavior of our framework in each intermediate step."
Advancing Large Language Models to Capture Varied Speaking Styles and Respond Properly in Spoken Conversations,2402.12786v2,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2402.12786v2.pdf,"The overview framework of Spoken-LLM. (c1, r1) and (c2, r2) are the current and response speech sample pairs. c1 and c2 are fed into the model individually.","In spoken dialogue, even if two current turns are the same sentence, their responses might still differ when they are spoken in different styles. The spoken styles, containing paralinguistic and prosodic information, mark the most significant difference between text and speech modality. When using text-only LLMs to model spoken dialogue, text-only LLMs cannot give different responses based on the speaking style of the current turn. In this paper, we focus on enabling LLMs to listen to the speaking styles and respond properly. Our goal is to teach the LLM that ""even if the sentences are identical if they are spoken in different styles, their corresponding responses might be different"". Since there is no suitable dataset for achieving this goal, we collect a speech-to-speech dataset, StyleTalk, with the following desired characteristics: when two current speeches have the same content but are spoken in different styles, their responses will be different. To teach LLMs to understand and respond properly to the speaking styles, we propose the Spoken-LLM framework that can model the linguistic content and the speaking styles. We train Spoken-LLM using the StyleTalk dataset and devise a two-stage training pipeline to help the Spoken-LLM better learn the speaking styles. Based on extensive experiments, we show that Spoken-LLM outperforms text-only baselines and prior speech LLMs methods.","Large Language Models (LLMs) have demonstrated remarkable capabilities in dialogue generation, natural language understanding, and commonsense reasoning. While LLMs mostly focus on text modality, speech represents the most natural form of human communication in our daily lives. In this work, we aim to inject speech modality for modeling spoken conversation with Multi-modal LLMs (MM-LLMs). The main goal is to develop a humanizing agent capable of listening, understanding, and engaging in dialogue with humans, ultimately leading to higher user satisfaction. 
Speech signals contain linguistic aspects (words, phonetics, syntax, and semantics), paralinguistic elements (emotions and speaker characteristics), and prosodic factors (speaking style, emphasis, and attitude). In human conversation, while the dialogue primarily relies on the lexical aspect, the speaking styles convey rich information beyond text, and can even alter the semantics of the spoken sentences. Neglecting spoken styles can lead to misinterpretation of communication or unnatural human interaction. For example, as shown in Figure, the current speech with the same current text (Looks like it might rain later this week though. ) but different speaking styles. The friendly speaking style leads to a cheerful response while speaking in a slow and neutral tone leans toward a sad and negative response. 
Although there are recent studies on MM-LLMs for speech/audio and text, most of the existing studies focus on content-centric Spoken Language Modeling (SLM), joint text and speech processing tasks or general audio perception and hearing ability. There is less attention on spoken dialogue with advanced methods and suitable datasets for modeling paralinguistics and speaking styles of spoken responses. 
To model spoken dialogue with a generative language model, dGSLM proposes a dual-tower SLM on discrete speech units to model two-channel spoken dialogue, but the generated spoken sentences lack semantic meaning. ParalinGPT organizes tasks in the sequence of current paralinguistic attribute prediction, response paralinguistic attribute prediction, and response text generation with autoregressive conditioning. However, it only uses the speech sentiment as speaking style, which might be primarily based on textual information, and how the speaking styles affect the spoken response is unclear. A concurrent work E-chat enhances LLM to generate responses in different emotional contexts, but the training and evaluation data are entirely generated by GPT-3.5 without human supervision, equivalent to distillation and prompting of GPT-3.5. It can only generate response text, constraining its capacity to control response style or speech-to-speech modeling. 
To overcome the current limitation, we collect a novel speech-to-speech conversational dataset named StyleTalk. This dataset is the first spoken conversation benchmark with the same dialogue context and input sentence in different speaking styles, accompanied by corresponding expressive spoken responses for speech-to-speech modeling. The dataset will be released upon the paper's acceptance. 
Based upon the StyleTalk dataset, we propose a multi-modal two-stage training method named Spoken-LLM for spoken dialogue modeling. Spoken-LLM is a fusion of the widely-used open-sourced LLM (Llama 2-Chat) and a self-supervised speech emotion representation model (emotion2vec). The proposed model can predict response speaking style and text, enabling the subsequent expressive Text-to-Speech (TTS) model to generate natural and diverse speech responses. We validate the performance through objective and subjective evaluations of spoken responses. With the same backbone model, the proposed method outperforms the text and speech LLM baseline in lexical/semantic similarity and response style F1 score. The human evaluation also indicates that the proposed method yields more reasonable and proper response speech than the text-only LLM baseline approach."
Automated Justification Production for Claim Veracity in Fact Checking: A Survey on Architectures and Approaches,2407.12853v1,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2407.12853v1.pdf,General AFC Pipeline; courtesy of.,"Automated Fact-Checking (AFC) is the automated verification of claim accuracy. AFC is crucial in discerning truth from misinformation, especially given the huge amounts of content are generated online daily. Current research focuses on predicting claim veracity through metadata analysis and language scrutiny, with an emphasis on justifying verdicts. This paper surveys recent methodologies, proposing a comprehensive taxonomy and presenting the evolution of research in that landscape. A comparative analysis of methodologies and future directions for improving fact-checking explainability are also discussed.","The huge increase in both user-generated and automated content has led to a significant amount of misinformation. This poses risks to uninformed readers, highlighting the need for scalable, automated methods for verification and fact-checking. While predicting the veracity of claims is essential, relying solely on predictions without providing explanations can be counterproductive, potentially reinforcing belief in false claims and perpetuating misinformation. 
Most fact-checking models use neural architectures, but interpreting these models is challenging. There is a need for fact-checking frameworks providing justifications to enhance effectiveness and trustworthiness. This survey presents recent efforts addressing automatic justification production for claim verification, emphasizing the move towards ""Explainable"" Automated Fact-Checking (AFC). Some work refers to the justification production process as the explanation generation process. In this survey, the term ""justification production"" is used following the work of. 
This survey's main contribution is as follows: Firstly, it introduces a multidimensional taxonomy for categorizing works based on various criteria. Secondly, it provides how research is progressing towards standard justifications. Thirdly, it conducts a comparative analysis of justification production approaches, pipeline architectures, input and output types. Lastly, it identifies challenges while proposing future directions in justification production. Appendix outlines the methodology utilized for literature compilation, detailing the search strategy and selection criteria employed for the papers that form the cornerstone of this survey."
Muffin or Chihuahua? Challenging Multimodal Large Language Models with Multipanel VQA,2401.15847v3,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2401.15847v3.pdf,Examples of Single-panel vs. multipanel image VQA. GPT-4V distinguishes muffin and chihuahua in the single-panel image input but struggles with the same content in the multipanel image.,"Multipanel images, commonly seen as web screenshots, posters, etc. , pervade our daily lives. These images, characterized by their composition of multiple subfigures in distinct layouts, effectively convey information to people. Toward building advanced multimodal AI applications, such as agents that understand complex scenes and navigate through webpages, the skill of multipanel visual reasoning is essential, and a comprehensive evaluation of models in this regard is important. Therefore, we introduce Multipanel Visual Question Answering (MultipanelVQA), a novel benchmark comprising 6,600 triplets of questions, answers, and multipanel images that specifically challenge models in comprehending multipanel images. Our evaluation shows that questions in the MultipanelVQA benchmark pose significant challenges to the state-of-the-art Multimodal Large Language Models (MLLMs) tested, even though humans can attain approximately 99%accuracy on these questions. Distinctively, the MultipanelVQA benchmark features synthetically generated multipanel images specifically crafted to isolate and assess the impact of various factors, such as the layout, on MLLMs 'multipanel image comprehension abilities. As a result, in addition to benchmarking the capabilities of MLLMs in understanding multipanel images, we analyze various factors of the multipanel image that affect MLLMs' performance with synthetic data and offer insights for enhancement. <https: //sites. google. com/view/multipanelvqa/home>.","Multimodal Large Language Models (MLLMs) have become a significant leap in the integration of visual and textual data processing, enabling more nuanced understanding and generation of content that blends both visual and linguistic elements. Being trained on extensive data, advanced MLLMs have shown remarkable proficiency in various tasks (e.g., image captioning and visual question answering) that require natural language understanding, visual-language grounding, visual reasoning, etc. 
As MLLMs become more competent, there is a trend of establishing increasingly challenging benchmarks that are often arduous for average humans to achieve. However, this raises a pertinent question: Have MLLMs advanced to the stage where elementary benchmarks easily handled by average humans pose little challenge to them? To answer this question, we target multipanel images, each involving a series of subfigures. These subfigures are presented together in certain layouts, such as web screenshots capturing multiple thumbnail images and posters utilizing multipanel formats to present a cohesive narrative or argument. We observe that while humans typically find interpreting multipanel images to be a straightforward task, MLLMs struggle with this challenge when presented with the entire multipanel image as input, as shown in Figure. 
This study aims to holistically evaluate MLLMs in understanding multipanel images. We introduce the MultipanelVQA benchmark with 6,600 triplets of multipanel images, questions and answers, challenging models to answer each question based on the multipanel image. There are three questions with distinct types for each multipanel image: identifying common or unique contents across subfigures, pinpointing content in specific subfigures through positional descriptions, and locating subfigures via visual grounding in a multi-choice format. Especially, the first type of question mainly tests the MLLMs 'ability to reason about contents and the other two question types also assess the MLLMs' understanding of multipanel image layouts in addition to the content reasoning ability. 
Uniquely, the multipanel images in the MultipanelVQA benchmark features a diverse mix of real-world web screenshots, posters and synthetic multipanel images, categorized into real-world data and synthetic data subsets. Unlike the real-world data that requires human annotation, the synthetic multipanel images are automatically generated by scripts with subfigures from two existed datasets. The script ensures the generated synthetic multipanel images have even distribution of various attributes such as the number of subfigures, their sizes, and the complexity of layouts, etc. As a result, based on the synthetic data, we are able to precisely isolate and pinpoint the impact of their attributes on the performance of MLLMs. 
We then benchmark popular open-sourced and proprietary MLLMs on the MultipanelVQA benchmark and conduct thorough error analysis with the help of the synthetic data, which delves into the reasons behind MLLMs 'difficulties in interpreting multipanel images. As a result, our main findings are 1) MLLMs are susceptible to content interference caused by the occurrence of multiple subfigures within the multipanel image. 2) The layout for subfigures has an impact on the MLLMs' performance on multipanel images. MLLMs tend to be more successful in understanding multipanel images with layouts with fewer subfigures and larger subfigure sizes. 3) Adding sequential numbers for subfigures as visual prompt can benefit some MLLMs that are sensitive to embedded texts in the input multipanel images. 
Last but not least, we explore how adding sequential numbers to subfigure captions in multipanel images, akin to the Set-of-Mark visual prompting method, improves MLLMs 'understanding of these images. We test MLLMs on multipanel images with and without sequential number captions for each subfigure. As a result, we observed that only GPT-4V and MiniGPT-v2 show a notable improvement when the sequential number is not only embedded in the image but also explicitly mentioned in the question. In conclusion, the contributions of this study are listed as follows: [itemize] leftmargin=* 
 0em 
 * We propose the MultipanelVQA benchmark with real-world and synthetic data that focus on evaluating the model' s ability to understand the content and layout of multipanel images. 
 * We benchmark several open-sourced and proprietary MLLMs with the MultipanelVQA benchmark and find that all models tested face a significant challenge in interpreting multipanel images despite their success on single-panel images. 
 * Benefited by the synthetic data with even distributions of various multipanel image attributes in the MultipanelVQA benchmark, we conduct thorough error analysis to uncover various factors that impact the model's performance, including subfigure content, layout, background, and visual text hint in multipanel images. 
 * Finally, we investigate the potential of adding subfigure captions in multipanel images as visual prompts to enhance the performance of MLLMs on multipanel image understanding."
WebVoyager: Building an End-to-End Web Agent with Large Multimodal Models,2401.13919v4,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2401.13919v4.pdf,"The overall workflow of WebVoyager. WebVoyager takes web tasks assigned by a human and automatically browses the web online. At each step, WebVoyager selects actions based on screenshots and text (the 'type' of the web element and its contents). Once the task is completed, the answers will be returned to the user. For example, for a user query: ""Find the cost of a 2-year protection for PS4 on Amazon. "", the agent interacts with Amazon online, locates the PS4, identifies the 2-year protection price, and returns ""$ 30.99"" to the user.","The rapid advancement of large language models (LLMs) has led to a new era marked by the development of autonomous applications in real-world scenarios, which drives innovation in creating advanced web agents. Existing web agents typically only handle one input modality and are evaluated only in simplified web simulators or static web snapshots, greatly limiting their applicability in real-world scenarios. To bridge this gap, we introduce WebVoyager, an innovative Large Multimodal Model (LMM) powered web agent that can complete user instructions end-to-end by interacting with real-world websites. Moreover, we establish a new benchmark by compiling real-world tasks from 15 popular websites and introduce an automatic evaluation protocol leveraging multimodal understanding abilities of GPT-4V to evaluate open-ended web agents. We show that WebVoyager achieves a 59.1%task success rate on our benchmark, significantly surpassing the performance of both GPT-4 (All Tools) and the WebVoyager (text-only) setups, underscoring the exceptional capability of WebVoyager. The proposed automatic evaluation metric achieves 85.3%agreement with human judgment, indicating its effectiveness in providing reliable and accurate assessments of web agents.","The recent advancement of large language models (LLMs), such as ChatGPT and GPT-4, have sparked significant interest in developing LLM-based autonomous agents for complex task execution. Recent studies have explored the construction of text-based web browsing environments and how to instruct large language model agents to perform web navigation. The primary challenge in these works lies in managing complex and verbose HTML texts, and solutions include simplifying and structuring HTML. 
However, existing approaches overlook a critical functionality of browsing: rendering HTML into visual webpages. Particularly, vision capability is crucial for utilizing tools such as web browsers, as rendered web pages are inherently designed with user experience (UX), emphasizing intuitive information and structured presentation. This design principle of rendering makes visual analysis more effective than mere HTML representation. At present, large multimodal models (LMMs), particularly GPT-4V (ision) and Gemini, demonstrate a remarkable ability to integrate intricate visual cues with textual information. Existing studies such as Pix2Struct and WebArena, have initiated explorations into using screenshots as inputs for decision-making in web navigation, yet these are preliminary and do not represent a deep exploration. Therefore, building multimodal web agents to leverage the environment rendered by browsers through screenshots, thus mimicking human web browsing behavior, is now a viable approach to enhance web navigation abilities. 
We introduce WebVoyager (Figure), a multimodal web agent designed to autonomously accomplish web tasks online from start to finish, managing the entire process end-to-end without any intermediate human intervention. WebVoyager processes the user query by making observations from screenshots and textual content in interactive web elements, formulates a thought on what action to take (such as clicking, typing, or scrolling, etc. ), and then executes that action on the websites. Inspired by Set-of-Mark Prompting, we mark interactive web elements on screenshots (see Figure) to facilitate decision-making for WebVoyager. 
Another challenge is the evaluation of an end-to-end web agent. Existing benchmarks, such as Mind2Web, primarily focus on stepwise and offline evaluation, where agents follow a predefined ""golden"" trajectory for action selection. This approach, however, may not fully account for the variety of viable strategies to accomplish a task, as it only reflects one possible plan. This limitation could lead to a biased evaluation and difficulties in fairly comparing different methods. To accurately evaluate the capabilities of web agents in end-to-end task completion, we propose an automated evaluation protocol using GPT-4V. Specifically, we save screenshots throughout the online navigation process and then use GPT-4V to evaluate these trajectories together with the final results automatically. Human evaluations are also conducted to verify the results and the analysis shows that our evaluation protocol achieves 85.3%agreement with human judges, indicating GPT-4V can serve as a reliable evaluator for online agents. 
We conduct evaluations on a newly collected dataset, which is semi-automatically generated using a self-instruct method, comprising 643 web tasks from 15 commonly accessed websites. We also evaluate WebVoyager on 90 web-related tasks of level 1 and level 2 from the GAIA, and 50 interactive open-web tasks from SeeAct. We compare our WebVoyager with 1) GPT-4 (All Tools), and 2) WebVoyager in a text-only setting which employs the textual accessibility tree proposed in WebArena to describe web pages. The results show that WebVoyager achieves a Task Success Rate of 59.1%on our new benchmark, significantly outperforming GPT-4 (All Tools) with a rate of 30.8%and the text-only setting with a rate of 40.1%, demonstrating the effectiveness of our method. Our research demonstrates the effectiveness of the WebVoyager method for web tasks, offering insights into the development of more intelligent and efficient web automation solutions."
ARIES: A Corpus of Scientific Paper Edits Made in Response to Peer Reviews,2306.12587v2,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2306.12587v2.pdf,"Overview of our tasks. In comment-edit alignment, a model is given a review comment and set of candidate edits derived from a source paper and a revised target paper, and it must align the comment to the edit (s) that are associated with it. In edit generation, a model is given a review comment and a source paper and must generate an edit that addresses the comment, possibly using placeholders for missing information.","We introduce the task of automatically revising scientific papers based on peer feedback and release ARIES, a dataset of review comments and their corresponding paper edits. The data is drawn from real reviewer-author interactions from computer science, and we provide labels linking each reviewer comment to the specific paper edits made by the author in response. We automatically create a high-precision silver training set, as well as an expert-labeled test set that shows high inter-annotator agreement. In experiments with 10 models covering the state of the art, we find that they struggle even to identify which edits correspond to a comment—especially when the relationship between the edit and the comment is indirect and requires reasoning to uncover. We also extensively analyze GPT-4's ability to generate edits given a comment and the original paper. We find that it often succeeds on a superficial level, but tends to rigidly follow the wording of the feedback rather than the underlying intent, and lacks technical details compared to human-written edits.","Feedback on paper drafts, whether from co-authors, readers, or reviewers, can be challenging to interpret and address. Consider a reviewer who wants authors to use a more realistic dataset in their evaluation. This could be expressed in a variety of ways: it could be stated as a direct request (""Apply the method to a realistic dataset""), or more indirectly as a criticism (""The evaluation is only on a synthetic dataset"") or question (""Is the current dataset truly representative of the real-world? ""). Similarly, an author editing the manuscript in response has several options. They could comply with the request, or clarify that no realistic datasets are publicly available, or even argue that the reviewer is mistaken and add a justification of their dataset's realism. Properly interpreting the request and then selecting a suitable edit in response requires deep reasoning and expertise. 
While existing NLP systems produce fluent and coherent text, their capabilities remain limited on the most demanding writing tasks that require interpretation and reasoning. Research on systems that can interpret complex writing feedback and edit documents in response could enable dramatically improved writing assistants, in scientific writing and elsewhere. However, as we discuss in, datasets for studying such tasks are limited, and scientific writing represents a particularly challenging and understudied domain. 
In this paper, we introduce and study the task of revising scientific papers based on peer feedback. To facilitate research on the task, we release ARIES (Aligned, Review-Informed Edits of Scientific Papers), a real-world dataset of computer science paper drafts from OpenReview, the corresponding reviewer feedback, and the author responses and revisions that address the feedback. To link the review comments to their corresponding edits, we devise an automatic method based on author responses to produce silver data at scale, and obtain labels from experts to form a gold test set. We believe ARIES is the first dataset to enable training and evaluation of models on contentful edits in a technical domain. 
Using the dataset, we formulate two novel tasks, shown in. The first is comment-edit alignment, in which a model determines which review comments made about a paper correspond to each of the edits made after the feedback. The second task is edit generation, in which a model generates edits directly from a given reviewer comment and paper text. The alignment task involves identifying rather than generating appropriate edits; it serves as a stepping stone toward the more challenging generative task and admits automatic evaluation. 
We evaluate ten baseline methods and find that the alignment task is challenging for existing models, including even large models such as GPT-4, and that comments and edits with indirect relationships are especially difficult. For the generation task, we find that GPT-4 does produce edits that are coherent and on-topic on a surface level, but fails to model the underlying intent; unlike real authors, it almost never makes edits that suggest the feedback is mistaken, often paraphrases the feedback rather than tightly integrating edits into the paper, and tends to include less technical detail. 
Our contributions are as follows: 
 
 * We propose the novel tasks of (a) aligning high-level draft feedback to specific edits and (b) generating revisions for scientific papers given reviewer feedback (). 
 * We construct ARIES, a real-world dataset containing 3.9K reviewer comments automatically matched to edits with 92%precision using author responses from OpenReview, and a carefully-curated test set of 196 human-annotated comments (). 
 * We evaluate a wide range of baseline methods on our comment-edit alignment task, finding that it is challenging even for modern LLMs. The best model (GPT-4) achieves only 27.0 micro-F1 compared to human performance of 70.7 (). 
 * We conduct an extensive analysis of edit generation with GPT-4, detailing several systemic differences between generated and real edits, and suggest future work directions ()."
Leveraging Large Language Models for Learning Complex Legal Concepts through Storytelling,2402.17019v4,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2402.17019v4.png,Illustration of the expert-in-the-loop pipeline. The left section demonstrates the procedure to produce an LLM-generated story from the concept. The lower section in the center shows how we use both the definition and story as input to produce LLM-generated reading comprehension (RC) questions. The center upper section shows that we first collect expert feedback on questions and regenerate questions with expert advice. The right section outlines the RCT experiment to see if LLM-generated stories improve comprehension in legal concepts.,"Making legal knowledge accessible to non-experts is crucial for enhancing general legal literacy and encouraging civic participation in democracy. However, legal documents are often challenging to understand for people without legal backgrounds. In this paper, we present a novel application of large language models (LLMs) in legal education to help non-experts learn intricate legal concepts through storytelling, an effective pedagogical tool in conveying complex and abstract concepts. We also introduce a new dataset LegalStories, which consists of 294 complex legal doctrines, each accompanied by a story and a set of multiple-choice questions generated by LLMs. To construct the dataset, we experiment with various LLMs to generate legal stories explaining these concepts. Furthermore, we use an expert-in-the-loop approach to iteratively design multiple-choice questions. Then, we evaluate the effectiveness of storytelling with LLMs through randomized controlled trials (RCTs) with legal novices on 10 samples from the dataset. We find that LLM-generated stories enhance comprehension of legal concepts and interest in law among non-native speakers compared to only definitions. Moreover, stories consistently help participants relate legal concepts to their lives. Finally, we find that learning with stories shows a higher retention rate for non-native speakers in the follow-up assessment. Our work has strong implications for using LLMs in promoting teaching and learning in the legal field and beyond.","Often individuals find themselves in certain high-stakes situations where they have to educate themselves on novel concepts such as new policies before voting, mortgage terms when buying a house or legal principles relevant to an ongoing lawsuit. Unfamiliar terms and nuanced use of language in these contexts can make it challenging for non-experts to make informed decisions, to have equal access to justice, or to participate in civic discourse and democracy. We present this work as a step towards enhancing general legal literacy, bridging the gap between non-experts and experts and promoting constructive and civic discourse. 
Storytelling is an important medium to communicate science to non-experts and teach professional knowledge to beginners. In legal contexts, storytelling has been used extensively to teach abstract legal concepts such as ethics, and has proven effective at explaining complex legal concepts such as legal mediation to the general public. However, the scalable implementation of legal storytelling education is severely limited by the high costs associated with legal experts. 
Large language models (LLMs) and their impressive text generation abilities have facilitated high-quality automated explanations and stories. Recent efforts have leveraged LLMs to generate accessible explanations of scientific or medical concepts for diverse audiences. used GPT-4 to generate explanations for legal concepts from statutory provisions. However, to the best of our knowledge, previous work has not: (1) used LLM-generated stories as a medium to explain complex concepts, especially in the under-explored legal domain, (2) generated and refined (via expert feedback) questions for the assessment of concept comprehension, nor (3) validated the effectiveness of LLM-generated stories in enhancing comprehension among non-experts.
In this work, we explore a novel application of LLMs that focuses on the use of generated stories and questions to facilitate the learning and assessment of legal concept understanding. We use a human-in-the-loop pipeline that combines LLM and expert input to generate stories and multiple-choice questions. We loop in both Prolific workers and legal experts to ensure that the LLM-generated content is of high-quality. Our pipeline presents a holistic approach to LLMs' application in the legal education domain, where both the learning intervention (stories) and assessment (reading comprehension questions) are generated and evaluated. By providing a reusable dataset and promising experiment results, our work has strong implications for the broader use of LLMs to enhance teaching and learning and to improve general legal literacy. Our contributions are as follows: 
 
 * We create a novel legal education dataset, LegalStories, which presents legal concepts with their definitions, LLM-generated stories and questions, and human annotations for future NLP and legal education research. 
 * We provide extensive comparisons of three LLMs, namely, LLaMA 2, GPT-3.5, and GPT-4, to generate legal stories and questions with both automatic and human evaluations. 
 * We conduct RCTs with both native and non-native English speakers to learn legal concepts, demonstrating that LLM-generated stories improve concept comprehension and interest in law among non-native speakers compared to Wikipedia definitions. We also find that LLM-generated stories consistently help both native and non-native participants in relating legal concepts to their personal lives."
Prompt Optimization via Adversarial In-Context Learning,2312.02614v3,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2312.02614v3.pdf,"adv-ICL orchestrates a minimax game between a Generator and a Discriminator, both powered by LLMs with few-shot prompts. The Generator crafts responses to unlabeled examples, while the Discriminator distinguishes between generated and ground truth outputs. Updates are made by a Prompt Modifier which modifies prompts based on the adversarial loss.","We propose a new method, Adversarial In-Context Learning (adv-ICL), to optimize prompts for in-context learning (ICL). Inspired by adversarial learning, adv-ICL is implemented as a two-player game between a generator and discriminator, with LLMs acting as both. In each round, given an input prefixed by task instructions and several exemplars, the generator produces an output. The discriminator then classifies the generator 's input-output pair as model-generated or real data. Based on the discriminator' s loss, a prompt modifier LLM proposes possible edits to the generator and discriminator prompts, and the edits that most improve the adversarial loss are selected. We show that applying adv-ICL results in significant improvements over state-of-the-art prompt optimization techniques for both open and closed-source models on","Generative Adversarial Networks (GANs) and adversarial learning have driven significant progress across a range of domains, including image generation, domain adaptation, and enhancing model robustness. At its core, adversarial learning frames training as a minimax game between a generator and a discriminator. The generator aims to generate output realistic enough that the discriminator classifies it as real (i.e., not generated), while the discriminator aims to accurately differentiate between generator output and real training samples. After each round, the parameters of both models are updated based on an adversarial loss, and the process repeats. As the generator improves, the discriminator improves alongside it, finding ""weak spots"" in generator output that may go undiscovered in non-adversarial training, ultimately resulting in better generator outputs. 
Despite success in other domains, applying adversarial learning to pre-training LLMs is impractical due to the data and computational overheads associated with training two models. Particularly for novel tasks where data is often scarce, it is desirable to have methods that can improve model performance using limited data. In this work, we solve this problem by applying adversarial learning to in-context learning (ICL), which has shown to be an effective method to improve model performance with few training samples. Though, effective, ICL has shown to be sensitive to changes in prompts. We introduce Adversarial In-Context Learning (adv-ICL), which applies insights from adversarial learning to prompt optimization for ICL. adv-ICL keeps model parameters fixed and instead updates model prompts in an adversarial manner. This alleviates compute and data requirements, while still allowing improvements in model performance. 
adv-ICL uses an adversarial objective and three main modules, implemented as LLMs, to optimize a model 's prompt for a given task, as shown in Figure. The first module is a generator (G), which is tasked with generating realistic, task appropriate output given a task instruction and an input. The second is a discriminator (D) which has the goal of classifying its inputs as real or produced byG. Finally, there is a prompt modifierMwhich is responsible for updating the prompts toGandD. As in typical adversarial learning, the learning objective is set up as a minimax game betweenGandD. In each round, Gproduces an output based on an input and a prompt consisting of a task instruction and several example inputs and outputs. Dthen classifies the pair constructed of the original input andG' s output as generated or real. Finally, Mproduces a number of possible updates toGandD 's prompts, the updates that most improve the adversarial loss fromD' s classification are selected, and the procedure repeats. Through this iterative update procedure adv-ICL is able to improveG's prompt, improving task performance. 
 
 We evaluate adv-ICL on13tasks with various open and closed-source LLMs, finding that adv-ICL outperforms other prompt optimization techniques by large margins across model configurations and tasks. For instance, we increase the accuracy of ChatGPT from 71.0%to 74.0%on MMLU, 79.9%to 82.3%on GSM8K, and 72.1%to 74.0%on BBH. Importantly, adv-ICL requires very few iterations and training samples, increasing performance significantly after only five rounds of training on twenty data points. Finally, adv-ICL is easy to implement, encouraging its use in real-world applications."
Multimodal Contextualized Semantic Parsing from Speech,2406.06438v1,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2406.06438v1.jpg,Example of VG-SPICE inputs as well as a plausible output to produce the correct next state context. New information that the agent is expected to add to the context is shown in green while already known information is noted in red. Grounding entities that have new information being added to them are noted in blue and orange. The current context is shown as a textually prompted representation of the actual knowledge graph (discussed in Section).,"We introduce Semantic Parsing in Contextual Environments (SPICE), a task designed to enhance artificial agents 'contextual awareness by integrating multimodal inputs with prior contexts. SPICE goes beyond traditional semantic parsing by offering a structured, interpretable framework for dynamically updating an agent' s knowledge with new information, mirroring the complexity of human communication. We develop the VG-SPICE dataset, crafted to challenge agents with visual scene graph construction from spoken conversational exchanges, highlighting speech and visual data integration. We also present the Audio-Vision Dialogue Scene Parser (AViD-SP) developed for use on VG-SPICE. These innovations aim to improve multimodal information processing and integration. Both the VG-SPICE dataset and the AViD-SP model are publicly available.","Imagine you are taking a guided tour of an art museum. During the tour as you visit each piece of art, your guide describes not only the artworks themselves but also the history and unique features of the galleries and building itself. Through this dialog, you are able to construct a mental map of the museum, whose entities and their relationships with one another are grounded to their real-world counterparts in the museum. We engage in this type of iterative construction of grounded knowledge through dialog every day, such as when teaching a friend how to change the oil in their car or going over a set of X-rays with our dentist. As intelligent agents continue to become more ubiquitous and integrated into our lives, it is increasingly important to develop these same sorts of capabilities in them. 
Toward this goal, this work introduces Semantic Parsing in Contextual Environments (SPICE), a task designed to capture the process of iterative knowledge construction through grounded language. It emphasizes the continuous need to update contextual states based on prior knowledge and new information. SPICE requires agents to maintain their contextual state within a structured, dense information framework that is scalable and interpretable, facilitating inspection by users or integration with downstream system components. SPICE accomplishes this by formulating updates as Formal Semantic Parsing, with the formal language defining the allowable solution space of the constructed context. 
Because the SPICE task is designed to model real-world and embodied applications, such as teaching a mobile robot about an environment or assisting a doctor with medical image annotations, there are crucial differences between SPICE and traditional text-based semantic parsing. First, SPICE considers parsing language within a grounded, multimodal context. The language in cases like these may have ambiguities that can only be resolved by taking into account multimodal contextual information, such as from vision. 
Furthermore, SPICE supports linguistic input that comes in the form of both speech and text. In real-world embodied interactions, language is predominantly spoken, not written. While modern automatic speech recognition (ASR) technology is highly accurate, it is still sensitive to environmental noise and reverberation, and representing the input language as both a waveform as well as a noisy ASR transcript can improve robustness. While we do not consider it here, the SPICE framework also supports paralinguistic input such as facial expressions, eye gaze, and hand gestures. 
We present a novel dataset, VG-SPICE, derived from the Visual Genome, an existing dataset comprised of annotated visual scene graphs representing constituent entities and relational prepositions, enhanced with additional processing and synthetic augmentation to form a foundational representation for SPICE tasks. VG-SPICE simulates the conversational construction of visual scene graphs, wherein a knowledge graph representation of the entities and relationships contained within an image must be collected from the visual inputs and audio dialogue. This dataset, along with an initial model trained for VG-SPICE, sets the baseline for future efforts. Figure shows an example of a typical VG-SPICE sample. The figure shows how potential semantic parses can be extracted from the visual scene and spoken utterance conditioned on what information is already known about the scene. 
The remainder of this paper is structured as follows: It begins with a detailed analysis of the SPICE task, introduces the VG-SPICE dataset, and presents our AViD-SP model. It then delves into experimental results, showcasing the model's ability to process and interpret context consistent with the SPICE framework. Finally we outline the implications and directions for future research. The main contributions include: 
 
 * A definition of the Semantic Parsing in Contextual Environments (SPICE) task, highlighting its challenges, scope, and significance in enhancing human-AI communication. 
 * The creation of a large, machine-generated SPICE dataset, VG-SPICE, leveraging existing machine learning models and the Visual Genome dataset, to motivate SPICE research. 
 * An initial baseline model, Audio-Vision Dialogue Scene Parser (AViD-SP), for VG-SPICE that integrates Language Models with Audio/Visual feature extractors, establishing a research benchmark for SPICE. As a component of AViD-SP, we also introduce a novel pretrained encoder adaption and multimodal fusion method, the Grouped Multimodal Attention Down Sampler (GMADS) to motivate the exploration of additional multimodal adaptation methods."
LangBridge: Multilingual Reasoning Without Multilingual Supervision,2401.10695v2,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2401.10695v2.pdf,"MGSM accuracy (%) of MetaMath models and models aligned with mT5-XL encoder (2B) via LangBridge (LB). In addition to the average (avg) accuracy, we also report the average accuracy of high-resource languages (hrl) and underrepresented languages (url) classified by.","We introduce LangBridge, a zero-shot approach to adapt language models for multilingual reasoning tasks without multilingual supervision. LangBridgeoperates by bridging two models, each specialized in different aspects: (1) one specialized in understanding multiple languages (e.g., mT5 encoder) and (2) one specialized in reasoning (e.g., Orca 2). LangBridgeconnects the two models by introducing minimal trainable parameters between them. Despite utilizing only English data for training, LangBridgeconsiderably enhances the performance of language models on low-resource languages across mathematical reasoning, code completion, logical reasoning, and commonsense reasoning. Our analysis suggests that the efficacy of LangBridgestems from the language-agnostic characteristics of multilingual representations. We publicly release our code and models.","Language models (LMs) are known to exhibit inferior performance in solving reasoning tasks such as math or coding in low-resource languages. This tendency primarily stems from the fact that LMs are predominantly trained on corpora comprised of a few high-resource languages. This results in low-resource languages being represented as long-tail knowledge. 
Prior works have mainly approached this problem by adapting English-centric LMs to other languages through continual training on the target language. However, scaling this approach to a large number of languages is challenging, as it requires targeted training corpora for each language. This issue is particularly pronounced for LMs such as MetaMath and Orca 2, which have undergone continuous domain-specific adaptation from Llama 2. These specialized, domain-specific datasets are typically in English, complicating multilingual support for the underlying LM. 
In this paper, we introduce LangBridge, a novel approach that adapts LMs to solve multilingual reasoning tasks without explicitly training on multilingual data. Inspired from the multimodal literature that integrates two independently pretrained modalities, we leverage the encoder from mT5 and introduce a small number of trainable parameters between the encoder and the target LM. Most importantly, our approach does not require multilingual supervision and solely relies on English data while generalizing to multiple languages during test time, resembling zero-shot cross-lingual transfer. 
We demonstrate the effectiveness of LangBridgeby applying our method to LMs specialized in diverse reasoning tasks of mathematical reasoning, code completion, logical reasoning. Our empirical results show LangBridgesubstantially enhances the multilingual reasoning performance of LMs. For example, LangBridgeapplied to MetaMath-13B leveraging mT5-XL encoder (2.2B) boosts the average accuracy on MGSM from 40.5%to 53.5%, matching the performance of PaLM-540B, which stands at 51.3%. We observe LangBridgealso significantly boosts LM performance on reasoning datasets that require intrinsic linguistic understanding such as specific subtasks of Big-Bench Hard and XCOPA. 
We hypothesize that the effectiveness of LangBridgeis anchored in the language-agnostic characteristics of multilingual representations. By mapping these representations to the target LM's input space, we conjecture that the LM is able to grasp the semantics of these representations. Since these representations are language-neutral, understanding them allows the LM to become less dependent on the specific language of the input, thereby enabling it to tackle tasks in languages it rarely encountered during pretraining. Our empirical analysis of LangBridge, using principal component analysis (PCA) and qualitative methods, supports this hypothesis."
Cognitive Visual-Language Mapper: Advancing Multimodal Comprehension with Enhanced Visual Knowledge Alignment,2402.13561v2,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2402.13561v2.png,"It illustrates the performance of LMMs on visual information-seeking questions. The bottom part shows the widely-used architecture of open-source LMMs, where the visual mapping network is usually pretrained on massive image-text captioning data. All LMMs including GPT-4V (Date: 2023.11.17) and Gemini-Pro make incorrect decisions.","Evaluating and Rethinking the current landscape of Large Multimodal Models (LMMs), we observe that widely-used visual-language projection approaches (e.g., Q-former or MLP) focus on the alignment of image-text descriptions yet ignore the visual knowledge-dimension alignment, i.e., connecting visuals to their relevant knowledge. Visual knowledge plays a significant role in analyzing, inferring, and interpreting information from visuals, helping improve the accuracy of answers to knowledge-based visual questions. In this paper, we mainly explore improving LMMs with visual-language knowledge alignment, especially aimed at challenging knowledge-based visual question answering (VQA). To this end, we present a Cognitive Visual-Language Mapper (CVLM), which contains a pretrained Visual Knowledge Aligner (VKA) and a Fine-grained Knowledge Adapter (FKA) used in the multimodal instruction tuning stage. Specifically, we design the VKA based on the interaction between a small language model and a visual encoder, training it on collected image-knowledge pairs to achieve visual knowledge acquisition and projection. FKA is employed to distill the fine-grained visual knowledge of an image and inject it into Large Language Models (LLMs). We conduct extensive experiments on knowledge-based VQA benchmarks and experimental results show that CVLM significantly improves the performance of LMMs on knowledge-based VQA (average gain by 5.0%). Ablation studies also verify the effectiveness of VKA and FKA, respectively.","Recent Large Multimodal Models (LMMs) such as GPT-4V, Gemini, MiniGPT-4, InstructBLIP, LLaVA, and many others, have achieved impressive performance in a variety of visual understanding and reasoning tasks, especially on Visual Question Answering (VQA). Current open-source LMMs are usually constructed by combining pertained visual encoders and Large Language Models (LLMs), as depicted in the bottom part of Figure, where a visual mapping network (e.g., Q-former, Linear, or MLP) is employed to project visual representations into the language space of LLMs. Although such LMMs have achieved powerful visual understanding capability similar to GPT-4V and Genimi on some image understanding tasks such as Image Captioning, Visual Dialogue, Visual Entailment, and VQA, they often fall short of knowledge-based VQA, which necessitates relevant knowledge to answer these visual questions. As the cases illustrated in Figure, these advanced LMMs (including GPT-4V and Gemini-Pro) can not give correct answers to simple visual information seeking questions: Who is the manufacturer of this aircraft; What country does this building belong to? . 
In light of this, rethinking the construction process of LMMs from the initial pretraining stages, we discover that these visual mapping networks trained on massive image-text captioning pairs simply transfer visual features to their language descriptions. They overlook the visual language knowledge-dimension alignment. i.e., connecting visuals to their relevant knowledge. As we know, visual knowledge plays a pivotal role in the way humans understand and interact with the world. It extends beyond the mere ability to recognize and interpret visuals, incorporating an understanding of spatial relationships, patterns, and symbols, which are essential components of human cognition. Additionally, previous works also demonstrated that introducing visual knowledge can improve the performance of pretrained language models on natural language understanding and open-ended text generation tasks. Inspired by these insights, we focus on enhancing LMMs through the introduction of visual-language knowledge alignment, going beyond the conventional scope of visual-language integration. 
To this end, we present a Cognitive Visual-Language Mapper (CVLM), which contains a pretrained Visual Knowledge Aligner (VKA) and a Fine-grained Knowledge Adapter (FKA). Specifically, we devise VKA based on a small language model that interacts with fine-grained image representation in each block. The output hidden states of VKA are fed into the LLM as the knowledge embedding tokens by a linear projection layer. To make VKA effectively capture image-relevant knowledge, we first train it on image-knowledge pairs collected from Wikipedia via the next tokens prediction. Like Q-former and prefix-tuning, we only fine-tune some learnable query tokens and the linear layer to acquire fixed-length visual knowledge representation and convert it into the representation space of LLM. In addition, considering that visual objects contain fine-grained visual knowledge, we introduce FKA to gain comprehensive visual knowledge of an image and distill valuable visual knowledge from the whole knowledge representation sequence. The output knowledge vectors of FKA are injected into each layer of LLMs to realize in-depth interactions between LLMs and detailed visual knowledge. By doing so, CVLM is capable of connecting visuals to relevant knowledge, enabling LMMs to utilize them during multimodal understanding and generation. 
To verify the effectiveness of CVLM, we conduct extensive experiments on image-centered, knowledge-based, and complex visual reasoning question-answering scenarios: VQAv2, OKVQA, A-OKVQA, Infoseek, TextVQA, and SeedBench. The experimental results show that CVLM significantly outperforms previous strong baselines such as LLaVA-v1.5. The ablation and case studies indicate that CVLM is capable of linking visual knowledge and improving performance on knowledge-intensive tasks via the introduced aligner and adapter. 
Our contributions can be summarized as follows: 
 
 * We present a cognitive visual-language mapper to achieve visual-language knowledge alignment, which contains a pretrained visual knowledge aligner and a fine-grained knowledge adapter that is used to distil and inject valuable visual knowledge into LLMs. 
 * To the best of our knowledge, we are the first to explore the visual-language knowledge alignment during the pretraining and finetuning stages of LMMs, connecting visuals to their knowledge via CVLM. 
 * Experimental results indicate that CVLM significantly improves the performance of LMMs on knowledge-intensive VQA. The ablation studies also verify the effectiveness of VKA and FKA on specific knowledge-based VQA."
"Angry Men, Sad Women: Large Language Models Reflect Gendered Stereotypes in Emotion Attribution",2403.03121v3,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2403.03121v3.pdf,"Stereotypical model biases in gendered emotion attribution for the event ""When I had a serious argument with a dear person"" from the ISEAR dataset. The model attributes woman with sadnessand man with anger. See Table for detailed explanations.","Large language models (LLMs) reflect societal norms and biases, especially about gender. While societal biases and stereotypes have been extensively researched in various NLP applications, there is a surprising gap for emotion analysis. However, emotion and gender are closely linked in societal discourse. E.g., women are often thought of as more empathetic, while men 's anger is more socially accepted. To fill this gap, we present the first comprehensive study of gendered emotion attribution in five state-of-the-art LLMs (openand closed-source). We investigate whether emotions are gendered, and whether these variations are based on societal stereotypes. We prompt the models to adopt a gendered persona and attribute emotions to an event like' When I had a serious argument with a dear person'. We then analyze the emotions generated by the models in relation to the gender-event pairs. We find that all models consistently exhibit gendered emotions, influenced by gender stereotypes. These findings are in line with established research in psychology and gender studies. Our study sheds light on the complex societal interplay between language, gender, and emotion. The reproduction of emotion stereotypes in LLMs allows us to use those models to study the topic in detail, but raises questions about the predictive use of those same LLMs for emotion applications.","Emotions are a ubiquitous experience, yet also vary from person to person. If a colleague publishes prolifically, some people might envythem, others admire their output, and a third might feel sadnessabout their inability to compete. But do these emotional patterns follow broader gender lines? 
How we talk about emotions signals cultural and societal gender stereotypes. Stereotypes can be neutral, positive, or negative generalizations about a specific social group. A gendered emotional stereotype is a generalization about how people feel based on their gender, e.g., ""women are emotional"" or ""men are angry"". While stereotypes are an important heuristic to free cognitive capacity and transmit information as quickly as possible, ""many of the stereotypes of historically powerless groups such as women, black people, or working‐class people variously involve an association with some attribute inversely related to competence or sincerity or both"". 
Given that emotions influence how we perceive and navigate the world, gendered emotional stereotypes limit how specific groups can be seen to engage in a situation, and shape their perceived characteristics. They also impact one 's own ability to conceptualise oneself. Women have historically been characterized as emotional and displaying more sympathy than men. These stereotypes have material consequences: men have been seen as unsuitable for care-giving jobs (e.g., nursing) and women for jobs supposedly requiring emotional distance (e.g., finance or technology). These stereotypes are deeply embedded in popular culture and thus risk being propagated in Large Language Models (LLMs). 
LLMs like LLaMA and GPT-4 use pre-training methods known to encode societal biases and stereotypes. While these issues has received much attention in machine translation as well as other NLP tasks, there is a notable gap in gendered stereotypes research for emotion analysis. Yet emotion analysis is a high-priority aspect in the recent European Union AI Act. For a comprehensive overview of emotion analysis in NLP, see. 
Recent work has harnessed persona-based prompting to reveal the varied stereotypes LLMs can produce. We leverage LLMs' persona capabilities and apply this framework to address the task of emotion attribution: given a persona and an event, the model has to generate an emotion experienced by that person, and an explanation. Figure shows an illustrative example. Then, we address two pivotal research questions (RQs): (RQ1) Do LLMs exhibit gendered emotions? 
 And, if so, (RQ2) are these differences shaped by actual differences in lived experiences or do they reflect gendered stereotypes? 
Contributions 1) We present the first study examining societal biases and stereotypes in emotion attribution in five state-of-the-art LLMs. 2) We combine LLMs 'persona capabilities with events from the ISEAR dataset to address the task of emotion attribution. 3) We provide a quantitative study based on over 200K completions generated by the five models for over 7,000 events and two personas, spanning over 400 unique emotions. 4) We qualitatively study the model explanations. 
We find strong evidence of gendered stereotyping across the five LLMs, which strongly aligns with findings in psychology and gender studies: models overwhelmingly link sadnesswith women and angerwith men. However, comparing to the gender and stated emotion of the subjects in the data set, we show this association does not correspond to men' s and women's lived experiences, raising questions about the use of LLMs in emotion applications. 
We publish all our data to support future studies on emotion and gendered stereotypes at <https: //github. com/MilaNLProc/emotion_gendered_stereotypes>."
STICKERCONV: Generating Multimodal Empathetic Responses from Scratch,2402.01679v2,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2402.01679v2.pdf,"An example of multimodal conversation in the STICKERCONV. Both parties can utilize the stickers to express their emotions, which enhances interactivity and expression. The assistant can empathize with the user according to the conversation (green text).","Stickers, while widely recognized for enhancing empathetic communication in online interactions, remain underexplored in current empathetic dialogue research, notably due to the challenge of a lack of comprehensive datasets. In this paper, we introduce the Agent for STICKERCONV (Agent4SC), which uses collaborative agent interactions to realistically simulate human behavior with sticker usage, thereby enhancing multimodal empathetic communication. Building on this foundation, we develop a multimodal empathetic dialogue dataset, STICKERCONV, comprising 12.9K dialogue sessions, 5.8K unique stickers, and 2K diverse conversational scenarios. This dataset serves as a benchmark for multimodal empathetic generation. To advance further, we propose PErceive and Generate Stickers (PEGS), a multimodal empathetic response generation framework, complemented by a comprehensive set of empathy evaluation metrics based on LLM. Our experiments demonstrate PEGS's effectiveness in generating contextually relevant and emotionally resonant multimodal empathetic responses, contributing to the advancement of more nuanced and engaging empathetic dialogue systems.","Increasing research indicates that utilizing stickers in online chats can effectively alleviate stress, augment personal happiness, and notably boost empathy. Prior studies on stickers primarily concentrated on sentiment analysis and recommendation systems, overlooking their vast potential in empathetic response generation. Most empathetic response generation tasks focus solely on textual modality, yet stickers in chats convey more abundant and intuitive emotional information, enhancing the expressiveness and emotional depth of responses. 
Integrating stickers with textual communication and interspersing stickers within the dialogue can yield more varied and superior-quality empathetic replies. A primary challenge in integrating stickers into empathetic response generation is developing a high-quality dataset to support this innovative multimodal communication. To address this, we leverage large language model (LLMs) for dataset construction. LLMs, with their extensive world knowledge and text processing capabilities, demonstrate near-human annotation abilities. However, applying LLMs directly have limitations in empathetic tasks, excelling in responding to explicit human instructions but lacking proactivity, a critical aspect of empathy. Empathy necessitates understanding others' emotions and the ability to actively express support and understanding. To mitigate this, we introduce a multi-agent system based on LLMs, Agent for STICKERCONV (Agent4SC). This system, through inter-agent interactions, utilizes stickers to simulate human-like dialogue scenarios. It not only generates text responses but also strategically selects suitable stickers, thereby effectively enhancing empathy. 
Based on Agent4SC, we build a multimodal empathetic dataset, STICKERCONV, that comprises 12.9K dialogue sessions and 5.8K unique stickers. STICKERCONVboasts an average of 5.22 stickers per dialogue session, mirroring the sticker usage patterns observed in human communication. Figure depicts an example of conversations in our dataset. To the best of our knowledge, this is the first multimodal empathetic dialogue dataset, with the particular utility of sticker as non-textual modal information to better facilitate empathy. 
Although Agent4SCeffectively generates multimodal empathetic responses, it is limited by expensive inference costs and specific sticker databases. To further advance the research on multimodal empathetic dialogue, we develop an end-to-end multimodal empathetic response generation framework, PEGS, with the ability to PErceive and Generate Stickers. Beyond the general ability to generate textual empathetic responses, PEGSreceives multimodal inputs and autonomously generates stickers based on the emotional and contextual aspects of the dialogue at the appropriate moment. Furthermore, to simulate human communications on social media in the real world, our model supports interleaved multiple image and text inputs. 
Misalignments between the modal quantities of predicted and golden responses can distort evaluation outcomes, and empathy is difficult to quantify due to its subjective nature. To address this, we propose a novel method for evaluating multimodal empathetic responses, focusing on empathy, consistency, and ranking. Utilizing the extensive world knowledge and anthropomorphic abilities of LLMs, this approach provides solid support for assessing multimodal empathetic replies. 
In conclusion, the main contributions of this work are as follows: 
 
 * We introduce an LLM-based multi-agent system, Agent for STICKERCONV (Agent4SC), which integrates stickers into empathetic dialogues, ensuring contextual consistency, variety, and empathy aligned with human interactions. Using Agent4SC, we create a multimodal empathetic dialogue dataset, STICKERCONV. 
 * We design PErceive and Generate Stickers (PEGS), a multimodal empathetic response generation framework that intuitively incorporates stickers based on the emotional and contextual dynamics of the dialogue. PEGSadeptly processes multimodal inputs, generating empathetic textual responses and using stickers appropriately to enhance these responses. 
 * We propose a method for assessing multimodal empathetic responses. It leverages LLM to evaluate the quality of these responses, with a specific focus on empathy, consistency, and ranking."
EXAMS-V: A Multi-Discipline Multilingual Multimodal Exam Benchmark for Evaluating Vision Language Models,2403.10378v1,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2403.10378v1.pdf,Data distribution for our EXAMS-Vdataset: languages and subjects.,"We introduce EXAMS-V, a new challenging multi-discipline multimodal multilingual exam benchmark for evaluating vision language models. It consists of 20,932multiple-choice questions across 20school disciplines covering natural science, social science, and other miscellaneous studies, e.g., religion, fine arts, business, etc. EXAMS-Vincludes a variety of multimodal features such as text, images, tables, figures, diagrams, maps, scientific symbols, and equations. The questions come in 11languages from 7language families. Unlike existing benchmarks, EXAMS-Vis uniquely curated by gathering school exam questions from various countries, with a variety of education systems. This distinctive approach calls for intricate reasoning across diverse languages and relies on region-specific knowledge. Solving the problems in the dataset requires advanced perception and joint reasoning over the text and the visual content of the image. Our evaluation results demonstrate that this is a challenging dataset, which is difficult even for advanced vision–text models such as GPT-4V and Gemini; this underscores the inherent complexity of the dataset and its significance as a future benchmark.","Large Language Models (LLMs) have recently demonstrated impressive skills in understanding and generating natural languages. This progress has paved the way for significant advancements in LLM-based vision models. Notable developments like GPT-4V and Gemini represent a new era in image understanding, exhibiting remarkable proficiency in interpreting and analyzing visual data alongside textual information. However, as Vision Language Models (VLMs) grow more sophisticated, existing benchmarks are becoming outdated, and unable to accurately assess these models 'performance. 
For LLM evaluation, standardized testing akin to school examinations has proven to be an effective measure of a model' s capabilities. A typical benchmark MMLU, which contains 57 subjects across science, engineering, and humanities, has become a de facto benchmark for LLM evaluation. Several other school exam datasets have also set the standard in evaluating LLMs in different languages. 
In terms of VLM, a comparable benchmarking framework is conspicuously absent. Existing benchmarks are (1) primarily monolingual, focused on English; (2) mostly not from school exams, leading to differences in methods of examining humans; (3) tend to keep images and text separate, which fails to challenge models with more complex tasks involving integrated visual elements like tables, symbols, and scientific notations. 
We introduce EXAMS-V, which addresses all these issues. First, this dataset represents a significant leap forward, treating visual and text content as a cohesive unit. This forces models to engage in more sophisticated processing, including distinguishing, preprocessing, and logical reasoning over combined textual and visual information. Additionally, EXAMS-Vhas a multilingual reach, covering 7language families, further enhancing its complexity and applicability. 
The key contributions of our paper include: 
 
 * We introduce a novel dimension to benchmarking vision language models, requiring them to reason over a unified snapshot that includes text, images, tables, graphs, and more. For this, we propose a new multimodal multilingual dataset, EXAMS-V, comprising 20,932questions, spanning 11languages and 20subjects. 
 * We evaluate the performance of state-of-the-art large language models and vision language models on our proposed dataset. 
Through EXAMS-V, we aim to set a new standard in evaluating VLMs, providing a more realistic and challenging benchmark that mirrors the complexity and diversity of real-world information processing."
Text Embedding Inversion Security for Multilingual Language Models,2401.12192v4,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2401.12192v4.pdf,"Schematic overview of a text embedding inversion attack. A user accesses an EaaS provider, while an attacker is eavesdropping. Although the attacker has no direct access to the embedding model, they can reliably decode the information stored in the embeddings.","Textual data is often represented as real-numbered embeddings in NLP, particularly with the popularity of large language models (LLMs) and Embeddings as a Service (EaaS). However, storing sensitive information as embeddings can be susceptible to security breaches, as research shows that text can be reconstructed from embeddings, even without knowledge of the underlying model. While defence mechanisms have been explored, these are exclusively focused on English, leaving other languages potentially exposed to attacks. This work explores LLM security through multilingual embedding inversion. We define the problem of black-box multilingual and cross-lingual inversion attacks, and explore their potential implications. Our findings suggest that multilingual LLMs may be more vulnerable to inversion attacks, in part because English-based defences may be ineffective. To alleviate this, we propose a simple masking defense effective for both monolingual and multilingual models. This study is the first to investigate multilingual inversion attacks, shedding light on the differences in attacks and defenses across monolingual and multilingual settings.","Industrial applications of natural language processing (NLP) typically utilize language models (LMs) and often rely on vector databases via frameworks such as Embeddings as a Service (EaaS). In this context, sentence embeddings are stored in a remote database, as opposed to raw text, allowing end-users to efficiently search across condensed representations. As embeddings are not human-readable, security of the encoded information may be naively assumed, however recent works have demonstrated that embeddings are no safer than raw text; they are susceptible to inversion attacks, whereby a malicious actor can train models to decode embeddings, thus exposing private information. Concretely, after gaining access to embeddings and the black-box embedder via the EaaS API, the malicious actor can train an external model, which approximates the inversion function that reconstructs the text from the embeddings. As such, there is a substantial threat to privacy if malicious actors are able to eavesdrop on communication channels between EaaS providers and customers, as illustrated in Figure. 
Previous work has shown that an exact match for data recreation can be obtained in specific settings, albeit with the limitation of assuming monolingual English models and embeddings. However, in real-world scenarios, eavesdroppers may not know the source language of the encoded text, as EaaS providers can have international clientele. Thus to assess the current level of risk posed to multilingual LMs, we introduce multilingual inversion attacks. As the first ever study in this direction, we focus specifically on exact text reconstruction, assuming that the language of a target embedding is unknown. Leveraging a state-of-the-art multilingual black-box encoder, we find that the trained model can reconstruct texts in certain languages more effectively than monolingual counterparts. Additionally, we also introduce cross-lingual inversion attacks, to ascertain whether inversion attacks can be successful when the target language is unknown by the attacker. We thus attempt cross-lingual text reconstruction (i.e., reconstructing German text with a model not trained on German reconstruction), introducing an Ad hoc Translation method to overcome the evaluation limitation of current string-matching metrics in this cross-lingual scenario. Finally, we assess the efficacy of an existing defense method by, ultimately finding that defenses intended for monolingual models fall short in protecting multilingual models. To this end, we introduce simple masking defense, which proves effective for both monolingual and multilingual models, and which also does not require additional model training. All our trained inversion models and code are open source, encouraging the research community to engage in development of defenses for vulnerable multilingual models. 
Overview of Multilingual Vec2Text, extending Vec2Text with Ad hoc Translation and Masking Defense Mechanism (outlined in the green dashed line frame). Given access to a target embedding e and query access to the embedder ϕ via an EaaS API, the inversion model ψ iteratively generates hypotheses ê to attain the target. The generated text x̂ is in German, and translated to English (AdTrans (x̂) ), to be compared with the target text x. The masking defense serves as an effective defense against inversion attacks while preserving utility in NLP tasks such as retrieval."
Context-aware Difference Distilling for Multi-change Captioning,2405.20810v2,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2405.20810v2.pdf,Three examples about multi-change captioning. (a) includes certain object changes; (b) consists of object and background changes; (c) shows both object changes and irrelevant viewpoint change. These changes are shown in colored boxes.,"Multi-change captioning aims to describe complex and coupled changes within an image pair in natural language. Compared with single-change captioning, this task requires the model to have higher-level cognition ability to reason an arbitrary number of changes. In this paper, we propose a novel context-aware difference distilling (CARD) network to capture all genuine changes for yielding sentences. Given an image pair, CARD first decouples context features that aggregate all similar/dissimilar semantics, termed common/difference context features. Then, the consistency and independence constraints are designed to guarantee the alignment/discrepancy of common/difference context features. Further, the common context features guide the model to mine locally unchanged features, which are subtracted from the pair to distill locally difference features. Next, the difference context features augment the locally difference features to ensure that all changes are distilled. In this way, we obtain an omni-representation of all changes, which is translated into linguistic sentences by a transformer decoder. Extensive experiments on three public datasets show CARD performs favourably against state-of-the-art methods. The code is available at <https: //github. com/tuyunbin/CARD>.","Change captioning aims to describe differences between a pair of similar images, which enables many important applications, such as automatic report generation about change conditions of surveillance areas and pathological changes between medical images. On the other hand, this task is more challenging than image captioning. This is because machines need to understand the contents of two images simultaneously, and further reason and caption all genuine changes between them, while resisting irrelevant viewpoint/illumination changes. 
Recently, single-change captioning has made remarkable progress. In a dynamic environment, however, the changes are usually the many-in-one, where multiple changes exist in an image pair. For instance, there are multiple object/background changes (Figure (a) (b) ). In other cases, object and viewpoint changes simultaneously appear (Figure (c) ). In above cases, unchanged objects commonly mingle with changed ones and even appear position misalignment under viewpoint changes. Such distractors pose a great challenge to identify and caption the genuine changes. 
There are a few attempts to address multi-change captioning. The pioneer work computed pixel differences of two images, which is sensitive to noise. Latest works tried to capture differences at representation space: some of them computed difference features by subtraction, while the others built the correlations between the patches of two images to model the change features for caption generation. 
Despite progress, the above endeavors in multi-change captioning have several limitations. (1) Direct subtraction between two images generalizes poorly to unaligned image pairs under viewpoint changes (Figure (c) ). (2) Directly correlating two images fails to sufficiently mine locally unchanged features as multiple objects change, because such features might mingle with the features of changed objects. (3) These methods focus on modeling locally difference features, which are useful to catch conspicuous changes. Nevertheless, certain local changes with weak features might be overlooked, e.g., the car occluded by its shadows in Figure (a). These limitations would result in obtaining unreliable difference features for the language decoder. 
We notice that the above methods capture differences between two images only based on local features, while neglecting the use of more comprehensive features. We argue that, to learn locally unchanged/changed features of two images, the model should first encapsulate their context features of commonality and difference. Such context features aggregate all similar/dissimilar semantics, termed common/difference context features. The former can help correlate and mine locally common features for deducing locally difference features, while the latter can augment the locally difference features to ensure all changes are distilled. 
In this paper, we propose a Context-Aware DiffeRence Distilling (CARD) network to learn the robust difference features under multi-change scenes. Specifically, given the featuers of two images, we first build intra-image interaction to help the model understand each image content of the pair. Then, we use CARD to decouple the common/difference context features from the image pair. Herein, the common context features of two images summarize joint semantics in between; the difference context feature in each image provides an independent space to preserve its all changed semantics. Besides, the consistency and independence constraints are designed to enforce the alignment and discrepancy of common and difference context features, respectively. Next, guided by the common context features, CARD models inter-image interaction to mine locally common features, which are removed from the pair to distill locally difference features. Subsequently, CARD augments the locally difference features via the difference context features, so as to construct an omni-representation of all changes, for generating descriptions by a transformer decoder.
Our key contributions are: (1) We propose CARD to first decouple common and difference context features, and then use them to facilitate modeling an omni-representation of all changes for multi-change captioning. (2) The consistency and independence constraints are customized to guarantee the alignment and discrepancy of decoupled common and difference context features. (3) Extensive experiments show our method achieves the state-of-the-art results on three public datasets."
Dataflow-Guided Retrieval Augmentation for Repository-Level Code Completion,2405.19782v1,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2405.19782v1.pdf,"A real-world example of repository-level code completion. The code LM CodeGen25-7B-mono fails to complete the last code line correctly when entering only the unfinished code (Zero-Shot). The model needs background knowledge relevant to newSignal, and the retrieval of this knowledge can be guided by dataflow.","Recent years have witnessed the deployment of code language models (LMs) in various code intelligence tasks such as code completion. Yet, it is challenging for pre-trained LMs to generate correct completions in private repositories. Previous studies retrieve cross-file context based on import relations or text similarity, which is insufficiently relevant to completion targets. In this paper, we propose a dataflow-guided retrieval augmentation approach, called DraCo, for repository-level code completion. DraCoparses a private repository into code entities and establishes their relations through an extended dataflow analysis, forming a repo-specific context graph. Whenever triggering code completion, DraCoprecisely retrieves relevant background knowledge from the repo-specific context graph and generates well-formed prompts to query code LMs. Furthermore, we construct a large Python dataset, ReccEval, with more diverse completion targets. Our experiments demonstrate the superior accuracy and applicable efficiency of DraCo, improving code exact match by 3.43%and identifier F1-score by 3.27%on average compared to the state-of-the-art approach.","Pre-trained language models (LMs) of code have shown remarkable performance in improving programming productivity. Instead of using a single code file, well-designed programs emphasize separating complicated functionality into independent modules. While facilitating collaborative development and software maintenance, it introduces the real-world problem of repository-level code completion: given an unfinished code file in a private repository, complete the following pieces of code at the cursor position. 
Despite pre-training on large-scale corpora, code LMs are still blind to unique naming conventions and programming styles in private repositories. Previous works finetune LMs to leverage cross-file context, which requires additional training data and is difficult to work with larger LMs. Recently, retrieval-augmented generation (RAG) is widely used to aid pre-trained LMs with external knowledge and maintain their parameters intact. For repository-level code completion, the retrieval database is the current private repository. The state-of-the-art approach, RepoCoder, iteratively incorporates a text similarity-based retriever and a code LM. 
As shown in Figure, the CodeGen25 Python model with 7 billion parameters assigns a value to the attribute channel of the object newSignal, which seems rational in the unfinished code but is actually outside the list of valid attributes. Due to the lack of similar code snippets in the repository, the text similarity-based approach also fails to complete the correct code line. From a programmer's perspective, one would explore the data origin of the variable newSignal in the last line. It comes from the call signal. getSignalByName, where the variable type of signal is RecordSignal imported from the module RecordSignal. After providing relevant background knowledge in the private repository, the model would know that the variable type of newSignal is the class Signal and thus call the correct function. 
Inspired by this programming behavior in private repositories, we propose DraCo, a novel dataflow-guided retrieval augmentation approach for repository-level code completion, which steers code LMs with relevant background knowledge rather than similar code snippets. Dataflow analysis is a static program analysis reacting to data dependency relations between variables in a program. In this work, we extend traditional dataflow analysis by setting type-sensitive dependency relations. We employ the standard RAG framework: (i) Indexing, which parses a private repository into code entities and establishes their relations through dataflow analysis, forming a repo-specific context graph for retrieval. (ii) Retrieval, which uses dataflow analysis to obtain fine-grained import information in the unfinished code and retrieves relevant code entities from the pre-built context graph. (iii) Generation, which organizes the relevant background knowledge as natural code and concatenates it with the unfinished code to generate well-formed prompts for querying code LMs. 
In addition to the existing dataset CrossCodeEval for repository-level code completion, we construct a new dataset, ReccEval, with diverse completion targets collected from Python Package Index (PyPI). We conduct experiments with popular LMs including adapted code LMs, specialized code LMs, and GPT models. Our experiments demonstrate that DraCoachieves generally superior accuracy across all settings. Furthermore, DraCois plug-and-play for various code LMs and efficient to real-time code completion. 
Our main contributions are outlined as follows: 
 * We design an extended dataflow analysis by setting type-sensitive data dependency relations, which supports more precise retrieval. 
 * We propose DraCo, a dataflow-guided retrieval augmentation approach for repository-level code completion. DraCobuilds a repo-specific context graph for retrieval and generates well-formed prompts with relevant background knowledge in real-time completion. 
 * We construct a Python dataset ReccEvalwith diverse completion targets. The experimental results show that DraCoimproves code exact match by 3.43%, identifier F1-score by 3.27%, and prompt generation time by 100
 ×on average compared to the second-best approach RepoCoder. Our source code and data are available at <https: //github. com/nju-websoft/DraCo>."
MemeGuard: An LLM and VLM-based Framework for Advancing Content Moderation via Meme Intervention,2406.05344v1,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2406.05344v1.pdf,An instance of the meme intervention task.,"In the digital world, memes present a unique challenge for content moderation due to their potential to spread harmful content. Although detection methods have improved, proactive solutions such as intervention are still limited, with current research focusing mostly on text-based content, neglecting the widespread influence of multimodal content like memes. Addressing this gap, we present MemeGuard, a comprehensive framework leveraging Large Language Models (LLMs) and Visual Language Models (VLMs) for meme intervention. MemeGuard harnesses a specially fine-tuned VLM, VLMeme, for meme interpretation, and a multimodal knowledge selection and ranking mechanism (MKS) for distilling relevant knowledge. This knowledge is then employed by a general-purpose LLM to generate contextually appropriate interventions. Another key contribution of this work is the Intervening Cyberbullying in Multimodal Memes (ICMM) dataset, a high-quality, labeled dataset featuring toxic memes and their corresponding human-annotated interventions. We leverage ICMM to test MemeGuard, demonstrating its proficiency in generating relevant and effective responses to toxic memes.
 Disclaimer: This paper contains harmful content that may be disturbing to some readers.","In today 's digital world, memes serve as a universal language for expression and engagement. However, as they become a powerful tool for rapid information dissemination, they are increasingly weaponized for cyberbullying and spreading toxic content, posing a challenge to existing content moderation systems. These systems struggle to decipher memes' nuanced meanings, typically reacting rather than proactively mitigating the harm of offensive content. [-10] Intervention represents a proactive approach to content moderation, going beyond simple detection to take preventive action against offensive content. Interventions aim to mitigate the harmful effects of toxic content and foster a more positive and respectful online discourse. However, existing intervention research has primarily been limited to text-based content such as hate speech, and misinformation. The excessive focus on text-based content overlooks the prevalence of multimodal content, which are major contributors to the content ecosystem in social media platforms. This exacerbates the potential for multimodal toxicity, enabling misuse of such mediums. A representative example of intervening in case of a cyberbullying meme is displayed in Figure.
Large Language Models (LLMs), and Visual Language Models (VLMs) have shown remarkable capability in understanding and generating human-like text and multimedia content. This has led to their application in various tasks within the domain of content moderation, but predominantly for detection purposes. Their ability to understand nuanced language and visual cues has been leveraged to detect toxic or harmful content, both in text and multimodal formats. Some attempts have also been made to use these models for intervention generation tasks, but these efforts have been largely restricted to text-based content. In this landscape, the zero-shot learning of these models presents a unique advantage, particularly for tasks like meme intervention, which are characterized by data scarcity. 
While these models hold considerable promise for content moderation, they are not without their limitations. First and foremost, vanilla LLMs and VLMs lack grounding in a knowledge base specific to the task at hand. This can lead to the generation of generic interventions that may fail to adequately address the bias, stereotype, assertions, and toxicity present in the meme content. In terms of visual content, while VLMs have performed well on a variety of traditional visual-linguistic tasks, they often struggle when it comes to memes. The primary reason behind this is the unique nature of memes, which are highly contextual and often rely on a shared understanding of internet subcultures for their interpretation. This makes the accurate assimilation and analysis of memes a challenging task. Finally, even when these models are grounded in a knowledge base for the task of meme intervention, there is a need to filter irrelevant and noisy information. Without appropriate filtering, these models might end up incorporating irrelevant or misleading information into their interventions.
In order to address these limitations inherent in LLMs and VLMs for meme intervention, we developed a comprehensive framework called MemeGuard. The development of MemeGuard was a multi-stage process designed to create a tool capable of understanding and effectively intervening in the spread of toxic memes. In the first stage, we developed a meme-aligned VLM (VLMeme), specifically fine-tuned to understand and interpret memes in all their complexity. This allowed our model to delve deeper into the content of memes. Next, we utilized this meme-aligned VLM to identify various facets of the meme, such as the underlying toxicity, bias, stereotypes, and claims being made, which provides valuable insights into the meme's potential harm. To address the challenge of irrelevant knowledge, we then proposed a Multimodal Knowledge Selection mechanism (MKS). This mechanism retained only the most relevant knowledge for the intervention generation process. In the subsequent stage, we utilized a general-purpose LLM grounded on this refined knowledge to generate appropriate interventions. This model took the insights provided by the VLM and the ranked knowledge to create contextually relevant and effective responses to toxic memes. Finally, to test the efficacy of our framework, we developed a high-quality labeled dataset featuring a variety of toxic and cyberbully memes with their corresponding human annotated intervention, Intervening Cyberbullying in Multimodal Memes (ICMM) dataset. To summarize, we make the following main contributions:
 A novel task of Meme Intervention to combat the toxicity of cyberbullying memes.
 A novel dataset, ICMM, to advance the research in this area.
 A novel framework, MemeGuard, that utilizes a meme-aligned VLM (VLMeme) to generate contextual information about the meme that is then used to generate the final intervention."
Long Context is Not Long at All: A Prospector of Long-Dependency Data for Large Language Models,2405.17915v1,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2405.17915v1.pdf,"Samples that carry longer dependencies better enhances LLMs' long-context modeling capabilities, even with a fixed training context window of 32k.","Long-context modeling capabilities are important for large language models (LLMs) in various applications. However, directly training LLMs with long context windows is insufficient to enhance this capability since some training samples do not exhibit strong semantic dependencies across long contexts. In this study, we propose a data mining framework ProLong that can assign each training sample with a long dependency score, which can be used to rank and filter samples that are more advantageous for enhancing long-context modeling abilities in LLM training. Specifically, we first use delta perplexity scores to measure the Dependency Strength between text segments in a given document. Then we refine this metric based on the Dependency Distance of these segments to incorporate spatial relationships across long-contexts. Final results are calibrated with a Dependency Specificity metric to prevent trivial dependencies introduced by repetitive patterns. Moreover, a random sampling approach is proposed to optimize the computational efficiency of ProLong. Comprehensive experiments on multiple benchmarks indicate that ProLong effectively identifies documents that carry long dependencies and LLMs trained on these documents exhibit significantly enhanced long-context modeling capabilities.","Large language models (LLMs) are widely used in many natural language processing (NLP) tasks. These tasks often require dealing with long text inputs, such as lengthy documents, long conversation histories in chatbots or large codebases. Therefore enhancing LLMs to model long-context inputs is a prominent desiderata. 
There are primarily two categories of approaches to expand the context window of an LLM. The first category fine-tunes LLMs with longer context windows, while the second category adjusts the LLM 's positional encoding or attention mechanism to accommodate larger position indices without further re-training. However, non-training methods often produce results inferior to those of fine-tuned LLMs, which model long-context inputs more effectively and generally achieve lower perplexity scores. 
Although reported to be feasible, simply fine-tuning LLMs with naively sampled long corpora does not ensure improved long context modeling capabilities. Some of these fine-tuned LLMs may still struggle to effectively process and utilize information from long input contexts even if they obtain a decently low perplexity score. This can lead to low performance in various downstream applications, even in some basic synthetic retrieval tasks. Nevertheless, few approaches try to tackle this long-context modeling issue from a data-centric perspective. 
As revealed by, the quality of fine-tuning data plays a critical role in enhancing the long-context modeling capabilities of LLMs. Besides, also report that high-quality corpora significantly outperform other factors in boosting long-context performance. Upon further exploration, we recognize that high-quality long-text data is characterized by the presence of long-range dependencies. The importance of encapsulating long-range dependencies in LLMs is also underscored by, which elucidates the benefits of integrating global dependencies into retrieval-augmented language models. 
However, such strong semantic dependencies are rare in typical training samples and diminish as the distance between segments increases. Even with identical sequence lengths, different samples may exhibit varying dependency density. Specifically, certain long training samples often comprise concatenated short documents that are randomly selected and do not have any semantic dependencies. Moreover, even for inherently long documents, like novels, most tokens depend only to a brief span of preceding context. This phenomenon can be simply concluded as ""long context is not long at all"", leading to challenges in model learning (Figure). Therefore, we argue that explicitly incorporating long-dependency data into the fine-tuning process can facilitate long context modeling. 
In this paper, we propose a novel framework (called ProLong) to mine long-dependency data. ProLong assigns a long-dependency score to each document, which serves as an indicator of the dependency density across long contexts. Documents with higher scores are deemed more advantageous for boosting long context modeling, therefore we can use these scores to rank and filter high quality corpus for LLM fine-tuning. Concretely, ProLong first partitions each document into fixed-length segments and evaluates dependency relationships between each segment pair from three perspectives: (i) dependency strength quantifies the difference in perplexities of a given segment when conditioned with or without its preceding segments. This metric measures the contribution of the preceding segment to the current one; (ii) dependency distance measures the positional gap and spatial relationship between two text segments; and (iii) dependency specificity employs entropy to ensure a non-uniform distribution of dependency strengths across all preceding segments, mitigating trivial dependencies introduced by repetitive patterns. A dependency score is assigned to each segment pair by combining the above three perspectives and a final long-dependency score for the entire document is computed by accumulating dependency scores of all segment pairs. 
Further, we adopt various strategies to optimize the computational efficiency of ProLong, including sampling among segments, evaluating perplexity scores with small models and curating test sets for rapid validation. Experiments on multiple benchmarks indicate that ProLong effectively identifies documents that carry long dependencies and LLMs trained on these documents exhibit significantly enhanced long-context modeling capabilities. Our contributions are summarized as follows: 
1. To the best of our knowledge, this is the first study to explore the relationship between dependency density and the quality of long-text data. 
2. We propose ProLong, a data mining framework for identifying long-dependency data. With ProLong, significant performance boosts are observed using only 50%of fine-tuning data. 
3. We provide an in-depth analysis of ProLong' s components, optimizing computational efficiency and making it practical for large-scale corpora. 
4. We develop two models, ProLong-7b/13b, using training samples derived by the ProLong framework. Experiments show that our models outperform equal-sized competitors on both language modeling and real long-context tasks."
Label-Synchronous Neural Transducer for E2E Simultaneous Speech Translation,2406.04541v1,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2406.04541v1.png,Illustration of the proposed LS-Transducer-SST. Linear denotes a linear classifier. Target-side CTC uses translations in the training objective computation.,"While the neural transducer is popular for online speech recognition, simultaneous speech translation (SST) requires both streaming and re-ordering capabilities. This paper presents the LS-Transducer-SST, a label-synchronous neural transducer for SST, which naturally possesses these two properties. The LS-Transducer-SST dynamically decides when to emit translation tokens based on an Auto-regressive Integrate-and-Fire (AIF) mechanism. A latency-controllable AIF is also proposed, which can control the quality-latency trade-off either only during decoding, or it can be used in both decoding and training. The LS-Transducer-SST can naturally utilise monolingual text-only data via its prediction network which helps alleviate the key issue of data sparsity for E2E SST. During decoding, a chunk-based incremental joint decoding technique is designed to refine and expand the search space. Experiments on the Fisher-CallHome Spanish (Es-En) and MuST-C En-De data show that the LS-Transducer-SST gives a better quality-latency trade-off than existing popular methods. For example, the LS-Transducer-SST gives a 3.1/2.9 point BLEU increase (Es-En/En-De) relative to CAAT at a similar latency and a 1.4 s reduction in average lagging latency with similar BLEU scores relative to Wait-k.","Simultaneous speech translation (SST) generates translations from input speech in a streaming fashion. Conventional cascaded SST performs streaming automatic speech recognition (ASR) followed by text-based simultaneous machine translation. Recently, end-to-end (E2E) SST has become popular and has advantages, including lower latency. However, E2E SST is challenging since it requires taking into account word re-ordering between source and target languages during the streaming process. Neural transducers, the dominant model for low-latency ASR, find this difficult due to their monotonic nature. Furthermore, E2E training results in severe data sparsity. 
Current SST methods are normally based on the Transformer attention-based encoder-decoder (AED) structure, which isn't naturally able to deal with streaming, as noted by. A popular approach to adapt the AED to SST is the Wait-k policy, which is a fixed read-write policy that uses a fixed number of wait duration steps before translation. However, it can be too aggressive or conservative in different cases. Alternative methods involve a flexible policy which lets the model decide how much input to read before generating the next translation token, such as monotonic multi-head attention (MMA) and Continuous Integrate-and-Fire (CIF) based SST system. However, these flexible policies normally rely on a latency loss to adjust the quality-latency trade-off at training, unlike the fixed Wait-k policy that can control latency at decoding only. 
To better address the challenges of E2E SST, this paper adapts the label-synchronous neural transducer developed for ASR to SST and denotes the resulting technique the LS-Transducer-SST. In the LS-Transducer-SST, an Auto-regressive Integrate-and-Fire (AIF) mechanism uses accumulated frame-level weights to dynamically determine when to emit translation tokens, based on which a label-level target-side encoder representation is extracted auto-regressively using an attention mechanism. Therefore, the LS-Transducer-SST is naturally equipped with both streaming and re-ordering capabilities. In addition, the prediction network of the LS-Transducer-SST works as a standard language model (LM) as its output is directly combined with the extracted encoder representation at the label level. As a benefit, the E2E SST data sparsity issue can be alleviated because the prediction network can effectively utilise monolingual text-only data, which is normally easy to collect, for tasks such as pre-training or text-based adaptation. While the standard AIF theoretically ensures low-latency output for the LS-Transducer-SST, to better control the quality-latency trade-off, a latency-controllable AIF is proposed, which controls the latency by adjusting the decision threshold of the accumulated frame-level weights. Furthermore, the latency-controllable AIF allows the quality-latency trade-off to be controlled not only during training but also during decoding, enabling the LS-Transducer-SST to combine the advantages of typical fixed and flexible SST policies. This paper focuses on low/medium-latency scenarios to keep the low-latency advantage of E2E SST. During decoding, to improve translation quality, a chunk-based incremental joint decoding is further proposed to refine and expand the search space. The proposed LS-Transducer-SST was evaluated on Fisher-CallHome Spanish (Es-En) and MuST-C En-De corpora and gave an improved quality-latency trade-off compared to existing popular SST methods. The main contributions are summarised below: 
 
 
 * LS-Transducer-SST, naturally equipped with streaming and reordering abilities, is proposed for SST and can help alleviate its data sparsity. 
 * A latency-controllable AIF is proposed to control the latency during decoding effectively. 
 * A chunk-based incremental joint decoding is proposed to expand the search space. 
 * Extensive experiments were conducted. Our code bridges the ESPnet and Fairseq toolkits and will facilitate future research."
A Modular Approach for Multimodal Summarization of TV Shows,2403.03823v9,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2403.03823v9.pdf,"Graphical depiction of our approach for long-form multimodal summarization where different subtasks are performed by five, specialized modules (shown in different colors). We use simplified summaries for display and show only four scenes. This full episode (As the World Turns aired 01-06-05, contains 29 scenes.","In this paper we address the task of summarizing television shows, which touches key areas in AI research: complex reasoning, multiple modalities, and long narratives. We present a modular approach where separate components perform specialized sub-tasks which we argue affords greater flexibility compared to end-to-end methods. Our modules involve detecting scene boundaries, reordering scenes so as to minimize the number of cuts between different events, converting visual information to text, summarizing the dialogue in each scene, and fusing the scene summaries into a final summary for the entire episode. We also present a new metric, PRisma (Precision and Recall Evaluation of Summary Facts), to measure both precision and recall of generated summaries, which we decompose into atomic facts. Tested on the recently released SummScreen3D dataset, our method produces higher quality summaries than comparison models, as measured with ROUGE and our new fact-based metric, and as assessed by human evaluators. Code for our experiments and metric are at <https: //github. com/Lou1sM/modular_multimodal_summarization>.","In this paper, we address the challenging task of summarizing television shows which has practical utility in allowing viewers to quickly recall plot points, characters, and events without the need to re-watch entire episodes or seasons. From a computational standpoint, the task serves as a testbed for complex reasoning over long narratives, involving multiple modalities, non-trivial temporal dependencies, inferences over events, and multi-party dialogue with different styles. An added difficulty concerns assessing the quality of generated summaries for long narratives, whether evaluations are conducted by humans or via automatic metrics. 
Most prior work on creative summarization does not consider the above challenges all at once, focusing either on the text modality and full-length narratives with complex semantics or on short video clips which last only a couple of minutes. A notable exception are, who incorporate multimodal information into a pre-trained textual summarizer by adding (and tuning) adapter layers. On the evaluation front, there is no single agreed-upon metric for measuring summary quality automatically, although there is mounting evidence that ROUGE does not discriminate between different types of errors, in particular those relating to factuality. 
While end-to-end models are a popular choice for summarization tasks, more modular approaches have been gaining ground recently for several reasons. Modules can be developed independently, and exchanged for better versions if available, new modules can be added to create new solutions or repurposed for different tasks, and dependencies between modules can be rearranged. Aside from greater controllability, modular approaches are by design more interpretable, since errors can be inspected and attributed to specific components. In this paper we delegate the end-to-end task of summarizing from multiple modalities (i.e., TV show video and its transcript) to more specialized modules, each responsible for handling different subtasks and their interactions. Our approach is depicted graphically in Figure. 
As scene breaks are not always given explicitly, we devise an algorithm to identify them from the order of the speaker names (row 1, Figure). Additionally, we select the optimal order in which to re-arrange scenes (row 2, Figure), as these often appear non-linearly (e.g., cuts between different subplots or flashbacks). Next, we produce summaries in a two-layer process. A vision-processing module converts the video to text using visual captioning (row 3, Figure), which leverages the strong specialized ability of vision-to-text models and allows us to treat the problem as one of text-to-text summarization. We also summarize each transcript scene independently with a module specialized for dialogue summarization (row 4, Figure). Finally, a high-level summarization module specialized for narrative summarization fuses the sequence of scene summaries into a final summary (row 5, Figure). 
We also propose a new metric for assessing the factuality of generated summaries by adapting FActScore, a recently introduced metric for detecting hallucination in text generation. We break the generated summary into atomic facts, and check what fraction of them are supported by the reference. This we term fact-precision. We also do the same in reverse, breaking the reference into facts and measuring what fraction are supported by the generated summary, which we term fact-recall. Our metric, PRisma (Precision and Recall Evaluation of Summary Facts), is the harmonic average of these two scores. Our contributions are: 
 0em 
 * We present a novel modular approach to multimodal summarization, where separate subtasks are performed by separate modules; 
 * Our modules involve detecting scene breaks, reordering each scene, summarizing the dialogue therein, converting the visual information to text, and fusing the scene-summaries into a final summary (see Figure); 
 * We present two novel algorithms, for determining the optimal order in which to place each scene, and for identifying where the scene breaks are located in the transcript; 
 * We devise a new metric for summarization, based on splitting text into atomic facts, which captures both precision and recall and correlates significantly with human judgments."
BvSP: Broad-view Soft Prompting for Few-Shot Aspect Sentiment Quad Prediction,2406.07365v1,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2406.07365v1.pdf,"A unseen aspect case is shown. The newly emerged category 'internet"" is not mentioned in the pre-defined set of aspect categories.","Aspect sentiment quad prediction (ASQP) aims to predict four aspect-based elements, including aspect term, opinion term, aspect category, and sentiment polarity. In practice, unseen aspects, due to distinct data distribution, impose many challenges for a trained neural model. Motivated by this, this work formulates ASQP into the few-shot scenario, which aims for fast adaptation in real applications. Therefore, we first construct a few-shot ASQP dataset (","Analyzing user reviews, social media posts, product evaluations, and other content on the web to extract sentiment information related to specific aspects helps in understanding users 'opinions and emotions regarding different aspects on the web. To monitor public opinion and support decision-making, the research field of sentiment analysis and opinion mining emerged. The aspect sentiment quad prediction (ASQP) task aims to extract aspect quadruplets from a review sentence to comprehensively understand users' aspect-level opinions. Recently, ASQP is gaining attention due to it involves predicting four fundamental aspect-level elements: 1) aspect term which is the concrete aspect description in the given text; 2) opinion term describing the exact opinion expression towards an aspect; 3) aspect category denoting the aspect type which refers to a pre-defined set, and 4) sentiment polarity indicating the sentiment class of the aspect. For example, given the sentence ""The room is clean. "", the aspect elements are ""room"", ""clean"", ""room_ overall"", and ""positive"", respectively. Accordingly, the ASQP is described as a quad (room, clean, room_ overall, positive). 
However, in practical situations, aspect categories are not immutable and frozen. New aspects emerge as people discuss emerging phenomena, trends, products, and more through social media, news articles, and other means on the internet. As the restaurant domain illustrated in Figure, the initial aspect category set is pre-defined. Yet as the infrastructure upgrades, new aspects, such as ""WiFi"", gradually appear. The sentence 's category, i.e.,""internet"" does not exist in the pre-defined categories. This imposes challenges to the model' s comprehensive and accurate understanding of the sentence. Moreover, the unseen aspect usually has a distribution shift, which is struggling for trained models to adapt accurately. 
=0.5cm 
Therefore, researching the few-shot ASQP task, i.e.,fast adaptation to unseen aspects with only a few labeled samples, becomes crucial, as it aligns more closely with real-world application scenarios. Yet, previous ASQP datasets either have a limited number of categories or long-tailed distribution. This task lacks a proper benchmark dataset. Therefore, we annotate a few-shot ASQP dataset, named 
 𝙵𝚂𝚀𝙿. This dataset aims to provide a more balanced representation and encompasses a wider range of categories, offering a comprehensive benchmark for evaluating few-shot ASQP. 
 
 Recent studies have employed generative methods to extract quads by converting input sentences into templated target sequences. Subsequently, by disentangling the formats of template, quads can be extracted. However, they have primarily concentrated on the utilization of a single template or incorporate multiple templates by considering different quad orders, thereby ignore the correlation among these various templates. To overcome this limitation, we introduce an innovative method called Broad-view Soft Prompting (BvSP). BvSP leverages a pre-trained language model and utilizes Jensen-Shannon (JS) divergence to select several templates, enabling a more harmonious view of the available templates. We further introduce soft prompting to fine-tune the pre-trained language model with these selected templates. The final prediction is obtained from multiple templates by using a voting mechanism. 
 
 In summary, our major contributions of this paper are as follows: 
 
 
 * We construct a new few-shot ASQP dataset𝙵𝚂𝚀𝙿which contains richer categories and is more balanced for the few-shot study. To the best of our knowledge, this is the first work to label the few-shot dataset in the ASQP task. 
 * We further propose BvSP, a various templates-based soft prompt learning method that improves quad prediction by taking into account the correlation between the different templates. 
 * Experimental results under four few-shot settings (i.e.,one-shot, two-shot, five-shot, and ten-shot) demonstrate that BvSP outperforms strong baselines and has significant gains in other public datasets. [𝚁𝚎𝚜𝚝15] <g r a p h i c s> [𝚁𝚎𝚜𝚝16] <g r a p h i c s> [𝚁𝚎𝚜𝚝𝚊𝚞𝚛𝚊𝚗𝚝] <g r a p h i c s> [𝙻𝚊𝚙𝚝𝚘𝚙] <g r a p h i c s> [𝙵𝚂𝚀𝙿] <g r a p h i c s> The category distribution is presented according to the number of instances. For example, the green section indicates the proportion of categories with the number of instances between 1 and 50."
RAVEL: Evaluating Interpretability Methods on Disentangling Language Model Representations,2402.17700v2,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2402.17700v2.pdf,"An overview of the Ravel benchmark, which evaluates how well an interpretability method can find features that isolate the causal effect of individual attributes of an entity.","Individual neurons participate in the representation of multiple high-level concepts. To what extent can different interpretability methods successfully disentangle these roles? To help address this question, we introduce Ravel (Resolving Attribute–Value Entanglements in Language Models), a dataset that enables tightly controlled, quantitative comparisons between a variety of existing interpretability methods. We use the resulting conceptual framework to define the new method of Multi-task Distributed Alignment Search (MDAS), which allows us to find distributed representations satisfying multiple causal criteria. With Llama2-7B as the target language model, MDAS achieves state-of-the-art results on Ravel, demonstrating the importance of going beyond neuron-level analyses to identify features distributed across activations. We release our benchmark at <https: //github. com/explanare/ravel>.","A central goal of interpretability is to localize an abstract concept to a component of a deep learning model that is used during inference. However, this is not as simple as identifying a neuron for each concept, because neurons are polysemantic – they represent multiple high-level concepts. 
Several recent interpretability works tackle this problem using a featurizer that disentangles the activations of polysemantic neurons by mapping to a space of monosemantic features that each represent a distinct concept. Intuitively, these methods should have a significant advantage over approaches that identify concepts with sets of neurons. However, these methods have not been benchmarked. 
To facilitate these method comparisons, we introduce a diagnostic benchmark, Ravel (Resolving Attribute–Value Entanglements in Language Models). Ravel evaluates interpretability methods on their ability to localize and disentangle the attributes of different types of entities encoded as text inputs to language models (LMs). For example, the entity type ""city"" has instances such as ""Paris"" or ""Tokyo"", which each have attributes for ""continent"", namely ""Europe"" and ""Asia"". An interpretability method must localize this attribute to a group of neurons 
 𝐍, learn a featurizerℱ (e.g., a rotation matrix or sparse autoencoder), and identify a featureF (e.g., a linear subspace of the residual stream in a Transformer) for the attribute. Ravel contains five types of entities (cities, people names, verbs, physical objects, and occupations), each with at least 500 instances, at least 4 attributes, and at least 50 prompt templates per entity type. 
 
 The metric we use to assess interpretability methods is based on interchange interventions (also known as activation patching). This operation has emerged as a workhorse in interpretability, with a wide swath of research applying the technique to test if a high-level concept is stored in a model representation and used during inference. 
 
 Specifically, we use the LM to process a prompt like ""Paris is in the continent of"" and then intervene on the neurons𝐍to fix the featureFto be the value it would have if the LM were given a prompt like ""Tokyo is a large city. "" If this leads the LM to output ""Asia"" instead of ""Europe"", then we have evidence that the featureFencodes the attribute ""continent"". Then, we perform the same intervention when the LM processes a prompt like ""People in Paris speak"". If the LM outputs ""French"" rather than ""Japanese', then we have evidence that the featureFhas disentangled the attributes"" continent ""and"" language"". 
 
 A variety of existing interpretability methods are easily cast in the terms needed for Ravel evaluations, including supervised probes, Principal Component Analysis, Differential Binary Masking (DBM: ), sparse autoencoders, and Distributed Alignment Search (DAS: ). Our apples-to-apples comparisons reveal conceptual similarities between the methods. 
 
 In addition, we propose multi-task training objectives for DBM and DAS. These objectives allow us to find representations satisfying multiple causal criteria, and we show that Multi-task DAS is the most effective of all the methods we evaluate at identifying disentangled features. This contributes to the growing body of evidence that interpretability methods need to identify features that are distributed across neurons."
Large Language Models as Zero-shot Dialogue State Tracker through Function Calling,2402.10466v4,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2402.10466v4.pdf,"Zero-shot DST performance comparison among (1) previous domain transfer approaches using small models; (2) previous prompting approaches exclusively relying on advanced proprietary LLMs; and (3) our approach, compatible with various LLMs, empowers various 7B and 13B models for superior performance and sets new state-of-the-art with GPT-4.","Large language models (LLMs) are increasingly prevalent in conversational systems due to their advanced understanding and generative capabilities in general contexts. However, their effectiveness in task-oriented dialogues (TOD), which requires not only response generation but also effective dialogue state tracking (DST) within specific tasks and domains, remains less satisfying. In this work, we propose a novel approach FnCTOD for solving DST with LLMs through function calling. This method improves zero-shot DST, allowing adaptation to diverse domains without extensive data collection or model tuning. Our experimental results demonstrate that our approach achieves exceptional performance with both modestly sized open-source and also proprietary LLMs: with in-context prompting it enables various 7B or 13B parameter models to surpass the previous state-of-the-art (SOTA) achieved by ChatGPT, and improves ChatGPT's performance beating the SOTA by 5.6%average joint goal accuracy (JGA). Individual model results for GPT-3.5 and GPT-4 are boosted by 4.8%and 14%, respectively. We also show that by fine-tuning on a small collection of diverse task-oriented dialogues, we can equip modestly sized models, specifically a 13B parameter LLaMA2-Chat model, with function-calling capabilities and DST performance comparable to ChatGPT while maintaining their chat capabilities. We have made the code publicly available.","Recent years have seen the rapid development of large language models (LLMs) that have demonstrated exceptional natural language understanding and generation capabilities. The integration of LLMs into industry applications, particularly as conversational assistants, is a notable trend. Fine-tuned with conversations between users and assistants, these models are further aligned with human preferences to enhance their ability to deliver fluent, helpful, and polite responses to user inquiries. Notable examples include proprietary systems such as ChatGPT and Claude, as well as open-source models such as LLaMA2-Chat, Vicuna, Baichuan. 
The primary focus of these chat-tuned LLMs has typically been on responding in general contexts. However, for another important type of conversation, task-oriented dialogues (TOD), the model is required to extract the intentions of users at each turn of the conversation, represented as slot-value pairs of per-domain predefined schemas; a process known as Dialogue State Tracking (DST). The challenge lies in the model 's ability to accurately summarize user needs over multiple turns of conversation and also strictly adhere to a domain-specific ontology. The most direct solutions necessitate training on curated domain-specific annotated data, a process that is notoriously costly and labor-intensive. Despite efforts in data augmentation and automated dataset creation using GPT-3, these methods struggle to generalize to unseen domains. To achieve zero-shot DST for unseen domains, prior approaches usually involved domain transfer methods. While such approaches are trained on alternative domains, they still require domains with matching annotation schema, and their performance has been far from satisfactory. 
LLMs exhibit remarkable capabilities for tackling various tasks without the need for task-specific fine-tuning, making them suited for zero-shot DST. However, while there have been initiatives to leverage ChatGPT for zero-shot DST, these methods tend to treat DST as a standalone task rather than chat completion, which the models, especially chat-tuned models, are more proficient in. They usually take the whole conversation as input along with detailed instructions to generate in domain-specific formats. This setup poses challenges due to the long task context and specific output requirements. Consequently, this works exclusively with advanced ChatGPT or Codex models but fails with less powerful LLMs. 
In this work, we introduce a novel approach FnCTOD, to address zero-shot DST with LLMs. Our method seamlessly integrates DST as a part of the assistant' s output during chat completion. Specifically, we treat the schema of each task-oriented dialogue domain as a specific function, and DST for this domain as the process of ""calling"" the corresponding function. We thus instruct LLMs to generate function calls along with the response in the assistant 's output. To achieve this, we convert the domain schema into function specifications, which include the function' s description and required arguments, and incorporate them into the system prompt of the LLM. Additionally, we integrate these function calls into the assistant 's output within the dialogue context. 
As shown in Figure, experimental results on the MultiWOZ benchmark represent a significant milestone. Our approach is the first that, without further fine-tuning, enables modestly sized open-source LLMs (7B or 13B parameters) to achieve comparable or superior performance compared to previous state-of-the-art (SOTA) prompting methods that relied exclusively on advanced proprietary LLMs such as ChatGPT and Codex. Furthermore, our approach beats the previous zero-shot SOTA by 5.6%Av. JGA, firmly establishing a new standard. It improves ChatGPT performance; beating previous individual best results for GPT-3.5 and GPT-4 by 4.8%and 14%, respectively. 
Additionally, we show that by fine-tuning a 13B LLaMA2-Chat model using a collection of 7,200 task-oriented dialogues — consisting of 200 randomly selected dialogues covering 36 diverse domains, from heterogeneous TOD datasets — we can equip it with function-calling DST abilities comparable to ChatGPT while still maintaining its response generation capabilities. 
The comparison with prior studies is summarized in Table and Figure. Our contribution is threefold: (1) Demonstration that the FnCTOD approach achieves outstanding performance with both open-source and proprietary LLMs through in-context prompting: enables open-source 7–13B models to surpass the previous SOTA achieved by ChatGPT, and enhances GPT-4' s performance by 14%, establishing a new SOTA. (2) Bridging the zero-shot DST performance gap between open-source models and ChatGPT by fine-tuning on a small collection of diverse dialogues. (3) Showing that function calling DST capabilities can be integrated into existing chat-tuned LLMs while preserving response capabilities."
Enhancing Dialogue State Tracking Models through LLM-backed User-Agents Simulation,2405.13037v1,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2405.13037v1.pdf,"The simulation process of our approach. The blue boxes are intentions for the user and the agent, the ' [RECOM] ', ' [EOF] ', and ' [EOD] ' are control identifiers.","Dialogue State Tracking (DST) is designed to monitor the evolving dialogue state in the conversations and plays a pivotal role in developing task-oriented dialogue systems. However, obtaining the annotated data for the DST task is usually a costly endeavor. In this paper, we focus on employing LLMs to generate dialogue data to reduce dialogue collection and annotation costs. Specifically, GPT-4 is used to simulate the user and agent interaction, generating thousands of dialogues annotated with DST labels. Then a two-stage fine-tuning on LLaMA 2 is performed on the generated data and the real data for the DST prediction. Experimental results on two public DST benchmarks show that with the generated dialogue data, our model performs better than the baseline trained solely on real data. In addition, our approach is also capable of adapting to the dynamic demands in real-world scenarios, generating dialogues in new domains swiftly. After replacing dialogue segments in any domain with the corresponding generated ones, the model achieves comparable performance to the model trained on real data.","Dialogue state tracking (DST) is a critical component of task-oriented dialogue systems, serving to track users 'goals and system actions in the conversation and facilitate precise information handling for communicating with external APIs. DST usually takes the form of key-value pairs, where the keys are denoted as slots which are defined in the system schema, outlining the specific information that the system aims to track or extract during the whole conversation. 
The design of DST models could be broadly categorized into two main types, classification-based DST models and generation-based DST models. Classification-based models select slot values from a set of candidates, assuming that the dialogue ontology is pre-defined and hence lacking generalization capability. Generation-based models directly generate the slot values to handle unseen domains and values. Recently, proposes a new DST framework LDST based on LLaMA. By using an instruction tuning method, LDST achieves performance on par with ChatGPT. 
Despite DST showing promising results, a significant challenge is that the annotation of dialogues entails significant costs. Furthermore, the dynamic nature of real-world demands highlights the urgent need to quickly generate utterances for new domains. Compared to other types of NLP data, collecting authentic dialogue data is particularly challenging. This difficulty is partly due to the dialogues frequently containing personal or sensitive information, which complicates data collection and sharing efforts. In response to these challenges, and inspired by the recent advancements of large language models (LLMs), we explore the use of these models for generating annotated DST data for data augmentation. By leveraging LLM' s cross-domain generation capability, we aim to create synthetic dialogues that can serve as replacements for manually annotated data, significantly reducing both financial cost and time constraints. 
In this paper, we propose a LLM-backed User-Agents Simulation (LUAS) algorithm to enhance DST. The process begins with the LLM generating a user profile that details the individual 's preferences for various tasks. Following this initial step, the LLM is prompted to simulate a conversation between the user and the agent. In these simulations, the user simulator makes requests and seeks recommendations or assistance, while the agent responds by understanding the user' s needs, providing suggestions, and taking appropriate actions. Through iterative conversations between the user and agent, complemented by a slot extractor also prompted by the LLM, we generate a substantial corpus of labeled, multi-turn dialogue data. 
To verify the effectiveness of our approach and the quality of the generated data, experiments are conducted on two public DST datasets, MultiWOZ 2.2 and MultiWOZ 2.4. Following, LLaMa 2 is finetuned with real data as a strong baseline. By using both the generated and the real data, finetuning LLaMa 2 can further improve the performance. Besides, by replacing dialogue segments of any domain with the generated data, the newly trained model achieves comparable performance to the model trained on the real data, which shows the capability of our method to meet the dynamic requirements of real-world scenarios, generating dialogues in new domains and preserving the promising performance. 
In summary, the contributions of our work can be categorized into four aspects: 
 
 * We propose a new framework that harnesses the power of GPT-4 to generate new labeled dialogue data, effectively reducing dialogue data collection and annotation costs. 
 * Experiment results on two datasets show the positive impact of the generated data on performance. 
 * Our method can swiftly generate data in new domains while maintaining promising performance. 
 * We believe that our approach holds promise for extension to other dialogue-related tasks."
KnowCoder: Coding Structured Knowledge into LLMs for Universal Information Extraction,2403.07969v2,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2403.07969v2.pdf,An illustration of KnowCoder schemas.,"In this paper, we propose KnowCoder, a Large Language Model (LLM) to conduct Universal Information Extraction (UIE) via code generation. KnowCoder aims to develop a kind of unified schema representation that LLMs can easily understand and an effective learning framework that encourages LLMs to follow schemas and extract structured knowledge accurately. To achieve these, KnowCoder introduces a code-style schema representation method to uniformly transform different schemas into Python classes, with which complex schema information, such as constraints among tasks in UIE, can be captured in an LLM-friendly manner. We further construct a code-style schema library covering over","Information Extraction (IE) aims to extract explicit and structured knowledge following the manually designed schemas. The IE schemas define high-level types of knowledge (i.e., concepts) and structures among them, which include various types of entities, relations, and events. To simultaneously extract various knowledge under different schemas via a single model, the Universal Information Extraction (UIE) task is proposed. Recently, Large Language Models (LLMs) have demonstrated general understanding abilities through large-scale pretraining, which drives their increasing utilization in UIE. However, their performance on UIE is still limited because of two main challenges: (1) the lack of a unified schema representation method that LLMs can easily understand; (2) the lack of an effective learning framework that encourages LLMs to accurately follow specific schemas for extracting structured knowledge. 
For the first challenge, the existing UIE models first represent different schemas in a universal way, such as classification labels, keywords, or a specifically-designed formal language. These schema representation methods have three main restrictions: (1) ignoring information like taxonomies (e.g., ""fairytale"" is a subclass of ""written work"") and constraints among concepts (e.g., ""spouse"" relation exists between two ""human"" entities); (2) classification labels or a specifically designed formal language is hard for LLMs to understand and follow; (3) designed for specific IE datasets and lacking a general schema library. 
To solve these restrictions, in this paper, we propose a kind of code-style schema representation method, with which various types of knowledge are generally defined as Python classes. As shown in Figure, the class inheritance mechanism is adopted to describe the concept taxonomies. A mechanism of type hint is employed to model constraints among different concepts. The class comments are used to provide clear definitions of concepts. And, the class methods are used to post-process the results according to specific IE guidelines. Upon this method, we construct a comprehensive code-style schema library covering over 
 29,000entity types,900relation types, and500event types based on Wikidata, the largest one for UIE, to the best of our knowledge, currently reported in the open literature. 
 
 For the second challenge, the existing learning framework for UIE directly conducts instruction tuning on LLMs to extract knowledge following specific and limited schemas. The enormous concepts in the constructed schema library challenge the existing training framework. To help LLMs better understand and follow these schemas, we propose an effective two-phase framework containing a schema understanding phase and a schema following phase. The former improves the ability of LLMs to understand different concepts in schemas via large-scale code pretraining on the schema definition code and corresponding instance code. The latter advances their abilities to follow specific schemas in an IE task via instruction tuning. After code pretraining on around 1.5B automatically constructed data, KnowCoder already attains remarkable generalization ability and achieves NER improvements compared to the base model, LLaMA2, by49.8%relative F1 point under the few-shot setting on NER. After instruction tuning on 1.5B automatically annotated data, KnowCoder experimentally demonstrates strong generalization ability on unseen schemas. Under the zero-shot setting, KnowCoder achieves average relative improvements up to12.5%on the NER task. Under the low-resource setting, KnowCoder gets average relative improvements up to21.9%on all the IE tasks. Additionally, based on our unified schema representation, various IE datasets can be simultaneously utilized to refine KnowCoder. After refinement, KnowCoder achieves consistent improvements across all IE tasks under the supervised setting, getting up to7.5%improvement on the relation extraction task, respectively. 
 
 In general, the main contributions of this paper include: 
 
 
 * We propose a code-style schema representation method to uniformly represent different schemas for UIE. Using this method, we construct a large code-style schema library covering more than30,000types of knowledge. 
 * We propose an effective learning framework for LLMs in a two-phase manner, which first enhances the schema understanding through code pretraining and then boosts schema following via instruction tuning. 
 * After training on billions of automatically annotated data and refining with human-annotated IE datasets, KnowCoder demonstrates superior performance on different IE tasks under the zero-shot, low-resource, and supervised settings. 
 * The constructed schema library, training data, code, and models are released for future research."
ERA-CoT: Improving Chain-of-Thought through Entity Relationship Analysis,2403.06932v2,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2403.06932v2.png,"The top of the figure represents the standard prediction process. The bottom of the figure shows the five-step inference process of ERA-CoT, which relies on the extraction of entities and the inference and analysis of relationships between entities to obtain the results.","Large language models (LLMs) have achieved commendable accomplishments in various natural language processing tasks. However, LLMs still encounter significant challenges when dealing with complex scenarios involving multiple entities. These challenges arise from the presence of implicit relationships that demand multi-step reasoning. In this paper, we propose a novel approach ERA-CoT, which aids LLMs in understanding context by capturing relationships between entities and supports the reasoning of diverse tasks through Chain-of-Thoughts (CoT). Experimental results show that ERA-CoT demonstrates the superior performance of our proposed method compared to current CoT prompting methods, achieving a significant improvement of an average of 5.1%on GPT3.5 compared to previous SOTA baselines. Our analysis indicates that ERA-CoT increases the LLM's understanding of entity relationships, significantly improves the accuracy of question answering, and enhances the reasoning ability of LLMs.","Large language models (LLMs) have shown remarkable in-context learning capabilities in various natural language processing (NLP) tasks, including machine translation, question answering, and named entity extraction, etc. Recently, prompting strategies like Chain-of-Thought (CoT) have garnered attention due to their capacity to significantly enhance LLMs reasoning capabilities. Considering the ability of CoT to guide LLMs in breaking down complex reasoning processes into simple steps, it stands out compared to standard zero-shot and few-shot methods. 
However, due to the presence of numerous entities such as characters, locations, etc. , and the multitude of implicit relationships among them in certain scenarios, CoT still faces significant challenges in handling these situations. Named Entity Recognition (NER) has typically been employed when addressing these tasks. NER is a sequence labeling task in nature, where the model needs to assign an entity-type label to each token within a sentence. Relation extraction is a category of methods for handling entity relationships within text passages. Various studies have also investigated the performance of LLMs in zero-shot relation extraction. However, without additional prompts, LLMs have limited entity and relation extraction capabilities. Considering the importance of contextual content in answering questions, addressing knowledge-intensive tasks also requires a comprehensive analysis of entity relationships. 
In this paper, we propose Entity Relationship Analysis with Chain-of-Thought (ERA-CoT), a novel framework to better address reasoning tasks in complex entity scenarios. First, we extract all the entities involved in the text; second, we extract the directly mentioned explicit relationships between entities based on the text; then, we infer the indirect implicit relationships between entities based on these explicit relationships and the hidden information in the text; after that, we let the model score the implicit relationships based on the reliability of the relationships, set a threshold for judging the reliability of the relationships, and eliminate the implicit relationships that are lower than the threshold; finally, answer the questions based on the previously extracted entities and the obtained implicit and explicit relationships. 
We conducted experiments on six widely adopted datasets and compared with four baseline methods. The results show that ERA-CoT outperforms baselines on nearly all benchmarks, achieving a significant improvement of about 5.1%on average. From the performance results, our method outperforms on all three types of reasoning problems: commonsense reasoning, mathematical reasoning, and logical reasoning. This indicates that enhancing the model's understanding of entity relationships can significantly boost the reasoning abilities and accuracy in answering questions of LLMs. Our main contributions can be summarized as follows."
On the Multi-turn Instruction Following for Conversational Web Agents,2402.15057v1,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2402.15057v1.pdf,Illustrations of different problems.,"Web agents powered by Large Language Models (LLMs) have demonstrated remarkable abilities in planning and executing multi-step interactions within complex web-based environments, fulfilling a wide range of web navigation tasks. Despite these advancements, the potential for LLM-powered agents to effectively engage with sequential user instructions in real-world scenarios has not been fully explored. In this work, we introduce a new task of Conversational Web Navigation, which necessitates sophisticated interactions that span multiple turns with both the users and the environment, supported by a specially developed dataset named Multi-Turn Mind2Web (MT-Mind2Web). To tackle the limited context length of LLMs and the context-dependency issue of the conversational tasks, we further propose a novel framework, named self-reflective memory-augmented planning (Self-MAP), which employs memory utilization and self-reflection techniques. Extensive experiments are conducted to benchmark the MT-Mind2Web dataset, and validate the effectiveness of the proposed method.","A longstanding objective in artificial intelligence is to develop AI agents that can execute complex tasks, thereby minimizing human effort in routine activities. With the advent of Large Language Models (LLMs), LLM-powered agents showcase exceptional planning capabilities in performing multi-turn interactions with diverse environments, which contribute to various real-world problem-solving. As shown in Figure (a), the web agent is designed to interpret the states of a webpage and execute a series of actions using keyboard and mouse inputs. Its purpose is to accomplish the tasks defined in natural language, such as booking tickets, through multi-turn interactions with the web-grounded environment. 
Despite the proficiency in executing each individual instruction, the capability of interacting with multi-turn user instructions remains under-explored, which is crucial for applying LLM-powered agents onto real-world applications. As the example shown in Figure (c), during a conversational web navigation session, users tend to request follow-up or co-referencing instructions without repeating previous information. They may also provide a succinct or brief instruction, which is similar to other conversation problems. Motivated by recent efforts on the investigation of conversational capabilities in the interactions with human users for LLMs, we propose a novel task, named Conversational Web Navigation. It requires the multi-turn interaction capabilities with both users and environment. In particular, we introduce a new dataset, named Multi-Turn Mind2Web (MT-Mind2Web). MT-Mind2Web is constructed by using the single-turn interactions from Mind2Web, an expert-annotated web navigation dataset, as the guidance to construct conversation sessions. 
In other conversational tasks, LLMs can answer conversational questions by utilizing their inherent knowledge from pretrained data or retrieval techniques to assess external databases (Figure (b) ). Compared with these tasks, the conversation history in conversational web navigation contains both the previous user-agent and agent-environment interactions, as the instruction completion relies on the dynamic environment status. Therefore, the history context can be much longer and noisier than that in the traditional conversation problems. 
In light of these challenges, we propose a novel framework, named self-reflective memory-augmented planning (Self-MAP). This framework is designed to maximize the utility of the limited memory space (i.e., input length limitation) of LLM-powered agents addressing the conversational web navigation problem. Specifically, we first construct a memory bank using the conversational interaction history, where each memory snippet stores each interaction step at each conversation turn. To reduce the noise from previous interactions, we propose a multifaceted matching approach to retrieve memory snippets that are semantically relevant and have similar trajectories. Furthermore, we design a reflection module to simplify the retrieved memory snippets by filtering out irrelevant information from the environment state. We then refine the retrieved memory snippets by generating reasoning rationales to enrich the memory information. Finally, we plan the next action by utilizing the self-reflective memory. 
To sum up, our contributions are as follows: 
 
 * To study the multi-turn instruction-following capability of web agents, we define the problem of conversational web navigation and introduce a novel dataset, namely MT-Mind2Web. 
 * We propose a self-reflective memory-augmented planning method (Self-MAP) that combines memory utilization and self-reflection for tackling the underlying challenges in the conversational web navigation task. 
 * We benchmark the MT-Mind2Web dataset with extensive baselines and provide comprehensive evaluations on different settings. Experimental results also validate the effectiveness of the proposed method."
I am a Strange Dataset: Metalinguistic Tests for Language Models,2401.05300v2,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2401.05300v2.png,An example highlighting the challenge presented by our task. All models that we tested on our dataset are close to chance-level.,"Statements involving metalinguistic self-reference (""This paper has six sections. "") are prevalent in many domains. Can current large language models (LLMs) handle such language? In this paper, we present ""I am a Strange Dataset"", a new dataset for addressing this question. There are two subtasks: generation and verification. In generation, models continue statements like ""The penultimate word in this sentence is"" (where a correct continuation is ""is""). In verification, models judge the truth of statements like ""The penultimate word in this sentence is sentence. "" (false). We also provide minimally different metalinguistic non-self-reference examples to complement the main dataset by probing for whether models can handle metalinguistic language at all. The dataset is hand-crafted by experts and validated by non-expert annotators. We test a variety of open-source LLMs (7B to 70B parameters) as well as closed-source LLMs through APIs. All models perform close to chance across both subtasks and even on the non-self-referential metalinguistic control data, though we find some steady improvement with model scale. GPT 4 is the only model to consistently do significantly better than chance, and it is still only in the 60%range, while our untrained human annotators score well in the 89–93%range. The dataset and evaluation toolkit are available at <https: //github. com/TristanThrush/i-am-a-strange-dataset>.","Self-reference plays a crucial role in the way we think about mathematics, theoretical computer science, recursive programming, understanding complex cases in hate speech detection, aptitude tests, and comedy. Some positions in the philosophy of mind consider self-referential capabilities to be a key aspect of higher intelligence or even consciousness. Of course, self-reference is also pervasive in how we communicate: at least one paper you read today is bound to contain ""In this paper"". 
In this paper, we focus on metalinguistic self-reference, the complex kind of self-reference in which language is used to make claims about itself, as in ""This sentence has five words"" and ""This paper has six sections"". Using such language involves reasoning about metalinguistic properties (counting words, naming parts of speech, etc. ) and resolving self-reference. Humans generally have no trouble with such language, and may even enjoy its playful and sometimes paradoxical nature. 
Recently, Large Language Models (LLMs) have demonstrated striking cognitive capabilities. But do they have the same mastery over metalinguistic self-reference as we do? See Figure for an example of the issue that LLMs face. To help address this question, we present a new task and dataset called ""I am a Strange Dataset"". We are inspired by Douglas Hofstadter's explorations of self-reference in language, and borrow part of the name from one of his books: ""I am a Strange Loop"". 
An example in ""I am a Strange Dataset"" is comprised of two self-referential statements that begin in the same way but have different endings (Figure). One is true and one is false. Crucially, the ending flips the truth value of the overall statement. There are two subtasks: generation and verification. In generation, the model must generate the true statement and reject the false one. In verification, models judge the truth of completed statements. To complement the main self-referential data, the dataset also contains metalinguistic non-self-reference examples. These are minimally different from the main examples and serve as controls to assess whether models can reliably handle metalinguistic statements in the absence of self-reference. In addition, all the examples in the dataset are tagged by expert annotators to further aid in error analysis. ""I am a Strange Dataset"" is validated by non-expert annotators. As a group, they have agreement rates in the 89–93%range, depending on which metric we use, as compared to chance rates at 50%. This further supports the claim that metalinguistic self-reference is relatively easy for humans. LLMs, by contrast, struggle: ""I am a Strange Dataset"" turns out to be so difficult that models are generally near chance both in generation and verification, and do not even succeed in the prerequisite metalinguistic non-self-reference case. That said, we do find some limited evidence that GPT 4 is getting some traction on the dataset: it is significantly above chance on all tested metrics (and seems to struggle especially with the self-referential data as compared to the non-self-referential controls). However, overall, it seems safe to say that ""I am a Strange Dataset"" poses a serious challenge for even the best present-day models."
TruthX: Alleviating Hallucinations by Editing Large Language Models in Truthful Space,2402.17811v2,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2402.17811v2.pdf,A case to show that TruthX can control LLM to generate truthful or hallucinatory coherent responses via editing one vector in LLM's internal representations.,"Large Language Models (LLMs) sometimes suffer from producing hallucinations, especially LLMs may generate untruthful responses despite knowing the correct knowledge. Activating the truthfulness within LLM is the key to fully unlocking LLM 's knowledge potential. In this paper, we propose TruthX, an inference-time intervention method to activate the truthfulness of LLM by identifying and editing the features within LLM' s internal representations that govern the truthfulness. TruthX employs an auto-encoder to map LLM 's representations into semantic and truthful latent spaces respectively, and applies contrastive learning to identify a truthful editing direction within the truthful space. During inference, by editing LLM' s internal representations in truthful space, TruthX effectively enhances the truthfulness of LLM. Experiments show that TruthX improves the truthfulness of 13 advanced LLMs by an average of 20%on TruthfulQA benchmark. Further analyses suggest that TruthX can control LLM to produce truthful or hallucinatory responses via editing only one vector in LLM's internal representations.","Large language models (LLMs) have demonstrated remarkable capabilities across a wide range of natural language processing (NLP) tasks. However, LLMs sometimes generate fluent, instruction-compliant yet untruthful responses, commonly referred to as ""hallucinations"". This phenomenon significantly undermines the credibility of LLMs in applications. Mitigating hallucinations of LLMs poses a substantial challenge, as hallucinations may stem from various factors, such as blindly following instructions, noisy data, lack of knowledge and the generation process. 
Preceding such factors, a more fundamental issue is: whether LLMs can consistently generate truthful responses, even when they possess the correct knowledge? Recent researches suggest ""no"" for this question. For instance, found that LLMs can generate truthful responses in some contexts while producing hallucinations in others. and discovered that LLMs can identify the presence of hallucinations generated by themselves through self-validation. directly pointed out the existence of the generation-discrimination gap in LLMs. All these findings indicate that LLMs, even equipped with correct knowledge, are still susceptible to producing hallucinations during the generation process. Further, some works found a correlation between the LLMs 'internal representations and the truthfulness of outputs, where some erroneous activations of internal representations lead LLMs to generate hallucinations even when they know the correct knowledge. Therefore, activating a well-trained LLM to generate truthful responses is the crucial first step in alleviating the hallucination of LLMs. 
To this end, we propose TruthX, a truthfulness enhancement approach by editing LLM' s internal representations in the truthful space. To edit LLM in the truthful space without compromising its generative capabilities, TruthX decouples the LLM's internal representations into truthful and semantic latent spaces respectively using an auto-encoder. Then, TruthX employs contrastive learning to probe representations with similar semantics but opposite truthfulness and those with similar truthfulness but different semantics within these two latent spaces. During inference, TruthX effectively regulates the truthfulness of LLM by editing it in the truthful space, while ensuring that the generation capability remains intact. Figure illustrates an example of TruthX controlling LLM to generate either truthful or hallucinatory coherent responses. 
Experimental results show that TruthX enhances the truthfulness of 13 advanced LLMs, including Llama, Mistral, Baichuan and Chatglm, by an average of 20%on TruthfulQA benchmark. Through further analyses, we get the following findings: 
 
 * TruthX exhibits superiority in truthfulness control. Editing LLMs along the truthful direction can enhance the truthfulness of responses, conversely, editing LLMs along the opposite direction yields highly hallucinatory responses. 
 * The truthful space extracted from homologous LLMs (i.e., trained sequentially) exhibits a high degree of similarity, so we can directly adopt a well-trained TruthX to different homologous models for truthfulness enhancement. 
 * Layer-wise analysis indicates that the representations in middle layers of LLMs exhibit a higher correlation with the truthfulness of responses."
Revisiting Demonstration Selection Strategies in In-Context Learning,2401.12087v2,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2401.12087v2.pdf,The different 8-shot performance of data-dependent methods (BM25 and TopK) and Our methods in SST-2. The colour in the number represents the relative performance between BM25 and TopK. We see that: 1) The data-dependent methods can not obtain optimal demonstrations under different models; 2) Our dataand model-dependent methods can achieve consistent improvement across different models.,"Large language models (LLMs) have shown an impressive ability to perform a wide range of tasks using in-context learning (ICL), where a few examples are used to describe a task to the model. However, the performance of ICL varies significantly with the choice of demonstrations, and previous research usually focuses on the data aspect ignoring the model 's effect. In this work, we first revisit the factors contributing to this variance from the model aspect, and find that the demonstration choice is both dataand model-dependent. We further propose a conjecture that the performance of a demonstration positively correlates with its contribution to the model' s understanding of the test samples, and accordingly propose a dataand model-dependent demonstration selection method, TopK + ConE. Empirically, our method yields consistent improvements in both language understanding and generation tasks with different model scales. Further analyses confirm that, besides the generality and stability under different circumstances, our method provides a unified explanation for the effectiveness of previous methods. Code is publicly available at <https: //github. com/Romainpkq/revisit_demon_selection_in_ICL>.","Large language models (LLMs, ) have achieved widespread success across many NLP tasks due to their remarkable emergent abilities. One of the most exciting emergent abilities is in-context learning (ICL, ), which utilizes only a few input-output examples to help LLMs make better predictions. ICL has shown its effectiveness in eliciting LLMs 'advanced capabilities and has (almost) become a common practice in tackling complex tasks. However, prior work has found that ICL is very sensitive to the choice of in-context examples and their order in the prompt, and even small changes can result in large variance. 
The sensitivity of ICL motivates researchers to explore methods to identify stable and high-performing demonstrations. Influenced by the success of leveraging a retrieval module to augment neural networks, the retrieval module has become a standard module in the ICL framework for retrieval demonstrations from a dataset. Extensive research has been conducted to search for demonstrations similar to the test samples. For example, proposed to select the samples that are closer to the test sample in the embedding space as in-context examples, and found that choosing the high word-overlap samples can also improve the ICL performance. 
Despite empirical success to some extent, the above methods usually only focus on the test data, overlooking the impact of models. To figure out what factors influence the choice of demonstrations, we revisit the performance of ICL from the model aspect, and accordingly propose a conjecture to understand the effective demonstrations. Specifically, we investigate ICL performance across different retrieval modules and inference models in. Experimental results show that the ICL performance can largely vary with different models even with the same demonstrations (see Figure as an example), indicating that the choice of demonstration is not only dependent on test data but also on the retrieval modules and inference models. We further propose a corresponding conjecture that effective demonstrations are those that enhance the inference model' s understanding of the test input, and the comparison results between shuffled test input and original test input demonstrate that the ICL performance positively correlates with model 's understanding of the test samples. 
Based on the above conjectures, we accordingly propose a demonstration selection method, denoted as TopK+ConE. Specifically, we initially employed the TopK method to narrow down the pool of demonstration candidates, followed by ranking these candidates based on the conditional entropy (estimated by the model itself) of the test sample input. Extensive experiments demonstrate the effectiveness of our method across different model scales. Further analyses show the universality and robustness, and provide a unified view of why previous demonstration selection methods work. Our contributions are summarized as follows: 
 
 * To the best of our knowledge, we are the first to study the impact of models on the demonstration selection methods. We substantiate that the choice of demonstrations is not only dependent on the test data but also on the retrieval module and inference model. 
 * We build the connection between ICL performance with the model' s understanding of test inputs. Our findings reveal that ICL performance positively correlates with the model 's understanding of the test samples. 
 * We propose a dataand model-dependent method TopK+ConE to effectively enhance the models' understanding of test input via reducing the conditional entropy of test input under the inference model. 
 * We achieve state-of-the-art performance on a series of tasks, and prove the effectiveness and universality of our method. Hopefully, our proposed best practice can be employed by more LLM participants."
Multimodal Table Understanding,2406.08100v1,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2406.08100v1.pdf,An overall performance comparison of Table-LLaVA 7B and existing MLLMs on various multimodal table understanding benchmarks. Table-LLaVA outperforms recent open-source MLLMs and is even competitive with the powerful GPT-4V on most tasks.,"Although great progress has been made by previous table understanding methods including recent approaches based on large language models (LLMs), they rely heavily on the premise that given tables must be converted into a certain text sequence (such as Markdown or HTML) to serve as model input. However, it is difficult to access such high-quality textual table representations in some real-world scenarios, and table images are much more accessible. Therefore, how to directly understand tables using intuitive visual information is a crucial and urgent challenge for developing more practical applications. In this paper, we propose a new problem, multimodal table understanding, where the model needs to generate correct responses to various table-related requests based on the given table image. To facilitate both the model training and evaluation, we construct a large-scale dataset named MMTab, which covers a wide spectrum of table images, instructions and tasks. On this basis, we develop Table-LLaVA, a generalist tabular multimodal large language model (MLLM), which significantly outperforms recent open-source MLLM baselines on 23 benchmarks under held-in and held-out settings. The code and data is available at <https: //github. com/SpursGoZmy/Table-LLaVA>. footnote-1 footnote-1 footnote-1","Tables are widely used to store and present data across various fields, e.g., financial analysis, scientific research and government reports. To make the most of the abundant tabular data, the table understanding (TU) technique has been proposed to automatically understand tables and perform table-based tasks, such as question answering and text generation. As a technique that could significantly elevate work efficiency in different industries, it has attracted ever-increasing research interest in recent years. 
Though considerable efforts have been dedicated to the table understanding problem, most previous models can only fulfill very limited tasks until the emergence of large language models (LLMs). With the help of powerful LLMs, we are getting closer to the vision that a versatile model can perform a variety of table-based tasks. However, existing table-oriented LLMs rely heavily on the prerequisite that all given tables must be converted into a certain text sequence (like Markdown or HTML) to be input to LLMs. Under some practical scenarios like scanned documents and webpage screenshots, it is difficult to obtain such high-quality textual table representations, and yet table images are more accessible. Moreover, humans can directly understand two-dimensional tables using the intuitive visual information, whereas LLMs can only interpret tables in a one-directional textual perspective, which may increase the difficulty of comprehending diverse table structures and colored table elements. In summary, for the sake of convenience and intuitiveness, it is a crucial and urgent challenge to explore how to directly digest tables using visual information. 
To promote the advancement of table understanding and its real-world applications, we propose the multimodal table understanding problem, where the model is required to generate correct responses to different table-related requests (e.g., questions) in an end-to-end fashion based on the table image. Despite the fact that recent multimodal large language models (MLLMs) have demonstrated excellent capabilities in many multimodal tasks, they cannot be directly extended to the proposed task. As shown in Figure, the performance of popular MLLMs like MiniGPT-4 and BLIP2 is close to zero on most tasks, revealing their weakness in understanding tabular data. More importantly, there is a lack of a comprehensive dataset that can support both the development and evaluation of generalist MLLMs towards multimodal table understanding. 
To address the above issue, we construct MMTab, the first open-source large-scale dataset for multimodal table understanding problem, based on 14 publicly available table datasets of 8 domains. We carefully design scripts to convert original textual tables in these datasets into table images highlighting a broad coverage of table structures and styles, and transform all task-specific samples into multimodal instruction-tuning samples with a unified format of <table image, input request, output response>. The resulting dataset contains (1) 150K table recognition samples on 97K table images for pre-training (named MMTab-pre). (2) 232K samples of 14 table-based tasks on 82K table images for instruction tuning (named MMTab-instruct). (3) 49K test samples on 23K table images composing 17 held-in and 7 held-out benchmarks (named MMTab-eval). During the dataset construction, data augmentations at multiple levels (e.g., table-level, task-level) were adopted to further improve the data diversity, and we also introduce multimodal table structure understanding tasks that have been overlooked in previous studies. 
Based on the curated dataset, we develop a versatile tabular MLLM named Table-LLaVA with an enhanced two-stage training paradigm. In the first stage, we pre-train LLaVA-1.5 with an extra table recognition task on the MMTab-pre, which requires the model to generate textual sequences (like HTML) given table images. This stage aligns the structures and elements within table images to textual modality and thus enhances the comprehension of the basic table structure and content. In the second stage, we continue to instruction-tuning the model with diverse table-based downstream tasks on the MMTab-instruct, which endows the model with the multimodal instruction-following ability for table-related requests. 
We compare Table-LLaVA with a series of open-source (M) LLMs and closed-source GPT-4V. Experimental results show that Table-LLaVA beats strong MLLM baselines on 17 held-in and 6 held-out benchmarks, and is even competitive with the powerful GPT-4V on 14 benchmarks with a subset of test samples. Extensive ablation experiments are conducted to reveal the contributions of different training data (e.g., the influence of table recognition pre-training data). We also explore the mutual influence between model's capacity for tabular tasks and non-tabular tasks. We hope this work could establish a strong base for future research on the multimodal table understanding problem and facilitate the progress of more general MLLMs. 
We conclude our contributions as follows: 
1) We make the first systematic exploration of the multimodal table understanding problem, which is complementary to the traditional text-only problem setting. 
2) Accordingly, we construct and release a large-scale dataset MM-Tab which covers diverse tables and data for different tasks, including a series of novel table structure understanding tasks. 
3) We develop a versatile tabular MLLM Table-LLaVA, which significantly outperforms a range of strong MLLM baselines under both held-in and held-out settings (Figure)."
Multi-Aspect Controllable Text Generation with Disentangled Counterfactual Augmentation,2405.19958v1,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2405.19958v1.pdf,The relevance scores of positive and negative sentiment in (a) AGNews and (b) Yelp. (a) The classifiers used for statistics are from. (b) The statistical data of Yelp are from.,"Multi-aspect controllable text generation aims to control the generated texts in attributes from multiple aspects (e.g., ""positive"" from sentiment and ""sport"" from topic). For ease of obtaining training samples, existing works neglect attribute correlations formed by the intertwining of different attributes. Particularly, the stereotype formed by imbalanced attribute correlations significantly affects multi-aspect control. In this paper, we propose MAGIC, a new multi-aspect controllable text generation method with disentangled counterfactual augmentation. We alleviate the issue of imbalanced attribute correlations during training using counterfactual feature vectors in the attribute latent space by disentanglement. During inference, we enhance attribute correlations by target-guided counterfactual augmentation to further improve multi-aspect control. Experiments show that MAGICoutperforms state-of-the-art baselines in both imbalanced and balanced attribute correlation scenarios. Our source code and data are available at <https: //github. com/nju-websoft/MAGIC>.","Controllable text generation (CTG) aims to generate texts adhering to given constraints reliably. The development of generative AI based on large language models (LLMs) draws increasing attention to CTG. Due to the demand for diverse attribute control, recent studies focus on a more practical and challenging setting, multi-aspect controllable text generation (MCTG). Different kinds of methods have been proposed, including weighted decoding, optimization in the language space, optimization in the latent semantic space, etc. 
Due to the difficulty of directly obtaining training data that satisfy arbitrary attribute combinations, existing methods reuse datasets with single-aspect annotations for MCTG, where each training sample only expresses a single attribute in one aspect. This neglects the fact that a sentence often couples multiple attributes due to the complexity of natural language. The co-occurrence of attributes within one sentence forms patterns corresponding to attribute correlations, serving as crucial dependencies for a generative model in inference. Meanwhile, the training corpus is derived from real life, where preferences in real life make certain combinations of attributes more common, leading to an imbalance in attribute correlations. As an example shown in Figure, in the AGNews dataset, since news with the topics of ""world"" and ""business"" are prone to correlating with negative elements, such as war or inflation, combinations of these topics and negative sentiment are more prevalent. In the Yelp dataset consisting of restaurant reviews with sentiment and food types, negative reviews also dominate. The imbalance in attribute correlations can lead the model to associate specific attributes, forming a stereotype that impacts multi-aspect control. An MCTG model can better fit attributes with higher co-occurrence frequencies, allowing it to learn the semantic information of these attributes more comprehensively. However, the model may neglect the learning of attributes with low co-occurrence frequencies, which hurts the control of these attribute combinations. 
To resolve the problem, we propose a multi-aspect controllable text generation method with disentangled counterfactual augmentation, called MAGIC. Specifically, we introduce attribute disentanglement with latent space optimization. It can disentangle the control factors of different attributes in the texts and generate the latent vectors with counterfactual features in the attribute latent space. During training, we employ counterfactual latent vectors to balance attribute correlations, thereby constructing a more semantically balanced attribute latent space. During inference, we enhance attribute correlations by the counterfactual latent vectors to improve multi-aspect control. 
We experiment on three-aspect control including sentiment, topic, and detoxification. We evaluate the relevance scores of attributes and assess the text quality in the scenarios of imbalanced and balanced attribute correlations. The results indicate that MAGICcan leverage attribute correlations and mitigate the imbalance issues, which leads to steady and superior performance in both imbalanced and balanced scenarios than state-of-the-art baselines on multi-aspect control. We further demonstrate the effectiveness of each module in MAGICthrough analytical experiments. 
Our main contributions are outlined as follows: 
 * To mitigate the issue of imbalanced attribute correlations, we propose a counterfactual feature augmentation model with attribute disentanglement. 
 * To improve multi-aspect control by leveraging attribute correlations, we introduce a text generation method based on target-guided attribute correlation augmentation. 
 * We experimentally validate the effectiveness of MAGIC. It outperforms state-of-the-art baselines on the imbalanced and balanced settings of multi-aspect control."
Reward-based Input Construction for Cross-document Relation Extraction,2405.20649v1,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2405.20649v1.pdf,"An illustrated comparison between Snippet and selected sentences using our REward-based Input Construction (REIC) for cross-document relation extraction. The figure depicts an example triplet (Kubuntu, x86-64, platform) with the text path ('Mir (software) ', 'X86-64'), including three bridge entities (Ubuntu, Linux, Intel) abbreviated as (Bridge1, Bridge2, Bridge3). Dash and solid arrows signify the selection process of Snippet and REIC, respectively, while gradient-colored arrows indicate connections between the head and tail entities. REIC selects important sentences from any position within a path to determine the relation between the head and tail entity, whereas Snippet only includes sentences located around the head or tail entity.","Relation extraction (RE) is a fundamental task in natural language processing, aiming to identify relations between target entities in text. While many RE methods are designed for a single sentence or document, cross-document RE has emerged to address relations across multiple long documents. Given the nature of long documents in cross-document RE, extracting document embeddings is challenging due to the length constraints of pre-trained language models. Therefore, we propose REward-based Input Construction (REIC), the first learning-based sentence selector for cross-document RE. REIC extracts sentences based on relational evidence, enabling the RE module to effectively infer relations. Since supervision of evidence sentences is generally unavailable, we train REIC using reinforcement learning with RE prediction scores as rewards. Experimental results demonstrate the superiority of our method over heuristic methods for different RE structures and backbones in cross-document RE. Our code is publicly available at <https: //github. com/aailabkaist/REIC>.","The task of relation extraction (RE) aims to identify relations between a pair of target entities in a given text. This task is fundamental in natural language processing (NLP) and provides crucial information for applications such as information retrieval and question answering. Most existing RE methods are limited to scenarios where the entity pair is within a single sentence or a single document. However, many scenarios require extracting relations across multiple documents, where the target entity pair may not coexist within the same document. Therefore, recent efforts to address these challenges have led to the proposal of cross-document RE by, and research in this area has received considerable attention. 
Unlike traditional RE research, cross-document RE involves extracting relational facts from large-scale long documents. For example, the DocRED dataset, developed for document-level RE, contains an average of 198 words per document. In contrast, the CodRED dataset, designed for cross-document RE, has an average of 2,416 words per document. Given the substantial length of documents in cross-document RE tasks, extracting document embeddings from pre-trained language models, a step common to all methods, poses considerable challenges. This is because pre-trained language models are limited by the maximum number of tokens they can process; for example, BERT typically has a limit of 512 tokens. 
A simple approach adopted in several studies is to use text snippets around the target entities as input to pre-trained language models, as shown in the dashed boxes in the middle panel of. Often, this text snippet is determined by either word proximity or document structure without any adjustment. However, if only a subset of sentences around the entities is extracted from long documents, there is a risk of missing important sentences that are crucial for RE. For example, as seen in the left panel of, using snippets around the entities fails to capture the relation 'platform' between the entity pair (Kubuntu, x86-64). 
In this paper, we introduce REward-based Input Construction (REIC), the first learning-based input sentence selection module specifically designed for cross-document RE. The right panel of displays the sentences selected by our model from the example. By identifying relational evidence sentences through the selection module, the RE module infers the relation between the target entities using the reasoning chain illustrated by the gradient-colored arrows in. Our approach is to develop a sentence selection module that computes the probability of selecting sentences based on the currently selected sentences. We show that this sentence selection process can be modeled as a Markov decision process (MDP). We specifically choose to use MDP because there is no supervision for sentence selection from corpus, so RE models have to perform iterative learning by trials. Subsequently, we train the sentence selection module using reinforcement learning (RL), where the relation prediction score obtained from the selected sentences serves as the reward. Through experimental validation, REIC outperforms other heuristic methods across various RE modules."
Semi-Supervised Spoken Language Glossification,2406.08173v1,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2406.08173v1.pdf,"Overview of the proposed S^3LG. It iteratively annotates and learns from pseudo labels to obtain the final SLG model f (θ̂^K) after total K iterations. For the k-th iteration, the synthetic data 𝒟_U^k-1 is conducted by randomly mixing the two complementary pseudo glosses generated by the fixed rules and the previously obtained SLG model f (θ̂^k-1), respectively. Note that at the first iteration, only the rule-based synthetic data is available.","Spoken language glossification (SLG) aims to translate the spoken language text into the sign language gloss, i.e., a written record of sign language. In this work, we present a framework named Semi-Supervised Spoken Language Glossification (S^3LG) for SLG. To tackle the bottleneck of limited parallel data in SLG, our S^3LG incorporates large-scale monolingual spoken language text into SLG training. The proposed framework follows the self-training structure that iteratively annotates and learns from pseudo labels. Considering the lexical similarity and syntactic difference between sign language and spoken language, our S^3LG adopts both the rule-based heuristic and model-based approach for auto-annotation. During training, we randomly mix these complementary synthetic datasets and mark their differences with a special token. As the synthetic data may be less quality, the S^3LG further leverages consistency regularization to reduce the negative impact of noise in the synthetic data. Extensive experiments are conducted on public benchmarks to demonstrate the effectiveness of the S^3LG. Our code is available at <https: //github. com/yaohj11/S3LG>.","Sign Language is the most primary means of communication for the deaf. Translating between sign and spoken language is an important research topic, which facilitates the communication between the deaf and the hearing. To support the development of applications, sign language gloss has been widely used as an intermediate step for generating sign language video from spoken language text or the inverse direction. The sign language gloss is the written representation of the signs. As a generally adopted way for sign language transcription, gloss is sufficient to convey most of the key information in sign language. In this work, we focus on the first step of the former task named spoken language glossification (SLG), which aims to translate the spoken language text into the sign language gloss. 
SLG is typically viewed as a low-resource sequence-to-sequence mapping problem. The previous methods rely on the encoder-decoder architectures to jointly align the embedding space of both languages in a data-driven manner. Since the data collection and annotation of sign language requires specialized knowledge, obtaining a large-scale text-gloss dataset is time-consuming and expensive. As a result, the performance of SLG models is limited by the quantity of parallel data. Witnessing the success of introducing monolingual data to enhance low-resource translation quality in neural machine translation (NMT), we are motivated to explore the accessible unlabeled spoken language texts to improve SLG. 
In this work, we present a framework named Semi-Supervised Spoken Language Glossification (S^3LG) to boost SLG, which iteratively annotates and learns from pseudo labels. To implement the above idea, the proposed S^3LG adopts both the rule-based heuristic and model-based approach to generate pseudo glosses for unlabeled texts. The rule-based synthetic data has high semantic accuracy, however, the fixed rules make it difficult to cover complex expression scenarios. The model-based approach on the other hand is more flexible for learning the correspondence between sign language and spoken language and generates pseudo gloss with higher synthetic diversity. These complementary synthetic datasets are randomly mixed as a strong supplement for the training of the SLG model. Besides, the model-based synthetic data is generated by the SLG model, which sets a good stage for iteratively re-training the SLG model. 
In addition, S^3LG introduces a simple yet efficient design from three aspects. Firstly, in each iteration, the training process is separated into two stages, i.e., pre-training and fine-tuning for domain adaptation. Secondly, to encourage the model to learn from the noisy pseudo labels, we apply the consistency regularization term to the training optimization and gradually increase the weight of the consistency regularization in the training curriculum. It enforces the consistency of the predictions with network perturbations based on the manifold assumption. Thirdly, to encourage the SLG model to learn complementary knowledge from different types of synthetic data, a special token is added at the beginning of input sentences to inform the SLG model which data is generated by the rule-based or model-based approach. Through end-to-end optimization, our S^3LG achieves significant performance improvement over the baseline. Surprisingly, the experiments show that the translation accuracy on low-frequency glosses is promisingly improved. We conjecture that the SLG model acts differently in annotating the high-frequency and low-frequency glosses, and such bias is mitigated by the rule-based synthetic data. 
In summary, our contributions are three-fold: 
 
 ∙We propose a novel framework S^3LG for SLG (namely, text-to-gloss translation), which iteratively annotates and learns from the synthetic data. It adopts two complementary methods, i.e., the rule-based heuristic and model-based approach for auto-annotation. 
 
 ∙We further leverage consistency regularization to reduce the negative impact of noise in the synthetic data. The biases of the SLG model on low-frequency glosses are mitigated by incorporating the rule-based synthetic data. 
 
 ∙We conduct extensive experiments to validate our approach, which shows encouraging performance improvement on the two public benchmarks, i.e., CSL-Daily and PHOENIX14T."
Fine-Grained Image-Text Alignment in Medical Imaging Enables Explainable Cyclic Image-Report Generation,2312.08078v5,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2312.08078v5.pdf,"Current vision-language models (VLM) achieve (a) global alignment and (b) local alignment by matching overall visual with textual features, and aligning patches with word features, respectively. (c) To exploit the relation between textual words and abnormal patches with varied sizes, our AdaMatch obtains adaptive patch features and aligns them with word features.","Fine-grained vision-language models (VLM) have been widely used for inter-modality local alignment between the predefined fixed patches and textual words. However, in medical analysis, lesions exhibit varying sizes and positions, and using fixed patches may cause incomplete representations of lesions. Moreover, these methods provide explainability by using heatmaps to show the general image areas potentially associated with texts rather than specific regions, making their explanations not explicit and specific enough. To address these issues, we propose a novel Adaptive patch-word Matching (AdaMatch) model to correlate chest X-ray (CXR) image regions with words in medical reports and apply it to CXR-report generation to provide explainability for the generation process. AdaMatch exploits the fine-grained relation between adaptive patches and words to provide explanations of specific image regions with corresponding words. To capture the abnormal regions of varying sizes and positions, we introduce an Adaptive Patch extraction (AdaPatch) module to acquire adaptive patches for these regions adaptively. Aiming to provide explicit explainability for the CXR-report generation task, we propose an AdaMatch-based bidirectional LLM for Cyclic CXR-report generation (AdaMatch-Cyclic). It employs AdaMatch to obtain the keywords for CXR images and 'keypatches' for medical reports as hints to guide CXR-report generation. Extensive experiments on two publicly available CXR datasets validate the effectiveness of our method and its superior performance over existing methods.","Inter-modality alignment, such as vision and language, has been an important task with growing interests in the field of computer vision, especially with the recent advancement in representation learning. Technologies like contrastive learning and self-supervised learning have dramatically improved state-of-the-art alignment performance. Recent vision-language models (VLMs) demonstrate two approaches: global contrastive alignment, which integrates images and texts at a global level, and local alignment, focusing on detailed connections between visual objects and textual words, as illustrated in Fig. . 
Current VLMs with local alignment either adopt the pre-trained object detector to extract region-of-interest (ROI) features from images and match the corresponding object features with textual words, or align the visual token from each patch and the textual token into the same embedding space. The former highly relies on the quality of the object detector and its predefined classes, which is less generalizable to new domains. The latter family of methods learns the alignment in a more automatic and data-driven manner. However, most of these methods depend on a pre-defined patch size and positions (e.g., grids) across images. In the most challenging cases, such as the analysis of medical image, lesions can exhibit a wide range of shapes, sizes, and positions. A fixed partition of image patches can lead to incomplete or ambiguous representations of the key imaging abnormalities. Therefore, it is highly desirable to adaptively exploit the fine-grained relationship between image embeddings derived from a more flexible patching scheme and textual embeddings. 
Another challenge in the current VLMs lies in their explainability: it is generally difficult to delineate the image-text relationship learned by the model, especially for the current medical VLMs. Current solutions to provide such explanations in medical VLMs leverage the attention maps from the intermediate layer to visualize the location of the abnormalities. Other methods utilize network gradients such as Grad-CAM to generate the heatmaps according to lesion types based on ground-truth reports. However, both maps can only show the general areas potentially associated with the corresponding text data rather than pinpointing a specific region. In addition, gradient-based methods need ground-truth reports, prohibiting them from functioning correctly beyond training data. It is thus highly necessary to develop a mechanism that could provide explicit and specific explanations of input image or text during inference time. 
To address these two challenges above, we propose a novel Adaptive patch-word Matching (AdaMatch) model to match fine-grained image regions of various sizes and positions with textual data. AdaMatch introduces an image encoder with multiple Adaptive Patch extraction (AdaPatch) modules to adaptively acquire the patches associated with certain text tokens. It then performs patch-word alignment based on contrastive learning. AdaMatch is specifically developed in the context of aligning radiology images (chest X-ray, CXR) and their corresponding radiology reports with the capability of achieving cyclic (CXR-to-report and report-to-CXR) generation based on the learned alignment. Our premise is that such a cyclic generation task would serve as the best use case and evaluation criterion for the desired fine-grained alignment. Also, fine-grained cyclic generation between CXR and report will provide natural explainability for how the model aligns two modalities: for any given text token, we can visualize its matching imaging manifestation; and for any image region within a CXR image, we can tell the type of lesion or anatomical region it belongs to. 
To implement the cyclic CXR-report generation, we propose an AdaMatch-based bidirectional model (AdaMatch-Cyclic). AdaMatch-Cyclic employs AdaMatch to identify the keywords for CXR images and the 'keypatches' for medical reports to guide the generation tasks. Since the potential keywords for CXR images cover a wide range and ground-truth reports cannot be used during inference, we predefine a textual codebook with the most common entities from medical reports as prior knowledge during fine-grained alignment. With the textual codebook, AdaMatch aligns it with the adaptive patches to obtain matched keywords to facilitate report generation. Next, a VQ-GAN model encodes the CXR image into image tokens, and a Large Language Model (LLM) takes image tokens, the matched keywords, and the instructions as input to generate medical reports. Similarly, we also build a visual codebook with the most commonly seen patches as 'keypatches', and use AdaMatch to obtain the matched keypatches from given text reports as hints for CXR generation. Utilizing medical reports, matched keypatches, and instructions, LLM generates image tokens, subsequently decoded by the VQ-GAN model to produce the resulting CXR image. Our contributions are summarized as follows: 
 
 * To exploit the fine-grained relation between CXR image patches and words of medical reports, we propose an Adaptive patch-word Matching (AdaMatch) model to obtain adaptive patches for abnormal regions and perform alignment between them and texts in medical reports. 
 * We devise an AdaMatch-based bidirectional LLM for Cyclic CXR-report generation (AdaMatch-Cyclic) to facilitate the bi-directional generation between CXR and reports. Moreover, we build the textual and visual codebook to utilize AdaMatch to extract useful keywords and keypatches for the report and CXR generation, respectively. 
 * Experiments on two publicly available chest X-ray datasets demonstrate the effectiveness of our method and its superior performance over the state-of-art methods."
Are LLM-based Evaluators Confusing NLG Quality Criteria?,2402.12055v2,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2402.12055v2.pdf,An example of prompting LLMs to evaluate the dialogue summarization on criteria for fluency.,"Some prior work has shown that LLMs perform well in NLG evaluation for different tasks. However, we discover that LLMs seem to confuse different evaluation criteria, which reduces their reliability. For further verification, we first consider avoiding issues of inconsistent conceptualization and vague expression in existing NLG quality criteria themselves. So we summarize a clear hierarchical classification system for 11 common aspects with corresponding different criteria from previous studies involved. Inspired by behavioral testing, we elaborately design 18 types of aspect-targeted perturbation attacks for fine-grained analysis of the evaluation behaviors of different LLMs. We also conduct human annotations beyond the guidance of the classification system to validate the impact of the perturbations. Our experimental results reveal confusion issues inherent in LLMs, as well as other noteworthy phenomena, and necessitate further research and improvements for LLM-based evaluation.","With the emergence of powerful large language models (LLMs) such as ChatGPT, LLM-based evaluators have been widely used for various natural language generation (NLG) tasks. In evaluation for common NLG tasks such as summarization, dialogue, and story generation, different aspects of quality (such as fluency and faithfulness) should be considered individually. Traditional evaluation metrics are either incapable of evaluating specific aspects, like BLEU and BERTScore, or they can only roughly assess a single aspect, like FactCC. In contrast, LLMs can be treated as akin to human annotators, with various definitions of aspects contained in the prompt for flexible evaluation. Some studies have shown LLM-based evaluators have achieved comparable performance with humans in many NLG tasks, suggesting LLMs to become promising candidates for automatic evaluation. 
However, during the explorations of LLM-based NLG evaluation, we observed two noteworthy phenomena that have not been revealed in previous work. First, the evaluation results from LLMs for a given aspect can achieve a higher correlation with human judgments on another clearly different aspect. Second, the correlations between LLM-generated scores across different aspects are significantly higher than those between human judgments accordingly. These lead us to question the reliability of LLM evaluations on required aspects, since LLMs seem to confuse different aspects. 
Understanding these issues is inseparable from aspects themselves at first, which stem from human evaluation for NLG tasks and are typically described by terms and definitions, forming corresponding specific criteria. Through semi-structured interviews, revealed that if aspects for evaluation lacked clear conceptualization, human annotators might conflate different aspects, such as fluency and grammaticality. pointed out the long-standing confusion of terms and definitions in human annotations, resulting in incomparable evaluations. Combining our investigation of previous work involving evaluation criteria, we believe that there are two distinct issues. The first is inconsistent conceptualization, where the definition is inconsistent with others for the same aspect but is clearly articulated. The second is ambiguous expression, where the definition is so vague that human annotators aren't sure what it really means. In Figure, we present an example of evaluation for fluency, where the criteria enclosed by the dashed box are selected from existing work and correspond to these two issues. 
Therefore, we should reduce the influence of the issues within the evaluation criteria as much as possible, so as to reveal the actual performance of LLMs on NLG evaluation across aspects. We collect many existing criteria from previous papers involved, and summarize a clear hierarchical classification system for aspects that are most commonly used. For each aspect, we construct five criteria with descriptions of different levels of detail, including default, detailed, and simplified ones, to explore the corresponding effects. Then, inspired by behavioral testing in NLP, we elaborately design a series of perturbation attacks based on the classification system to conduct targeted analyses on both proprietary LLMs (GPT-3.5 and GPT-4) and specifically fine-tuned LLMs like Prometheus. Different from previous related work, each of our perturbations is designed for a specific aspect to better verify the variances in evaluation for aspects that are related or not. We also engage human annotators to check our perturbations and expected impacts to enhance the reliability of our attack tests. To sum up, our contributions and findings are as follows: 
 
 * To the best of our knowledge, we are the first to explore the capabilities of LLMs in distinguishing aspects during NLG evaluation and the impacts of different criteria descriptions, bridging human and LLM-based evaluation. 
 * We summarize a classification system containing 11 common aspects and propose 18 aspect-targeted perturbation attacks, which have been verified by human annotators, to test the fine-grained evaluation behaviors of LLMs. 
 * Our experimental results reveal the confusion across different aspects in LLM-based evaluation, even for the powerful GPT-4, which necessitate attention and in-depth research. The related resources have been released, aiming to facilitate future relevant work."
Linear Transformers with Learnable Kernel Functions are Better In-Context Models,2402.10644v2,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2402.10644v2.pdf,"Results on the MQAR dataset, designed to measure In-Context Learning capabilities of an architecture. ReBased outperforms all baselines except Attention across different sequence lengths and model sizes. See Section for more details.","Advancing the frontier of subquadratic architectures for Language Models (LMs) is crucial in the rapidly evolving field of natural language processing. Current innovations, including State Space Models, were initially celebrated for surpassing Transformer performance on language modeling tasks. However, these models have revealed deficiencies in essential In-Context Learning capabilities – a domain where the Transformer traditionally shines. The Based model emerged as a hybrid solution, blending a Linear Transformer with a kernel inspired by the Taylor expansion of exponential functions, augmented by convolutional networks. Mirroring the Transformer's in-context adeptness, it became a strong contender in the field. In our work, we present a singular, elegant alteration to the Based kernel that amplifies its In-Context Learning abilities evaluated with the Multi-Query Associative Recall task and overall language modeling process, as demonstrated on the Pile dataset.","Large Language Models (LLMs) are revolutionizing the field of natural language processing and establishing new benchmarks across various tasks. Nevertheless, despite their triumphs, most of these models are built on Transformer frameworks that employ attention mechanisms. These mechanisms scale poorly with long text sequences, leading to impractical computational complexity for extending contextual processing. 
To address this constraint, several alternatives to Transformers were proposed. suggested replacing the exponential function in the attention mechanism with the kernel function to change the order of computations and thus move away from quadratic complexity of the sequence length. However, when compared to vanilla Transformers, this approach leads to a drop in performance. Furthermore, the kernel function selection is a topic still in need of consideration. An alternative way to define a linear model is to utilize State Space Models (SSMs), which are capable of producing quality that is comparable to Transformers when measured with perplexity on language modeling. 
Notably, both Linear Transformers and SSMs can be described as Recurrent Neural Networks (RNNs), which have their limitations when it comes to managing lengthy dependencies within texts since memory capacity can be overrun as the volume of information increases. Additionally, while the hidden state of RNNs is larger for Linear Transformers than for SSMs, the latter showed higher text modeling quality. The introduction of the Based model attempted to address the abovementioned challenges by utilizing a hybrid architecture based on a Linear Transformer with a novel kernel function derived from a Taylor expansion of an exponential function. demonstrated that the Based model was less prone to performance issues when working with longer content than other models when assessed on the Multi-Query Associative Recall (MQAR) task. Nonetheless, even the Based model experiences a drop in performance when faced with extensive contexts relative to the conventional transformer architecture. 
A profound comprehension of the processes occurring within the Based architectures is essential for their advancement. Upon examining how attention scores are distributed, we argue that the kernel function previously adopted in Based cannot be considered optimal, resulting in limitations when dealing with lengthy context and small model capacity. To address this issue, we introduce ReBased (Revisited Based), a novel variation of the Linear Transformer model that improves the use of attention kernels. The crux of our development lies in addressing the inability of Based to disregard specific tokens with zero probability during the attention process. By refining the kernel function and incorporating new architectural modifications, we have created a model that improves accuracy on tasks involving retrieving information from long sequences of tokens while simplifying the calculation of the attention mechanism. 
When testing our enhanced architecture on the MQAR task, we found that ReBased surpasses the original Based model across a variety of contexts and model sizes. Additionally, after training with the Pile dataset, we observed that ReBased performs better than its predecessor at In-Context Learning and excels at modeling associative dependencies measured through improved perplexity metrics."
To Generate or to Retrieve? On the Effectiveness of Artificial Contexts for Medical Open-Domain Question Answering,2403.01924v2,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2403.01924v2.pdf,"MedGENIE performance (Flan-T5-base, Fusion-In-Decoder) on USMLE-style questions. Comparison against fine-tuned open-source baselines with a maximum of 10B parameters, using the MedQA (4 options) test set. Model size displayed on a log scale.","Medical open-domain question answering demands substantial access to specialized knowledge. Recent efforts have sought to decouple knowledge from model parameters, counteracting architectural scaling and allowing for training on common low-resource hardware. The retrieve-then-read paradigm has become ubiquitous, with model predictions grounded on relevant knowledge pieces from external repositories such as PubMed, textbooks, and UMLS. An alternative path, still under-explored but made possible by the advent of domain-specific large language models, entails constructing artificial contexts through prompting. As a result, ""to generate or to retrieve"" is the modern equivalent of Hamlet's dilemma. This paper presents MedGENIE, the first generate-then-read framework for multiple-choice question answering in medicine. We conduct extensive experiments on MedQA-USMLE, MedMCQA, and MMLU, incorporating a practical perspective by assuming a maximum of 24GB VRAM. MedGENIE sets a new state-of-the-art in the open-book setting of each testbed, allowing a small-scale reader to outcompete zero-shot closed-book 175B baselines while using up to 706× fewer parameters. Our findings reveal that generated passages are more effective than retrieved ones in attaining higher accuracy.","Question answering is a challenging task that requires complex reasoning on explicit constraints described in the question and unstated domain knowledge. Open-domain question answering (ODQA) aims to tackle natural questions across various topics without predefined evidence. This setting mirrors real-world scenarios where there cannot be a labeled passage for every potential user inquiry. ODQA has particular significance in medicine due to the high-quality standards it demands, including a deep understanding of specialized terminology and background concepts, and an effective recall of expert insight for clinical decision making. 
Recent efforts have transitioned from a closed-book strategy, where models rely solely on their opaque parametric knowledge, to an open-book alternative, allowing them to consult external sources for grounding. In particular, the retrieve-then-read framework is a common thread, where the input is augmented with relevant knowledge chunks retrieved from an external datastore, which can be unstructured (e.g., PubMed, textbooks) or structured (e.g., UMLS). However, performance is highly dependent on the quality of the retriever. Developing custom retrieval modules generally requires extensive question–context pairs or intensive computational resources, particularly when dealing with massive sources. Furthermore, the retrieved fragments may be incomplete or not specifically tailored to the query, leading to noise. 
In parallel, medical large language models (LLMs) have gained increasing research interests to aid professionals and improve patient care. After pre-training on an extreme-scale collection of specialized text corpora, they implicitly encode an impressive amount of domain knowledge that can be evoked through prompting, akin to summoning a genie from a lamp. This facilitates a paradigm shift towards a generate-then-read approach, wherein contexts are directly generated by an LLM. Despite preliminary work, there is an ongoing debate on ""whether generative augmentation is preferable to retrieval augmentation"". 
In this paper, we introduce MedGENIE, the first generate-then-read framework for multiple-choice medical ODQA. Specifically, we study the effectiveness of grounding generalist LLMs and small language models (SLMs) on multi-view contexts generated by a medical LLM via in-context learning (ICL) and fine-tuning, respectively. To foster accessibility and match prevalent hardware configurations, we assume a low-resource infrastructure with 24GB VRAM. 
We evaluate MedGENIE on three standard ODQA benchmarks designed to quantify professional medical competencies: MedQA-USMLE, MedMCQA, and MMLU-Medical. MedGENIE demonstrates significant performance gains, improving the accuracy of few-shot LLM readers on all testbeds by up to ≈16 points. By fine-tuning the reader, MedGENIE allows Flan-T5-base to outcompete closed-book zero-shot 175B LLMs and supervised 10B baselines on MedQA (Figure), using up to 706× fewer parameters. Furthermore, our research demonstrates a clear inclination of cutting-edge rerankers towards favoring generated contexts over retrieved ones. When treated as knowledge sources or incorporated into human-curated ones, generated passages notably enhance the effectiveness of retrieve-then-read workflows (up to ≈6 extra points). RAGAS evaluation confirms the quality of generated contexts, even allowing for more faithful answers from the reader. Finally, we release a comprehensive dataset of ≈1 million artificial contexts in the medical field, adhering to principles of open science and encouraging further research endeavors."
SC2: Towards Enhancing Content Preservation and Style Consistency in Long Text Style Transfer,2406.04578v1,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2406.04578v1.pdf,Comparisons of content learning between existing approaches and the proposed method: (a) evaluating the relevance between text x and its style; (b) evaluating the relevance between source text x_s and target text x_t; and (c) joint evaluating the relevance between text x and its style as well as content.,"Text style transfer (TST) aims to vary the style polarity of text while preserving the semantic content. Although recent advancements have demonstrated remarkable progress in short TST, it remains a relatively straightforward task with limited practical applications. The more comprehensive long TST task presents two challenges: (1) existing methods encounter difficulties in accurately evaluating content attributes in multiple words, leading to content degradation; (2) the conventional vanilla style classifier loss encounters obstacles in maintaining consistent style across multiple generated sentences. 
In this paper, we propose a novel method SC2, where a multilayer Joint Style-Content Weighed (JSCW) module and a Style Consistency loss are designed to address the two issues. The JSCW simultaneously assesses the amounts of style and content attributes within a token, aiming to acquire a lossless content representation and thereby enhancing content preservation. The multiple JSCW layers further progressively refine content representations. We design a style consistency loss to ensure the generated multiple sentences consistently reflect the target style polarity. Moreover, we incorporate a denoising non-autoregressive decoder to accelerate the training. We conduct plentiful experiments and the results show significant improvements of SC2 over competitive baselines. Our code: <https: //github. com/jiezhao6/SC2>.","Text style transfer (TST) aims to generate a text exhibiting a desired style based on the source text (e.g., negative → positive), while endeavoring to faithfully preserve the semantic content. The applications of TST cover a wide range of user-centric natural language generation tasks, such as personalized dialogue systems, educational platforms, and writing assistants. 
Given the scarcity of parallel data (i.e., text pairs conveying the same content but differing in styles) and the labor-intensive nature of annotating such pairs, existing research has predominantly focused on unsupervised TST. Recent contributions in this domain, including studies by, have demonstrated significant progress. Despite notable success, these works primarily concentrate on the transfer of a single sentence, which we call short TST. This is a relatively simple task and is difficult to apply to complex scenarios, i.e., long TST such as transferring news articles and novels. It is challenging to achieve desirable content preservation and style consistency across multiple sentences for these methods. In a very recent study, first concentrated on the long TST task and proposed StoryTrans, which learns content representations and fills stylistic tokens in separate stages. While somewhat effective, challenges persist in preserving content and ensuring style consistency. 
Content Preservation: The critical factor for preserving content lies in accurately assessing the amount of content attribute (CA) within a token to improve the content representation. In traditional approaches, the content learning primarily involves explicitly or implicitly removing style tokens. In these processes, they evaluate the relevance between text and style, which solely consider the amount of style attribute (SA) within a token and neglect the CA amount (Figure (a) ). This results in tokens with both strong style and content attributes, such as ""euphonious"", potentially receiving higher SA scores, making them more drastic to be removed and consequently leading to content degradation. On the other hand, in StoryTrans, the disentanglement of content from style is achieved by encouraging texts with distinct styles to be close together in the content space (Firure (b) ). However, owing to the non-parallel nature of the data, this unavoidably results in a loss of content information. 
Style Consistency: To control the style polarity of generated text, existing methods employ a style discriminator that operates on the entire output text. However, in the context of long TST, it becomes challenging for the discriminator to ensure that the style of the generated multiple sentences is consistent. As a result, some generated sentences might exhibit strong target style polarity while others remain weak, which creates a less reader-friendly outcome. Therefore, we argue that maintaining style (polarity) consistency across multiple sentences is crucial. 
To tackle the above issues, we propose a novel approach aimed at enhancing content preservation and maintaining style consistency in long TST. Our approach achieves these objectives by carefully designing a multilayer Joint Style-Content Weigher (JSCW) module and a Style Consistency loss, thus we call it SC2. (1) The JSCW utilizes the convolution operation to measure the SA amount within the center token. Simultaneously, it assesses the CA amount by computing and integrating the content relevance of a token across all sentences. Then by normalizing these two amounts and weighting the CA amount to the tokens' representations, we obtain preliminary content representation. Furthermore, we employ multiple JSCW layers to progressively refine content representations. Finally, we fuse the target style information to content representations and feed them to the decoder to generate target text. (2) For the other challenge, we design a contrastive learning-based style consistency loss. It brings each generated sentence closer to the previously generated sentences or target sentences in the corpus, while farther away from the sentences in the source text. 
Additionally, within the unsupervised long TST setting, directly employing an autoregressive (AR) decoder substantially slows down the training since the masked self-attention technique cannot be exploited. Hence, drawing inspiration from the research on non-AR (NAR) text generation, we incorporate an auxiliary denoising NAR decoder. It parallelly generates pseudo-target texts, which are then fed into the AR decoder to accelerate the training process. Our main contributions are summarized as follows: 
 
 * We propose to explicitly and simultaneously assess the SA and CA amounts within tokens to learn lossless content representations and show that this idea can significantly improve content preservation in long TST. 
 * We first propose the concept of style consistency for long TST, and design a corresponding loss to encourage the generated text to consistently maintain style polarity across multiple sentences. 
 * Extensive experiments are conducted on both Chinese and English datasets to verify the effectiveness of the proposed SC2. The results demonstrate significant improvements over competitive baselines."
NewsBench: A Systematic Evaluation Framework for Assessing Editorial Capabilities of Large Language Models in Chinese Journalism,2403.00862v4,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2403.00862v4.pdf,"The key components and processes to evaluate editorial capabilities of an LLM with our evaluation framework, NewsBench. The numbers inside the brackets indicate the number of test samples that we construct for each group of evaluations. The bold border boxes are the overall scores for Short Answer Questions (SAQs) and Multiple Choice Questions (MCQs) on Safety Adherence (SA) and Journalistic Writing Proficiency (JWP), respectively.","We present NewsBench, a novel evaluation framework to systematically assess the capabilities of Large Language Models (LLMs) for editorial capabilities in Chinese journalism. Our constructed benchmark dataset is focused on four facets of writing proficiency and six facets of safety adherence, and it comprises manually and carefully designed 1,267 test samples in the types of multiple choice questions and short answer questions for five editorial tasks in 24 news domains. To measure performances, we propose different GPT-4 based automatic evaluation protocols to assess LLM generations for short answer questions in terms of writing proficiency and safety adherence, and both are validated by the high correlations with human evaluations. Based on the systematic evaluation framework, we conduct a comprehensive analysis of eleven popular LLMs which can handle Chinese. The experimental results highlight GPT-4 and ERNIE Bot as top performers, yet reveal a relative deficiency in journalistic safety adherence in creative writing tasks. Our findings also underscore the need for enhanced ethical guidance in machine-generated journalistic content, marking a step forward in aligning LLMs with journalistic standards and safety considerations. The evaluation framework and experimental results are expected to provide an in-depth understanding of the editorial capabilities of LLMs and speed up the development of LLMs in journalism.","The increasing availability of Large Language Models (LLMs) with Application Programming Interfaces (APIs), like OpenAI's ChatGPT, has further accelerated the adoption of the LLM technology across a variety of application scenarios. However, while LLMs offer significant benefits to Natural Language Processing (NLP), their non-deterministic and black-box nature has sparked discussions and concerns about ensuring the responsible and ethical utilization of this advanced technology. Although general safety evaluation benchmarks and safeguard measures, including the OpenAI moderation API designed to prevent toxic and harmful content, have been proposed and some put in place, there is a need for specialized benchmarks tailored to the unique rules, responsibilities, and styles of various professional domains and scenarios. In journalism, the significant role it plays in informing the general public and its potential to influence public perception demands a higher and more specific ethical and safety standard. 
There are an increasing number of LLMs being applied in Chinese journalism to complete editorial tasks, such as headline generation, summarization, continuation writing, expansion writing and refinement. Despite considerable discussions among the academia and industry on comprehending, regulating, and mitigating the risks associated with LLMs in journalism, there is a notable absence of a standardized benchmark or systematic evaluation framework that assess the alignment of LLMs with journalistic ethics and safety standard and integrates them with common journalistic editorial tasks. 
Drawing on discussions about AI safety in journalism, this paper introduces NewsBench, a systematic evaluation framework which is focused on assessing the editorial capabilities of LLMs for not only journalistic writing proficiency but also safety adherence. For journalistic writing proficiency, we focus on language fluency, logical coherence, style alignment, and instruction fulfilment, while for safety adherence we consider six facets including civil language, bias and discrimination, personal privacy, social harm, journalistic ethics, and illegal activities. We construct the benchmark dataset with 1,267 test samples in the types of multiple choice and short answer questions in five editorial tasks including headline generation, summarization, continuation of writing, expansion of writing, and style refinement from 24 news domains. Additionally, NewsBench incorporates two automatic evaluation protocols for assessing LLM generations for short answer questions in terms of writing proficiency and safety adherence. Utilizing this comprehensive framework, we have evaluated eleven popular LLMs which can handle Chinese, providing insights into their performance across a diverse range of journalistic tasks and safety considerations. 
The main contributions of the paper are as follows: 
 
 * We propose an evaluation framework for systematically evaluating LLMs on journalistic writing and safety, and we release 1,267 manually designed test samples featuring two types of short answer and multiple choice questions across five editorial tasks. 
 * Two GPT-4 based evaluation protocols for journalistic writing proficiency and safety compliance are developed and validated by human annotation. 
 * We conduct a comparative analysis and error assessment of eleven popular LLMs, identifying their strengths and weaknesses for editorial tasks in Chinese journalism. GPT-4 and ERNIE Bot are identified as leading models while they still have limitations in adhering to journalistic ethics in creative writing tasks, and LLMs with fewer parameters but more training tokens are performing better than those larger ones with fewer training tokens on our benchmark dataset."
Enhancing Noise Robustness of Retrieval-Augmented Language Models with Adaptive Adversarial Training,2405.20978v1,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2405.20978v1.pdf,"An illustrative example of the RAG process applied to question answering. The model predicts the correct answer with accurate retrieved text. However, it fails to produce the right answer when the retrieved text contains misleading or inaccurate information.","Large Language Models (LLMs) exhibit substantial capabilities yet encounter challenges, including hallucination, outdated knowledge, and untraceable reasoning processes. Retrieval-augmented generation (RAG) has emerged as a promising solution, integrating knowledge from external databases to mitigate these challenges. However, inappropriate retrieved passages can potentially hinder the LLMs 'capacity to generate comprehensive and high-quality responses. Prior RAG studies on the robustness of retrieval noises often confine themselves to a limited set of noise types, deviating from real-world retrieval environments and limiting practical applicability. In this study, we initially investigate retrieval noises and categorize them into three distinct types, reflecting real-world environments. We analyze the impact of these various retrieval noises on the robustness of LLMs. Subsequently, we propose a novel RAG approach known as Retrieval-augmented Adaptive Adversarial Training (RAAT). RAAT leverages adaptive adversarial training to dynamically adjust the model' s training process in response to retrieval noises. Concurrently, it employs multi-task learning to ensure the model's capacity to internally recognize noisy contexts. Extensive experiments demonstrate that the LLaMA-2 7B model trained using RAAT exhibits significant improvements in F1 and EM scores under diverse noise conditions. For reproducibility, we release our code and data at: <https: //github. com/calubkk/RAAT>.","Large language models (LLMs) have garnered substantial attention in both academic and industrial research within the domain of artificial intelligence due to their remarkable capabilities. Despite their immense power, LLMs face challenges such as hallucinations and outdated knowledge. Moreover, a lack of domain knowledge may hinder their performance on domain-specific tasks. To mitigate these challenges, recent studies improve LLMs by retrieving passages from external databases and pretending them in context, constituting a framework known as retrieval-augmented language models (RALMs). 
However, RALMs also present significant limitations. Previous studies have empirically demonstrated that retrieved noisy passages are problematic for LLMs, resulting in performance degradation. We term this issue as the noise robustness problem of RALMs. As illustrated in Figure, the model can provide correct answers when the retrieving context is accurate and related to the query. However, when the retrieved context contains misleading or inaccurate information, the model may yield incorrect answers. As the retriever inherently cannot achieve complete accuracy, the presence of noise in the retrieved context is inevitable. Therefore, designing robust algorithms against retrieved noises is of great practical importance. 
Recently, several studies have attempted to enhance the noise robustness of RALMs through noisy training, which involves incorporating retrieved noisy contexts into fine-tuning data. While noisy training exhibits promise, its effectiveness heavily relies on the composition of the training dataset. Incorrectly introducing noises to the training data can lead to model overfitting, adversely affecting generalization. In practice, meticulous adjustment of the type and intensity of noises is essential to ensure the model 's proficiency across various tasks and datasets. This demands significant experimentation and tuning, adding complexity to the development process. Moreover, the lack of clear classification for retrieval noises in current studies stands in contrast to the diverse range of noises encountered in real retrieval environments. 
This paper systematically explores three types of retrieval noises: (i) contexts that are superficially related to the query but lack the correct answer (Relevant retrieval noise), (ii) contexts that are irrelevant to the query (Irrelevant retrieval noise), and (iii) contexts that are topically related to the query but contain incorrect information (Counterfactual retrieval noise). Our empirical study indicates that LLMs exhibit varying robustness to these three types of noise. Compared to entirely irrelevant texts, texts that are superficially related to the query or those containing counterfactual details often lead to more misinformation. 
In response to diverse types of noises, we propose a novel approach named Retrieval-augmented Adaptive Adversarial Training (RAAT), which employs adaptive adversarial training to dynamically regulate the model' s training process in response to retrieved noisy texts. Concretely, RAAT generates adversarial samples (noises) by considering the model's sensitivity to different types of noises during training, which aligns with the min-max paradigm of adversarial training. Moreover, RAAT utilizes multi-task learning to encourage the LLMs to generate tokens that are aware of noises, thereby enabling the model to internally recognize retrieved noisy contexts and improve the overall generation performance. 
The main contributions of this paper can be summarized as follows: 
 
 * We systematically explore three types of retrieval noises and investigate the sensitivity of LLMs to these diverse types of noises. 
 * We propose a novel adaptive adversarial training method (called RAAT) to enhance the robustness of RALMs against various retrieval noises. RAAT dynamically adjusts the training process of the model in diverse noise environments. In addition, it integrates multi-task learning to encourage the model to improve its ability to discern different types of noises. 
 * We set up a benchmark (named RAG-Bench) for assessing the noise robustness problem of RALMs based on three open-domain question-answering datasets. Experimental results demonstrate that our RAAT method enhances robustness across diverse retrieval noise environments."
Meta-Task Prompting Elicits Embeddings from Large Language Models,2402.18458v2,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2402.18458v2.png,"The highest decoding probabilities are largely allocated to stop words that carry little useful information when conducting a meaning compression prompting, even if employing a constraint of ""in one word"" following. Although the general semantic, movie, is contained, other aspects of this sentence are missing, like sentiments.","We introduce a new unsupervised text embedding method, Meta-Task Prompting with Explicit One-Word Limitation (MetaEOL), for generating high-quality sentence embeddings from Large Language Models (LLMs) without the need for model fine-tuning. Leveraging meta-task prompting, MetaEOLguides LLMs to produce embeddings through a series of carefully designed prompts that address multiple representational aspects. Our comprehensive experiments demonstrate that embeddings averaged from various meta-tasks are versatile embeddings that yield competitive performance on Semantic Textual Similarity (STS) benchmarks and excel in downstream tasks, surpassing contrastive-trained models. Our findings suggest a new scaling law, offering a versatile and resource-efficient approach for embedding generation across diverse scenarios.","The advent of Large Language Models (LLMs) such as GPT-3 and LLaMA has marked a significant milestone in the field of natural language processing (NLP), introducing promising unsupervised methods for various NLP tasks by leveraging task-related instructions or prompts. These tasks also include the generation of sentence embeddings, which aims to produce sentence representations that can be applied across a wide range of scenarios. They have been applied to intrinsic tasks like Semantic Textual Similarity (STS), to downstream tasks including information retrieval, and to sentiment classification and beyond. By employing specific prompts, researchers have begun to explore the potential of extracting meaningful sentence embeddings directly from the hidden states of LLMs without the need for explicit training. These prompt-based approaches generate embeddings without the need for any fine-tuning or in-context learning, which is a substantial improvement over approaches that require extensive fine-tuning to achieve high performance. 
Initial efforts in this domain, as highlighted by works like, have focused on unsupervised techniques that extract sentence representations directly from LLMs. These methods typically involve using fill-in-the-blanks prompts, such as This sentence: "" [TEXT] "" means in one word: "", to embed a sentence into a single token representation by using the output hidden state of the last token as the sentence 's embedding. While they perform well, these approaches also reveal the inherent challenges of this task: embeddings may be overly simplistic or misaligned with the intended semantic nuances of the sentences. 
In a pilot experiment illustrated in Figure, we demonstrate that a previous prompt-based method can struggle to capture a sentence' s meaning, especially when the usage of the sentence is associated with multiple aspects. When probing the probability distribution for the next token during decoding, which reflects the embedding quality of the last token, the highest probabilities are mostly distributed to frequent stop words. Although the general movie topic appears, other meaningful aspects like sentiments are missing. 
A straightforward solution to mitigate this issue is to provide LLMs with task-specific instructions. This approach involves instructing the model with prompts explicitly designed for a particular task, thereby tailoring the embeddings to better suit the specific requirements of that task. However, considering the wide range of distinct tasks that an embedding may be used for, this would be impractical. Furthermore, while task-specific embeddings are effective for their corresponding tasks, they may fail to generalize well across different tasks. 
Inspired by the principles of the usage-based theory of language acquisition, which asserts that the essence of meaning is rooted in the practical utilization of language, our approach aims to generate broad embeddings through the use of meta-task prompting, inspired by meta-task prompted training and hyper-prompt techniques. By defining a suite of meta-tasks, each tailored to a distinct application context, MetaEOLprompts LLMs to consider multiple representational tokens from a variety of perspectives. This multifaceted approach enables the extraction of more diverse and nuanced contextualized token embeddings that collectively form a comprehensive sentence embedding. 
Extensive experiments empirically show that: (i) Simply averaging embeddings from different meta-tasks without any training leads to general embeddings that are competitive to contrastive-trained models on STS tasks and can achieve the best average result on several downstream tasks. (ii) Incrementally integrating more meta-tasks (ranging from one to four) yields consistent improvements across STS tasks, showcasing high generalities, and highlighting the significant impact of meta-task integration on overall performance. (iii) The final layer is not always the most effective for STS tasks and with a simple proportional layer selection strategy, we achieve the best results with a 70B model, which points to a potential scaling law."
A Sentiment Consolidation Framework for Meta-Review Generation,2402.18005v2,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2402.18005v2.pdf,"The three-layer framework of the underlying information consolidation logic in meta-reviewing (P: Positive, P^+: Strongly positive, N: Negative, N^+: Strongly negative).","Modern natural language generation systems with Large Language Models (LLMs) exhibit the capability to generate a plausible summary of multiple documents; however, it is uncertain if they truly possess the capability of information consolidation to generate summaries, especially on documents with opinionated information. We focus on meta-review generation, a form of sentiment summarisation for the scientific domain. To make scientific sentiment summarization more grounded, we hypothesize that human meta-reviewers follow a three-layer framework of sentiment consolidation to write meta-reviews. Based on the framework, we propose novel prompting methods for LLMs to generate meta-reviews and evaluation metrics to assess the quality of generated meta-reviews. Our framework is validated empirically as we find that prompting LLMs based on the framework — compared with prompting them with simple instructions — generates better meta-reviews.","Notable strides have been made in abstractive text summarization with the advancement of Large Language Models (LLMs) over recent years. With even a simple instruction such as ""tl; dr"" or ""please write a summary"", these models can generate plausible summaries which are found more preferred over those written by humans. However, it is uncertain if these models truly possess the ability of information consolidation, especially when summarizing documents that are composed of opinionated information. The models may take shortcuts to generate texts instead of correctly understanding and aggregating information from the source documents and they may generate abstractive summaries with incorrect overall sentiment. 
Automated sentiment summarization holds significant importance and there have been sentiment summarization datasets; however, most of them are in the product review domain. These datasets are less interesting for investigating information consolidation as (1) the summaries are extractive, composed of a simple combination of extracted snippets, and (2) the summary of product reviews is about extracting the majority sentiment (which is a simple consolidation function). To address this, in this paper, we propose the task of scientific sentiment summarization, taking the meta-reviews in scientific peer review as summaries. The investigation of meta-review generation presents an exciting opportunity for exploring the intricate process of multi-document information consolidation that involves complex judgement. This is because (1) meta-reviewers are supposed to understand not only all the reviews from different reviewers but also the multi-turn discussions between the reviewers and the author and write their comments to support the acceptance decision of the manuscript, (2) the logic of arguments (from reviewers and authors) has to be taken into account to arrive at the final sentiment in the meta-reviews and it is not a matter of majority voting and (3) meta-reviews have to recognize and resolve conflicts and consensus among reviewers. 
In this paper, we hypothesize that human meta-reviewers follow a three-layer sentiment consolidation framework as shown in Figure to write meta-reviews based on reviews and multi-turn discussions in the peer review process. Human and automatic annotation is then conducted to extract sentiments and expressions on various review facets (e.g., novelty and soundness) from corresponding source documents (i.e., reviews and discussions) and these judgements play a critical role in generating the meta-reviews. We also propose two evaluation metrics which focus on assessing sentiments in generated meta-reviews, and experiments empirically validate our proposed three-layer framework when they are integrated as prompts for LLMs to generate meta-reviews. 
Contributions of our paper: 
∙ 
 * We hypothesize that human meta-reviewers follow a three-layer sentiment consolidation framework when writing meta-reviews; 
 * We collect human annotations on meta-reviews and corresponding source documents based on the consolidation framework; 
 * We propose two automatic metrics (reference-free and reference-based) to evaluate the sentiment in the generated meta-reviews. 
 * Experiments validate the empirical effectiveness of the framework when we incorporate it as prompts for LLMs to generate meta-reviews."
MuggleMath: Assessing the Impact of Query and Response Augmentation on Math Reasoning,2310.05506v3,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2310.05506v3.png,Comparison of test set accuracy on GSM8K for models of varying scales after fine-tuning on AugGSM8K subsets with different query augmentation strategies.,"In math reasoning with large language models (LLMs), fine-tuning data augmentation by query evolution and diverse reasoning paths is empirically verified effective, profoundly narrowing the gap between open-sourced LLMs and cutting-edge proprietary LLMs. In this paper, we conduct an investigation for such data augmentation in math reasoning and are intended to answer: (1) What strategies of data augmentation are more effective; (2) What is the scaling relationship between the amount of augmented data and model performance; and (3) Can data augmentation incentivize generalization to out-of-domain mathematical reasoning tasks? To this end, we create two new dataset AugGSM8K and AugMATH, by complicating and diversifying the queries and sampling multiple reasoning paths from GSM8K and MATH. We obtained a series of LLMs called MuggleMath by fine-tuning LLaMA models on AugGSM8K and AugMATH. MuggleMath substantially achieves new state-of-the-art on GSM8K and MATH. A log-linear relationship and a segmented log-linear are presented between MuggleMath's performance and the amount of augmented data on GSM8K and MATH, respectively. We also find that it is weak in out-of-domain math reasoning generalization from AugGSM8K to MATH and from AugMATH to GSM8K, which suggests that augmenting queries that cover a broader range of subjects is more beneficial for generalization. We release our codes and augmented data in <https: //github. com/OFA-Sys/gsm8k-ScRel>.","The emergence of large language models (LLMs) has profoundly revolutionized the field of natural language processing, exhibiting versatile performance in various tasks like code generation, instruction following, long context question answering, and math reasoning. Math reasoning as a representative reasoning task is widely studied to access the reasoning abilities in LLMs. Proprietary LLMs, such as GPT-3.5, and GPT4 have shown exceptional mathematical reasoning abilities, while there remains a substantial gap between open-source LLMs, such as GPT-J and LLaMA) and the cutting-edge proprietary models. 
To enable better mathematical reasoning abilities in open-sourced LLMs, they generally undergo a fine-tuning stage on supervised reasoning datasets. A series of efforts are committed to enhancing the mathematical reasoning capabilities of open-source LLMs, where a mainstream approach involves first augmenting new mathematical problems and answers, followed by supervised fine-tuning on the augmented dataset. This type of approach has achieved good results, and in this paper, we would like to explore what are the key factors affecting the effectiveness of data augmentation for mathematical reasoning tasks and the scaling relationship between the amount of data augmentation and model performance. Specifically, with the help of proprietary models (GPT-3.5 and GPT-4), we applied five types of mathematical problem augmentation methods based on human experience in creating variations of mathematical problems similar to. We further generated multiple reasoning paths for each augmented problem since distinct reasoning paths can also enhance chain-of-thought reasoning. We obtained two new datasets called AugGSM8K and AugMATH after data augmentation on two widely used mathematical reasoning datasets GSM8K and MATH. By supervised fine-tuning on the open-source LLaMA and LLaMA-2 LLMs on AugGSM8K and AugMATH, we obtained a series of models dubbed MuggleMath. We find that with sufficient amounts of data, MuggleMath achieves a new state-of-the-art on GSM8K and MATH. In addition to this, we find a log-linear relationship between the performance of MuggleMath and the amount of data augmentation over a range of data volumes on GSM8K and a segmented log-linear relationship on MATH. 
Although MuggleMath achieves strong performance on the GSM8K and MATH test set, the rationales for performance improvement by data augmentation remain unclear. We are therefore interested in the specific reason behind the performance improvement and whether it brings enhancement in LLMs' mathematical reasoning capabilities generally. 
To validate the generalization of MuggleMath, we conduct multi-task learning and analyze the transferability with AugGSM8K and AugMATH. We found that LLMs trained with supervised learning after data augmentation on GSM8K only bring marginal improvements to performance on MATH and the similar conclusion is fit for AugMATH and GSM8K. By visualizing the data distribution in the embedding space of LLaMA-2-7B, we observe that the embedding distribution of problems in AugGSM8K is very close to that of GSM8K, but significantly different from the problem distribution in the MATH dataset. The reason behind can be attributed to the fact that GSM8K and MATH have different reasoning difficulty, response style and require different mathematical knowledge. 
The main contributions of our work can be summarized as follows: 
 
 * By augmenting GSM8K and MATH with various queries and multiple reasoning paths, we curate GSM8K and MATH to two new datasets named AugGSM8K and AugMATH. 
 * We utilize AugGSM8K and AugMATH for fine-tuning the LLaMA and LLaMA-2 models to obtain MuggleMath, which greatly improves the in-domain performance of the open-sourced LLMs on GSM8K and MATH, achieving new state-of-the-art performances. 
 * We find a log-linear relationship between the accuracy of the model on the test set and the amount of data augmentation within a certain range while the coefficient is similar to augmenting new human-written samples on GSM8K. When it comes to MATH, a a segmented log-linear relationship is found. 
 * We demonstrate that the performance gains from data augmentation on GSM8K and MATH are difficult to generalize to each other, which indicates a need of diverse original queries in augmenting math data."
BinaryAlign: Word Alignment as Binary Sequence Labeling,2407.12881v1,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2407.12881v1.png,"Example of alignment of an approximate translation, as often encountered in real-world applications. Links in red indicate situations where one word is aligned with several contiguous or non-contiguous words. The green line represent a situation where a word is untranslated which happens in many language pairs.","Real world deployments of word alignment are almost certain to cover both high and low resource languages. However, the state-of-the-art for this task recommends a different model class depending on the availability of gold alignment training data for a particular language pair. We propose BinaryAlign, a novel word alignment technique based on binary sequence labeling that outperforms existing approaches in both scenarios, offering a unifying approach to the task. Additionally, we vary the specific choice of multilingual foundation model, perform stratified error analysis over alignment error type, and explore the performance of BinaryAlign on non-English language pairs. We make our source code publicly available.","Word alignment refers to the task of uncovering word correspondences between translated text pairs. The automatic prediction of word alignments dates back to the earliest work in machine translation with the IBM models where they were used as hidden variables that permit the use of direct token to token translation probabilities. While state of the art machine translation techniques have largely abandoned the use of word alignment as an explicit task other use cases for alignments have emerged including lexical constraint incorporation, analysing and evaluating translation models, and cross-lingual language pre-training. 
In many real-world applications word alignment must be performed across several languages, often including languages with manually annotated word alignment data and others lacking such annotations. We refer to those languages as high and low-resource languages respectively. While word alignment for high-resource languages can be learned in a few-shot or fully supervised setting depending on the amount of data, for low-resource languages zero-shot learning strategies must be employed due to data scarcity. 
State-of-the-art supervised techniques formalize the task of word alignment as a collection of SQuAD-style span prediction problems while in zero-shot settings the best performing methods induce word alignment from the contextualized word embeddings of mulitingual pre-trained language models (mPLMs). From a practical perspective, this discrepancy in the preferred method adds complexity to the deployment of word alignment models in real-world applications where both high and low-resource languages must be supported. 
We observe a deeper issue that both span prediction and contextualized word embeddings are sub-optimal as each induces a bias in word alignment models that limits their accuracy. Span prediction methods cannot robustly deal with discontinuous word alignments without relying on complex post-processing and hyper-parameter tuning. Contextualized word embeddings method cannot deal effectively with untranslated words and one-to-multiple alignments because they rely on a softmax function that normalizes predictions at a sentence-level while in word alignment; one token being aligned to token T does not mean that another token is less likely to be aligned to T. This poses word alignment as a single-label classification problem, while in reality it is better viewed as a series of binary classifications applied to each possible pair of words. Figure shows some cases of one-to-multiple alignments, non-contiguous spans and untranslated words. 
In this paper, we present BinaryAlign, a novel word alignment solution that outperforms the state-of-the-art in zero-shot, few-shot and fully-supervised settings. In particular, we reformulate word alignment as a set of binary classification tasks in which an individual alignment prediction is made for each possible pair of words. This reformulation of the task outperforms all previous approaches over five different language pairs with varying levels of supervision."
SyllabusQA: A Course Logistics Question Answering Dataset,2403.14666v2,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2403.14666v2.pdf,"Domain diversity in covering 36 majors. For visual clarity, we show representative majors.","Automated teaching assistants and chatbots have significant potential to reduce the workload of human instructors, especially for logistics-related question answering, which is important to students yet repetitive for instructors. However, due to privacy concerns, there is a lack of publicly available datasets. We introduce, an open-source dataset with 63 real course syllabi covering 36 majors, containing 5,078 open-ended course logistics-related question-answer pairs that are diverse in both question types and answer formats. Since many logistics-related questions contain critical information like the date of an exam, it is important to evaluate the factuality of answers. We benchmark several strong baselines on this task, from large language model prompting to retrieval-augmented generation. We introduce Fact-QA, an LLM-based (GPT-4) evaluation metric to evaluate the factuality of predicted answers. We find that despite performing close to humans on traditional metrics of textual similarity, there remains a significant gap between automated approaches and humans in terms of fact precision.","In educational applications, artificial intelligence (AI) approaches have shown significant promise in improving learning outcomes, by automatically providing feedback to students or engaging in tutoring dialogues with them. The key idea is to use AI to create an on-demand virtual teaching assistant to interact with many students simultaneously; see, e.g., Khamigo from Khan Academy. These approaches can scale up the effort of expert human teachers and tutors, and relieve them from doing repetitive tasks so that they can focus on providing personalized feedback or designing new learning content. In higher education, one promising avenue for AI-powered teaching assistants to reduce human effort is course logistics-related question answering (QA): answering student questions on logistics whose answers can be directly found or inferred from the syllabus. 
There exist many approaches for automated QA in online courses (both logistics-related and content-related), using tools from rule-based AI systems and expert systems with knowledge bases to end-to-end text generation. Recently, large language model (LLM) -based approaches have shown great promise to improve the coverage and answer quality over traditional QA approaches. See Section in the Supplementary Material for a detailed discussion on related work. Unfortunately, these approaches are mostly developed and evaluated on proprietary data due to student privacy concerns, which prevents more researchers from contributing to the development of automated QA systems for education. 
For evaluation, especially in logistics-related QA that often contains critical information, the factuality of predicted answers is more important than measuring surface textual features. Moreover, text similarity metrics may not be suitable for some open-ended natural language generation tasks. As an example, the answer ""The final exam will be on Dec 15"", has high surface-level textual similarity with the reference answer, ""The final exam is on Dec 14"", but contains a critical factual error that may lead to significant negative consequences to students. Meanwhile, human instructors and teaching assistants often answer student questions in a concise way, without giving any unnecessary information. Therefore, it is important for LLM-based approaches to generate answers that are both concise and precise. 
 
Contributions In this paper, we introduce the dataset for course logistics-related QA. We publicly release this dataset and hope that it can be a benchmark for future work on developing and evaluating automated QA approaches for teaching assistance. Our contributions are: 
First, we collect a diverse set of 63 real course syllabi covering 36 majors across 12 universities, and employ crowd annotators to write 5,078 logistics-related QA pairs with the goal of simulating what students would ask in a real-world course. 
Second, we detail the diverse composition of syllabi and QA pairs in, in terms of syllabi domain, question types, answer sources, and different language styles. We lay out metrics to evaluate different aspects of open-ended automated QA approaches, in terms of both surface textual similarity and more importantly, the factuality of predicted answers grounded in the syllabus. 
Third, we conduct extensive experiments to benchmark the QA performance of several strong baselines on. Overall, LLM-based approaches perform similar to humans on surface-level textual similarity metrics but worse on factuality metrics. We found that fine-tuning on real QA pairs from performs much better than LLM prompting approaches and that retrieval-augmented generation is especially important. 
To the best of our knowledge, is the first publicly available real-world course logistics-related QA dataset. assesses automated QA models on various natural language understanding aspects including handling challenging question types, from reasoning-based ones to adversarial ones, understanding of long input documents sourced from diverse course majors, processing complex input formatting including tables and schedules, and answering open-ended questions in a similar way as human instructors and TAs. Table highlights the difference between and existing datasets; see also Section for a detailed discussion."
Bridging the Preference Gap between Retrievers and LLMs,2401.06954v2,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2401.06954v2.png,"We observe a preference gap when alternating the ranking and selection of information in RAG. Experiments are conducted with retrieving passages using GTR and using top K of them as additional context for a frozen Palm2-S LLM. Different colors indicate different datasets (detailed in Sec. ) and the Y-axis shows the relative percentage. Alternating the selection (Top-1) of information significantly affects (either positively or negatively) the LLM's performance, while randomizing the ranking of multiple selected items (Top-5) does not have a comparable impact (the metrics are detailed in Sec. ). Note the impact on NQ is even too small to be visible.","Large Language Models (LLMs) have demonstrated superior results across a wide range of tasks, and Retrieval-augmented Generation (RAG) is an effective way to enhance the performance by locating relevant information and placing it into the context window of the LLM. However, the relationship between retrievers and LLMs in a RAG is still under-investigated. Most existing work treats the retriever and the LLM as independent components and leaves a gap between retrieving human- ""friendly"" information and assembling a LLM- ""friendly"" context. In this work, we examine a novel bridge mechanism. We validate the ranking and selection assumptions of retrievers in the context of RAG and propose a framework that chains together supervised and reinforcement learning to train a bridge model that optimizes the connection between the retriever and the LLM. Empirical results demonstrate the effectiveness of our method in both question-answering and personalized generation tasks.","Large language models (LLMs) such as GPT-4 and PaLM 2, have demonstrated impressive performance on a wide variety of tasks. Retrieval-augmented generation (RAG), which retrieves knowledge items from an external data source and puts it into the context window of LLMs, has produced significantly enhanced results in many NLP tasks. 
However, most works on RAG study retrievers and LLMs separately. On one hand, most retrievers are designed to be human-friendly, usually based on the general belief in classic information retrieval literature that ranking is paramount, as humans typically read from top to bottom. On the other hand, LLMs exhibit preferences different from humans and yield accurate results only when the information in the prompt aligns with these preferences. This discrepancy leads to sub-optimal design in current RAG systems, a phenomenon we term preference gap. This gap manifests in various aspects. For example, the general belief in ranking may not align with LLM 's preferences due to the self-attention mechanism of Transformers, which can focus on any token regardless of its position. Another aspect is selection; while humans can easily disregard irrelevant information in a context, it has been shown that LLMs are highly sensitive to irrelevant content. There likely exist more aspects that further diverge the LLM' s preference from that of humans, e.g., repetition. Repeated information is generally considered detrimental for retrieval systems, but it may be useful to LLMs for weighting the relevance of context items. 
We empirically investigate this preference gap, focusing specifically on ranking and selection. As shown in Fig. , when we randomize the ordering of top-5 retrieved items (in our case, passages), the performance of RAG only varies by around 1%. However, the variation exceeds 5%when the LLM is only presented with the top-1 passage under each order (therefore it encounters different selections of information). This indicates the general belief in ranking does not always apply to LLMs, and that the selection of information could be more crucial. This finding confirms the existence of the preference gap between retrievers and LLMs, and it is critical to bridge this preference gap to enhance the performance of RAG. To the best of our knowledge, this is a novel insight that may guide future designs of RAG systems. 
Existing work has tried to finetune the LLMs to align with the retriever or adjust the retriever to align with the LLM. However, finetuning LLMs, especially at the scale of GPT-4 or Palm 2, is often expensive. Similarly, it is difficult to adjust production-level retrievers such as Google or Bing. Even when the retriever is adjustable, existing efforts often focus on re-ranking the retrieved results and fail to address other aspects of preference such as selection or repetition. Instead, we propose a novel and practical framework called BGM (Bridging the Gap between retrievers and LLMs), which keeps the retriever and LLM fixed and trains a bridge model in between. The bridge model aims to transform the retrieved information into a format that LLMs prefers and can effectively work with. 
Without loss of generality, we structure the bridge model as a sequence-to-sequence (seq2seq) model, which allows dynamically selecting items, re-ranking them, and potentially broader operations like repeating some of them in the retrieval-augmented prompt. Training such a bridge model is challenging as there are usually no ground truth labels on ideal item sequences for retrieval augmentation. Existing work has tried to derive supervisory signals for ranking from RAG's downstream task performance, such as perplexity distillation. Nevertheless, these methods only provide pointwise supervisory signals for each item independently. Directly applying the same idea to obtain sequential supervision is infeasible, since it would require feeding all possible item sequences into the LLM to obtain perplexity. We developed a greedy search approach to solve this problem (Sec. ). Moreover, we find sequential supervision can be too sparse to effectively train such a seq2seq model (Table). To address this issue, we employed reinforcement learning (RL) on the SL trained bridge model, regarding the downstream task performance as the reward and the bridge model as a policy model. Chaining SL and RL provides increased supervision from the downstream task. It also offers the flexibility to explore more advanced strategies, such as repetition, in forming the optimal passage sequence. 
Our experiments reveal that BGM can enhance the performance of various downstream tasks, such as Question Answering (QA) and personalized generation, across a spectrum of datasets, from public QA and amazon reviews to private email conversations. Notably, the modified passages retrieved by BGM surpass the performance of strong retrievers and baseline reranking models. This underscores the significance and promise of the ""bridge"" approach in the realm of RAG. In summary, our contributions can be summarized as follows: 
 
 
 * We empirically establish the existence of the preference gap between retrievers and LLMs, and introduce BGM to address this gap. 
 
 * We propose a seq2seq bridge model to jointly accomplish reranking and selection, adapting the retrieved information to be LLM-friendly. We employ a SL and RL training scheme to optimize this adaptation process. 
 
 * We evaluate BGM with diverse tasks, including QA and text generation, with publicly available and personalized datasets. The evaluation underscores the effectiveness of BGM in bridging the preference gap and improving RAG performance in downstream tasks."
Large Language Models Can Learn Temporal Reasoning,2401.06853v6,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2401.06853v6.pdf,Our framework (TG-LLM) performs temporal reasoning in two steps: 1) Text-to-Temporal Graph translation: generate (relevant) temporal graph given the context and keyword (extracted from questions); 2) Temporal Graph Reasoning: perform Chain-of-Thought reasoning over the temporal graph.,"While large language models (LLMs) have demonstrated remarkable reasoning capabilities, they are not without their flaws and inaccuracies. Recent studies have introduced various methods to mitigate these limitations. Temporal reasoning (TR), in particular, presents a significant challenge for LLMs due to its reliance on diverse temporal concepts and intricate temporal logic. In this paper, we propose TG-LLM, a novel framework towards language-based TR. Instead of reasoning over the original context, we adopt a latent representation, temporal graph (TG) that enhances the learning of TR. A synthetic dataset (TGQA), which is fully controllable and requires minimal supervision, is constructed for fine-tuning LLMs on this text-to-TG translation task. We confirmed in experiments that the capability of TG translation learned on our dataset can be transferred to other TR tasks and benchmarks. On top of that, we teach LLM to perform deliberate reasoning over the TGs via Chain-of-Thought (CoT) bootstrapping and graph data augmentation. We observed that those strategies, which maintain a balance between usefulness and diversity, bring more reliable CoTs and final results than the vanilla CoT distillation.","As one of the fundamental abilities, temporal reasoning (TR) plays an important role in human perception. It is not just about understanding basic concepts such as ordering or duration; it extends to more intricate aspects, e.g., task planning or causal relation discovery. Recently, large language models (LLMs) have emerged with some reasoning capabilities. However, there is observation that they still can not perform TR sufficiently well, preventing their applications from solving complex real-world problems. In particular, TR requires a combination of various skills including mathematical and logical reasoning as well as commonsense knowledge. 
Recent works mainly adopt general approaches to investigate and improve the TR capability of LLMs. For example, benchmark the leading LLMs on different TR tasks with standard Input/Output prompt, few-shot in-context learning (ICL) and Chain-of-Thought (CoT) reasoning. Similarly, designs several specific types of prompts as prompt tuning. introduce pre-defined Python programs/rule-based templates to perform supervised fine-tuning (SFT). In addition, adopt some extra strategies, which include specific pre-training, instruction tuning and reinforcement learning. 
Despite the effectiveness of such methods, they either ignore or not explicitly involve the intrinsic nature of TR. Humans perform complex TR on a timeline of events which are aligned with the entities and relations. These temporal concepts (e.g., ordering, duration, frequency, typical time) are then rigorously defined based on the timeline information. In other words, the aligned timeline (more generally, the temporal graph, TG) serves as a latent representation to help humans understand the patterns in TR. However, due to the lack of ground truth, the high-quality TG translation is a challenging task for most TR benchmarks. To solve this problem, we propose a synthetic dataset (TGQA), which is fully controllable and requires minimal supervision. We demonstrate the capability of TG translation learned on our dataset can be transferred to other TR tasks and benchmarks. 
Given a reliable TG, the key challenges of teaching TR to LLMs include: (1) How can one introduce the necessary arithmetic and commonsense knowledge involved in TR? Prior work shows that explicitly introducing knowledge into context enhances the performance of LLMs. In this paper, we first identify all the valid time expressions, and then generate related knowledge (e.g., temporal relation and time gap between the timestamps, and the relative order of the gaps). (2) How can one teach LLM to perform deliberate reasoning? Generally, there exist two roadmaps: (i) translating natural language into logical statements, and using external symbolic engine for reasoning; (ii) using LLMs directly as the reasoning engine. For (i), the difficulty lies in accurate translation and the limited expressive power of formal logic. For (ii), there is no guarantee for the correctness of generated intermediate steps especially with insufficient training data. In this paper, we adopt (ii) with the proposed bootstrapping method to generate reliable intermediate steps for supervised fine-tuning. We further improve the model performance with graph data augmentation, which mitigates the data deficiency in TR tasks. 
To be specific, our contributions are summarized as follows: 
 
 * We propose a new paradigm, TG-LLM, for language-based TR. In this framework, we first translate the context into a latent representation (temporal graph), and then perform reasoning on it. Extensive experiments prove that our novel approach results in superior performance compared to the baselines. 
 * We design two approaches including Chain-of-Thought bootstrapping and graph data augmentation to teach LLM to generate consistent and faithful CoTs, which brings better performance than the vanilla CoT distillation. 
 * We present a pipeline to create a synthetic dataset (TGQA) for question answering that requires TR. It is fully controllable and requires minimal supervision for text-temporal graph alignment. We show in experiments that fine-tuning on our dataset benefits LLM on other TR tasks and benchmarks."
HyperMoE: Towards Better Mixture of Experts via Transferring Among Experts,2402.12656v4,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2402.12656v4.pdf,"A trade-off in MoE: (a) A small number of selectable experts can maintain sparsity but limits the availability of expert knowledge. (b) Increasing the number of selectable experts can improve performance but decrease sparsity. (c) Transferring partial knowledge from the unselected experts E_2,3 to the selected experts E_1 can improve the availability of expert knowledge while maintaining sparsity.","The Mixture of Experts (MoE) for language models has been proven effective in augmenting the capacity of models by dynamically routing each input token to a specific subset of experts for processing. Despite the success, most existing methods face a challenge for balance between sparsity and the availability of expert knowledge: enhancing performance through increased use of expert knowledge often results in diminishing sparsity during expert selection. To mitigate this contradiction, we propose HyperMoE, a novel MoE framework built upon Hypernetworks. This framework integrates the computational processes of MoE with the concept of knowledge transferring in multi-task learning. Specific modules generated based on the information of unselected experts serve as supplementary information, which allows the knowledge of experts not selected to be used while maintaining selection sparsity. Our comprehensive empirical evaluations across multiple datasets and backbones establish that HyperMoE significantly outperforms existing MoE methods under identical conditions concerning the number of experts. Our code is publicly available at <https: //github. com/Bumble666/Hyper_MoE> ","The accelerated advancement of large language models has culminated in their widespread application across various domains, including healthcare, education, and social interactions. The remarkable capabilities of these models are attributed to the enhancements in their scale. Nevertheless, the scaling of dense models is often hampered by significant computational demands, posing a challenge to developing the Natural Language Processing (NLP) community. In response, sparse activation models have emerged as a solution, activating only a subset of parameters for different inputs, thus mitigating computational costs. One of the most representative methods is the Mixture of Experts (MoE, ), which routers different inputs to specific groups of experts, thereby enlarging the model 's capacity without increasing computational burdens. 
The key to effectively reducing computational costs lies in the sparsity of expert selection, with the number of experts selected for each token being kept at a lower level. In practical applications or experiments, existing works usually select only one or two experts per input. However, increasing the number of selected experts per token can enhance the availability of expert knowledge and improve the performance of downstream tasks. This scenario positions MoE model in a predicament akin to a zero-sum game: a choice between increasing the number of available experts to improve performance or preserving a lower level of available experts to ensure sparsity, as depicted in Figure. 
To mitigate this contradiction, one solution would be to use the knowledge of other experts to assist the sparsely selected experts. This is similar to multi-task learning, which transfers knowledge among related tasks. Some works suggest using hypernetworks to generate task-specific knowledge to enhance positive transfer between tasks. Inspired by this, we aim to increase the availability of expert knowledge by transferring the knowledge of unselected experts while sparsely selecting experts. 
In this paper, we propose HyperMoE, a novel MoE framework built upon hypernetworks, which captures the information from every expert by leveraging expert-shared hypernetwork while achieving positive expert transfer by generating conditioned modules individually. We refer to the information as cross-expert information. Specifically, a HyperMoE consists of HyperExperts, which are generated based on the information of unselected experts and serve as supplementary information for selected experts while maintaining sparsity. 
We further improve upon this by introducing the concept of cross-layer Hypernetworks: A hypernetwork is shared among all transformer layers, which enables information flow among MoEs in different layers. This brings additional efficiency in terms of parameters and computational costs: Despite the additional computation, our method only experienced a decrease of approximately 15%in training speed and 10%in inference speed compared to the standard MoE. 
We evaluate HyperMoE on 20 representative NLP datasets across diverse tasks: sequence classification, extractive question answering, summarization, and text generation. Extensive experimental results show that HyperMoE outperforms baselines, including Switch Transformer with MoE architecture. This demonstrates the effectiveness of our method in transferring knowledge to experts, which increases the utilization of expert knowledge while keeping the number of experts selected at a low level. 
To summarise, our core contributions are: 
 
 * We propose a novel HyperMoE architecture with HyperExpert for MoE framework, which resolves the inherent tension between maintaining sparse expert selection and ensuring sufficient expert availability within MoE. 
 * HyperMoE outperforms baselines based on Switch Transformer across a diverse set of NLP tasks, confirming our approach' s effectiveness. 
 * We show the relevance between selection embeddings, which are based on the context of unselected experts, and selected experts, indicating that the selection embeddings effectively encode the information of knowledge that the currently selected experts need."
Aligning Large Language Models with Human Preferences through Representation Engineering,2312.15997v3,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2312.15997v3.png,Illustration of different apporaches. (a) Reinforcement learning from human feedback (RLHF); (b) Direct preference optimization (DPO); (c) Hindsight instruction relabeling (HIR); (d) Representation alignment from human feedback (RAHF).,"Aligning large language models (LLMs) with human preferences is crucial for enhancing their utility in terms of helpfulness, truthfulness, safety, harmlessness, and interestingness. Existing methods for achieving this alignment often involve employing reinforcement learning from human feedback (RLHF) to fine-tune LLMs based on human labels assessing the relative quality of model responses. Nevertheless, RLHF is susceptible to instability during fine-tuning and presents challenges in implementation. Drawing inspiration from the emerging field of representation engineering (RepE), this study aims to identify relevant representations for high-level human preferences embedded in patterns of activity within an LLM and achieve precise control of model behavior by transforming its representations. This novel approach, denoted as Representation Alignment from Human Feedback (RAHF), proves to be effective, computationally efficient, and easy to implement. Extensive experiments demonstrate the efficacy of RAHF in not only capturing but also manipulating representations to align with a broad spectrum of human preferences or values, rather than being confined to a singular concept or function (e.g.,honesty or bias). RAHF's versatility in accommodating diverse human preferences shows its potential for advancing LLM performance. Code is available at <https: //github. com/LiuAmber/RAHF>.","While large language models (LLMs) learn broad-ranging world knowledge and a degree of reasoning proficiency, precise control over their behavior proves challenging due to the unsupervised nature of their pre-training. For each query, instruction-tuned LLMs exhibit the capacity to generate multiple responses that are both semantically and syntactically coherent by some sampling techniques. While such ability enables the models to provide diversity that is essential for chat agents, some responses may contain harmful, unethical, socially biased, and negative, even illegal content. 
Existing methods steer LLMs to align with human preferences often using reinforcement learning (RL), with reinforcement learning from human feedback (RLHF) emerging as the most successful one. However, the underlying learning algorithms exhibit a considerable degree of complexity, sensitivity to hyperparameters, instability during training, and necessitate additional training in the reward model and value network, leading to substantial computational costs. 
In addressing the aforementioned challenges posed by RL-based methods, several computationally lightweight alternatives have been proposed to simplify the human preference-matching process. Two prominent paradigms among these alternatives include contrastive learning and Hindsight instruction relabeling (HIR). Contrastive learning-based methods optimize a language model policy by increasing the relative probability of preferred responses over dispreferred ones, while HIR methods transform human feedback into instructions by relabeling the original ones, indicating the relative quality of provided responses. A common characteristic shared by these two paradigms is their capability to align language models with human preferences through reward-free fine-tuning. 
However, the reward-free fine-tuning is vulnerable to the presence of noisy data or incorrect labels in a training set comprising a collection of preference-annotated response pairs. Instances of dull sentences or very brief responses may appear repeatedly in such a training set, potentially introducing bias into the models. The exclusion of such instances from the training set renders it impossible for LLMs to glean insights into human preferences expressed in these instances. In contrast, RL-based methods adopt a different strategy, wherein a reward function is first extracted from a dataset of response rankings, and then this reward function can be applied to train an LLM, effectively mitigating the model 's direct exposure to noisy data or incorrect labels within the dataset. 
In this study, we aim to seek for a computationally lighter and reward-free algorithm that can effectively harness human preference expressed in datasets meanwhile safeguarding LLMs from the influence of noisy data. Inspired by the recent advance in representation engineering, we initially locate relevant representations and activity patterns associated with high-level human preferences within an LLM, and subsequently gain precise control over its behavior by manipulating its internal representations. In the neural architecture, network weights determine neural activity, neural activity determines the networks' output, and the networks 'output determines the networks' behavior. Instead of focusing on neurons and their connections, we see aligning LLMs with human feedback as an outcome of representational spaces, implemented by patterns of activity across populations of neurons. We first identify the differences in model activities between preferred and dispreferred stimuli, and then control its behavior by leveraging the identified differences in representations (see Figure). We introduce two methods for controlling representations and demonstrate the efficacy of these representation engineering (RepE) approaches in aligning LLMs with a broad spectrum of human preferences through a collection of response pairs. 
To validate the effectiveness of our approach in aligning with human preferences, we conducted extensive comparative experiments on the generated results. Our method outperformed RLHF and other RL-free approaches in human evaluations and automated metrics such as general abilities and GPT-4 evaluations. Notably, the underlying algorithms exhibit simplicity in implementation and straightforwardness in training."
ARAIDA: Analogical Reasoning-Augmented Interactive Data Annotation,2405.11912v2,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2405.11912v2.pdf,Comparison between manual annotation and interactive annotation.,"Human annotation is a time-consuming task that requires a significant amount of effort. To address this issue, interactive data annotation utilizes an annotation model to provide suggestions for humans to approve or correct. However, annotation models trained with limited labeled data are prone to generating incorrect suggestions, leading to extra human correction effort. To tackle this challenge, we propose Araida, an analogical reasoning-based approach that enhances automatic annotation accuracy in the interactive data annotation setting and reduces the need for human corrections. Araida involves an error-aware integration strategy that dynamically coordinates an annotation model and a k-nearest neighbors (KNN) model, giving more importance to KNN's predictions when predictions from the annotation model are deemed inaccurate. Empirical studies demonstrate that Araida is adaptable to different annotation tasks and models. On average, it reduces human correction labor by 11.02%compared to vanilla interactive data annotation methods.","Data annotation is a challenging task that involves a tradeoff between annotation quality and budget. While some platforms offer a cost-effective solution by relying on ML models to annotate data automatically, the quality of such annotations is often compromised. It is particularly true in the limited data annotation scenario where the annotation budget is limited or when unlabeled data are scarce. 
Human-machine interactive annotation methods were introduced to reduce annotation effort while maintaining annotation quality. As illustrated in Fig. , these methods introduce an annotation model to suggest labels (model annotations) to human annotators. The annotators accept a suggested label if it is correct. Otherwise, they have to correct the label manually. Compared to manual annotation, interactive annotation requires less human effort because human annotators only have to verify the model annotations instead of coming up with an answer from scratch, leading to potential speedup of the annotation process. 
Evidently, the annotation model 's accuracy is crucial because incorrect suggestions require additional human effort to rectify. Existing methods update the annotation model based on previously accepted or corrected data (ground-truth annotation), aiming to reduce human corrections by improving prediction accuracy at each iteration. However, in the context of limited data annotation, the annotation model lacks sufficient labeled training data to reach a reasonable accuracy and is prone to providing incorrect suggestions. For example, in the span relation annotation example shown in Fig. (blue), the annotation model continues to make mistakes on similar examples ( [car, tyre] ) even after the human annotator corrects the label' [tree, leaf] => component '. As a result, this leads to more human corrections. Such a problem is crucial for interactive annotation and has been identified by recent work, but it has yet to be addressed. 
Inspired by cognitive studies on efficient learning, finding that the human brain can learn from a few examples because our brain is continuously building analogies during the learning process of concepts to facilitate comprehension, we propose Analogical Reasoning-Augmented Interactive Data Annotation (Araida), which is designed to improve interactive annotation under the limited data annotation setting. Araida provides an annotation reference to the annotation model by retrieving previously human-labeled examples in the proximity of the example in consideration using the k-nearest neighbors (KNN) method. As illustrated in Fig. (red), the final suggestion combines the model annotation and the annotation reference provided by KNN via an error-aware integration strategy. This strategy dynamically coordinates the annotation model and KNN, giving more importance to KNN' s prediction if the predicted label from the annotation model is estimated to be inaccurate. 
We conduct simulated experiments for the limited data annotation task and estimate the human annotation effort based on the number of human corrections (or the number of suggestion errors) following and. We test Araida on different word-level and sentence-level annotation tasks, combining with different annotation models (i.e., classic and LLM-based models). The result shows that Araida consistently improves different annotation models' accuracy across various tasks. On average, it reduces human corrections by 11.02%. Further analysis attributes this improvement to the few-shot capability of the KNN module and the error-aware integration strategy that effectively synergizes complementary annotations. In summary, our contributions are as follows: 
 
 
 * Calling attention to the limited data annotation scenario. We highlight the under-trained problem of the annotation model, which is crucial in practice but overlooked in interactive annotation. 
 * Introducing Araida that involves a KNN module and an error-aware integration strategy to alleviate the under-trained problem by facilitating coordination between the two model annotators (i.e., the vanilla annotation model and KNN). 
 * Demonstrating the efficacy of Araida in enhancing suggestion accuracy, reducing human corrections, and showcasing its flexibility to combine with various annotation models through extensive experiments."
CLAMBER: A Benchmark of Identifying and Clarifying Ambiguous Information Needs in Large Language Models,2405.12063v2,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2405.12063v2.png,"Investigation on the identification accuracy when handling ambiguous (i. e, Acc@1) versus unambiguous queries (i. e, Acc@0). We report the results under Zero-shot w/o CoT setting. Small-scale LLMs tend to classify most queries as ambiguous.","Large language models (LLMs) are increasingly used to meet user information needs, but their effectiveness in dealing with user queries that contain various types of ambiguity remains unknown, ultimately risking user trust and satisfaction. To this end, we introduce CLAMBER, a benchmark for evaluating LLMs using a well-organized taxonomy. Building upon the taxonomy, we construct ∼ 12K high-quality data to assess the strengths, weaknesses, and potential risks of various off-the-shelf LLMs. Our findings indicate the limited practical utility of current LLMs in identifying and clarifying ambiguous user queries, even enhanced by chain-of-thought (CoT) and few-shot prompting. These techniques may result in overconfidence in LLMs and yield only marginal enhancements in identifying ambiguity. Furthermore, current LLMs fall short in generating high-quality clarifying questions due to a lack of conflict resolution and inaccurate utilization of inherent knowledge. In this paper, CLAMBER presents a guidance and promotes further research on proactive and trustworthy LLMs. Our dataset is available at <https: //github. com/zt991211/CLAMBER>.","Given well-defined user queries, large language models (LLMs) have demonstrated remarkable proficiency in facilitating the information search process. They provide more precise search results with the help of the inherent knowledge stored within LLMs. Nonetheless, as evidenced by previous studies, the practical utility of LLMs is hindered by unclear and ambiguous user queries in real-world scenarios. For instance, in a query like ""what are the strategies for saving? "", the term ""saving"" can have multiple interpretations, such as ""saving money"" or ""saving from sins"", depending on the user 's actual need. This necessitates LLMs proactively identifying (i.e., determine if the query is ambiguous or not) and clarifying the ambiguities rather than providing potentially incorrect answers that may not align with the user' s true needs, ultimately risking user trust and satisfaction. 
Driven by this concern, recent works have explored LLMs' capacity to address ambiguous queries. However, these investigations have been somewhat fragmented, lacking a comprehensive taxonomy, leading to incomplete and inconsistent handling of ambiguity distributions. As a notable example, they are often limited to contextual ambiguity, where the given context is insufficient for producing a definitive answer. In the era of LLMs, there should be more emphasis on the LLM-oriented ambiguity that may occur when inherent knowledge stored within LLMs have conflict understanding about the query. Consequently, it still remains unclear which ambiguities LLMs can effectively identify and clarify, along with the challenges that LLMs persistently encounter in this regard. 
To this end, we introduce CLAMBER (Clarifying Ambiguous Query), a novel benchmark for comprehensively evaluating LLMs in identifying and clarifying various ambiguities using a well-organized taxonomy. Drawing inspiration from the input-process-output framework for evaluating collaborative systems, we establish a taxonomy that consolidates both input understanding and task completion perspectives into three primary dimensions, as illustrated in Table. These dimensions are further conceptualized into eight fine-grained categories to facilitate in-depth evaluation. Building upon this taxonomy, we construct ∼ 12K data for analyzing the pros and cons of LLMs when identifying and clarifying ambiguities. 
With CLAMBER, we comprehensively evaluate strengths, weaknesses, and potential risks of various LLMs. Our findings indicate that ChatGPT outperforms other small-scale LLMs, especially excelling in identifying and clarifying ambiguities in multifaceted queries, such as ""What is the largest manufacturer in China? "", which does not specify the type of ""manufacturer"". However, they still encounter numerous challenges: 1) current LLMs, despite leveraging chain-of-thought (CoT) and few-shot prompting, face challenges in identifying ambiguities. Our results suggest that CoT and few-shot prompting may lead to the over-confidence issue in small-scale LLMs, impacting ambiguity identification negatively. Even with a large number of shots and CoT support, LLMs only achieve a marginal improvement. Moreover, current LLMs struggle to leverage contextual cues to disambiguate pronouns, highlighting the inadequacy in deducing underlying ambiguities. 2) Current LLMs fail to ask high-quality clarifying questions, due to the inability of knowing their knowledge gap. Despite LLMs recognize a query containing ambiguities, their lack of conflict resolution and inaccurate use of inherent knowledge results in uncertainty about which ambiguity to clarify. This prompts the need of developing effective methods for LLMs to resolve conflicts and accurately utilize their inherent knowledge. 
In this paper, CLAMBER stands as a valuable resource to provide guidance and insight into evaluating LLMs and addressing ambiguous information needs for future improvements. In conclusion, our contributions are threefold: 
 
 
 * We introduce a taxonomy for categorizing various query ambiguities. This taxonomy combines three primary dimensions, detailed as eight categories for facilitating fine-grained evaluations. 
 * We present a novel benchmark called CLAMBER, tailored to the characteristics of LLMs. It contains ∼ 12K data featuring ambiguous user queries across diverse categories. 
 * With CLAMBER, we evaluate the off-the-shelf LLMs in an inclusive manner. Our findings shed light on why current LLMs struggle to identify and clarify ambiguities. These insights will guide future research in this field."
Multimodal Reasoning with Multimodal Knowledge Graph,2406.02030v2,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2406.02030v2.pdf, (a) The inadequate knowledge encapsulated within textual KG results in the incorrect answer. (b) Our MR-MKGproduces the correct answer by reasoning with richer multimodal information.,"Multimodal reasoning with large language models (LLMs) often suffers from hallucinations and the presence of deficient or outdated knowledge within LLMs. Some approaches have sought to mitigate these issues by employing textual knowledge graphs, but their singular modality of knowledge limits comprehensive cross-modal understanding. In this paper, we propose the Multimodal Reasoning with Multimodal Knowledge Graph (MR-MKG) method, which leverages multimodal knowledge graphs (MMKGs) to learn rich and semantic knowledge across modalities, significantly enhancing the multimodal reasoning capabilities of LLMs. In particular, a relation graph attention network is utilized for encoding MMKGs and a cross-modal alignment module is designed for optimizing image-text alignment. A MMKG-grounded dataset is constructed to equip LLMs with initial expertise in multimodal reasoning through pretraining. Remarkably, MR-MKGachieves superior performance while training on only a small fraction of parameters, approximately 2.25%of the LLM's parameter size. Experimental results on multimodal question answering and multimodal analogy reasoning tasks demonstrate that our MR-MKGmethod outperforms previous state-of-the-art models. ^* Corresponding author.","Recently, Large Language Models (LLMs) have demonstrated their superiority and robustness across a variety of NLP tasks. To further unlock the potential of LLMs, researchers have attempted to endow them with multimodal reasoning capabilities, as exemplified by visual LLMs like BLIP-2, MiniGPT-4, LLaVA, etc. Although these models have made significant strides in enabling reasoning with both images and text, they are still prone to hallucinations, often caused by inadequate or outdated information. 
Fine-tuning Large Language Models (LLMs) to update their knowledge base is often a time-consuming and costly process. An alternative strategy, as suggested by, involves leveraging knowledge graphs (KGs) as a means to directly augment LLMs with the requisite knowledge. Although recent efforts have focused on employing textual KGs, their singular modality limits LLMs 'ability to process and reason with multimodal information (as illustrated in Figure a). This limitation leads us to consider the use of multimodal knowledge graphs (MMKGs) instead of textual KGs (See Figure b). 
In this paper, we propose the Multimodal Reasoning with Multimodal Knowledge Graphs (MR-MKG) method, designed to expand the multimodal knowledge of LLMs by learning from MMKGs. In particular, MR-MKGfirst encodes the retrieved MMKG using a relation graph attention network (RGAT), which generates knowledge node embeddings that are able to capture complex graph structures. Then, knowledge and visual adapter layers are designed to bridge the cross-modal gap, mapping both knowledge nodes and visual embeddings to word embedding of LLMs, respectively. Finally, embeddings of knowledge nodes, image and text are concatenated to form the prompt and subsequently forwarded to LLMs to provide guidance and instructions. In addition, we introduce a novel cross-modal alignment module to optimize the image-text alignment through a matching task within MMKGs. To equip the model with initial expertise in multimodal reasoning, we first pretrain MR-MKGon a customized MMKG-grounded dataset, which is constructed by matching each VQA instance with a corresponding MMKG, derived from the scene graph of its image and containing essential knowledge for answering questions. 
To thoroughly evaluate our MR-MKGmethod, we conduct comprehensive experiments on multimodal question answering and multimodal analogy reasoning tasks, spanning various LLM sizes and training configurations. The experimental results confirm that MR-MKGeffectively processes and utilizes knowledge from MMKGs for multimodal reasoning, outperforms previous state-of-the-art models with a 1.95%increase in accuracy and a 10.4%improvement in the Hits@1 metric. Importantly, MR-MKGfreezes both LLM and the visual encoder, with only a small fraction of the parameters, approximately 2.25%of the LLM' s parameter size, being updated. In summary, our main contributions are three-fold: 
 
 * To the best of our knowledge, we are the first to investigate the problem of expanding multimodal reasoning capabilities of LLMs by utilizing knowledge derived from MMKGs. 
 * We propose the MR-MKGmethod, specifically designed to extract valuable knowledge from MMKGs and seamlessly integrate multimodal information into LLMs. Additionally, we also develop a MMKG-grounded dataset for initially enhancing multimodal reasoning. 
 * We extensively evaluate MR-MKGon two multimodal reasoning tasks. MR-MKGachieves state-of-the-art performance by significant margins, outperforming recent baseline methods."
RAGTruth: A Hallucination Corpus for Developing Trustworthy Retrieval-Augmented Language Models,2401.00396v2,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2401.00396v2.pdf,Frequency of different types of hallucination by task.,"Retrieval-augmented generation (RAG) has become a main technique for alleviating hallucinations in large language models (LLMs). Despite the integration of RAG, LLMs may still present unsupported or contradictory claims to the retrieved contents. In order to develop effective hallucination prevention strategies under RAG, it is important to create benchmark datasets that can measure the extent of hallucination. This paper presents RAGTruth, a corpus tailored for analyzing word-level hallucinations in various domains and tasks within the standard RAG frameworks for LLM applications. RAGTruth comprises nearly 18,000 naturally generated responses from diverse LLMs using RAG. These responses have undergone meticulous manual annotations at both the individual case and word levels, incorporating evaluations of hallucination intensity. We not only benchmark hallucination frequencies across different LLMs, but also critically assess the effectiveness of several existing hallucination detection methodologies. We show that using a high-quality dataset such as RAGTruth, it is possible to finetune a relatively small LLM and achieve a competitive hallucination detection performance when compared to the existing prompt-based approaches using state-of-the-art LLMs such as GPT-4. Furthermore, the finetuned model can effectively mitigate hallucination in LLM responses.","Large language models (LLMs) have achieved remarkable success in a variety of tasks, including text generation, machine translation, and question answering. However, one of the key challenges in deploying LLMs in real-world applications is their tendency to hallucinate. Hallucination in the context of LLMs usually refers to a situation where the model generates content that is not based on factual or accurate information. The occasional generation of outputs that appear plausible but are factually incorrect significantly undermine the reliability of LLMs in real-world scenarios, such as medical diagnoses and news summarization. 
To reduce hallucination, various methods have been developed that can be applied at different stages of LLM lifecycle, including pre-training, supervised fine-tuning, RLHF, and inference. In terms of detection, methods are developed by examining the model 's intrinsic state, comparing it with external data and tools, or leveraging the LLM' s inherent powerful capabilities for self-checking. Retrieval-augmented generation (RAG) is extensively used to supply LLMs with updated, relevant knowledge, significantly mitigating hallucination. Nevertheless, even with RAG and other enhancements, LLMs still produce statements that are either unfounded or contradict the information provided in the retrieved references. 
Despite the growing awareness of the hallucination phenomenon, the understanding of hallucination in LLMs is still in its early stages. One key challenge is the lack of high-quality, large-scale datasets specifically designed for hallucination detection. This issue is particularly acute in RAG settings. Due to the relatively low hallucination ratio, a substantial increase in annotation resources is needed. Existing datasets for LLM hallucination detection are predominantly synthesized. For instance, in, prompts conflicting with conventional knowledge are purposely generated to trigger hallucinations. While these approaches are efficient at generating hallucinations, the resulting artificial hallucinations can substantially differ from those that naturally occur. In, hallucination datasets are developed by manual annotations of naturally produced LLM responses. However, these datasets are of limited size and are not specifically focused on the RAG scenario. 
In this paper, we introduce a large-scale high-quality dataset specifically designed for word-level hallucination detection for RAG applications. Using this dataset, we have conducted an extensive benchmarking of mainstream LLMs to assess their tendency to generate hallucinations, as well as evaluate current methods for hallucination detection. Additionally, we have demonstrated superior performance in identifying hallucinations by fine-tuning LLM with RAGTruth dataset. Our key contributions are: 
 * We propose RAGTruth, a large-scale word-level hallucination evaluation dataset specifically for the RAG scenario across several common tasks. It consists of nearly 18,000 fully annotated natural responses generated from major open-source and closed-source LLMs. 
 * We perform a comprehensive comparison of different hallucination detection methods at both the passage and word levels. 
 * We present a baseline method of fine-tuning LLM for hallucination detection. It is shown that by fine-tuning the Llama-2-13B model on the RAGTruth training data, we can achieve results competitive to the existing prompt-based approaches using GPT-4. This shows the potential of developing better hallucination detection methods using RAGTruth. 
 * We show that by using our finetuned hallucination detector, it is possible to significantly reduce the occurrence of hallucinations in the responses from LLMs. The improvement holds even for models with inherently low hallucination rates, such as GPT-4."
CLOMO: Counterfactual Logical Modification with Large Language Models,2311.17438v4,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2311.17438v4.png,Demonstration of CLoMo. An LLM is given an argument and two premises. The LLM needs to modify the statements in Argument such that the logical relation switch to stand in instead of.,"In this study, we delve into the realm of counterfactual reasoning capabilities of large language models (LLMs). Our primary objective is to cultivate the counterfactual thought processes within LLMs and rigorously assess these processes for their validity. Specifically, we introduce a novel task, Counterfactual Logical Modification (CLoMo), and a high-quality human-annotated benchmark. In this task, LLMs must adeptly alter a given argumentative text to uphold a predetermined logical relationship. To effectively evaluate a generation model's counterfactual capabilities, we propose an innovative evaluation metric, the decomposed Self-Evaluation Score (SES) to directly evaluate the natural language output of LLMs instead of modeling the task as a multiple-choice problem. Analysis shows that the proposed automatic metric aligns well with human preference. Our experimental results show that while LLMs demonstrate a notable capacity for logical counterfactual thinking, there remains a discernible gap between their current abilities and human performance. Code and data are available at <https: //github. com/Eleanor-H/CLOMO>.","Despite large language models perform strikingly in plenty of reasoning benchmarks, late studies observe an internal inconsistency in their reasoning processes. The inconsistency is attributed to misunderstanding and misapplication of logical relations. However, logical relations in complex language reasoning are not yet properly quantified and evaluated. 
Current studies on evaluating model reasoning are limited in both form and content. On the one hand, benchmarking complex reasoning is generally applying discrimination tasks such as multiple-choice questions, where accuracy and pass rate serve as the main evaluation metric. However, such evaluations over-simplify the goal of uncovering essential and subtle pitfalls in complex reasoning. For example, the reasoning processes could contain misconceptions in logical relations but give correct answers due to the data distribution. Therefore, evaluating the generated content would provide a more realistic measurement of model reasoning. On the other hand, unlike widely studied reasoning tasks such as math reasoning and standard exams, counterfactual reasoning as a fundamental evaluation of logical relations is less explored in the context of large language models. Previous literature studies counterfactual reasoning either in a multiple-choice manner or applying labored human study to evaluate counterfactual generation, leaving an effective evaluation of counterfactual generation unexplored. 
In our study, we delve into the realm of evaluating large language models ' (LLMs) ability to generate counterfactually coherent thoughts. Figure demonstrates the paradigm. Specifically, we proposed an innovative evaluation system that quantitatively measures the evolution of information in statement pairs, ensuring that they adhere to a specified logical relationship. Our approach includes designing a specialized task where models are presented with mismatched argument-premise pairs bound by a specific logical relation. The objective for these models is to adeptly modify the argument text until the specified logical relation is satisfactorily established. In conjunction with this task, we have created the first dataset of its kind, comprising dual argument-premise pairs, each annotated with a defined logical relation. This dataset is vital for facilitating logically restricted counterfactual modifications, and we have enriched it with human-written modifications to serve as a benchmark for evaluation. 
Our experimental investigations encompass a range of large language models, including the latest GPT-4o, GPT-4, and GPT-3.5-Turbo, as well as smaller models from the LLaMA and LLaMA 2 families. Through these experiments, we have discerned that the task of CLoMo poses a significant challenge. It becomes evident that these models' current counterfactual logical reasoning capabilities fall short of the desired proficiency. This observation underscores the need for further advancements in enhancing the counterfactual reasoning abilities of existing language models, paving the way for more sophisticated and logically coherent AI systems. 
The contributions of this paper are three-fold: 
 
 
 * We propose the task of Counterfactual Logical Modification and contribute a corresponding CLoMoto evaluate the counterfactual reasoning capability of LLMs in the scenario of complicated textual logical reasoning. 
 * We propose the decomposed Self-Evaluation Score (SES) for the logically consistent generation of large language models. 
 * We conduct experiments on LLMs (GPT-3.5, GPT-4) and small language models (the LLaMA and LLaMA 2 families) and find that CLoMo is a very challenging task and the counterfactual logical reasoning ability of the existing model needs to be improved."
Exploring Hybrid Question Answering via Program-based Prompting,2402.10812v1,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2402.10812v1.pdf,Example of hybrid question answering task with the corresponding program.,"Question answering over heterogeneous data requires reasoning over diverse sources of data, which is challenging due to the large scale of information and organic coupling of heterogeneous data. Various approaches have been proposed to address these challenges. One approach involves training specialized retrievers to select relevant information, thereby reducing the input length. Another approach is to transform diverse modalities of data into a single modality, simplifying the task difficulty and enabling more straightforward processing. In this paper, we propose HProPro, a novel program-based prompting framework for the hybrid question answering task. HProPro follows the code generation and execution paradigm. In addition, HProPro integrates various functions to tackle the hybrid reasoning scenario. Specifically, HProPro contains function declaration and function implementation to perform hybrid information-seeking over data from various sources and modalities, which enables reasoning over such data without training specialized retrievers or performing modal transformations. Experimental results on two typical hybrid question answering benchmarks HybridQA and MultiModalQA demonstrate the effectiveness of HProPro: it surpasses all baseline systems and achieves the best performances in the few-shot settings on both datasets.","Question answering systems have attracted significant attention and made considerable progress in recent years. However, real-world data often exists in diverse formats and originates from multiple sources. Consequently, researchers turn their focus to the hybrid question answering (HQA) task, which necessitates mixed reasoning across various types of data. The HQA task is challenging due to the vast amount of information and the organic coupling of heterogeneous data sources. Reasoning over such diverse data requires the ability to understand multiple data types simultaneously. For instance, as depicted in Figure, the model must engage in reasoning over both the table and the extensive passages and images linked in hyperlinks to make accurate predictions. 
To tackle these challenges, recent approaches focus on training domain-specific models to retrieve or rank elements such as table rows, passages, or images, selecting the most relevant ones to enhance the subsequent reasoning process. Since real-world heterogeneous data is vast and constantly updated, even if these approaches demonstrate promising performance on their focused datasets, their applicability to such intricate data is still limited. Furthermore, some existing approaches tend to transform diverse modalities of data into a single modality, such as image captioning, or table-to-text generation, to reduce the task difficulty. However, such approaches are constrained by the performance of modal transformation models, which often result in the loss of information. In a word, these approaches highly rely on data distribution, and the complexity of real-world heterogeneous data makes them exorbitant. 
In contrast to previous approaches, we argue that the solution of solving the HQA task should be agnostic to data distribution. Consequently, we advocate for an optimal solution devising a procedure for determining how to find an answer, rather than merely generating the answer itself. Noticing that the program could elucidate the reasoning process employed to arrive at the answer (as depicted in Figure), in the current era of large language models (LLMs), leveraging a program can serve as an advantageous solution since LLMs are an excellent program generator. Moreover, the process of program generation necessitates the incorporation of various functions into the program, enabling information-seeking across diverse sources and modalities of data. 
Based on the aforementioned considerations, in this paper, we introduce a novel program-based prompting framework HProPro (Hybrid Program-Based Prompting) for HQA task. HProPro considers the solution as a process of code generation and execution, integrating external customized functions under the few-shot setting. To facilitate the utilization of customized functions, HProPro incorporates two key components: Function Declaration during the code generation phase and Function Implementation during the execution phase, which is shown in Figure. During the function declaration stage, HProPro defines the function name and formal parameters, utilizing them as prompts to generate code. Subsequently, in the function implementation stage, HProPro implements the declared functions, serving for the direct execution of the generated code. By defining different functions, HProPro can support reasoning over data from various modalities, making it a flexible and scalable framework. Importantly, HProPro eliminates the need to convert different modalities of data into a single modality beforehand. Instead, it acquires information within the origin modal by the functions themselves. To the best of our knowledge, HProPro is the first work to explore the power of LLMs in handling heterogeneous data without requiring domain-specific retrieval or modal transformation. Experiments demonstrate that HProPro significantly outperforms previous methods. 
In summary, our contributions are as follows: 
 
 * We introduce HProPro, a program-based prompting framework that enables reasoning over heterogeneous data without domain-specific retrieval and modal transformation. 
 * We implement a few-shot code generation and execution pipeline, calling various functions by function declaration and implementation to perform information-seeking across data from different sources and modalities. 
 * Experiments show the effectiveness that HProPro achieves the best performances under the few-shot settings on HybridQA and achieves state-of-the-art performances under all settings on MultiModalQA."
Uncertainty Aware Learning for Language Model Alignment,2406.04854v1,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2406.04854v1.pdf,"Illustration of feature clustering. Compared to SFT, UAL-based models show more convergence in the feature space, which we detailed our exploration in Section.","As instruction-tuned large language models (LLMs) evolve, aligning pretrained foundation models presents increasing challenges. Existing alignment strategies, which typically leverage diverse and high-quality data sources, often overlook the intrinsic uncertainty of tasks, learning all data samples equally. This may lead to suboptimal data efficiency and model performance. In response, we propose uncertainty-aware learning (UAL) to improve the model alignment of different task scenarios, by introducing the sample uncertainty (elicited from more capable LLMs). We implement UAL by a simple fashion – adaptively setting the label smoothing value of training according to the uncertainty of individual samples. Analysis shows that our UAL indeed facilitates better token clustering in the feature space, validating our hypothesis. Extensive experiments on widely used benchmarks demonstrate that our UAL significantly and consistently outperforms standard supervised fine-tuning. Notably, LLMs aligned in a mixed scenario have achieved an average improvement of 10.62%on high-entropy tasks (i.e., AlpacaEval leaderboard), and 1.81%on complex low-entropy tasks (i.e., MetaMath and GSM8K).","Large language models (LLMs), represented by GPT-4, Claude, and Llama2, have recently achieved significant success in a series of natural language generation and understanding tasks. The emergence of alignment methods has further enhanced the capabilities of LLMs, for example, the ability to follow human instructions or to achieve better zero-shot performance. The current popular alignment approaches including RLHF, consider supervised fine-tuning (SFT) an essential part that contributes significantly. 
It is undeniable that the current SFT paradigm achieves considerable success. Some previous studies find that high-quality, complex, and diverse data contribute significantly to alignment. However, the data can exhibit different levels of uncertainty. For data points in highly technical, scientific, or specialized settings, context may have little or no ambiguity and a limited set of correct answers. In contrast, other dialogues might feature varied and dynamic social contexts with idiomatic language uses. Appendix presents a pair of such examples. Nevertheless, the common SFT paradigm applies the same level of supervision to all samples in the training set, overlooking the intrinsic uncertainty of the data. 
Furthermore, due to the phenomenon of catastrophic forgetting, SFT-aligned models may perform worse than their foundational models. The Deepseek technical report reveals that the SFT model often underperforms compared to its base model on several benchmarks, while LLMs generally show diminished performance on general tasks when forging their agent capability. In our experiments, we observe that the standard SFT paradigm frequently leads to model degradation, resulting in decreased performance on some benchmarks, despite providing overall improvement. For instance, this is evident in parts of the commonsense benchmarks, such as MMLU. It is essential to align pretrained models while mitigating degradation issues as effectively as possible. Therefore, we propose: Uncertainty Hypothesis. In an ideal paradigm, to further enhance alignment performance, the model should attend to samples differently based on their properties during alignment. Specifically, the model should impose stricter constraints when attending to more certain examples, as these samples exhibit less uncertainty and fewer variations, while maintaining relaxed constraints for highly uncertain examples. 
To design our uncertainty-aware learning (UAL) method and address potential model degradation caused by SFT, we propose measuring uncertainty through a coarsely-grained approach that incorporates an autonomous judge (e.g., GPT-4) to assess the uncertainty of each sample in the training set. Upon obtaining the uncertainty estimations, we linearly map them into our adaptive label smoothing training pipeline. Further details of our algorithm are presented in Section. 
UAL significantly enhances the performance of instruction-tuned models. We apply this method across various model architectures and alignment datasets, observing that the UAL paradigm consistently outperforms the vanilla SFT paradigm on prominent benchmarks, including MMLU and TruthfulQA, among others. As Figure shows, UAL also helps the model improve in different scenarios. Moreover, compared to its foundational model, as Figure presents, the SFT-aligned model brings same-class tokens closer together in the feature space, and UAL enhances this tendency even further. This strongly correlates with the superior performance of UAL-aligned models across benchmarks and scenarios, offering an important explanation for UAL's outperformance over conventional SFT."
Interpretable User Satisfaction Estimation for Conversational Systems with Large Language Models,2403.12388v2,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2403.12388v2.jpg,Illustration of user utterances with satisfaction patterns (green) and dissatisfaction patterns (red).,"Accurate and interpretable user satisfaction estimation (USE) is critical for understanding, evaluating, and continuously improving conversational systems. Users express their satisfaction or dissatisfaction with diverse conversational patterns in both general-purpose (ChatGPT and Bing Copilot) and task-oriented (customer service chatbot) conversational systems. Existing approaches based on featurized ML models or text embeddings fall short in extracting generalizable patterns and are hard to interpret. In this work, we show that LLMs can extract interpretable signals of user satisfaction from their natural language utterances more effectively than embedding-based approaches. Moreover, an LLM can be tailored for USE via an iterative prompting framework using supervision from labeled examples. Our proposed method, Supervised Prompting for User satisfaction Rubrics (SPUR), not only has higher accuracy but is more interpretable as it scores user satisfaction via learned rubrics with a detailed breakdown.","General-purpose conversational systems such as ChatGPT and Copilot are revolutionizing how people live and work. Understanding when and why users are satisfied or dissatisfied is critical for the continuous improvement of these systems. It helps system developers identify areas of improvements, conduct effective A/B experiments, and optimize underlying models. Unsurprisingly, developing machine learning models for User Satisfaction Estimation (USE) has captured significant attention from the research community. 
When estimating user satisfaction, simply classifying that a user is satisfied or dissatisfied is insufficient. Understanding the reason why a user is satisfied or dissatisfied is just as valuable. For example, frequent query reformulation presents opportunities for prompt recommendation and conversations where users explicitly correct a bot's mistakes can suggest examples for model alignment. See Figure for an illustration. However, most existing work has focused on improving classification accuracy and has overlooked interpretability. Representation learning-based approaches are relatively opaque due to their use of neural models (e.g., embeddings) and thus offer little insight into conversational patterns that indicate satisfaction/dissatisfaction. Similar limitations apply to reward models for training LLMs, e.g., RLHF and RLAIF. In this case, the learned model produces a continuous ""reward"" score that aims to distinguish outputs that a human prefers without explaining why a conversation has a higher score than others. To our knowledge, these reward models have not been directly used for USE, but we treat it as a baseline due to their ability to rank outputs with respect to human preferences. 
Some prior work addressed the interpretation needs of USE via featurized ML models. Examples include, which evaluated user satisfaction based on human-annotated features assessing task success and dialogue costs, and, which proposed domain-independent features that evaluate response quality. However, the growth of LLM-based conversational systems (e.g., ChatGPT, Bing Copilot) means user queries in conversational systems may now be across multiple domains and intents (e.g., task-oriented, QA, chitchat). As such, approaches based on domain-specific features have limited generalizability to these diverse conversational patterns. 
In this work, we make the key observation that LLMs can achieve both high classification accuracy and fine-grained interpretability at the same time – through their ability to reason about user conversational patterns and identify salient pattern classes that generalize and produce accurate predictions. We propose Supervised Prompting for User satisfaction Rubrics (SPUR). We consider a few-shot scenario, where a small number of training examples are available, and develop a supervised, iterative prompting framework that uses an LLM to (1) extract signals of satisfaction from user utterances in a labeled training set, (2) summarize the reasons into rubrics for identifying satisfaction/dissatisfaction conversational patterns, and (3) apply the rubrics to predict satisfaction labels on unseen conversations. 
In addition to being more accurate, our approach provides an interpretable rubric for understanding the conversational patterns that indicate user satisfaction/dissatisfaction. Notably, our approach can be used to learn SAT/DSAT patterns automatically for different conversational systems. In our experimental results, we show the distributions of patterns in different types of systems and demonstrate how these patterns (1) correlate to overall user satisfaction, and (2) differ across domains. 
Moreover, we show that we can scale the application of the learned rubrics in two ways. First, we show that we can distill individual rubric items into an embedding-based model that can be applied at scale without the need for LLM prompting. Next, we show that we can add rubric items as features to an embedding-based model to increase the accuracy of embedding-only models on datasets with more available training data. 
The main contributions of our work include: 
 
 * We propose Supervised Prompting for User satisfaction Rubrics (SPUR), a novel framework for estimating user satisfaction in conversational systems with LLMs. 
 * We show the SPUR prompting process extracts patterns into clear and interpretable rubrics that guide the LLM to classify user satisfaction and show that diverse rubrics are learned automatically for different domains. 
 * We show SPUR outperforms existing methods across different types of conversational systems when training data is limited and provide insights into the factors that influence user satisfaction. 
 * We use knowledge distillation to scale the application of learned rubrics and show the rubrics can continuously improve performance as more training data is available."
Measuring Political Bias in Large Language Models: What Is Said and How It Is Said,2403.18932v1,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2403.18932v1.pdf,An overview of our proposed framework for measuring political bias in LLM-generated content. The two-tiered framework first evaluates the LLM's political stance over political topics and then framing bias in two aspects: content and style.,"We propose to measure political bias in LLMs by analyzing both the content and style of their generated content regarding political issues. Existing benchmarks and measures focus on gender and racial biases. However, political bias exists in LLMs and can lead to polarization and other harms in downstream applications. In order to provide transparency to users, we advocate that there should be fine-grained and explainable measures of political biases generated by LLMs. Our proposed measure looks at different political issues such as reproductive rights and climate change, at both the content (the substance of the generation) and the style (the lexical polarity) of such bias. We measured the political bias in eleven open-sourced LLMs and showed that our proposed framework is easily scalable to other topics and is explainable.","As the pervasiveness of AI in human daily life escalates, extensive research has illuminated its limitations and potential harms such as gender and racial biases and hallucinations. Among these, political bias in AI, a notably crucial yet underexplored facet, poses significant risks by potentially distorting public discourse and exacerbating societal polarization. 
Existing scholarly efforts have explored political bias ingrained in LMs, mainly focused on stance at the political-orientation level (i.e., left or right/liberal or conservative). The political orientation tests-based methodology (e.g., The Political Compass test) is often employed, yet it may be inadequate to fully capture the complex dynamics of bias within LLM-generated content. Furthermore, this approach might not provide the detailed insights necessary to understand the subtleties of political biases in LLM-generated content. 
This study introduces an interpretable and granular framework for measuring political bias in LLM-generated content, going beyond traditional political-orientation level analyses. Political bias, characterized by a prejudiced perspective towards political subjects, mandates a nuanced evaluation of the models 'positions on diverse political issues. This bias predominantly manifests through framing, which entails the deliberate selection and emphasis of specific informational elements, both in content and style, to shape perceptions. Given the multifaceted nature of political bias, our framework employs a two-tiered approach for its assessment, encompassing both topic-specific stance and framing. 
 To assess the models' stance on distinct political topics, we implement a method of extreme anchor comparison, quantifying the similarity between model outputs and two opposed stances – advocacy and opposition – across various political subjects. Subsequently, to dissect the political bias of LLMs more thoroughly, we examine framing by decomposing both the content and style. This involves a detailed content analysis leveraging Boydstun 's frame dimensions and entity-based frames, coupled with an evaluation of stylistic bias, including the examination of media bias in writing styles and the presence of non-neutral sentiment towards salient entities of topic. Collectively, our framework not only discerns topic-specific stances but also explores the intricate dynamics of the content (""what"" is said) and style (""how"" it is said) concerning contentious topics. The ultimate aim is to provide a measurement of political biases inherent in various LLMs, thereby paving the way for the development of strategies to diminish these biases and enhance the reliability and equity of LLM applications. 
Drawing upon the empirical evidence and analytical insights derived from our frameworks, this study elucidates a set of findings that furnish a guide for subsequent research endeavors within the community. The key discoveries include: (1) LLMs show different political views depending on the topic, such as being more liberal on reproductive rights and more conservative on immigration; (2) Even when LLMs agree on a topic, they focus on different details and present information differently; (3) LLMs often discuss topics related to the US; (4) topic-level analysis aligns with previous finding that LLMs usually lean towards liberal ideas; (5) Larger models aren' t necessarily more neutral in their political views; (6) Models from the same family can have different political biases; (7) the impact of multilingual capabilities (e.g., Yi-chat, Jais-chat) on the thematic focus of content, diverging from models primarily trained in English. By facilitating both model-specific and comparative analyses, our framework seeks to advance the development of AI systems that are safer and more aligned with ethical standards. We will open-source the codebase."
Fortify the Shortest Stave in Attention: Enhancing Context Awareness of Large Language Models for Effective Tool Use,2312.04455v4,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2312.04455v4.pdf," (a) Task illustration: Presented with multiple key-value pairs and a target key (highlighted in bold), the model is required to accurately retrieve and generate the value associated with this key from an extensive context. (b) We illustrate the position-related fluctuation in accuracy of Llama-2-7B on this in-context retrieval task. (c) The pattern of the attention score exhibits fluctuations, which we term the ""attention waveform"". Our study reveals a connection between the position-related fluctuations in LLMs' performance and this attention waveform.","In this paper, we demonstrate that an inherent waveform pattern in the attention allocation of large language models (LLMs) significantly affects their performance in tasks demanding a high degree of context awareness, such as utilizing LLMs for tool-use. Specifically, the crucial information in the context will be potentially overlooked by model when it is positioned in the trough zone of the attention waveform, leading to decreased performance. To address this issue, we propose a novel inference method named Attention Buckets. It allows LLMs to process their input through multiple parallel processes. Each process utilizes a distinct base angle for the rotary position embedding, thereby creating a unique attention waveform. By compensating an attention trough of a particular process with an attention peak of another process, our approach enhances LLM's awareness to various contextual positions, thus mitigating the risk of overlooking crucial information. In the largest tool-use benchmark, our method elevates a 7B model to achieve state-of-the-art performance, comparable to that of GPT-4. On other benchmarks and some RAG tasks, which also demand a thorough understanding of contextual content, our Attention Buckets also exhibited notable enhancements in performance.","Recent works that augmenting large language models (LLMs, e.g., GPT series) with tools have achieved advancements in various fields, such as human-computer interactions, automating multi-modal tasks, and enhancing the overall efficiency of language-related applications. In this paradigm, upon receiving a user 's intent, a large language model accesses multiple tools, typically in the form of APIs. It then selects the most suitable one by referring to the relevant tool documentation, and provides an accurate and suitable response. Considering the integration of extensive information into the context, tool-use tasks demand a high level of context understanding and awareness from LLMs. 
Despite the achievements made by current LLM-based tool-use frameworks, in our practical experience, we observed that LLMs exhibit varying levels of awareness concerning different positions within the context. For instance, LLMs may overlook certain tools within the context, resulting in a failed call; however, by altering the position of these tools, the task can be successfully executed. Such variations significantly affect the performance of LLMs in tool-use. This observation is consistent with the findings from a previous study that investigated a simple in-context retrieval task. When LLMs are presented with multiple key-value pairs and instructed to retrieve the value associated with a specific key, the index of the queried target key results in significant fluctuations in accuracy. Figure (a) provides a visual representation of the instructions for this task. Figure (b) shows this fluctuation we replicated using the Llama-2-7B. In our study, we go beyond the superficial fluctuations previously observed and identify that these position-related performance differences are closely associated with the model' s fluctuating attention allocation. Specifically, we observed a waveform pattern in the attention ""intensity"" (referred to as the attention waveform in this paper) when LLMs retrieve the same token from the context, as illustrated in Figure (c). We demonstrate that if the position of the crucial information coincides with a trough in the attention waveform, the model may overlook it, leading to decreased accuracy. 
Based on insight above, we argue that by shifting essential information away from the attention waveform 's trough zone, we can reduce the risk of LLMs missing crucial details, thus enhancing the efficacy of tool-use. Because crucial information within the context is inaccessible in practice, we propose the following approach to circumvent this challenge: We process the context through multiple parallel executions, where each execution is assigned a unique rotary angle base of the rotary position embedding, resulting a distinct waveform pattern (See for details). By ensuring these attention waveforms are ""complementary, "" — for any position where one waveform reaches its trough, another waveform reaches its peak — we enhance the LLM' s context awareness across various positions. We then aggregate the output distributions from these parallel executions and compute their weighted sum. This sum is subsequently decoded to generate the final prediction token. 
An analogy can aid in understanding our approach: Imagine a wooden bucket with some shorter staves, which allow water to leak out. Similarly, the attention mechanism, at each angle base, has limited awareness of specific positions in the context. We utilize models to process the context with different angle bases. This results in the troughs of one attention wave being fortified by the peaks of another, analogous to how the longer staves in one bucket compensate for the shorter staves in another. Consequently, we name our proposed method Attention Buckets. 
We achieve the state-of-the-art on the largest tool-use benchmark ToolBench and another benchmark ToolAlpaca. In ToolBench, we augment the performance of a 7B LLM to levels competitive with those of GPT-4. In addition to our achievements in tool-use, we also demonstrate our method 's potential in general retrieval-augmented generation (RAG) tasks, which also demand a high degree of contextual awareness. In summary, we make three major contributions: (1) For LLMs with RoPE, we propose and verify an explanation for the variation in their awareness of different positions within the context. We establish a relationship between this variation and the attention waveform. (2) By leveraging the insights from our proposed explanation, we develop a novel approach Attention Buckets to enhance LLMs' context awareness. (3) Through extensive experiments, we empirically validate the efficacy of our proposed method."
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations,2403.14112v3,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2403.14112v3.pdf,Construction of CHARM. CHARM encompasses both global and Chinese-specific commonsense. CHARM consists closely-interconnected reasoning and memorization tasks.,"We introduce CHARM, the first benchmark for comprehensively and in-depth evaluating the commonsense reasoning ability of large language models (LLMs) in Chinese, which covers both globally known and Chinese-specific commonsense. We evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5 representative prompt strategies for improving LLMs 'reasoning ability, such as Chain-of-Thought. Our findings indicated that the LLM' s language orientation and the task 's domain influence the effectiveness of the prompt strategy, which enriches previous research findings. We built closely-interconnected reasoning and memorization tasks, and found that some LLMs struggle with memorizing Chinese commonsense, affecting their reasoning ability, while others show differences in reasoning despite similar memorization performance. We also evaluated the LLMs' memorization-independent reasoning abilities and analyzed the typical errors. Our study precisely identified the LLMs' strengths and weaknesses, providing the clear direction for optimization. It can also serve as a reference for studies in other fields. We will release CHARM at <https: //github. com/opendatalab/CHARM>.","Commonsense reasoning is important for the enhancement of the large language models (LLMs) towards artificial general intelligence (AGI), therefore requires thorough evaluations. Numerous benchmarks evaluate the commonsense reasoning of LLMs, but most are English-based, limiting non-English evaluations. This paper focuses on assessing LLMs 'commonsense reasoning in a Chinese context. Currently, some commonsense reasoning benchmarks in Chinese are simply English translations, which overlooks unique Chinese cultural, linguistic, regional, and historical aspects. These factors matter when Chinese users use the LLM, hence should be included in benchmarks. To effectively tackle this, we introduce CHARM, the benchmark designed to thoroughly and in-depth assess the abilities of LLMs in Chinese commonsense reasoning. It covers two domains: globally accepted commonsense (global domain) and Chinese-specific commonsense (Chinese domain). The latter includes 7 aspects: History (H), Traditional Culture and Arts (CA), Daily Life and Customs (LC), Entertainment (E), Public Figures (F), Geography (G), and Chinese Language (L). Therefore CHARM allows a thorough evaluation of LLMs' reasoning in a Chinese context. 
Prompt strategies like Chain of Thought (CoT) can significantly improve LLMs 'reasoning performance. Particularly, as the training corpus of LLMs is primarily in English, studies have shown that for non-English reasoning tasks, some LLMs perform better when reasoning in English than the native language. We evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5 representative prompt strategies. The result showed that prompt strategies' effectiveness depends on the LLMs 'orientation and the benchmark task' s domain, which enriches prior research and guides performance assessment and strategy choice for non-English LLMs. 
LLMs 'commonsense reasoning relies on memorization. Exploring the correlation between memorization and reasoning offers insights into LLMs, aiding deeper understanding and suggesting ways to enhance these abilities. Some benchmarks aid the research of memorization-reasoning relationships by incorporate tasks for assessing knowledge memorization and application (like reasoning). However, they used the existing and disparate datasets for different tasks, resulting in a lack of intrinsic connections between these tasks. For instance, the question Q_rea tests the LLM' s reasoning with the knowledge piece K. However, in memorization tasks, there probably is not any matching questions to determine if the LLM has effectively memorized K. Hence, if the LLM fails on Q_rea, it 's unclear whether due to poor reasoning or forgetfulness of K. This results in the disjointed evaluation of memorization and reasoning, failing to uncover their intrinsic links. To address this limitation, we selected suitable reasoning tasks from CHARM' s Chinese domain, and built related memorization questions for each reasoning question (see Figure). This design produces the closely-interconnected reasoning and memorization tasks, therefore allows for not only the concurrent evaluation of the two abilities, but also the assessment of memorization-independent reasoning, providing the clear guidance for the LLMs 'enhancement. 
The contributions of this paper are as follows: 
 
 * We present CHARM, the first benchmark for comprehensively evaluating the LLMs' commonsense reasoning ability in Chinese, by encompassing not only the global but also the Chinese-specific commonsense. 
 
 * We evaluated the representative prompt strategies on CHARM. Results showed that LLMs 'orientation and the task' s domain affect prompt strategy performance, which enriches previous research findings. 
 
 * In CHARM, we built closely-interconnected reasoning and memorization tasks in Chinese commonsense domain, allowing for in-depth understanding the correlation between these abilities and precisely identifying the LLMs' strengths and weaknesses. The design approach could serve as the reference for other fields."
Model Composition for Multimodal Large Language Models,2402.12750v2,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2402.12750v2.pdf,Illustration of various approaches for multimodal large language models: (a) aligning LLM with a multimodal encoder and (b) joint training with multiple modal encoders and (c) our proposed model composition method that creates a versatile model from existing MLLMs through a training-free and extensible process.,"Recent developments in Multimodal Large Language Models (MLLMs) have shown rapid progress, moving towards the goal of creating versatile MLLMs that understand inputs from various modalities. However, existing methods typically rely on joint training with paired multimodal instruction data, which is resource-intensive and challenging to extend to new modalities. In this paper, we propose a new paradigm through the model composition of existing MLLMs to create a new model that retains the modal understanding capabilities of each original model. Our basic implementation, NaiveMC, demonstrates the effectiveness of this paradigm by reusing modality encoders and merging LLM parameters. Furthermore, we introduce DAMC to address parameter interference and mismatch issues during the merging process, thereby enhancing the model performance. To facilitate research in this area, we propose MCUB, a benchmark for assessing ability of MLLMs to understand inputs from diverse modalities. Experiments on this benchmark and four other multimodal understanding tasks show significant improvements over baselines, proving that model composition can create a versatile model capable of processing inputs from multiple modalities.","Recent advancements in Multimodal Large Language Models (MLLMs) have established them as the forefront of multimodal learning paradigms. The prevalent approach involves aligning modality encoders with large language models (LLMs) through extensive modality-text paired data and then fine-tuning with modality-specific instruction data. This paradigm has been successfully applied to a wide range of modalities such as image, audio, video, and point cloud, resulting in the emergence of a diverse array of MLLMs with unique modal capabilities. 
There are also some efforts to enable a single MLLM to handle multiple modalities. One way to achieve this is to align the LLM with a multimodal encoder such as ImageBind with only image-text data (Figure (a) ). This approach leverages the inherent alignment of different modalities within the multimodal encoder, allowing the MLLM to comprehend various modalities to a certain degree. However, the absence of modality-specific instruction data often results in suboptimal performance. Another approach entails the concurrent training of the MLLM with multiple modality encoders (Figure (b) ). For example, ChatBridge connects image, video and audio encoders with the LLM through a joint training process with multimodal instruction data (i.e., video-audio chats). This kind of methods show potential but faces two major challenges. First, it is resource-heavy to collect paired data across multiple modalities. Second, adapting these models to new modalities requires additional training, adding to the complexity and resource demands of the development process. 
Given the limitations of current approaches, we propose and study a more practical setting: model composition for MLLMs (Figure (c) ). Our primary research question is simple yet fundamental: Can we create a new MLLM by combining existing MLLMs to inherit their understanding of different modalities without training? Model composition for MLLMs is advantageous for two key reasons: (1) it eliminates the need for the resource-heavy process of training and gathering multimodal data, and (2) it promises enhanced adaptability, facilitating seamless incorporation of new modalities. 
Some recent studies, such as X-InstructBLIP, serve as pioneering efforts in model composition for MLLMs. These works primarily train projectors to align different encoders with a single LLM and demonstrate the ability to process multiple modalities concurrently. However, a critical limitation is their applicability only to MLLMs with frozen language model weights. This constraint restricts the range of models that can be utilized, and impairs the overall performance of the MLLMs. 
In this paper, we first propose a framework for model composition for MLLMs. Our implementation, named NaiveMC, is elegantly simple: for the MLLMs to be composed, we directly reuse their modality-specific encoders and merge their LLM parameters. We demonstrate that MLLMs, as long as initialized from the same LLM, can achieve zero-shot multi-modality expansion through this model composition framework, regardless of whether the parameters of the LLM have been fine-tuned. 
Furthermore, to mitigate parameter interference in the composition process and optimize the performance of the composite model, we propose DAMC, an advanced framework with parameter Decoupling and Adjustment for Model Composition. By separating modality-specific parameters from language model parameters during initial MLLM training, DAMC allows for the selective merging of textual parameters, reducing cross-modal interference. Moreover, DAMC introduces an adaptive parameter adjustment mechanism to ensure optimal compatibility and effectiveness of the composite model, achieving a balanced and efficient multi-modality expansion. 
To assess the efficacy of our proposed frameworks, we conduct comprehensive experiments on tasks that require an integrated understanding of inputs from diverse combinations of four prevalent modalities: image, audio, video, and point cloud. To facilitate the research in model composition for MLLMs, we also build MCUB, a benchmark specifically designed for evaluating the capability to concurrently comprehend multiple modalities by identifying commonalities across inputs from various modalities. Experimental results indicate that our frameworks enable the composition of existing MLLMs from different modalities without requiring further training, yielding a versatile and high-performing multimodal model adept at handling any combination of these modalities. 
Our contributions are three-fold: 
 
 * We propose the concept of model composition for MLLMs, realized through the NaiveMC framework, which allows for seamless integration of different MLLMs without additional training, enabling zero-shot multi-modality expansion. 
 * We introduce DAMC, an advanced model composition framework that employs parameter decoupling and adaptive adjustment to mitigate parameter interference and optimize composite model performance across multiple modalities. 
 * We create MCUB, a benchmark designed to evaluate the unified understanding of diverse modalities, and demonstrate the efficacy of our model composition frameworks via extensive experiments on various multimodal understanding tasks and MCUB."
Draft & Verify: Lossless Large Language Model Acceleration via Self-Speculative Decoding,2309.08168v2,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2309.08168v2.pdf,"Visualization of the self-speculative decoding process. The verification stage evaluates all drafted tokens in a single forward pass, with accepted tokens marked in green and rejected tokens highlighted in red. Each verification step also predicts one more token, which is denoted in blue.","We present a novel inference scheme, self-speculative decoding, for accelerating Large Language Models (LLMs) without the need for an auxiliary model. This approach is characterized by a two-stage process: drafting and verification. The drafting stage generates draft tokens at a slightly lower quality but more quickly, which is achieved by selectively skipping certain intermediate layers during drafting. Subsequently, the verification stage employs the original LLM to validate those draft output tokens in one forward pass. This process ensures the final output remains identical to that produced by the unaltered LLM. Moreover, the proposed method requires no additional neural network training and no extra memory footprint, making it a plug-and-play and cost-effective solution for inference acceleration. Benchmarks with LLaMA-2 and its variants demonstrated a speedup up to 1.99×.","Transformer-based Large Language Models (LLMs), such as GPT-3/4, PaLM, and LLaMA, have been widely adopted in various real-world applications. However, their inference costs have raised significant concerns, especially for latency-sensitive scenarios. The main efficiency bottleneck is the autoregressive decoding process. This process decodes each output token sequentially, leading to a high number of Transformer calls; furthermore, each Transformer call is typically memory bandwidth-bound, resulting in low computation utility and thus longer wall-clock time. For instance, decoding 128 tokens autoregressively using LLaMA-2-13B on an A100 GPU can take up to 100× longer than a sequence-level forward pass on the same number of tokens, highlighting the substantial inefficiency inherent in the current decoding process. 
Established model compression techniques such as quantization, pruning, and distillation have been employed to alleviate these costs. While these solutions have proven extremely effective, they usually require changing the model architecture, changing the training procedure, re-training or fine-tuning the models, and do not maintain identical outputs. 
In parallel to model compression, speculative execution is being explored to accelerate the autoregressive decoding process. These methods train an auxiliary draft model that can quickly generate some draft output tokens. Subsequently, the original LLM, referred to as the verify model, then checks the acceptability of these draft tokens with one single forward pass. This verification step ensures that the outputs are derived from the original LLM's probability distribution. 
However, an essential issue of existing speculative execution methods is the need to identify or train a suitable draft model that can generate outputs consistent with the verify model. It becomes more tricky when the LLM is already a fine-tuned model, e.g.,LLaMA-2-Chat, CodeLLaMA. How to find or train a draft model that can effectively mimic the outputs of such a tailored model is a formidable task, with no straightforward or guaranteed solutions. Furthermore, the introduction of an additional draft model escalates the GPU memory overhead, increasing deployment challenges particularly on devices with restricted memory capacity. 
In this paper, we present self-speculative decoding, a novel approach to accelerate the inference of LLMs. This method builds on the principles of speculative execution, but with a unique twist: it utilizes one LLM for both drafting and verification stages. The key insight driving our approach is the observation that skipping certain layers in LLMs does not significantly compromise the generation quality. As such, by selectively bypassing some intermediate layers, we can use the LLM itself to generate draft tokens. These tokens are then verified by the original LLM in a single forward pass. illustrates this two-stage decoding process. The blue arrow indicates the inference path of the original model, while the green arrow depicts the inference path during the drafting stage. Notably, both inference paths share the same model so we do not need a standalone draft model with extra memory overhead. 
Implementing self-speculative decoding poses two main challenges: (a) determining which layers and the number of layers to skip during drafting, and (b) deciding the timing to stop generating draft tokens. To tackle the first challenge, we formulate it as an optimization problem, which accepts the combinations of layers to bypass as input and aims to minimize the average inference time per token. We employ Bayesian optimization to solve this problem. The optimization is performed offline at the model level, and the searched layer combinations are fixed. The second challenge pertains to determining the optimal number of draft tokens (K) to generate. As shown in, the choice of K significantly influences the end-to-end speedup: for an acceptance rate below 80%, K=1 is optimal, and for rates above 80%, a larger K is necessary. This observation underscores that a static K is not universally applicable. To tackle this variability, we introduce an adaptive draft-exiting mechanism, which stops generating draft tokens once its confidence level drops below a threshold. This intervention prevents unnecessary computation and potential discard of additional draft tokens, thereby enhancing efficiency. 
To summarize, our main contributions are: (1) Inference scheme: we propose self-speculative decoding, a practical, plug-and-play solution for inference acceleration that does not require further neural network training and avoids additional memory overhead; (2) Optimization strategies: we adopt Bayesian optimization to select which layers to skip during drafting and propose a simple yet effective method to adaptively determine the number of draft tokens; (3) Evaluation: we evaluate our method on text summarization and code generation tasks, and the experimental results indicate that our method can achieve up to 1.99× in end-to-end speedup."
Measuring Meaning Composition in the Human Brain with Composition Scores from Large Language Models,2403.04325v3,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2403.04325v3.png,Comparing Composition Scores with fMRI data during naturalistic listening comprehension.,"The process of meaning composition, wherein smaller units like morphemes or words combine to form the meaning of phrases and sentences, is essential for human sentence comprehension. Despite extensive neurolinguistic research into the brain regions involved in meaning composition, a computational metric to quantify the extent of composition is still lacking. Drawing on the key-value memory interpretation of transformer feed-forward network blocks, we introduce the Composition Score, a novel model-based metric designed to quantify the degree of meaning composition during sentence comprehension. Experimental findings show that this metric correlates with brain clusters associated with word frequency, structural processing, and general sensitivity to words, suggesting the multifaceted nature of meaning composition during human sentence comprehension.","When encountering words such as ""milk"" and ""pudding"", the human mind effortlessly combines them to form a complex concept, such as a milk-flavored pudding. This combinatory process is a fundamental aspect of human language comprehension and production, enabling us to generate an infinite array of meanings from a finite set of words. Despite extensive neurolinguistic research into the localization of meaning composition in the human brain, understanding the detailed mechanism of how a complex meaning is constructed from its components and how it is processed by the human brain has become a challenging problem. One of the primary difficulties lies in the absence of a suitable computational metric to quantify the extent of meaning composition. This absence significantly complicates quantitative analyses of meaning composition in the human brain. 
Recent advancements in Large Language Models (LLMs) offer promising insights into this problem. By training on large-scale natural language corpora and aligning with human preferences, these computational models achieve unprecedented levels of proficiency in understanding and generating natural languages. In addition to their high performance, studies have shown that their internal states correlate with human behavioral and neural data, suggesting shared principles between their algorithms and the human brain. Given this background, it is natural to inquire whether we can develop a computational metric to quantify meaning composition from the internal states of LLMs. 
Motivated by this inquiry, our study introduces a novel model-based metric, the Composition Score, to evaluate meaning composition in the human brain. Leveraging the key-value memory interpretation of the Feed-Forward Network (FFN) modules in the transformer model, this metric computes the composition of memory-induced vocabulary distributions within the FFN blocks given an input prefix, thereby reflecting the degree of meaning composition of each word. To assess its validity, we examine the patterns of Composition Scores using the novel ""The Little Prince"" in English and compare them with other control variables such as word frequency and syntactic node count based on top-down, bottom-up, and left-corner parsing. Additionally, we correlate Composition Scores with an openly available fMRI dataset where participants listened to ""The Little Prince"" in the scanner. Our findings reveal that: 
 
 * The Composition Score exhibits partial correlation with word frequency and syntactic node counts but reveals more intricate patterns; 
 * The Composition Score is associated with a broader brain cluster and exhibits a higher regression score with the fMRI data compared to the control variables; 
 * Brain regions associated with the Composition Score encompass those underlying word frequency, structural processing, and general sensitivity to words, indicating the multifaceted nature of meaning composition."
Complex Reasoning over Logical Queries on Commonsense Knowledge Graphs,2403.07398v2,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2403.07398v2.pdf,An example of conjunctive logical queries and their verbalization as complex commonsense inferences.,"Event commonsense reasoning requires the ability to reason about the relationship between events, as well as infer implicit context underlying that relationship. However, data scarcity makes it challenging for language models to learn to generate commonsense inferences for contexts and questions involving interactions between complex events. To address this demand, we present Com^2 (COMplex COMmonsense), a new dataset created by sampling multi-hop logical queries (e.g., the joint effect or cause of both event A and B, or the effect of the effect of event C) from an existing commonsense knowledge graph (CSKG), and verbalizing them using handcrafted rules and large language models into multiple-choice and text generation questions. 
Our experiments show that language models trained on Com^2 exhibit significant improvements in complex reasoning ability, resulting in enhanced zero-shot performance in both in-domain and out-of-domain tasks for question answering and generative commonsense reasoning, without expensive human annotations.","Large language models struggle to effectively perform reasoning when presented with complex tasks, such as reasoning about multiple events and their relationships. This shortcoming is due to both the inherent difficulty of reasoning over multiple pieces of information, as well as a lack of adequate-scale, supervised training datasets for learning. Unfortunately, complex and multi-hop commonsense reasoning benchmarks are both technically challenging and financially expensive to curate. Consequently, previous efforts either constructed datasets (a) with simpler reasoning structures, such as single-hop chains, (b) using distant supervision based on one-hop inference, or (c) with human-annotations, but at a relatively small scale. 
To alleviate this training data bottleneck, recent works have explored extracting and formulating questions from existing CommonSense Knowledge Graphs (CSKGs; ), which store commonsense triples. However, using CSKGs to produce high-quality reasoning datasets poses several challenges. First, while the shared entities in commonsense triples encode a complex, interconnected graph structure, the sparsity of this structure limits the number of potential questions that encode more than one reasoning hop. Second, triples in CSKGs are represented in a context-free manner, such as the event ""PersonX gets tired of it"" in, yielding ambiguous (and sometimes incorrect) human annotations in the CSKG, e.g., ATOMIC has an error rate of over 10%. These errors propagate when triples are naively combined to construct reasoning questions. Finally, also because triples in CSKGs are represented in a context-free manner, additional context must be added to make questions fluent, a problem exacerbated in multi-hop settings where the entities of multiple reasoning hops must be coherently verbalized together. 
In this paper, we construct Com^2 (COMplex COMmonsense), a novel commonsense reasoning dataset using multi-hop queries in commonsense knowledge graphs to construct question answer pairs requiring complex narrative reasoning. To build this dataset, we use conjunctive logical queries, a subset of First-Order Logical queries that use existential quantifiers and conjunction. The multi-hop projection operation involves inferring hidden contexts, while the intersection operation enables reasoning among multiple events, encompassing common cause or effect, and abduction. For example, in, an intersection of two triples can be verbalized to a short narrative, and the process of inferring the common tail can be seen as an abduction of the hidden cause between the two heads. 
To address the challenges above, we propose to first densify the CSKG to merge nodes with high semantic similarity, increasing the connectivity of the graph. Then, we use an off-the-shelf plausibility scorer to filter out low quality triples, avoiding error propagation as we construct more complicated queries. Finally, we verbalize the queries to a natural language context with handcrafted rules and Large Language Models to derive coherent and informative narrative contexts for our questions. Our final Com^2 dataset comprises 790K question-answer pairs (both with multiple-choice and generative answer settings), including 1.3K examples that we manually verify for evaluation. 
Our results demonstrate the challenges faced by even powerful LLMs and supervised question answering models on the Com^2 dataset, underscoring the difficulty of performing complex multi-hop reasoning. Moreover, fine-tuning question answering models and generative commonsense inference models on Com^2 leads to substantial improvements across eight commonsense reasoning datasets, showing the efficacy of our framework for boosting commonsense reasoning ability. 
To conclude, our contributions are three-fold. First, we present a pipeline for sampling and verbalizing complex logical queries from CSKGs, to form a complex commonsense reasoning benchmark, Com^2, with minimal human effort. Second, we benchmark the complex reasoning ability of various state-of-the-art language models and question answering models on Com^2. Finally, we validate the benefit of fine-tuning on Com^2 on eight zero-shot commonsense reasoning datasets."
Learning to Plan and Generate Text with Citations,2404.03381v3,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2404.03381v3.pdf,"Query (top), followed by most relevant (abridged) passages, and summaries (bottom) with in-line citations. Summary (a) is the output of a vanilla sequence-to-sequence model trained to generate long answers with citations. Summaries (b) and (c) are the output of models with abstractive and extractive plans, respectively. Citations for plan-based models can have different formats (e.g., references to the question plan; see Section).","The increasing demand for the deployment of LLMs in information-seeking scenarios has spurred efforts in creating verifiable systems, which generate responses to queries along with supporting evidence. In this paper, we explore the attribution capabilities of plan-based models which have been recently shown to improve the faithfulness, grounding, and controllability of generated text. We conceptualize plans as a sequence of questions which serve as blueprints of the generated content and its organization. We propose two attribution models that utilize different variants of blueprints, an abstractive model where questions are generated from scratch, and an extractive model where questions are copied from the input. Experiments on long-form question-answering show that planning consistently improves attribution quality. Moreover, the citations generated by blueprint models are more accurate compared to those obtained from LLM-based pipelines lacking a planning component.","Large language models (LLMs) have demonstrated remarkable abilities to engage in creative conversations, summarize information from contextual cues, and deliver zero-shot performance on a wide range of previously unseen predictive and generative tasks. They are also becoming increasingly useful in information-seeking scenarios, ranging from answering simple questions to generating responses to search-like queries. 
The increasing demand for the deployment of LLMs in information-seeking scenarios has further spurred efforts in creating verifiable systems, which generate responses to queries along with supporting evidence. The evidence can take the form of a URL pointing to a short segment of text which supports an answer, an attribution report with evidence snippets, quotes cited verbatim from pages retrieved from a search engine, and references to passages extracted while browsing. In fact, this last type of evidence has been recently adopted in the form of in-line citations by commercial search engines such as BingChat and perplexity. ai. 
Regardless of how the evidence is presented, recent approaches tend to rely on a retrieval system (e.g., a commercial search engine) to obtain passages relevant to a query, while an LLM conditions on them to generate a response. Other work generates an answer to the input query first and subsequently retrieves relevant evidence in a post-processing step. Alternatively, the retrieved evidence can be used to further revise the generated response rendering it more consistent with the evidence. 
Despite recent efforts, it remains an open question how to best develop models with a built-in mechanism for attribution to external evidence. A related question is whether said mechanism contributes to generating more factually faithful output. Large-scale evaluation studies paint a worrying picture. find that long-form responses from existing search engines frequently contain unsupported statements or inaccurate citations, while show that model performance on attribution varies greatly (between 46%and 71%) across different architectures for the simpler question answering task. 
In this paper, we explore the attribution capabilities of plan-based models which have been shown to be less prone to hallucinations and more controllable. We focus on long-form question answering which aims to generate summaries from a set of passages that answer a specific query. We simulate how a search engine might synthesize passages of high relevance to a user query by assuming access to a retriever, and some way of verifying the output, i.e., by citing sources (see Figure). Our models operate on retrieved passages and learn to plan and generate summaries with attribution. On account of being more expressive, plan-based models allow us to formalize different forms of attribution, e.g., plans can be verified via citations to passages, while summaries can be verified through citations to the plan, passages, or both. 
Our models conceptualize text plans as a sequence of questions operating as blueprints for generation, determining what to say and in which order. Questions as a planning mechanism are ideally suited to attribution, since they provide a natural link between retrieved passages and their summaries. We define two models which differ on whether the question-based plan is generated (see Figure (b) ) or copied from input passages (Figure (c) ) and explore whether explicit planning has any bearing on citation quality. Our contributions can be summarized as follows: 
 0em 
 * We develop automatic methods to annotate training data with plans and citations, and fine-tune several Transformer models to generate attributed text. 
 * Experimental results on the AQuAMuSe dataset demonstrate that plans consistently improve attribution quality. Furthermore, summary quality improves with an extractive blueprint model. 
 * Out-of-domain experiments on the ALCE benchmark show that, once acquired, attribution is a robust skill (across information-seeking tasks). In terms of attribution quality, our models are competitive with (and sometimes better than) pipelines that heavily rely on large language models."
Language Models can Exploit Cross-Task In-context Learning for Data-Scarce Novel Tasks,2405.10548v3,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2405.10548v3.pdf,"In this working example, we aim to solve a question from MedMCQA using demonstrations from BoolQ. To do so, we sample a semantically similar demonstration from the source task and then use this demonstration along with task descriptors of the source and target tasks to generate a cross-task prompt that is fed to an LLM.","Large Language Models (LLMs) have transformed NLP with their remarkable In-context Learning (ICL) capabilities. Automated assistants based on LLMs are gaining popularity; however, adapting them to novel tasks is still challenging. While colossal models excel in zero-shot performance, their computational demands limit widespread use, and smaller language models struggle without context. This paper investigates whether LLMs can generalize from labeled examples of predefined tasks to novel tasks. Drawing inspiration from biological neurons and the mechanistic interpretation of the Transformer architecture, we explore the potential for information sharing across tasks. We design a cross-task prompting setup with three LLMs and show that LLMs achieve significant performance improvements despite no examples from the target task in the context. Cross-task prompting leads to a remarkable performance boost of 107%for LLaMA-2 7B, 18.6%for LLaMA-2 13B, and 3.2%for GPT 3.5 on average over zero-shot prompting, and performs comparable to standard in-context learning. The effectiveness of generating pseudo-labels for in-task examples is demonstrated, and our analyses reveal a strong correlation between the effect of cross-task examples and model activation similarities in source and target input tokens. This paper offers a first-of-its-kind exploration of LLMs' ability to solve novel tasks based on contextual signals from different task examples.","Large Language Models (LLMs) have revolutionized the state of Natural Language Processing for the past few years. With the ability of In-Context Learning (ICL), one can adopt an LLM to almost any task without costly gradient updates. At the same time, automated assistants, built on top of foundational LLMs, are being popularized. A crucial challenge with this escalating usage popularity is handling novel tasks. Humongous models like GPT-4 are able to deliver up-to-par performance even in a zero-shot regime. However, the computational requirements of deploying such large-scale models counteract the practicality of their usage en masse. Relatively smaller LMs, on the other hand, suffer drastically in the absence of in-context examples. The availability of labeled examples usually varies across the use cases of the language model. For example, in an NLP research setup, expert users can quickly come up with a few handwritten examples. However, when we consider mass-scale usage of average users who are not experienced prompt engineers or need quick answers, the zero-shot performance of a model becomes extremely crucial. For example, assuming the popular usage of ChatGPT, very few non-expert users would opt to write down examples while asking ChatGPT to perform some tasks. 
This naturally raises the question of whether one can make an LLM generalize from labeled examples of a predefined set of tasks to an input defining a novel task. In the world of biological neurons, such abilities are commonplace: inculcating specific limb usage into an untrained limb while training the opposite limb, or relatively easier adoption of newer skills from the culminated experience of older skills. Drawing a blunt parallel between biological neurons and LLMs would be naive. However, one can find a supporting intuition in the mechanistic interpretation of the Transformer architecture. One can argue that if information pathways necessary to solve a novel task are similar to those corresponding to some different task from a task library, an LLM may gather useful information across tasks. Earlier evidence found by also elicits intuitive motivation as they showed that LLMs can learn to infer from cross-lingual examples if proper alignment is provided. 
In this work, we design a cross-task prompting setup (Section) using three different LLMs: LLaMA-2 7B and 13B, and GPT 3.5; we select 50 different pairs of tasks where one serves as a source (i.e., context example task) and the other as target. Despite no examples from the target task presented in the context, LLMs can produce a staggering improvement over the zero-shot regime; on average, cross-task prompting improves performance by 107%for LLaMA-2 7B, 18.6%for LLaMA-2 13B and 3.2%for GPT 3.5 (Section). With multiple source tasks, cross-task performance is even better than, if not comparable, usual in-task prompting (Contribution # 1). However, learning from examples of different tasks is heavily sensitive to the choice of source task for a given target and the LLM is prone to copy the label space of the source task into the target. To circumvent this, we propose a pseudo-labeling based approach: in a data-scarce setup, cross-task prompting with majority voting is first employed to generate noisy, in-task examples; these are subsequently used for standard few-shot prompting (Contribution # 2). Finally, we provide introductory analysis towards interpreting cross-task signal transfer by dissecting the model activations. We find that the cross-task signal transfer is abrupt and happens at later layers, with the effective layers widely varying for different target tasks (Contribution # 3). In a nutshell, this is the first exploration of LLMs' ability to learn to solve novel tasks based on the contextual signals of different task examples."
CharacterEval: A Chinese Benchmark for Role-Playing Conversational Agent Evaluation,2401.01275v2,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2401.01275v2.pdf,"An example of the CharacterEval, including the dialogue, scene and character's profile.","Recently, the advent of large language models (LLMs) has revolutionized generative agents. Among them, Role-Playing Conversational Agents (RPCAs) attract considerable attention due to their ability to emotionally engage users. However, the absence of a comprehensive benchmark impedes progress in this field. To bridge this gap, we introduce CharacterEval, a Chinese benchmark for comprehensive RPCA assessment, complemented by a tailored high-quality dataset. The dataset comprises 1,785 multi-turn role-playing dialogues, encompassing 11,376 examples and featuring 77 characters derived from Chinese novels and scripts. It was carefully constructed, beginning with initial dialogue extraction via GPT-4, followed by rigorous human-led quality control, and enhanced with in-depth character profiles sourced from Baidu Baike. CharacterEval employs a multifaceted evaluation approach, encompassing thirteen targeted metrics on four dimensions. To facilitate the convenient evaluation for these subjective metrics in CharacterEval, we further developed CharacterRM, a role-playing reward model based on human annotations, which has a higher correlation with human judgment compared to GPT-4. Comprehensive experiments on CharacterEval demonstrate that Chinese LLMs exhibit more promising capabilities than GPT-4 in Chinese role-playing conversation. Source code, data source and reward model will be publicly accessible at <https: //github. com/morecry/CharacterEval>.","The development of large language models (LLMs) has marked the beginning of a new era in conversational AI, and opened up a wide range of application possibilities, particularly in agent-based interactions. The automated agents, equipped with the emerging capabilities of LLMs such as planning, reasoning, and in-context learning, can perform complex tasks for humans without any supervision. Among the diverse agents, the Role-Playing Conversational Agent (RPCA), designed to offer emotional value instead of the productivity, attracts amount of interest. 
RPCA represents a unique category within the realm of conversational agents, distinguished by their capability for immersive interaction. Different from traditional dialogue systems, which typically focus on chit-chat, knowledge-based, personalized and empathetic dialogue, RPCAs engage users in dynamic scenarios, where LLM agents are assumed as specific characters or roles, often derived from existing composition such as novels, films, cartoons, and games. The development of connections between fictional characters and humans has the potential to not only deepen the impact of cultural works but also improve human engagement. Furthermore, RPCAs hold significant application value in their ability to offer emotional value to users, positioning fictional characters as virtual friends. The multifaceted nature of RPCAs has sparked considerable attention, leading to a surge in both research and application development (e.g., Character AI, Tongyi Xingchen and Glow). However, these implementations of RPCAs vary significantly in both approach and objectives, presenting a challenge in systematically assessing and comparing their capabilities. Therefore, we propose the CharacterEval, a Chinese role-playing conversation benchmark for advancing RPCA development. 
To develop a benchmark, the primary problem is the construction of a dataset. While there are existing datasets, their quality are concerning, which are either generated by LLMs or suffering from significant noise due to the extractive methods. These limitations render the evaluation results unreliable for the RPCA 's actual capabilities. To address it, we constructed a Chinese role-playing conversation dataset comprising 1,785 multi-turn role-playing dialogues, encompassing 11,376 examples and 77 leading characters, drawn from diverse Chinese novels and scripts. Our process began with the collection of well-known sources across various genres. After that, GPT-4 was employed to extract dialogue scenes, utterances, and behaviors of the leading roles of these sources. Following basic preprocessing and the removal of dialogues with fewer turns, we invited annotators to assess the quality of the dialogues. Their task was to identify and retain high-quality dialogues, while discarding those of lower quality. Additionally, we crawled detailed character profiles from Baidu Baike, composing a comprehensive dataset for RPCA evaluation. The example from the dataset is as Figure shows. 
Otherwise, role-playing conversation is a complicated task that requires not only mimicking a character' s behavior and utterance but also maintaining the character's knowledge, as well as the excellent multi-turn ability. Considering this, we proposed a multifaceted evaluation approach including thirteen specific metrics on four dimensions for a fair and thorough assessment of RPCAs, Our evaluation approach considered the conversational ability, character consistency, role-playing attractiveness, and utilized a personality back-testing method to evaluate the personality accuracy of a RPCA. To assess conversational ability, we measured the conversational fluency, coherence, and consistency at both the sentence and conversation levels. Character consistency is the most crucial in role-playing conversation. Hence, we evaluated knowledge and persona consistency to measure how vividly an RPCA can simulate a character. This involves assessing knowledge exposure, accuracy, and hallucination for knowledge consistency, and evaluating behavior and utterance consistency for persona consistency. Considering that RPCAs are entertainment-oriented, role-playing attractiveness is also a important elements. We assessed this through human-likeness, communication skill, expression diversity, and empathy. Finally, we introduced personality back-testing. With the collected Myers-Briggs Type Indicator (MBTI) personality types as a reference, we let RPCAs do the MBTI assessment and calculate the MBTI accuracy (personality back-test) as implemented in. 
For convenient re-implementation, we invited 12 annotators to score responses generated by different models for the subjective metrics in our evaluation system. Based the human judgments, we developed a role-playing reward model—CharacterRM, whose correlation with human could surpass state-of-the-art LLM GPT-4. On CharacterEval, We conducted comprehensive evaluations for existing LLMs, encompassing both openand closed-source models. Experimental results shows the broad prospect of existing Chinese LLM while GPT-series models do not take the predominance in Chinese role-playing conversation. 
In summary, our contributions of are as follows: 
 
 * We create a large-scale, high-quality dataset for RPCA evaluation, consisting of 1,785 multi-turn role-playing dialogues, 11,376 example, featuring 77 leading characters from diverse Chinese novels and scripts. 
 * We propose CharacterEval, a new benchmark for RPCAs, which contain comprehensive set of evaluation principles, encompassing thirteen specific metrics on four dimensions. 
 * We develop CharacterRM, a role-playing reward model for evaluating RPCAs in several subjective metrics, achieving the better performance than GPT-4 on correlation with human. 
 * We conducted thorough evaluations of existing LLMs on CharacterEval, including openand closed-source, and derived valuable findings from the results."
Generative Cross-Modal Retrieval: Memorizing Images in Multimodal Language Models for Retrieval and Beyond,2402.10805v1,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2402.10805v1.pdf,Real cases from GPT4 illustrate the necessity of visual outputs for LLMs.,"The recent advancements in generative language models have demonstrated their ability to memorize knowledge from documents and recall knowledge to respond to user queries effectively. Building upon this capability, we propose to enable multimodal large language models (MLLMs) to memorize and recall images within their parameters. Given a user query for visual content, the MLLM is anticipated to ""recall"" the relevant image from its parameters as the response. Achieving this target presents notable challenges, including inbuilt visual memory and visual recall schemes within MLLMs. To address these challenges, we introduce a generative cross-modal retrieval framework, which assigns unique identifier strings to represent images and involves two training steps: learning to memorize and learning to retrieve. The first step focuses on training the MLLM to memorize the association between images and their respective identifiers. The latter step teaches the MLLM to generate the corresponding identifier of the target image, given the textual query input. By memorizing images in MLLMs, we introduce a new paradigm to cross-modal retrieval, distinct from previous discriminative approaches. The experiments demonstrate that the generative paradigm performs effectively and efficiently even with large-scale image candidate sets.","Recently, we have witnessed the explosive development of generative large language models (LLMs), such as GPT series and LLaMA. Undergone extensive pretraining on document corpora and instruction tuning, these language models have demonstrated an impressive ability to memorize a lot of knowledge in their parameters and effectively recall them to answer users 'instructions and queries. As shown in Figure, GPT4 could directly respond to the user' s question, ""Who is Sheldon Cooper? "", without any external document or database. Building upon the advancements of LLMs, multimodal LLMs (MLLMs) have been developed to expand the capabilities beyond text and allow users to express their needs using visual input. 
Despite the impressive capabilities of LLMs and MLLMs, their responses are limited to textual outputs. For instance, a user might ask, ""What does Sheldon Cooper look like? "" as shown in Figure. While the MLLM tries to describe the person 's appearance, it is often said that ""an image is worth a thousand words. "" It would greatly enhance the response capabilities of MLLMs if they could give visual outputs, like a photograph in this case. 
A straightforward solution is to enhance MLLMs with external image synthesis tools, like diffusion models and Generative Adversarial Networks, for visual output capabilities. However, a significant challenge with these modules is their propensity to produce unrealistic or hallucinatory images, which cannot accurately describe real-world images, such as a photograph of ""Sheldon Cooper"". The integration of an image retrieval module seems a more viable solution. Nonetheless, such a combination often encounters a transition gap between two independent modules. Considering the massive benefits of LLMs in memorizing textual knowledge, a bold and innovative idea emerges: Is it possible to equip MLLMs with the ability to memorize visual information within their parameters for retrieval and beyond? In this light, we formulate a generative cross-modal retrieval task: given a user query for visual content, MLLMs are expected to recall desired images from their parameters directly as the response. 
Accomplishing this task poses a significant challenge, necessitating the presence of two essential abilities of MLLMs: 1) Visual memory. As the prerequisite requirement, the MLLM model must possess the capability to memorize visual information within its parameters. This goes beyond simply encoding images into dense vectors within a vector database. It necessitates a distinct, differentiable, and integrated visual memory scheme within MLLMs' parameters. 2) Visual recall. Given a textual query, the MLLM should be able to recall the relevant visual information from the complicated visual memory bank. Above this, for user comprehension, the activated visual information must be grounded to the complete and original images rather than mere patches or fragmented visuals. 
In this work, we propose a novel GeneRAtive Cross-modal rEtrieval framework, GRACE, to overcome the above issues. GRACE assigns images unique identifiers, where each identifier is a distinct string representing an image. Based on the identifiers, GRACE comprises two training steps, as illustrated in Figure. 1) Learning to memorize. Given an image, the MLLM is trained to generate the corresponding identifier string via the standard text generation loss. The goal of this phase is for the MLLM to effectively learn and memorize the associations between the visual content of images and their respective identifiers. 2) Learning to retrieve. The MLLM is trained to generate the identifier string of the relevant image while given a textual query. In this way, the MLLM learns to associate user queries with visual memory. After the two training steps above, GRACE enables generative cross-modal retrieval: given a textual query, the MLLM generates an identifier string corresponding to a real image. 
We delve into GRACE from various perspectives, including different identifier types, effectiveness, and efficiency of the generative paradigm. We evaluate GRACE on text-image matching datasets to verify the feasibility of generative cross-modal retrieval. Without any image 's visual information during inference, GRACE performs comparably to the advance one-tower approaches (e.g., CLIP) and demonstrates higher efficiency with large-scale image sizes. It is acknowledged that as a new retrieval paradigm, GRACE still lags behind one-tower approaches. One-tower approaches are only applicable to ranking stage due to their low efficiency, while GRACE and CILP are specifically designed for the retrieval stage. By comprehensive analysis, we hope to comprehensively understand its capabilities and limitations. 
We believe exploring generative cross-modal retrieval holds great significance. 
 
 * Benefiting from inbuilt visual memory within MLLMs, GRACE introduces a new paradigm to cross-modal retrieval. GRACE transforms the original matching problem into a generation problem, eliminating the need for negative samples during training and retrieval index during inference. No matter the size of the image set, the retrieval efficiency remains constant. This new cross-modal retrieval paradigm leaves much room for investigation. 
 * Inbuilt visual memory serves for retrieval, yet its utility extends beyond mere retrieval. In Section, we demonstrate that the MLLM could describe the memorized image and even answer questions about the memorized images, just like humans do. This opens up the possibility of injecting personalized visual experiences of humans into MLLMs for them to memorize and understand an individual' s journey, and accomplish more visual tasks."
Self-Training with Pseudo-Label Scorer for Aspect Sentiment Quad Prediction,2406.18078v1,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2406.18078v1.pdf,Illustration of our pseudo-label scorer.,"Aspect Sentiment Quad Prediction (ASQP) aims to predict all quads (aspect term, aspect category, opinion term, sentiment polarity) for a given review, which is the most representative and challenging task in aspect-based sentiment analysis. A key challenge in the ASQP task is the scarcity of labeled data, which limits the performance of existing methods. To tackle this issue, we propose a self-training framework with a pseudo-label scorer, wherein a scorer assesses the match between reviews and their pseudo-labels, aiming to filter out mismatches and thereby enhance the effectiveness of self-training. We highlight two critical aspects to ensure the scorer's effectiveness and reliability: the quality of the training dataset and its model architecture. To this end, we create a human-annotated comparison dataset and train a generative model on it using ranking-based objectives. Extensive experiments on public ASQP datasets reveal that using our scorer can greatly and consistently improve the effectiveness of self-training. Moreover, we explore the possibility of replacing humans with large language models for comparison dataset annotation, and experiments demonstrate its feasibility.","Aspect-Based Sentiment Analysis (ABSA) aims to recognize aspect-level opinions and sentiments from user-generated content. This problem has consistently attracted interest owing to its proficiency in distilling and summarizing fine-grained opinions from vast data. The most representative and challenging task in ABSA is Aspect Sentiment Quad Prediction (ASQP). This task formulates aspect-level opinions and sentiments as quadruples, each consisting of an aspect term, aspect category, opinion term, and sentiment polarity. For example, given a review ""the food is great and reasonably priced, "" the output of ASQP would be { (food, food_ quality, great, positive), (food, food _ prices, reasonably priced, positive) }. 
As a fine-grained problem, ABSA faces the challenge of insufficient labeled data, which is particularly severe in the ASQP task. This issue limits the performance of existing models. Many efforts explore data augmentation methods to alleviate this issue. They synthesize new samples by modifying existing ones, applying self-training techniques, or utilizing generative methods. However, a significant limitation of these methods is that the synthetic samples often inevitably contain mismatches between sentences and labels, which can adversely affect model learning. 
To reduce such mismatches, this paper introduces a pseudo-label scorer for data augmentation. As illustrated in Figure, the scorer assesses the degree of match between the review and its pseudo-label. If we have a sufficiently robust scorer, we can filter out all mismatched samples, thereby significantly enhancing the effectiveness of data augmentation. We propose that the effectiveness and reliability of this scorer hinge on two critical aspects: (1) the quality of the training dataset and (2) its architecture along with the training objective. We discuss these two aspects below. 
For the first aspect, previous works typically produce negative labels by modifying real labels using heuristic rules. However, such negative labels are usually simplistic and patterned, limiting the scorer 's learning. To overcome this limitation, we create a human-annotated comparison dataset. Specifically, we train an ASQP model with existing labeled data, use it to infer several pseudo-labels for unlabeled data, and then have human annotators choose the most appropriate pseudo-labels. The labels chosen by annotators are designated as positive labels, while the rest as negative labels. Our dataset, in contrast to the rule-based datasets, is more challenging and better aligned with human judgment. 
For the second aspect, previous works formalize label-scoring as a question-answering problem or embed the discriminative matching token into the label. However, our findings suggest that these methods underperform in complex tasks like ASQP, due to their limited capacity to model the interactions between reviews and pseudo-labels. Recent works in preference optimization reveal that the language model itself can serve as a scorer. This motivates us to use the conditional likelihoods that a generative model assigns to a pseudo-label as the measure of its quality. Compared with the previous methods, this approach enables the scorer to examine the plausibility of a pseudo-label in a token-by-token fashion, thus offering a more comprehensive and effective scoring. We then fine-tune this scorer on our comparison dataset using ranking-based objectives. 
Upon developing this pseudo-label scorer, we apply it in a data augmentation framework, specifically opting for the self-training framework due to its simplicity. We conduct extensive experiments on public ASQP datasets to examine its effectiveness and further investigate the following questions: (1) how does the pseudo-label scorer perform using our comparison data and model architecture? ; (2) is it feasible to replace humans with large language models to annotate the comparison data? ; and (3) how to utilize the scorer to filter out low-quality samples? Furthermore, inspired by, we extend the application of this scorer, employing it as a reranker for multiple candidate labels, and assess its impact and effectiveness. 
Our contributions are summarized as follows: (1) To the best of our knowledge, we are the first to apply a pseudo-label scorer to data augmentation in the ASQP task. (2) We investigate how to enhance the scorer' s effectiveness and reliability from both dataset and model architecture perspectives. (3) We empirically demonstrate that the proposed pseudo-label scorer can significantly and consistently enhance the performance of existing models."
Self-Training with Direct Preference Optimization Improves Chain-of-Thought Reasoning,2407.18248v1,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2407.18248v1.pdf,"Our approach demonstrates superior performance on the GSM8K benchmark while minimizing the required compute cost, including both training and inference. Compute cost calculations are based on the methodology outlined by.","Effective training of language models (LMs) for mathematical reasoning tasks demands high-quality supervised fine-tuning data. Besides obtaining annotations from human experts, a common alternative is sampling from larger and more powerful LMs. However, this knowledge distillation approach can be costly and unstable, particularly when relying on closed-source, proprietary LMs like GPT-4, whose behaviors are often unpredictable. In this work, we demonstrate that the reasoning abilities of small-scale LMs can be enhanced through self-training, a process where models learn from their own outputs. We also show that the conventional self-training can be further augmented by a preference learning algorithm called Direct Preference Optimization (DPO). By integrating DPO into self-training, we leverage preference data to guide LMs towards more accurate and diverse chain-of-thought reasoning. We evaluate our method across various mathematical reasoning tasks using different base models. Our experiments show that this approach not only improves LMs' reasoning performance but also offers a more cost-effective and scalable solution compared to relying on large proprietary LMs.","Making language models (LMs) perform mathematical reasoning is a valuable, yet challenging research objective. Recent efforts have focused on enhancing large-scale LMs 'reasoning abilities through various methods, including chain-of-thought prompting, continual pretraining, and adding external verifiersq. However, the research question of how to enhance the reasoning capabilities of smaller-sized LMs remains relatively under-explored. 
Recent studies demonstrate that the reasoning capabilities of smaller LMs can be significantly enhanced through learning from the outputs of larger and more advanced LMs, such as Codex, PaLM, and GPT-4. While this method is straightforward to implement, the associated costs can be substantial. The computational demand, measured in floating-point operations (FLOPs), increases considerably when using large LMs. Additionally, the reliance on proprietary large LMs for data annotation not only incurs high economic costs but also raises concerns regarding the sustainability and scalability of such practices. For instance, highlighted that while employing large LMs as annotators can largely enhance the performance of smaller LMs, it introduces a clear trade-off between economic costs and performance gains. 
 All methods presented here are integrated with an external calculator except for the Codex distillation by. 
Another line of research focuses on exploring enhancements through self-improvement methods. These methods diverge from using outputs from larger models, instead encouraging LMs to learn from their own generated data. The effectiveness of these techniques is evident, yet their success largely depends upon the inherent capabilities of the base models. For example, initiated self-improvement by few-shot prompting GPT-J, a relatively large LM which has 6 billion parameters, to generate rationales – an emergent ability typically reserved for large models. However, the extent to which small-scale LMs can gain from self-improvement remains uncertain. 
In this work, we introduce a novel enhancement to the conventional self-training framework by incorporating Direct Preference Optimization (DPO). This integration specifically targets performance objectives within chain-of-thought reasoning, with a particular focus on mathematical reasoning. The clear-cut nature of mathematical solutions enables straightforward validation of a model' s outputs, facilitating the creation of a preference dataset for DPO. Our empirical results indicate that this method notably enhances the reasoning capabilities of LMs while also reducing computational overhead. We visualize the relationship between the GSM8K performance and computational cost across various specialized models in Figure. It can be observed that our method not only achieves strong performance, but also reduces computational demands by effectively utilizing self-generated data for learning. Overall, the main contribution of this work can be summarized as follows: 
 
 * We propose a novel extension to the classic self-training framework by integrating Direct Preference Optimization, demonstrating its effectiveness across various math reasoning tasks. 
 * Our method significantly enhances the reasoning abilities of language models while requiring minimal computational resources, optimizing both performance and efficiency. 
 * We present an efficient method for integrating LMs with external tools, which significantly boosts downstream task performance without notably compromising inference speed."
Investigating and Mitigating the Multimodal Hallucination Snowballing in Large Vision-Language Models,2407.00569v4,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2407.00569v4.pdf,"An example of the LVLM assisting a visually impaired person to cross the street. The model is misled by the generated hallucination and mistakenly suggests the user to cross the street, although it can give correct advice independently. [colback=green1] Green and [colback=red1] red colors highlight the correct answer and hallucinations, respectively.","Though advanced in understanding visual information with human languages, Large Vision-Language Models (LVLMs) still suffer from multimodal hallucinations. A natural concern is that during multimodal interaction, the generated hallucinations could influence the LVLMs 'subsequent generation. Thus, we raise a question: When presented with a query relevant to the previously generated hallucination, will LVLMs be misled and respond incorrectly, even though the ground visual information exists? To answer this, we propose a framework called MMHalSnowball to evaluate LVLMs' behaviors when encountering generated hallucinations, where LVLMs are required to answer specific visual questions within a curated hallucinatory conversation. Crucially, our experiment shows that the performance of open-source LVLMs drops by at least 31%, indicating that LVLMs are prone to accept the generated hallucinations and make false claims that they would not have supported without distractions. We term this phenomenon Multimodal Hallucination Snowballing. To mitigate this, we further propose a training-free method called Residual Visual Decoding, where we revise the output distribution of LVLMs with the one derived from the residual visual input, providing models with direct access to the visual information. Experiments show that our method can mitigate more than 24%of the snowballed multimodal hallucination while maintaining capabilities.","Large Vision-Language Models (LVLMs) have shown remarkable abilities in observing and understanding the real world in human languages. However, multimodal hallucinations, in which LVLMs provide responses misaligned with the corresponding visual information, remain to be the Achilles 'heel. 
Previous research has revealed that hallucinations generated by large language models may accumulate due to models' over-commitment to early mistakes, leading to more mistakes that they otherwise would not make, especially for the user-model interaction scenarios such as conversation. However, the extent to which accumulated multimodal hallucinations mislead LVLMs into generating false claims requires further exploration. In this work, we conducted an investigation into this issue for the first time. As shown in Figure, we seek the answer to the question: When presented with a query relevant to the previously generated hallucination that contradicts the visual information, can models make the correct judgment when they could have given a correct answer independently? We conduct a preliminary studyon GPT-4V, LLaVA 1.5, and mPLUG-Owl2. Similar to the setting of Figure, given an image, we start a conversation by asking the model to describe the image in detail. When observing hallucinations in the LVLM 's responses, we continue to ask a relevant question according to the model-generated hallucination. In addition, we ask the same question separately to see if the model can answer it correctly without distractions. As demonstrated in Figure (a), we find that when the text context contains relevant hallucination, the model performance declines significantly, compared to the model response when asking the same question separately. We further select those question samples that the LVLM can correctly answer separately, and manually identify the response change when asking the same question with the related model-generated hallucinatory context. As Figure (b) depicts, we find that more than 59%of the answers are semantically the same as the generated hallucination, indicating that they were misled by the previously generated hallucinations. 
To systematically investigate this phenomenon, we propose to identify whether the LVLM is misled by hallucinations via checking if a specific claim is flipped due to previous hallucinations. We design a framework called MMHalSnowball to construct hallucinatory visual conversations, where models are required to answer the question based on the image and the hallucinatory conversation. The result shows that LVLMs' multimodal hallucinations are easy to mislead the later generation because their strong language capabilities make them prone to be over-confident in the hallucinated context, thereby generating false claims that they normally would not support, which we term as Multimodal Hallucination Snowballing. 
In addition to mitigating this issue, we further proposed a training-free decoding method called Residual Visual Decoding (RVD). By residual connecting the visual information and the current user instruction, distributions that emphasizing the visual information are derived to revise the original output distribution. Our RVD achieves more than 24%of improvements in reducing the multimodal hallucination snowballing while maintaining the contextual modeling ability."
Beyond Traditional Benchmarks: Analyzing Behaviors of Open LLMs on Data-to-Text Generation,2401.10186v3,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2401.10186v3.pdf,"To benchmark LLMs, we download unlabeled structured data from public APIs and prompt LLMs to generate texts based on the data. We annotate semantic errors in the outputs using reference-free metrics.","We analyze the behaviors of open large language models (LLMs) on the task of data-to-text (D2T) generation, i.e., generating coherent and relevant text from structured data. To avoid the issue of LLM training data contamination with standard benchmarks, we design Quintd – a tool for collecting novel structured data records from public APIs. We find that open LLMs (Llama 2, Mistral, and Zephyr) can generate fluent and coherent texts in zero-shot settings from data in common formats collected with Quintd. However, we show that the semantic accuracy of the outputs is a major issue: both according to human annotators and our reference-free metric based on GPT-4, more than 80%of the outputs of open LLMs contain at least one semantic error. We publicly release the code, data, and model outputs.","Large language models (LLMs; ) have already left a mark in many areas of natural language processing (NLP). Surprisingly, their applicability to the task of data-to-text (D2T) generation remains underexplored, with limited evaluation on a handful of well-established benchmarks only. Generating text from structured data is arguably challenging for LLMs, given the specifics of D2T generation, such as long inputs, complex non-linear structure, and strict requirements on semantic accuracy. However, a more significant issue is the lack of testing grounds. The current D2T generation benchmarks are not only getting saturated, but also promote optimization towards traditional reference-based evaluation metrics, which were shown to correlate poorly with human judgment. When it comes to the models, using closed LLMs is increasingly considered a bad research practice due to its non-reproducibility. On top of that, contamination of LLM training data with standard benchmarks further restricts the space for experiments. 
In this paper, we propose an approach that allows us to analyze model behavior in D2T generation on novel, real-world structured data records with reference-free evaluation metrics. We begin by realizing that unlabeled data are plentiful. To leverage the data for our experiments, we introduce Quintd – a tool for collecting structured data from five domains in standard formats: JSON, CSV, and Markdown. We choose the domains so that the data can be directly used as input for five distinct D2T generation tasks. Our tasks include generating weather forecasts, sports reports, product descriptions, chart captions, and entity descriptions (see). Next, we collect a set of 1,000 inputs with Quintdand use the inputs as an ad-hoc benchmark (called Quintd-1) for testing the abilities of LLMs for D2T generation. We assume that the data formats in Quintd-1are common in the LLMs' pretraining corpora, so we specify the task using instructions instead of standard finetuning with human-written outputs, capitalizing on the zero-shot abilities of instruction-tuned LLMs (§). 
We push towards better reproducibility by focusing on open LLMs, which – apart from being more accessible – also achieve increasingly better results across tasks. For our experiments, we use three open LLMs with 7B parameters: Llama 2, Mistral, and Zephyr. We also use GPT-3.5 as a closed model baseline for the final experiments. Given the behavioral nature of the experiments with LLMs, we put emphasis on reporting model behavior throughout the process (§). 
Another piece of the puzzle is reference-free evaluation: using the input data as a ground for comparison instead of reference outputs (§). We focus on identifying semantic errors in the model outputs, i.e., the information that is not supported by the input data. We use two separate evaluation methods: manual annotations from human crowdworkers and a custom automatic metric based on GPT-4. We annotate the errors on the level of individual words, getting fine-grained annotations of error spans in several categories. 
Based on our results, we provide general recommendations for D2T generation with open LLMs across tasks and formats (§). Our main findings are as follows: 
 
 * Open LLMs can generate fluent outputs from structured data in common formats under zero-shot settings. 
 * Semantic accuracy is a major obstacle: both human annotators and GPT-4-based metric report that over 80%of outputs of open LLMs on our data contain a semantic error. 
 * Long data inputs cause practical issues, including the need for long-context models, increased GPU memory requirements, and unavailability of few-shot approaches. 
 * Outputs can be empirically improved by following several rules-of-thumb for preprocessing the model input, such as including units, removing unnecessary fields, or prefixing the model answer."
One Prompt To Rule Them All: LLMs for Opinion Summary Evaluation,2402.11683v2,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2402.11683v2.png,"G-Eval vs. Op-I-Prompt. On closed-source model (ChatGPT-3.5) our Op-I-Prompt shows comparable performance whereas on open-source model (Mistral-7B) our approach outperforms G-Eval on 7 dimensions: fluency (FA), coherence (CO), relevance (RE), faithfulness (FA), aspect coverage (AC), sentiment consistency (SC), and specificity (SP). Check Figure for more details.","Evaluation of opinion summaries using conventional reference-based metrics often fails to provide a comprehensive assessment and exhibits limited correlation with human judgments. While Large Language Models (LLMs) have shown promise as reference-free metrics for NLG evaluation, their potential remains unexplored for opinion summary evaluation. Furthermore, the absence of sufficient opinion summary evaluation datasets hinders progress in this area. In response, we introduce the SummEval-Op dataset, encompassing 7 dimensions crucial to the evaluation of opinion summaries: fluency, coherence, relevance, faithfulness, aspect coverage, sentiment consistency, and specificity. We propose Op-I-Prompt, a dimension-independent prompt, along with Op-Prompts, a dimension-dependent set of prompts for opinion summary evaluation. Our experiments demonstrate that Op-I-Prompt emerges as a good alternative for evaluating opinion summaries, achieving an average Spearman correlation of 0.70 with human judgments, surpassing prior methodologies. Remarkably, we are the first to explore the efficacy of LLMs as evaluators, both on closed-source and open-source models, in the opinion summary evaluation domain.","Opinion summarization systems predominantly use traditional metrics such as Rouge and BertScore for automatic evaluation, however, they have been shown to have poor correlations with human judgments. Moreover, these metrics fall short of comprehensively evaluating opinion summaries. Additionally, obtaining reference-based datasets at a large scale is an expensive process. 
Recently, Large Language Models (LLMs) have been utilized as reference-free evaluators for Natural Language Generation (NLG) outputs. The idea is to prompt a powerful LLM such as ChatGPT-3.5/GPT-4 to evaluate an output on certain criteria. However, their suitability has not been explored at all for evaluating opinion summaries. Moreover, these approaches have been tested only on closed-source models (ChatGPT-3.5/GPT-4) primarily because of the limitations of the open-source models in following instructions and producing the desired output. 
To this end, we first create SummEval-Op, a reference-free opinion summarization dataset covering 7 dimensions, for the e-commerce domain. Next, we present Op-I-Prompt and Op-Prompts tailored for opinion summary evaluation. We investigate their suitability to both closed-source and open-source models. Experiments reveal that Op-I-Prompt emerges as a good alternative for evaluating opinion summaries across all 7 dimensions. 
Our contributions are: 
 
 * SummEval-Op, an opinion summary evaluation benchmark dataset, consisting of a total of 2,912 summary annotations, assessing 13 opinion summaries for 32 products from the Amazon test set. The evaluation covers 7 dimensionsfluency, coherence, relevance, faithfulness, aspect coverage, sentiment consistency, and specificity related to the evaluation of opinion summaries (Section). 
 * Op-I-Prompt, a dimension-independent prompt and Op-Prompts, a dimension-dependent set of prompts, enabling opinion summary evaluation for all the 7 dimensions. Experiments indicate that the Op-I-Prompt generally outperforms existing approaches on both closed-source and open-source models by 9%on average in correlation with human judgments (Figure, Section). To the best of our knowledge we are the first to test the applicability of different prompt approaches on open-source LLMs. 
 * Benchmarking of 9 recent LLMs (closed and open-source) on the aforementioned 7 dimensions for the task of opinion summarization, which to the best of our knowledge is first of its kind (Table, Section). 
 * Detailed analysis, comparing an open-source LLM against a closed-source LLM acting as evaluators for automatic evaluation of opinion summaries on 7 dimensions. Analysis indicates that Op-I-Prompt emerges as a good alternative for evaluating opinion summaries showing a high correlation (spearman correlation of 0.70 on average) with humans when compared with alternatives (Section)."
VIEScore: Towards Explainable Metrics for Conditional Image Synthesis Evaluation,2312.14867v2,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2312.14867v2.pdf,We study the correlation between MLLMs and human perspectives on rating images.,"In the rapidly advancing field of conditional image generation research, challenges such as limited explainability lie in effectively evaluating the performance and capabilities of various models. This paper introduces VIEScore, a Visual Instruction-guided Explainable metric for evaluating any conditional image generation tasks. VIEScoreleverages general knowledge from Multimodal Large Language Models (MLLMs) as the backbone and does not require training or fine-tuning. We evaluate VIEScoreon seven prominent tasks in conditional image tasks and found: (1) VIEScore (GPT4-o) achieves a high Spearman correlation of 0.4 with human evaluations, while the human-to-human correlation is 0.45. (2) VIEScore (with open-source MLLM) is significantly weaker than GPT-4o and GPT-4v in evaluating synthetic images. (3) VIEScoreachieves a correlation on par with human ratings in the generation tasks but struggles in editing tasks. With these results, we believe VIEScoreshows its great potential to replace human judges in evaluating image synthesis tasks.","Diffusion models have become a focal point in AI research for image synthesis. Over the past year, several new models have been introduced to enhance control over image generation. However, comprehensively evaluating AI-synthesized images remains a challenging and unresolved issue. While metrics like LPIPS, CLIP-Score, and DreamSim were proposed, they have certain limitations: (1) these metrics are agnostic the end task, which can fail to measure the desired aspects of the generated images, (2) the score is opaque with limited explainability. These limitations heavily restrict their effectiveness in assessing conditional image generation. Some research work relied on human-driven evaluation methods. While humans excel at understanding and interpreting visual content, such methods in the context are facing challenges such as scalability limits and preference subjectivity issues. This reliance on human judgment highlights the need for more uniform evaluation methods in the field. To solve the mentioned issues, we formulate the problem definition with our desired properties, as presented in equation. The function f takes an instruction I, a synthesized image O, and C^* which is a set of conditions (e.g.,style, subject, background, canny-edge, etc). The score function should produce the intermediate rationale in the form of natural language before generating the final score according to the prompt instruction I: 
 f_VIE (I, O, C^*) = (rationale, score) The function f can be any Multimodal Large Language Model (MLLM) such as GPT-4 and LLaVA, which can take input images to generate human-like text responses. Unlike the automatic metrics, MLLM can receive human instructions and produce rationale. With such motivation, we introduce VIEScore (Visual Instruction-guided Explainable Score), a framework to assess synthetic images in different conditional image generation tasks. VIEScorehas multiple advantages compared to auto-metrics and human evaluation. It includes: 
Task Awareness. Existing metrics were often designed to measure a certain aspect of generated images. For example, LPIPS measures the perceptual similarity of a pair of images, while CLIP-Score measures the text alignment of one single image. As a consequence, these metrics cannot be adapted to evaluate other tasks. VIEScoreacts as a silver bullet to tackle all conditional image generation evaluation processes due to its instruction-guiding property. It can be carefully adjusted with different instruction requirements. 
Explainability. The existing metrics normally output a single float-point score, which cannot offer detailed insights into the 'rationale' behind its evaluations. Such a score makes it difficult to interpret the decisions from the metric output. Instead, VIEScorecan offer the rationale in the form of natural languages to help humans understand the reasoning process. As depicted in Figure, the rationale can significantly improve the trustworthiness of VIEScore. 
While the ultimate goal is to derive an MLLM that can rate images like humans, in this paper we also explore how well MLLMs can assess synthetic images compared to human evaluation and present insights and challenges on state-of-the-art MLLMs towards human evaluators, as shown in Figure."
RAID: A Shared Benchmark for Robust Evaluation of Machine-Generated Text Detectors,2405.07940v2,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2405.07940v2.pdf,Detectors for machine-generated text are often highly performant on default model settings but fail to detect more unusual settings such as using random sampling with a repetition penalty.,"Many commercial and open-source models claim to detect machine-generated text with extremely high accuracy (99%or more). However, very few of these detectors are evaluated on shared benchmark datasets and even when they are, the datasets used for evaluation are insufficiently challenging—lacking variations in sampling strategy, adversarial attacks, and open-source generative models. In this work we present RAID: the largest and most challenging benchmark dataset for machine-generated text detection. RAID includes over 6 million generations spanning 11 models, 8 domains, 11 adversarial attacks and 4 decoding strategies. Using RAID, we evaluate the out-of-domain and adversarial robustness of 8 openand 4 closed-source detectors and find that current detectors are easily fooled by adversarial attacks, variations in sampling strategies, repetition penalties, and unseen generative models. We release our data along with a leaderboard to encourage future research.","Large Language Models (LLMs) have been able to fool humans into thinking their outputs are human-written for roughly four years. In that short span of time we have seen LLM-generated text be used for targeted phishing attacks, mass spam and harassment, disinformation campaigns, and spurious scientific publication. In order to document and eventually mitigate such harms, we must develop robust automatic detectors of machine-generated text. 
Many exciting and inventive methods have been proposed in recent years for detecting generated text. However, when evaluating these methods, authors typically generate their own evaluation datasets and fail to test their models on shared resources—making it difficult to verify claims of accuracy and robustness. This has led to an erosion of trust in the efficacy of automatic detection methods and a generally fatalistic sentiment towards detection among researchers and practitioners. 
To combat this trend, in this work, we introduce the Robust AI Detection (RAID) benchmark. RAID is the largest and most challenging benchmark of generated text ever released, consisting of 6M+ generations spanning 11 generators, 8 domains, 11 adversarial attacks, and 4 decoding strategies. Using RAID, we benchmark 12 detectors (8 openand 4 closed-source). We find that detectors have difficulty generalizing to unseen models and domains and that simple changes such as changing the sampling strategy, adding a repetition penalty, and adversarially modifying text lead to marked decreases in performance."
LLM in a flash: Efficient Large Language Model Inference with Limited Memory,2312.11514v3,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2312.11514v3.pdf,"Average inference latency for a single token when only half of the model's memory is available: Our method selectively loads parameters on demand for each token generation step. The latency represents the time required to repeatedly load parameters from flash memory, combined with the time needed for computations.","Large language models (LLMs) are central to modern natural language processing, delivering exceptional performance in various tasks. However, their substantial computational and memory requirements present challenges, especially for devices with limited DRAM capacity. This paper tackles the challenge of efficiently running LLMs that exceed the available DRAM capacity by storing the model parameters in flash memory, but bringing them on demand to DRAM. Our method involves constructing an inference cost model that takes into account the characteristics of flash memory, guiding us to optimize in two critical areas: reducing the volume of data transferred from flash and reading data in larger, more contiguous chunks. Within this hardware-informed framework, we introduce two principal techniques. First, ""windowing"" strategically reduces data transfer by reusing previously activated neurons, and second, ""row-column bundling"", tailored to the sequential data access strengths of flash memory, increases the size of data chunks read from flash memory. These methods collectively enable running models up to twice the size of the available DRAM, with up to 4x and 20x increase in inference speed compared to naive loading approaches in CPU and GPU, respectively. Our integration of sparsity awareness, context-adaptive loading, and a hardware-oriented design paves the way for effective inference of LLMs on devices with limited memory.","In recent years, large language models (LLMs) have demonstrated strong performance across a wide range of natural language tasks. However, the unprecedented capabilities of these models come with substantial computational and memory requirements for inference. LLMs can contain hundreds of billions or even trillions of parameters, which makes them challenging to load and run efficiently, especially on personal devices. 
Currently, the standard approach is to load the entire model into DRAM (Dynamic Random Access Memory) for inference. However, this severely limits the maximum model size that can be run. For example, a 7 billion parameter model requires over 14GB of memory just to load the parameters in half-precision floating point format, exceeding the capabilities of most personal devices such as smartphones. While it is possible to employ techniques such as quantization to reduce the model size, still, this cannot address the main limitation of loading the entire model into DRAM. 
To address this limitation, we propose to store the model parameters in flash memory, which is at least an order of magnitude larger than DRAM. Then, during inference, we directly load the required subset of parameters from the flash memory, avoiding the need to fit the entire model in DRAM. To this end, our work makes several contributions: 
 * First, we study the hardware characteristics of storage systems (e.g., flash, DRAM). We show that hardware constraints such as capacity and bandwidth limitations can have significant considerations when designing efficient algorithms for serving LLMs from flash (Section). 
 * Motivated by our findings, we propose several techniques that can help with (i) reducing the required data transfer, (ii) increasing the transfer throughput, and (iii) managing loaded parameters efficiently in DRAM (Section). 
 * Finally, as partially demonstrated in Figure, we show that our proposed techniques for optimizing the cost model and selectively loading parameters on demand allows us to run models 2x larger than the device's DRAM capacity and speed up inference up to 4x, 7x, and 20x compared to naive implementation in CPU, Metal and NVIDIA GPU backends, respectively (Section)."
Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models,2306.05424v2,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2306.05424v2.png,"Architecture of Video-ChatGPT. Video-ChatGPT leverages the CLIP-L/14 visual encoder to extract both spatial and temporal video features. This is accomplished by averaging frame-level features across temporal and spatial dimensions respectively. The computed spatiotemporal features are then fed into a learnable linear layer, which projects them into the LLMs input space. In our approach, we utilize the Vicuna-v1.1 model, comprised of 7B parameters, and initialize it with weights from LLaVA.","Conversation agents fueled by Large Language Models (LLMs) are providing a new way to interact with visual data. While there have been initial attempts for image-based conversation models, this work addresses the under-explored field of video-based conversation by introducing Video-ChatGPT. It is a multimodal model that merges a video-adapted visual encoder with an LLM. The resulting model is capable of understanding and generating detailed conversations about videos. We introduce a new dataset of 100,000 video-instruction pairs used to train Video-ChatGPT acquired via manual and semi-automated pipeline that is easily scalable and robust to label noise. We also develop a quantitative evaluation framework for video-based dialogue models to objectively analyze the strengths and weaknesses of video-based dialogue models. Code: <https: //github. com/mbzuai-oryx/Video-ChatGPT>. [1] Equal contribution.","The surge of deep learning applications for video understanding has lead to major advancements in video-related tasks. However, the current video understanding models are still unable to hold an open-ended conversation about the video content in a coherent manner. A video-based dialogue model can revolutionize video search, surveillance operations and help summarize key events and abnormal event detection. Above all, it can provide a unified human-understandable interface to video-related tasks such as action recognition, localization, detection, segmentation, retrieval, and tracking. Further, such a capability is of great interest as it will demonstrate the model's ability to encode temporal and spatial cues, contextual relationships and long-term dependencies. 
Recent advancements in multimodal understanding are largely based on the combination of pretrained image models with Large Language Models (LLMs) but generally do not consider video inputs. It is therefore interesting to leverage the vast capabilities of LLMs for video understanding tasks in a way that would not only maintain the temporal and spatial characteristics but also be adept at generating human-like conversations about videos. In this paper, we introduce Video-ChatGPT, a novel multimodal model that merges the representational abilities of a pretrained visual encoder and the generative powers of an LLM, capable of understanding and conversing about videos. 
Video-ChatGPT leverages an adapted LLM that integrates the visual encoder of CLIP with Vicuna as a language decoder, fine-tuned on generated instructional image-text pairs. Our approach further adapts the design for spatiotemporal video modeling and fine-tunes the model on video-instruction data to capture temporal dynamics and frame-to-frame consistency relationships available in video data. In contrast to other concurrent works for video-based conversation, Video-ChatGPT excels at temporal understanding, spatial consistency and contextual comprehension as demonstrated by our extensive evaluations. 
A fundamental contribution of this work is the creation of a dataset of 100,000 video-instruction pairs using a combination of human-assisted and semi-automatic annotation methods. Each pair consists of a video and its associated instruction in the form of a question-answer. This provides Video-ChatGPT with a large and diverse dataset to learn from, increasing its video-specific understanding, attention to temporal relationships and conversation capabilities. 
Moreover, we introduce the first quantitative video conversation evaluation framework for benchmarking, allowing for a more accurate evaluation of the performance of video conversation models. This framework evaluates models on a variety of capabilities, such as correctness of information, detail orientation, contextual understanding, temporal understanding, and consistency. 
The contributions of this work are as follows,
 
 
 * We propose Video-ChatGPT, a video conversation model capable of generating meaningful conversations about videos. It combines the capabilities of LLMs with a pretrained visual encoder adapted for spatiotemporal video representations. 
 * We introduce 100,000 high-quality video instruction pairs together with a novel annotation framework that is scalable and generates a diverse range of video-specific instruction sets. 
 * We develop the first quantitative video conversation evaluation framework for benchmarking video conversation models. We demonstrate Video-ChatGPT to perform well compared to concurrent conversational engines for videos such as Video Chat."
Generating Coherent Sequences of Visual Illustrations for Real-World Manual Tasks,2405.10122v1,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2405.10122v1.png,The properties of the elements in illustrations should remain coherent throughout the whole sequence.,"Multistep instructions, such as recipes and how-to guides, greatly benefit from visual aids, such as a series of images that accompany the instruction steps. While Large Language Models (LLMs) have become adept at generating coherent textual steps, Large Vision/Language Models (LVLMs) are less capable of generating accompanying image sequences. The most challenging aspect is that each generated image needs to adhere to the relevant textual step instruction, as well as be visually consistent with earlier images in the sequence. To address this problem, we propose an approach for generating consistent image sequences, which integrates a Latent Diffusion Model (LDM) with an LLM to transform the sequence into a caption to maintain the semantic coherence of the sequence. In addition, to maintain the visual coherence of the image sequence, we introduce a copy mechanism to initialise reverse diffusion processes with a latent vector iteration from a previously generated image from a relevant step. Both strategies will condition the reverse diffusion process on the sequence of instruction steps and tie the contents of the current image to previous instruction steps and corresponding images. Experiments show that the proposed approach is preferred by humans in 46.6%of the cases against 26.6%for the second best method. In addition, automatic metrics showed that the proposed method maintains semantic coherence and visual consistency across the sequence of visual illustrations.","When humans undertake a task with numerous intricate steps, merely reading a step description is limiting, leaving the user to imagine and infer some of the more nuanced details. Complementing the textual step instructions with images enhances the user experience by better communicating and representing the text semantics and ideas. 
Although prompt-based image generation has advanced significantly, state-of-the-art (SOTA) models such as Latent Diffusion Models (LDMs) still struggle when generating image sequences to accompany textual instruction steps. The challenge lies in effectively combining two key aspects: (a) accurately portraying the actions outlined in the step instructions, and (b) ensuring coherence between successive images to avoid confusing the user. Existing storytelling approaches operate mostly on linear storytelling and use synthetic cartoon datasets with explicit sequence information, i.e., the textual prompts describe the images appropriately and have no implicit co-references. These aspects limit the applicability of existing methods to real-world scenarios (Figure), where there is a lack of informative prompts accompanying images, and dependencies between prompts are not necessarily linear. 
In this paper, we explore the generation of image sequences within two domains: recipe instructions, and Do It Yourself (DIY) guides, both showing increasing online consumption. In these domains, accuracy and coherence are of utmost importance to ensure that the result of all manual actions is correct, and that the user is correctly guided to the target output, Figure. These domains contain (i) complex sequential manual tasks of detailed actions, (ii) coherence requirements for the images accompanying the sequence step descriptions, and (iii) a non-linear sequential structure, where steps may be related to earlier steps–not necessarily the previous step. 
To tackle these challenges, we propose to extend Latent Diffusion Models, with an LLM decoder to semantically condition the reverse diffusion process in the sequence of steps and a copy mechanism to select the best LDM initialisation. The image generation process is conditioned on the current step and the previous steps, to increase semantic coherence. In addition, our method initializes the reverse diffusion process with a latent vector iteration copied from a previous generation process to ensure the visual coherence of the generated image. Through this dual attendance to past textual and visual items in the sequence, we aim to achieve semantic coherence, which pertains to the presence and persistence of objects in consecutive images, and visual coherence, which aims to ensure the consistency of backgrounds and visual object properties across successive images. 
Extensive automatic and manual evaluations confirmed that our model outperforms strong baselines in terms of the overall quality of the generated sequence of illustrations in the cooking and DIY domains."
CritiqueLLM: Towards an Informative Critique Generation Model for Evaluation of Large Language Model Generation,2311.18702v2,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2311.18702v2.pdf,"Overview of Eval-Instruct. Starting from referenced pointwise grading data, our proposed multi-path prompting method can apply pointwise-to-pairwise and referenced-to-reference-free prompting strategies to acquire evaluation data in other tasks and settings via two different paths. Cross validation is adopted to filter out the contradictory data from these two paths and further improve the data quality.","Since the natural language processing (NLP) community started to make large language models (LLMs) act as a critic to evaluate the quality of generated texts, most of the existing works train a critique generation model on the evaluation data labeled by GPT-4's direct prompting. We observe that these models lack the ability to generate informative critiques in both pointwise grading and pairwise comparison especially without references. As a result, their generated critiques cannot provide fine-grained distinguishability on generated texts, causing unsatisfactory evaluation performance. In this paper, we propose a simple yet effective method called Eval-Instruct, which can first acquire pointwise grading critiques with pseudo references and then revise these critiques via multi-path prompting to obtain informative evaluation data in different tasks and settings, including pointwise grading and pairwise comparison with / without references. After fine-tuning on these data, the resulting model CritiqueLLMis empirically shown to outperform ChatGPT and all the open-source baselines and even achieve comparable evaluation performance to GPT-4 in system-level correlations of pointwise grading. We also demonstrate that our generated critiques can act as scalable feedback to further improve the generation quality of strong LLMs like ChatGPT.","Recently, large language models (LLMs) have been improved rapidly and approached human-level performance on various natural language processing (NLP) tasks, such as question answering, text summarization, dialogue generation, and code generation. How to automatically measure the performance of LLMs has now become an essential research problem and attracted extensive attention. Strong evaluation methods are expected to provide high-quality critiques (including not only rating scores but also explanations) that act as scalable feedback and guide LLMs to improve persistently. 
Traditional evaluation metrics, usually based on n-gram overlap between generated texts and reference texts (such as BLEU and ROUGE), have limited effectiveness. Recent works mostly resort to model-based evaluation metrics, especially LLM-based ones. Since most of the best-performing LLMs such as ChatGPT and GPT-4 can only be accessed via OpenAI APIs, researchers start to automatically collect evaluation data by directly prompting GPT-4 and train their own evaluation models, aiming to avoid potential risks of commerical APIs, such as high cost, unstable usage, and data leakage. 
However, we argue that these evaluation models are still struggling to generate informative critiques in different evaluation tasks including pointwise grading and pairwise comparison. Especially in the challenging reference-free setting, these models tend to generate general critiques without fine-grained distinguishability on generated texts, causing unsatisfactory evaluation performance. 
In this work, we propose a simple yet effective method called Eval-Instruct, which can automatically construct informative instruction-tuning data for different evaluation tasks and settings, including pointwise grading and pairwise comparison with / without references. Our main idea is to fully utilize referenced pointwise grading critiques, which are shown to possess rich information with the assistance of references and elaborate prompt design, to construct evaluation data for other tasks and settings. Specifically, after acquiring pointwise grading critiques with pseudo references via GPT-4, we devise a multi-path prompting method including two strategies: 1) Pointwise-to-Pairwise Prompting aims to inject pointwise grading critiques into pairwise critiques and enrich them with more information about the respective quality of text pairs. 2) Referenced-to-Reference-Free Prompting is targeted at removing direct comparison with references in referenced critiques, while keeping other details to improve the specificity of reference-free critiques. The evaluation data in different tasks and settings can be acquired via different paths consisting of these two strategies. And we also design a cross validation mechanism to improve the data quality of reference-free pairwise comparison because both of the two paths reach this task. After fine-tuning on the data of all the tasks and settings, the resulting model CritiqueLLMis empirically shown to outperform all the open-source baselines and even achieve comparable performance with GPT-4 in system-level correlations of pointwise grading. We also show the potential of CritiqueLLMto act as effective feedback to enhance the performance of LLMs like ChatGPT. 
Our main contributions are as follows: 
 
 * We propose an evaluation data construction method called Eval-Instruct to automatically acquire informative evaluation data in both pointwise grading and pairwise comparison with / without references. 
 * We conduct extensive experiments on CritiqueLLM, which is fine-tuned on the data constructed by Eval-Instruct. Experimental results on three instruction following benchmark datasets show that our model can outperform all the open-source baselines and even perform comparably with GPT-4 in system-level correlations of pointwise grading. 
 * We reveal the potential of CritiqueLLMto guide LLMs to improve persistently by showing the positive impact of our generated critiques as scalable feedback on the generation quality of LLMs."
Effects of diversity incentives on sample diversity and downstream model performance in LLM-based text augmentation,2401.06643v3,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2401.06643v3.png,"Overview of our methodology. For each dataset, we randomly sample 6 samples per label that are used as seed sentences for LLM data augmentation. There, we collect data in in 2 rounds - 1st only using the prompt method and then in parallel for prompt method and 3 different diversity incentive methods. These are added together to form the datasets. BERT-large or Mistral classifier is fine-tuned 5 or 3 times respectively on each of the collected data and then evaluated. We repeat the entire process 5 times.","The latest generative large language models (LLMs) have found their application in data augmentation tasks, where small numbers of text samples are LLM-paraphrased and then used to fine-tune downstream models. However, more research is needed to assess how different prompts, seed data selection strategies, filtering methods, or model settings affect the quality of paraphrased data (and downstream models). In this study, we investigate three text diversity incentive methods well established in crowdsourcing: taboo words, hints by previous outlier solutions, and chaining on previous outlier solutions. Using these incentive methods as part of instructions to LLMs augmenting text datasets, we measure their effects on generated texts' lexical diversity and downstream model performance. We compare the effects over 5 different LLMs, 6 datasets and 2 downstream models. We show that diversity is most increased by taboo words, but downstream model performance is highest with hints.","The emergence of large language models (LLMs) such as GPT-4, LLaMA, etc. , has sparked interest in using them to augment textual datasets. In these scenarios, the number of samples is expanded by paraphrasing existing ones through LLM prompting. The created paraphrases are then added to the original dataset and used for downstream model training. Such methods have been explored for various domains such as sentiment classification, news classification and health symptoms classifications. However, investigation of the effect of various prompts, specific instructions, and selection of seed data inspired by crowd in the text augmentation process when using LLMs is lacking. 
Crowdsourcing is an established practice for collecting training or validation examples for a variety of NLP tasks. Scenarios of data collection using human workers can be similar to those of data augmentation: workers create paraphrases on existing sentences chosen from a dataset. The aim of such data collection is to increase the data diversity and subsequent performance of classifiers trained on the data. To increase the diversity, various methods are used in crowdsourcing to guide workers. These include taboo words - where most significant words from the collected data are identified and listed in the worker instructions to be avoided during paraphrasing, chaining - where outliers in the previous paraphrases are identified and used as seed sentences in the next round of data collection, and hints where previous outlier paraphrases are used as examples in the instructions. The hints method itself is similar to LLM in-context learning, where examples are included in the instructions for the model to achieve better performance. All of these diversity incentive methods report increased diversity of paraphrases and some also report increased performance of the classifiers trained on the so-collected data. 
This work is inspired by the parallels between crowdsourcing and LLM prompting and by the performance of diversity incentive methods on the diversity of paraphrases and the performance of models trained on them. We investigate the effects of the three diversity incentive methods (originating in crowdsourcing) on data augmentation using LLMs. The baseline, taken from a previous study, is a simple prompting for paraphrases. Measuring paraphrase diversity and downstream performance of classification models, we assess whether the diversity incentives (added to the base prompt) improve LLM outputs similarly as in crowdsourcing scenarios. To our knowledge, this is the first work to investigate the effects of diversity incentive methods on LLMs. 
In this paper, we answer the following research questions: 
 
 RQ1: Does the usage of diversity incentive methods on LLMs yield more diverse paraphrases? (compared to base prompting) RQ2: Do classifiers achieve better performance if trained on data augmented using diversity incentive methods on LLMs? (compared to base prompting) To answer these questions, we have conducted a data augmentation experiment using 5 different LLMs on 6 different datasets in the tasks of sentiment (movie and app reviews), news, and intent (flight and voice assistant commands) classification. In this experiment, we repeatedly collect LLM paraphrases using different diversity incentive methods. Then, we compare the lexical diversity of the collected data and the performance of downstream classifiers. Additionally, we also conduct an ablation study, where we modify the diversity incentive methods with random data to validate, that the inputs used by these methods (e.g., most influential taboo words, outlier paraphrases) contribute to the method's performance and a combination of the best performing methods for lexical diversity and model performance. In total, we collected 253,500 paraphrases. 
The most prominent findings are the following: 1) We do not observe statistically significant improvements in lexical diversity of the generated datasets, but only minor improvements using the taboo method, 2) The hints method increases the performance of classification models trained on such data compared to the baseline, while also reducing standard deviation and thus increasing the stability of results, 3) The chaining method and taboo method both do not significantly affect the performance of classification models trained on such data compared to the baseline."
Faithful Logical Reasoning via Symbolic Chain-of-Thought,2405.18357v2,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2405.18357v2.pdf,An illustrative example of logical reasoning via Chain-of-Thought and our proposed Symbolic CoT (SymbCoT).,"While the recent Chain-of-Thought (CoT) technique enhances the reasoning ability of large language models (LLMs) with the theory of mind, it might still struggle in handling logical reasoning that relies much on symbolic expressions and rigid deducing rules. To strengthen the logical reasoning capability of LLMs, we propose a novel Symbolic Chain-of-Thought, namely SymbCoT, a fully LLM-based framework that integrates symbolic expressions and logic rules with CoT prompting. Technically, building upon an LLM, SymbCoT 1) first translates the natural language context into the symbolic format, and then 2) derives a step-by-step plan to solve the problem with symbolic logical rules, 3) followed by a verifier to check the translation and reasoning chain. Via thorough evaluations on 5 standard datasets with both First-Order Logic and Constraint Optimization symbolic expressions, SymbCoT shows striking improvements over the CoT method consistently, meanwhile refreshing the current state-of-the-art performances. We further demonstrate that our system advances in more faithful, flexible, and explainable logical reasoning. To our knowledge, this is the first to combine symbolic expressions and rules into CoT for logical reasoning with LLMs. Code is open at <https: //github. com/Aiden0526/SymbCoT>.","Achieving human-like logical reasoning capabilities is crucial for realizing AGI, which plays a pivotal role in enabling intelligent systems to engage in problem-solving, decision-making, and critical thinking. Recently, LLMs have demonstrated unprecedented capabilities in semantic understanding, casting a beacon of hope toward achieving AGI. Further enhancing LLMs to achieve human-level reasoning abilities, particularly in logical reasoning, is of paramount importance. Logical reasoning stands out as a quintessential form of reasoning that, unlike other types, is crucial and challenging. It epitomizes a cognitive process characterized by rigorous evidence evaluation, argument construction, and logical deduction. The latest trend is integrating LLMs with symbolic solvers to enhance their performance. Unfortunately, these efforts have been limited to using LLMs merely as text-to-symbolic translators, with the core reasoning still reliant on traditional external reasoners. Such an approach, first, does not intrinsically strengthen LLMs 'capability in logical reasoning. Besides, over-reliance on external symbolic solvers often results in inflexibility, information omission, and unexplainability. 
On another note, the concept of CoT has been introduced to mimic human thinking processes by encouraging LLMs to explicitly consider intermediate steps during problem-solving and to provide rationales for decisions, thereby enhancing the reliability of the reasoning process. CoT has been successfully integrated into a wide array of tasks, significantly improving LLMs' reasoning capabilities, sometimes even matching human performance in certain scenarios. There is growing interest in applying CoT for logical reasoning, and developing advanced strategies such as self-consistency and Tree-of-Thought for enhancement. However, applying basic CoT directly to logical reasoning is inherently limited, due to the abstractive nature of language expression. Logical reasoning demands rigorous logical calculations, heavily relying on both symbolic expressions and rigid deducing rules to represent the internal structure of problems. Plain texts often fall short of supporting such precise logic, especially in scenarios that demand strict logical representation. For instance, as shown in Fig. , when tackling a logical reasoning problem, utilizing symbolic representations like First-Order Logic (FOL) is more representative and precise than fully natural language rationales in CoT, enabling strict logical reasoning through clear inference rules. 
To address these challenges, we introduce a novel Symbolic CoT (namely SymbCoT) for logical reasoning. Unlike existing state-of-the-art (SoTA) LLM-based symbolic reasoning systems, SymbCoT is entirely facilitated by LLMs without relying on any external reasoners/tools, i.e., encompassing both the initial translation and subsequent reasoning phases. Fig. provides a high-level illustration of the overall system workflow. Technically, SymbCoT comprises four main modules: Translator, Planner, Solver, and Verifier. Notably, SymbCoT is characterized by the following three core aspects: 
 1.5em2.2em1.87em1.7em1em1em 
 1) SymbCoT integrates symbolic expressions into CoT to describe intermediate reasoning processes, facilitating more precise logical calculations. However, relying solely on symbolic representation still has its limitations, as it often fails to capture certain content, such as implicit intentions or crucial contextual information embedded within questions. Yet LLMs excel at interpreting such nuanced information and contexts. Thus, we consider a combination of symbolic and natural language expressions to leverage the mutual strengths of both: freely expressed implicit intents and contextual information in natural language and rigorous expression in symbolic forms. 
 2) Unlike the straightforward prompting of ""thinking step by step"" in vanilla CoT, SymbCoT considers a plan-then-solve architecture. This involves decomposing the original complex problem into a series of smaller, more manageable sub-problems, which are then addressed one by one. This way, the entire reasoning process becomes more trackable, enabling a clearer and more structured approach to problem-solving. 
 3) Furthermore, we devise a retrospective verification mechanism. At both the translation and subsequent problem-solving stages, we retrospectively validate the correctness of each step's outcome, by tracing back to the original given condition. This verification process ensures the accuracy and reliability of the operations performed during the reasoning process. 
 
In experiments, we test SymbCoT with symbolic expressions of FOL and Constraint Optimization (CO) on five logical reasoning datasets using both GPT-3.5 and GPT-4. Results demonstrate that SymbCoT significantly enhances the reasoning capabilities of vanilla CoT, outperforming current SoTA solutions clearly. We further demonstrate that the more complex the logical reasoning task, the more pronounced the improvement of SymbCoT over vanilla CoT, further with the verification mechanism ensuring the faithfulness of the reasoning process. Our in-depth analysis reveals that fully LLM-based logical reasoning can offer better symbolic syntax robustness, human-readable explanations, and fuller utilization of information. 
In summary, our technical contributions are: 
 * proposing a fully LLM-based logical reasoning framework based on CoT, demonstrating that LLMs can achieve robust logical reasoning capabilities without external reasoning tools. Compared to existing SoTA solutions relying on external resolvers, SymbCoT offers better robustness against translation errors and more human-understandable explanations. 
 * innovatively integrating the strengths of symbolic forms and natural language expressions, enabling precise reasoning calculations while fully interpreting implicit information and capturing rich contexts. 
 * introducing a plan-then-solve architecture for CoT reasoning, along with a retrospective verification mechanism, enhancing the faithfulness of the reasoning process."
Combining Supervised Learning and Reinforcement Learning for Multi-Label Classification Tasks with Partial Labels,2406.16293v1,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2406.16293v1.pdf,"A. A partially annotated data sample in DocRE task. B. Severe imbalanced distribution of annotated positive (red scatters) and negative labels (blue and orange scatters) corresponding to the DocRED dataset. Orange scatters are actually-positive labels reannotated by Re-DocRED dataset C. Results of training on incomplete DocRED and testing on reannotated Re-DocRED and DocGNRE. SSR-PU is sensitive to prior estimation, while ours is prior agnostic. D. Performance comparison in DocRE task.","Traditional supervised learning heavily relies on human-annotated datasets, especially in data-hungry neural approaches. However, various tasks, especially multi-label tasks like document-level relation extraction, pose challenges in fully manual annotation due to the specific domain knowledge and large class sets. Therefore, we address the multi-label positive-unlabelled learning (MLPUL) problem, where only a subset of positive classes is annotated. We propose Mixture Learner for Partially Annotated Classification (MLPAC), an RL-based framework combining the exploration ability of reinforcement learning and the exploitation ability of supervised learning. Experimental results across various tasks, including document-level relation extraction, multi-label image classification, and binary PU learning, demonstrate the generalization and effectiveness of our framework.","Multi-Label Classification (MLC) task treats a problem that allows instances to take multiple labels, and traditional Supervised Learning (SL) methods on MLC heavily rely on human-annotated data sets, especially neural approaches that are data-hungry and susceptible to over-fitting when lacking training data. However, in many MLC tasks that generally have dozens or hundreds of sizes of class sets, incompleteness in the acquired annotations frequently arises owing to the limited availability of expert annotators or the subjective nature inherent in human annotation processes. . Therefore, we focus on the fundamentally important problem, typically termed Multi-Label Positive-Unlabelled Learning (MLPUL), which involves learning from a multi-label dataset in which only a subset of positive classes is definitely annotated, while all the remaining classes are unknown (which could be positives or negatives). For instance, as shown in A& B, human annotators find it hard to completely annotate all the relations due to the confusion of understanding relation definitions and long-context semantics in document-level relation extraction (DocRE) task. 
Illustration of our RL framework. ⊕ represents union operation. We iteratively update the policy network and critic network. The augmented training data are curated for the critic network."
Spatially-Aware Speaker for Vision-and-Language Navigation Instruction Generation,2409.05583v1,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2409.05583v1.pdf,"Extracting 3D scene relationships from house environments (a, b) can improve instruction generation by including object references (c).","Embodied AI aims to develop robots that can understand and execute human language instructions, as well as communicate in natural languages. On this front, we study the task of generating highly detailed navigational instructions for the embodied robots to follow. Although recent studies have demonstrated significant leaps in the generation of step-by-step instructions from sequences of images, the generated instructions lack variety in terms of their referral to objects and landmarks. Existing speaker models learn strategies to evade the evaluation metrics and obtain higher scores even for low-quality sentences. In this work, we propose SAS (Spatially-Aware Speaker), an instruction generator or Speaker model that utilises both structural and semantic knowledge of the environment to produce richer instructions. For training, we employ a reward learning method in an adversarial setting to avoid systematic bias introduced by language evaluation metrics. Empirically, our method outperforms existing instruction generation models, evaluated using standard metrics. Our code is available at <https: //github. com/gmuraleekrishna/SAS>.","Incorporating language understanding in robots has been a long-standing goal of the NLP and robotic research community. Specifically, the Vision-Language Navigation (VLN) task requires robots to follow natural language instructions grounded on vision to navigate in human living spaces. Although humans generally follow navigational instructions well, training robots to follow natural language instructions remains a challenging problem. Detailed navigation instructions may include landmarks, actions, and destinations. Recent work has succeeded in improving instruction understanding of robots by augmenting instruction and trajectory training data. They showed that using machine-generated instructions from a large number of navigational paths sampled from real houses helps robots navigate successfully even in previously unseen environments. However, there is still room for improvement, as the quality of machine-generated instructions is clearly lower compared to human annotations. 
In this work, we present a novel instruction generation model that can produce a variety of human-like instructions using semantic and structural cues from the environment. Our method uses rooms, interesting landmarks, objects, inter-object relations, object locations, and spatial features to produce richer instructions that can be used by robots and humans alike (Fig. ). Our Spatially-Aware Speaker (SAS) model generates information-rich instructions by leveraging expert demonstrations that map trajectories to verbal directions. Incorporating spatial references within these instructions is critical, as they convey the environmental layout, highlight key landmarks relevant to the actions taken, and gauge the progression of navigation. At its core, SAS employs a sequence learning framework that is fine-tuned through a combination of adversarial learning rewards and multiple objectives aimed at enhancing its linguistic generation capabilities. 
The architecture of SAS is based on an Encoder-Decoder model, which processes a sequence of viewpoints and corresponding actions that define a navigational path, subsequently generating a coherent set of instructions. During the encoding phase, the model extracts vital information from visual inputs, such as object categories (e.g., cupboard, bed), spatial relationships between objects (e.g., on top of, near, under), object placements, and significant landmarks within the viewpoint (e.g., bedroom, kitchen). These elements are combined with navigational actions to form a comprehensive vision-action representation that computes the temporal order. 
The decoding phase acquires linguistic capabilities by linking this latent representation to the instructions encountered during training. An adversarial learning objective is introduced to encourage the generation of varied sentences, mitigating the potential biases that automatic evaluation metrics introduce. Through this novel approach, SAS outperforms existing instruction generation models on VLN datasets evaluated using standard language evaluation metrics. 
Our contribution is as follows. 
 
 * We introduce a novel speaker model (SAS) that can incorporate semantic and structural viewpoint features into the instruction. 
 * We develop an adversarial reward learning strategy, that rewards diverse instructions, to train our SAS model. 
 * We introduce a large scale silver dataset for automatic data augmentation."
HiRoPE: Length Extrapolation for Code Models Using Hierarchical Position,2403.19115v2,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2403.19115v2.pdf,"Illustration of the hierarchical position in source code, such as function-level and token-level positions. We also show a simplified abstract syntax tree of the code in the bottom left corner.","Addressing the limitation of context length in large language models for code-related tasks is the primary focus of this paper. Existing LLMs are constrained by their pre-trained context lengths, leading to performance issues in handling long complex code sequences. Inspired by how human programmers navigate code, we introduce Hierarchical Rotary Position Embedding (HiRoPE), a novel approach that enhances the traditional rotary position embedding into a hierarchical format based on the hierarchical structure of source code. HiRoPE offers easy integration into existing LLMs without extra training costs. Our method is extensively evaluated with various LLMs, demonstrating stable performance in tasks such as language modeling and long code completion. We also introduce a new long code understanding task with real-world code projects, in hopes of promoting further development in this code-related field. Theoretically and experimentally, we find that HiRoPE also addresses the out-of-distribution issue in position encoding. Our HiRoPE significantly expands the context length capabilities of LLMs, enabling inference at lengths exponentially greater than the training length.","Large language models (LLMs) such as LLaMA-2, and CodeLLaMA have achieved significant performances in code-related tasks. These Transformer-based models excel in code comprehension and generation but face a notable challenge: the limitation of maximum context length. LLMs are typically pre-trained with a context length ranging from 2k to 16k tokens, which often proves insufficient for complex, extended source code. Exceeding this length limitation during inference may lead to performance degradation for these code models, particularly in tasks like project-level code completion or long code generation. 
Various methods have been developed to extend the context window of LLMs. Some approaches involve fine-tuning on extensive texts, which can be resource-intensive and potentially lead to overfitting and loss of performance on shorter sequences. There are also some training-free methods. However, these methods usually use window attention rely on local information, and ignore the long dependency in code. It is essential to incorporate certain structural characteristics of the code into position encoding to efficiently model these long-distance code dependencies. 
Our work diverges from these methods by focusing on the hierarchical information of source code in position encoding, inspired by how human programmers navigate code. Traditional positional encoding uses token counts for positioning, and treats code as plain text. However, human programmers often use hierarchical information in the code, representing positions in the code efficiently through multi-level hierarchical positions. We propose a hierarchical position approach that identifies token positions within specific levels, such as functions or statements. Figure shows the comparison of the traditional position and our hierarchical position. It is clear that the hierarchical positional encoding, benefiting from the full utilization of structural information in the code, can more conveniently locate positional information within long code sequences. This method could more effectively model long dependencies in source code. 
Following such inspirations, we introduce a novel approach, Hierarchical Rotary Position Embedding (HiRoPE), which enhances the popular rotary position embedding (RoPE) into a hierarchical format. HiRoPE differentiates itself by extracting hierarchical information from the source code and splitting the RoPE dimension to represent different hierarchical levels. It simultaneously models token-level relative location and higher-level relative location information. We also add a window mechanism to ensure stability with short texts, aligning with traditional positional encoding. 
HiRoPE is a plug-and-play solution, easily integrated into existing LLMs without additional training costs. Our extensive experiments with popular LLMs on tasks like language modeling and token completion in long code contexts demonstrate its effectiveness. We compare HiRoPE with existing length extrapolation methods using long code benchmarks such as LCC and RepoBench. We also introduce a new long code understanding task named code symbol understanding with real-world code libraries. Theoretically and experimentally, we find that HiRoPE effectively addresses the out-of-distribution issue in position encoding. Our HiRoPE significantly expands the context length capabilities of LLMs, enabling inference at lengths exponentially greater than the training length. We believe our work with HiRoPE not only addresses a critical length limitation in LLM applications but also opens new avenues for long-structured data modeling research. 
In summary, we make the following main contributions: 
 
 * We propose Hierarchical RoPE (HiRoPE), enhancing the traditional rotary position embedding into a hierarchical format based on the hierarchical structure of source code, providing improved extrapolation capabilities. 
 * We conducted comprehensive experiments with LLMs on various long code tasks involving language modeling and code completion. We also introduce a new long code understanding task with real-world code projects, in hopes of promoting further development in this code-related field. 
 * We demonstrate that HiRoPE effectively addresses the out-of-distribution issue in position encoding, enabling inference at lengths exponentially greater than the training length."
What is the Best Way for ChatGPT to Translate Poetry?,2406.03450v1,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2406.03450v1.pdf,Comparison between the framework of the traditional translation method and the proposed Explanation-Assisted Poetry Machine Translation (EAPMT).,"Machine translation (MT) has historically faced significant challenges when applied to literary works, particularly in the domain of poetry translation. The advent of Large Language Models such as ChatGPT holds potential for innovation in this field. This study examines ChatGPT's capabilities in English-Chinese poetry translation tasks, utilizing targeted prompts and small sample scenarios to ascertain optimal performance. Despite promising outcomes, our analysis reveals persistent issues in the translations generated by ChatGPT that warrant attention. To address these shortcomings, we propose an Explanation-Assisted Poetry Machine Translation (EAPMT) method, which leverages monolingual poetry explanation as a guiding information for the translation process. Furthermore, we refine existing evaluation criteria to better suit the nuances of modern poetry translation. We engaged a panel of professional poets for assessments, complemented evaluations by using GPT-4. The results from both human and machine evaluations demonstrate that our EAPMT method outperforms traditional translation methods of ChatGPT and the existing online systems. This paper validates the efficacy of our method and contributes a novel perspective to machine-assisted literary translation.","Foreign poems translated into Chinese still have to be like poems. 
 —— 
Poetry translation is widely regarded as one of the most challenging tasks in the field of translation. When translating a foreign poem into Chinese, the resulting text should still be recognizable as a poem. In discussing poetry, we often refer to specific genres or styles. Unlike classical poetry, the term ""modern"" in modern poetry refers to the poetic styles of the 20th and 21st centuries, which represent a significant departure from traditional forms. The primary characteristic of modern poetry is its embrace of freedom and lack of restrictions. Modern poetry is specifically characterized by open forms, diverse genres, a break from conventional narratives, and innovative language combinations. Unlike classical poetry, rhythm is no longer an essential feature of modern poetry. Consequently, when translating modern poetry, it is not necessary to adhere to the original poem 's rhythm. However, the poeticity must not be overlooked; the poetic essence of the source poem must be preserved throughout the translation process. 
For Chinese poetry, there are significant differences between various genres. Classical Chinese poetry is characterized by strict constraints on format, meter, sentence length, and rhyme. In contrast, modern poetry is free from these constraints and breaks away from the rigid structures of classical poetry. 
Previous work has successfully applied machine translation to poetry, but these poems typically have clear format or rhyme restrictions, which differ significantly from modern Chinese poetry. Recent research has taken an innovative approach by first obtaining an initial translation of the input prose using traditional Neural Machine Translation (NMT) methods. This initial translation is then mapped to a set of masked sequences via a designed heuristic method. Finally, these sequences are used to generate poetry translations through a pre-trained Masked Language Modeling (MLM) technique. 
Another study compared the differences between machine translation and human translation of Arabic poetry into English. The authors concluded that machine translation is not suitable for translating Arabic poetry into English as it fails to comprehend the socio-cultural background of poetry creation and the contextual nuances, particularly the genre-specific elements. 
Promisingly, the artificial intelligence chatbot ChatGPT, released by OpenAI, has demonstrated excellent performance across various tasks and domains, including translation tasks. Although previous work has studied the performance of ChatGPT on translation tasks, and recent studies have explored the application of ChatGPT to poetry-related tasks, these investigations primarily focused on poetry generation. For instance, recent research examined the effectiveness of ChatGPT-4 in generating Arabic poetry and found the results to be unsatisfactory. The study highlighted several issues with the text generated by ChatGPT-4, including poor language quality, superficial content, lack of emotion, inconsistent speech, inappropriate word choices, and an ease of recognition by human evaluators. 
Unlike previous work, this paper focuses on the capabilities of ChatGPT in translating English poetry into modern Chinese poetry. We explored optimal strategies for utilizing ChatGPT to translate poetry and evaluated its maximum performance in this specific task. Inspired by and, we investigated ChatGPT' s performance on modern poetry translation tasks by designing appropriate prompts and providing example shots for the model. 
Experimental results demonstrate the effectiveness of our designed prompts. Despite these promising outcomes, our analysis reveals persistent issues in the translations generated by ChatGPT that warrant further attention. Consequently, we propose a new poetry translation method called Explanation-Assisted Poetry Machine Translation (EAPMT). Our method leverages the explanation of monolingual poetry as guidance information to achieve high-quality translations from English poetry to modern Chinese poetry. Furthermore, existing evaluation criteria are typically designed for ordinary texts or poems with specific restrictions and are not fully applicable to modern poetry. Therefore, we refined these criteria to better capture the nuances of contemporary poetry translation. We engaged a panel of professional poets for assessments and complemented their evaluations with those conducted using GPT-4. The results from both human and machine evaluations demonstrate that our EAPMT method outperforms traditional translation techniques of ChatGPT and existing online systems. 
The contributions of our work are as follows: 
 
 * We are the first to examine ChatGPT 's capabilities in English-Chinese modern poetry translation tasks. 
 * We construct and release a high-quality bilingual poetry dataset. 
 * We identify the optimal prompts and examples (shots) for ChatGPT to effectively translate poetry. 
 * We propose a novel method for poetry translation that uses monolingual poetry explanations as guiding information. This method significantly enhances ChatGPT' s performance in translating modern poetry and can be extended to other language pairs and models of language understanding and generation. 
 * We design a new framework for human evaluation criteria specifically applicable to modern poetry translation and engage several professional poets to evaluate the translation results."
Representation Learning with Conditional Information Flow Maximization,2406.05510v2,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2406.05510v2.png,Venn information diagram comparison of our CIFM with existing principles. The learned representations by each principle is circled by the red dashed line.,"This paper proposes an information-theoretic representation learning framework, named conditional information flow maximization, to extract noise-invariant sufficient representations for the input data and target task. It promotes the learned representations have good feature uniformity and sufficient predictive ability, which can enhance the generalization of pre-trained language models (PLMs) for the target task. Firstly, an information flow maximization principle is proposed to learn more sufficient representations for the input and target by simultaneously maximizing both input-representation and representation-label mutual information. Unlike the information bottleneck, we handle the input-representation information in an opposite way to avoid the over-compression issue of latent representations. Besides, to mitigate the negative effect of potential redundant features from the input, we design a conditional information minimization principle to eliminate negative redundant features while preserve noise-invariant features. Experiments on 13 language understanding benchmarks demonstrate that our method effectively improves the performance of PLMs for classification and regression. Extensive experiments show that the learned representations are more sufficient, robust and transferable.","The goal of deep representation learning is to transform the raw observational data into low-dimensional representations that are essential for various downstream tasks. In recent years, information-theoretic representation learning has been widely studied, aiming to discover useful representations in a principled manner. The InfoMax principle has extensive applications in the field of self-supervised representation learning. In supervised scenarios, minimizing the standard cross-entropy is actually equivalent to maximizing the mutual information between the representations and the target task. But InfoMax tends to preserve potential redundant features that are irrelevant to the given target, leading to biased representations. Another noteworthy line of information-theoretic research is built upon the information bottleneck (IB) principle, which aims to discover compact and informative representations that can reduce redundant features from the inputs. IB seeks to find a maximally compressed representation of the input that preserves as much information as possible about the target, striking a balance between compression and prediction."
GPT is Not an Annotator: The Necessity of Human Annotation in Fairness Benchmark Construction,2405.15760v1,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2405.15760v1.png,"Comparison of human post-evaluation results for GPT-WinoQueer (blue, left) and GPT-WinoSemitism (orange, right) datasets. GPT-WQ evaluation results are generally worse, with a lower proportion of correct responses and higher proportions of grammatically incorrect, opposite, and hallucinated responses.","Social biases in LLMs are usually measured via bias benchmark datasets. Current benchmarks have limitations in scope, grounding, quality, and human effort required. Previous work has shown success with a community-sourced, rather than crowd-sourced, approach to benchmark development. However, this work still required considerable effort from annotators with relevant lived experience. This paper explores whether an LLM (specifically, GPT-3.5-Turbo) can assist with the task of developing a bias benchmark dataset from responses to an open-ended community survey. We also extend the previous work to a new community and set of biases: the Jewish community and antisemitism. Our analysis shows that GPT-3.5-Turbo has poor performance on this annotation task and produces unacceptable quality issues in its output. Thus, we conclude that GPT-3.5-Turbo is not an appropriate substitute for human annotation in sensitive tasks related to social biases, and that its use actually negates many of the benefits of community-sourcing bias benchmarks.","Though seemingly ubiquitous, large language models (LLMs) still treat users unequally and exhibit harmful social biases. Quantitative LLM bias measurement is a necessary first step to understanding and mitigating bias-related harms of AI systems. Measurement is essential because it allows model creators to understand potential fairness issues with their models, downstream users to compare models and choose those that are relatively fair in their use context, and fairness researchers to determine whether debiasing methods are effective. 
The current standard for bias measurement in LLMs is paired sentence bias benchmarks, which consist of pairs of similar sentences and generally rely on comparing the model's probability of predicting the stereotypical sentence to the probability of predicting a contrasting sentence. There are significant quality and grounding issues with most current benchmarks, especially those developed via crowd-sourcing. Current methods for community-sourced benchmark development, which mitigate some of the problems with crowd-sourcing, require significant human effort for annotation of survey responses. This work is time-consuming; in an unfunded, community-led benchmark development effort, this is either cost-prohibitive or requires asking annotators to work for free. This annotation also places a significant psychological burden on annotators. 
In order to maintain the usefulness and participatory nature of community-sourced bias benchmarks while reducing the financial and psychological costs, we tested model-assisted harm extraction. The main contributions of our work are as follows: 
 
 * We introduce WinoSemitism, a community-sourced benchmark for antisemitism, generalizing method of. 
 * We create GPT-WinoQueer and GPT-WinoSemitism, which are versions of the WQ and WS datasets annotated by GPT-3.5-Turbo instead of human experts. 
 * We provide a thorough quantitative and qualitative comparison of human-annotated and model-annotated datasets, finding significant quality issues with model annotations."
Multi-modal Preference Alignment Remedies Degradation of Visual Instruction Tuning on Language Models,2402.10884v2,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2402.10884v2.png,"From a visual-instruction-tuned pre-trained model, we generate 4 completions for a given image-question prompt. These answers are then presented to Gemini to obtain granular annotations given a labeling guide. We construct a preference dataset of (image-text prompt, preferred completion) and (image-text prompt, rejected completion). We benchmarked DPO, Rejection sampling, and SteerLM alignment methods, in addition to a pure SFT baseline using Gemini-provided answers directly.","Multi-modal large language models (MLLMs) are expected to support multi-turn queries of interchanging image and text modalities in production. However, the current MLLMs trained with visual-question-answering (VQA) datasets could suffer from degradation, as VQA datasets lack the diversity and complexity of the original text instruction datasets with which the underlying language model was trained. To address this degradation, we first collect a lightweight, 5k-sample VQA preference dataset where answers were annotated by Gemini for five quality metrics in a granular fashion and investigate standard Supervised Fine-tuning, rejection sampling, Direct Preference Optimization (DPO) and SteerLM algorithms. Our findings indicate that with DPO, we can surpass the instruction-following capabilities of the language model, achieving a 6.73 score on MT-Bench, compared to Vicuna 's 6.57 and LLaVA' s 5.99. This enhancement in textual instruction-following capability correlates with boosted visual instruction performance (+4.9%on MM-Vet, +6%on LLaVA-Bench), with minimal alignment tax on visual knowledge benchmarks compared to the previous RLHF approach. In conclusion, we propose a distillation-based multi-modal alignment model with fine-grained annotations on a small dataset that restores and boosts MLLM's language capability after visual instruction tuning.","Recent advancements in artificial intelligence have led to the rise of multi-modal large language models (MLLMs), which combine textual and visual interpretation capabilities in a single model. However, effectively blending multi-modality in one system has proven non-trivial. Integrating diverse data forms often creates internal representation conflicts, giving rise to the issue known as ""catastrophic forgetting"". The diversity constraint in visual question answering (VQA) datasets could be attributed as a source of the issue. VQA tasks typically focus on descriptive queries about image contents, whereas textual datasets encompass a broader range of complex cognitive tasks, including reasoning, writing, summarization, and coding. This discrepancy in dataset complexity is a key factor contributing to the observed performance degradation in MLLMs. Our evaluation of models such as BLIP-2, InstructBLIP, and LLaVA against language instruction-following benchmarks like MT-Bench and AlpacaEval revealed diminished language capabilities in comparison to their linguistic backbones. For instance, LLaVA, built on the Vicuna-13b LLM, demonstrated a decline in MT-Bench performance from 6.57 to 5.92, even underperforming the Vicuna-7B model. 
Driven by the limitations observed in distillation-based instruction tuning, particularly its constrained generalizability and the narrow performance improvements on tasks outside the training distribution, this study investigates the efficacy of distillation-based preference alignment in addressing modality conflict in MLLMs. The decision to explore this avenue is predicated on the hypothesis that integrating AI-generated preference data can provide a more granular and nuanced alignment with human expectations, potentially mitigating the adverse effects of modality conflict. 
This study rigorously evaluates three baseline methodologies—Direct Preference Optimization (DPO), SteerLM, and Rejection Sampling—as potential solutions to utilize the distilled preference data to enhance the instruction-following capabilities and address the modality conflict inherent in MLLMs. Each of these methods offers a unique approach to model alignment, from the direct optimization of preferences in DPO to the conditional supervision in SteerLM and the selective acceptance in Rejection Sampling. Our empirical analysis reveals that DPO, in particular, demonstrates a pronounced efficacy in reconciling the performance disparities observed between textual and visual modalities. By leveraging a refined preference dataset, fine-tuned with the DPO objective and supplemented with comprehensive annotations from advanced AI models, DPO not only addresses the modality conflict but also significantly enhances the model 's performance across a spectrum of benchmarks. The results indicate that, through the application of DPO, MLLMs can achieve a more robust alignment with human-like preferences, thereby mitigating the adverse effects of catastrophic forgetting and modality conflict and elevating the models' capabilities to a level that surpasses traditional instruction tuning methods. 
Our main contributions are: 
Exploration of Modality Degradation: This work is the first to identify and address modality degradation in MLLMs, a phenomenon where visual instruction tuning detrimentally impacts language instruction capabilities. Our systematic investigation into this issue contributes novel insights to the field, laying the groundwork for further research in mitigating such degradation. 
Efficient and scalable preference alignment pipeline as remedy Our data collection strategy employs a granular quality metric annotation format, leveraging cost-effective commercial APIs. This scalable approach enables the efficient production of high-quality datasets. We are able to surpass LLaVA and Vicuna's language instruction-following capability with DPO on a 6k dataset."
Towards Better Understanding of Contrastive Sentence Representation Learning: A Unified Paradigm for Gradient,2402.18281v2,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2402.18281v2.pdf,"Average Spearman's correlation on Semantic Textual Similarity tasks for ineffective optimization objectives before (""ori"") and after (""mod"") modifications under different backbones.","Sentence Representation Learning (SRL) is a crucial task in Natural Language Processing (NLP), where contrastive Self-Supervised Learning (SSL) is currently a mainstream approach. However, the reasons behind its remarkable effectiveness remain unclear. Specifically, many studies have investigated the similarities between contrastive and non-contrastive SSL from a theoretical perspective. Such similarities can be verified in classification tasks, where the two approaches achieve comparable performance. But in ranking tasks (i.e., Semantic Textual Similarity (STS) in SRL), contrastive SSL significantly outperforms non-contrastive SSL. Therefore, two questions arise: First, what commonalities enable various contrastive losses to achieve superior performance in STS? Second, how can we make non-contrastive SSL also effective in STS? To address these questions, we start from the perspective of gradients and discover that four effective contrastive losses can be integrated into a unified paradigm, which depends on three components: the Gradient Dissipation, the Weight, and the Ratio. Then, we conduct an in-depth analysis of the roles these components play in optimization and experimentally demonstrate their significance for model performance. Finally, by adjusting these components, we enable non-contrastive SSL to achieve outstanding performance in STS.","Sentence Representation Learning (SRL) explores how to transform sentences into vectors (or ""embeddings""), which contain rich semantic information and are crucial to many downstream tasks in Natural Language Processing (NLP). In the era of Large Language Models (LLMs), SRL also plays an important role in providing the embedding models for Retrieval-Augmented Generation (RAG, ). The quality of sentence embeddings is usually measured through Transfer tasks (TR) and Semantic Textual Similarity tasks (STS). 
Contrastive Self-Supervised Learning (SSL) is now a prevalent approach in SRL, which is introduced by and. It optimizes the representation space by reducing the distance between a sentence (or ""anchor"") and semantically similar sentences (or ""positive samples""), as well as increasing the distance between the sentence and semantically dissimilar sentences (or ""negative samples""). 
While the mechanisms underlying contrastive SSL can be intuitively understood, its effectiveness in SRL has not been thoroughly explored. Specifically, there are still some conflicts between the existing conclusions: (1) In theoretical perspective, machine learning community has found that contrastive SSL shares many similarities with non-contrastive SSL (e.g.,alignment & uniformity, Barlow Twins, and VICReg) (2) However, in practical applications, contrastive and non-contrastive SSL show comparable performance only in classification tasks (e.g., in Visual Representation Learning (VRL) and TR in SRL). In contrast, for ranking tasks (i.e., STS in SRL), contrastive SSL seems be the only effective method, significantly outperforming non-contrastive SSL. These inconsistent conclusions suggest that to make the obtained representations suitable for STS, certain unique factors must be present in the optimization objectives, which have rarely been explored in the existing literature. 
In this work, we attempt to identify the key factors that enable contrastive SSL to be effective in STS. Specifically, we would like to answer two questions: (1) What commonalities enable various contrastive losses to achieve superior performance in STS? (2) How can we make non-contrastive SSL, which is similar to contrastive SSL but ineffective in STS, effective? We first analyze the commonalities among four effective losses in SRL from the perspective of gradients. From this analysis, we find that all gradients can be unified into the same paradigm, which is determined by three components: the Gradient Dissipation, the Weight, and the Ratio. By statistically analyzing the values of these three components under different representation space distributions, we propose three conjectures, each corresponding to the role of a component in optimizing the representation space. Subsequently, we construct a baseline model to empirically validate our conjectures and demonstrate the significance of these components to model performance by varying them in the baseline. 
After understanding the key factors that enable contrastive losses to be effective, we are able to analyze the reasons behind the poor performance of non-contrastive SSL in STS from the perspective of three components in the paradigm. We find that these ineffective losses do not perform as well as effective ones across these components. Therefore, by adjusting these components, we manage to make them function and achieve improved model performance in STS (refer to Figure). 
Briefly, our main contributions are as follows: 
 
 * We propose a unified gradient paradigm for effective losses in SRL, which is controlled by three components: the Gradient Dissipation, the Weight, and the Ratio (Section); 
 * We analyze the roles in optimization for each component theoretically. Further, we propose and validate the conjectures on their effective roles in performing STS tasks (Section); 
 * With the guidance of our analysis results, we modify four optimization objectives in non-contrastive SSL to be effective in STS by adjusting the three components (Section)."
Emergent Word Order Universals from Cognitively-Motivated Language Models,2402.12363v2,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2402.12363v2.pdf,We compare the word orders that are challenging for LMs to those that are infrequent in attested languages (). We examine the advantage of cognitively-motivated LMs () in simulating the word-order universals (the world's word-order distribution) with their inductive biases ().,"The world's languages exhibit certain so-called typological or implicational universals; for example, Subject-Object-Verb (SOV) languages typically use postpositions. Explaining the source of such biases is a key goal of linguistics. We study word-order universals through a computational simulation with language models (LMs). Our experiments show that typologically-typical word orders tend to have lower perplexity estimated by LMs with cognitively plausible biases: syntactic biases, specific parsing strategies, and memory limitations. This suggests that the interplay of cognitive biases and predictability (perplexity) can explain many aspects of word-order universals. It also showcases the advantage of cognitively-motivated LMs, typically employed in cognitive modeling, in the simulation of language universals. <g r a p h i c s> -2-2 <https: //github. com/kuribayashi4/word-order-universals-cogLM> ","There are thousands of attested languages, but they exhibit certain universal tendencies in their design. For example, Subject-Object-Verb (SOV) word order often combines with postpositions, while SVO order typically employs prepositions. Researchers have argued that such implicational universals are not arbitrary but shaped by their advantage for humans. 
Such language universals have been recently studied through neural-based computational simulation to elucidate the mechanisms behind the universals. The languages which emerge, however, have typically not been human-like. Such mismatch arguably stems from the lack of human-like cognitive biases in neural agents, but injecting cognitive biases into systems and showing their benefits has proved challenging. 
In this study, expanding on a study of word-order biases in language models (LMs: ), we demonstrate the advantage of cognitively-motivated LMs, which can simulate human cognitive load during sentence processing well, and thus predict many implicational word-order universals in terms of their inductive biases (Figure). Specifically, we train various types of LMs in artificial languages with different word-order configurations (). Our experiments show that perplexities estimated by cognitively-motivated LMs () correlate better with frequent word-order configurations in attested languages than standard LMs (). This confirms that such biases are a potential source of the word-order universals as well as demonstrate the plausibility of cognitively-motivated LMs as models of human language processing."
Speech Translation with Speech Foundation Models and Large Language Models: What is There and What is Missing?,2402.12025v3,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2402.12025v3.png,Architectural building blocks of ST models based on the combination of an SFM and an LLM.,"The field of natural language processing (NLP) has recently witnessed a transformative shift with the emergence of foundation models, particularly Large Language Models (LLMs) that have revolutionized text-based NLP. This paradigm has extended to other modalities, including speech, where researchers are actively exploring the combination of Speech Foundation Models (SFMs) and LLMs into single, unified models capable of addressing multimodal tasks. Among such tasks, this paper focuses on speech-to-text translation (ST). By examining the published papers on the topic, we propose a unified view of the architectural solutions and training strategies presented so far, highlighting similarities and differences among them. Based on this examination, we not only organize the lessons learned but also show how diverse settings and evaluation approaches hinder the identification of the best-performing solution for each architectural building block and training choice. Lastly, we outline recommendations for future works on the topic aimed at better understanding the strengths and weaknesses of the SFM+LLM solutions for ST.","The natural language processing (NLP) landscape has recently undergone a paradigm shift with the emergence of foundation models. Among them, Large Language Models (LLMs) have revolutionized text-based NLP, showcasing remarkable capabilities across a wide range of NLP tasks. This unprecedented success has spurred research into creating foundation models for other modalities, including speech processing. 
Building on the translation abilities of LLMs and the remarkable speech recognition and understanding capabilities achieved by Speech Foundation Models (SFMs), researchers are now actively exploring their combination. The resulting large multimodal models leverage, on the one hand, the SFM ability to encode speech content into rich and high-level representations and, on the other, the extensive linguistic knowledge of the LLM to generate fluent outputs and address a wide range of tasks. Focusing on the speech-to-text translation (ST) task – the scope of this paper – the rapid pace of the advancements has led to multiple parallel endeavors, resulting in a variety of solutions. While all these efforts have the merit of demonstrating the viability and effectiveness of this line of work, their contemporaneity, along with methodological inconsistencies, hinders a fair comparison. For this reason, we provide a systematic analysis of the proposed SFM+LLM solutions for ST with the multiple goals of identifying their similarities and differences, organizing the lessons learned, and suggesting future research directions, along with best practices for insightful evaluations. At its core, this paper addresses two key questions: 
 
 1.2triangular flagWhat is There? We survey the publicly available works that propose an SFM+LLM solution for ST, resulting in 9 papers (henceforth referred to as 1.2keycap: 1, . . . ,1.2keycap: 9), and analyze them () focusing on two orthogonal aspects: 
 1.2gearArchitectural Building Blocks (): We delve into the SFM+LLM architectures, identifying a common abstraction made of 5 building blocks and underscoring similarities and differences in the SFM and LLM choices, along with the strategies adopted for combining them;
 1.2gearTraining and Evaluation (): We inspect the training data, tasks, and strategies employed in the studies, as well as evaluation data and supported language pairs, gathering insights about promising solutions, and highlighting the sparsity of the current landscape;
 1.2triangular flagWhat is Missing? We conclude by underscoring the importanceof establishing a standard training setting based on open data to ease direct comparability across works, and by identifying aspects that need further investigation to better understand the potential ofSFM+LLM combination for ST ()."
D2LLM: Decomposed and Distilled Large Language Models for Semantic Search,2406.17262v1,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2406.17262v1.pdf,"Architecture of D2LLM: The teacher model is decomposed into three segments corresponding to the input of the [baseline= (char. base) ] [shape=circle, draw, inner sep=2pt, scale=0.6] (char) 1; query, [baseline= (char. base) ] [shape=circle, draw, inner sep=2pt, scale=0.6] (char) 2; passage, and [baseline= (char. base) ] [shape=circle, draw, inner sep=2pt, scale=0.6] (char) 3; prompt, represented in light red, light blue, and light purple. Its output, represented by a dark purple square, is the classification token embedding, which, after a linear layer, yields logits. The student model maintains the query and passage components ( [baseline= (char. base) ] [shape=circle, draw, inner sep=2pt, scale=0.6] (char) 1; and [baseline= (char. base) ] [shape=circle, draw, inner sep=2pt, scale=0.6] (char) 2; ) from the teacher but adds [baseline= (char. base) ] [shape=circle, draw, inner sep=2pt, scale=0.6] (char) 3; the PMA and IEM to capture the interplay between the query and passage, as well as their combined interaction with the prompt. It also outputs classification token embeddings respectively for symmetric and asymmetric search and then derives logits via a linear layer.","The key challenge in semantic search is to create models that are both accurate and efficient in pinpointing relevant sentences for queries. While BERT-style bi-encoders excel in efficiency with pre-computed embeddings, they often miss subtle nuances in search tasks. Conversely, GPT-style LLMs with cross-encoder designs capture these nuances but are computationally intensive, hindering real-time applications. In this paper, we present D2LLMs—Decomposed and Distilled LLMs for semantic search—that combines the best of both worlds. We decompose a cross-encoder into an efficient bi-encoder integrated with Pooling by Multihead Attention and an Interaction Emulation Module, achieving nuanced understanding and pre-computability. Knowledge from the LLM is distilled into this model using contrastive, rank, and feature imitation techniques. Our experiments show that D2LLM surpasses five leading baselines in terms of all metrics across three tasks, particularly improving NLI task performance by at least 6.45%. The source code is available at <https: //github. com/codefuse-ai/D2LLM>.","Semantic search has become an integral part of natural language processing, tasked with sifting through extensive texts to find passages that best match a user 's query based on underlying semantic links. It transcends the non-semantic techniques, such as TF-IDF and BM25, by resolving lexical mismatches and enabling more precise text matching. As a result, semantic search has significant impacts across various fields, including information retrieval, question answering, dialogue systems, item recommendation, fact checking, and retrieval-augmented generation. 
The major challenge of semantic search lies in devising a model that is both accurate and efficient in pinpointing the most relevant passages for any given query. The current go-to models, particularly the compact BERT-style bi-encoders or dual encoders, independently convert queries and passages into vectors and judge their relevance through measures such as cosine similarity. This process is praised for efficiency—enabling pre-computation and on-the-fly querying of passage vectors. However, this streamlined method comes at an accuracy cost. Within the rigidity of the bi-encoder' s similarity space, subtle but critical nuances may be lost, such as when differentiating between symmetric search tasks (e.g., finding similar questions to ""What are the symptoms of the flu? "") and asymmetric search tasks (e.g., matching that same query to a comprehensive answer detailing symptoms). The bi-encoders 'constrained interaction mode limits their comprehension of the distinct informational roles that queries and passages play. Additionally, bi-encoders are bound to a laborious and multi-stage training process, starting with pretraining on massive datasets of weakly-supervised text pairs and ending with finetuning on diverse and extensive labeled datasets. This process is heavily data-intensive and usually limited by the variety of data available. Moreover, the small size of bi-encoders often means they excel within their training domain but fall short when generalizing to new, unseen domains. 
On the flip side, GPT-style Large language models (LLMs) with cross-encoder designs overcome these limitations by jointly processing queries and passages, thereby forming a single, interactive input. This method enables a granular understanding of textual relationships, as it involves concatenating the query and passage, with directive prompts such as ""Are these questions similar? "" or ""Does this passage answer the question? "" to guide the model through both symmetric and asymmetric search tasks. Updated LLMs arrive pre-loaded with a broad spectrum of world knowledge, eliminating the need for domain-specific pretraining and facilitating rapid adaptation. The remarkable zero-shot learning ability ensures their robust performance even for novel domains. However, this accurate analysis incurs a toll on computational efficiency; it precludes the caching of passage vectors and necessitates fresh inference for each new query-passage pairing, which can hinder the practicality of cross-encoder LLMs in situations demanding real-time, voluminous processing. 
In this paper, we seek to bring the best of both worlds together with the introduction of D2LLMs, which stands for Decomposed and Distilled LLMs for semantic search. Our proposed framework begins by decomposing an LLM-based cross-encoder into a bi-encoder coupled with an Interaction Emulation Module (IEM). The bi-encoder, equipped with Pooling by Multihead Attention (PMA) of token embeddings resulting from a pretrained LLM, efficiently generates separate embeddings for queries and passages, allowing passage vectors to be pre-stored while ensuring the model' s adaptability. The IEM goes further, intricately mapping the relationships between queries and passages. It features dedicated branches for handling symmetric and asymmetric search tasks. We then distill the high-level knowledge from the original LLM-based cross-encoder (the teacher) into our decomposed model (the student) through a series of teacher-guided methodologies, including contrastive imitation, rank imitation, and feature imitation. Our contributions can be summarized as: 
 
 * We introduce D2LLM, a new semantic search solution that combines the speed of bi-encoders with the accuracy of LLM-based cross-encoders. This method breaks down the complex cross-encoder into a more manageable student model comprising a bi-encoder, a PMA, and an IEM. 
 * We transfer the teacher's linguistic expertise to the student through a comprehensive knowledge distillation strategy, encompassing contrastive imitation, rank imitation, and feature imitation techniques. 
 * Our empirical results reveal that D2LLM outperforms five leading methods in three benchmark tasks, with a particularly notable 14.39%improvement over the second-best LLaRA and a 6.45%lead over the heavily finetuned benchmark BGE model in the NLI task."
Arabic Diacritics in the Wild: Exploiting Opportunities for Improved Diacritization,2406.05760v1,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2406.05760v1.pdf," (a) The nine Arabic diacritics commonly used in Modern Standard Arabic, grouped by function; and four examples of diacritic clusters. (b) A visually annotated example of a diacritized phrase meaning 'and the bright suns [lit. and-the-suns the-bright] '. Diacritics are marked in red; and so are the undiacritized vowel-lengthening helping letters. Silent letters appear in dotted boxes.","The widespread absence of diacritical marks in Arabic text poses a significant challenge for Arabic natural language processing (NLP). This paper explores instances of naturally occurring diacritics, referred to as ""diacritics in the wild, "" to unveil patterns and latent information across six diverse genres: news articles, novels, children's books, poetry, political documents, and ChatGPT outputs. We present a new annotated dataset that maps real-world partially diacritized words to their maximal full diacritization in context. Additionally, we propose extensions to the analyze-and-disambiguate approach in Arabic NLP to leverage these diacritics, resulting in notable improvements. Our contributions encompass a thorough analysis, valuable datasets, and an extended diacritization algorithm. We release our code and datasets as open source.",
Disinformation Capabilities of Large Language Models,2311.08838v2,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2311.08838v2.pdf,"Summary of how many generated texts we consider dangerous or safe. Dangerous texts are disinformation articles that could be misused by bad actors. Safe texts contain disclaimers, provide counterarguments, argue against the user, etc. Note that GPT-4 annotations are generally slightly biased towards safety.","Automated disinformation generation is often listed as an important risk associated with large language models (LLMs). The theoretical ability to flood the information space with disinformation content might have dramatic consequences for societies around the world. This paper presents a comprehensive study of the disinformation capabilities of the current generation of LLMs to generate false news articles in the English language. In our study, we evaluated the capabilities of 10 LLMs using 20 disinformation narratives. We evaluated several aspects of the LLMs: how good they are at generating news articles, how strongly they tend to agree or disagree with the disinformation narratives, how often they generate safety warnings, etc. We also evaluated the abilities of detection models to detect these articles as LLM-generated. We conclude that LLMs are able to generate convincing news articles that agree with dangerous disinformation narratives.","The threat of LLMs generating disinformation at scale is one the most commonly cited risks of their further development. The capability to generate an arbitrary amount of human-like texts can be a powerful tool for disinformation actors willing to influence the public by flooding the Web and social media with content during influence operations. 
The recent wave of instruction-tuned LLMs that started to appear in late 2022 only exacerbated this issue as they proved to be capable of closely following arbitrary instructions. The growing capabilities of LLMs, their growing availability (caused by capable open source models and improvements in inference libraries), and improvements in prompting techniques are all concerning in the context of disinformation generation. 
Despite all this, very little is known about the disinformation capabilities of the current generation of LLMs. While there is a body of existing work, the experimental evaluation of certain features or capabilities is often absent or anecdotal. Our goal in this paper is to fill this gap and provide a comprehensive evaluation of instruction-tuned models prompted to generate English disinformation ""news articles"". We do this by observing how different LLMs behave when they are asked to generate texts about various harmful disinformation narratives, such as narratives about health-related hoaxes. 
We manually evaluated 1,200 generated texts to ascertain how much they agree or disagree with the prompted disinformation narratives, how many novel arguments they use, and how closely they follow the desired news article text style (grammar, structure, vocabulary, etc. ). We observed whether there are differences in how capable different LLMs are at generating disinformation (there are), how well their safety filters work (with a few exceptions, not very well), or how detectable these generated texts are (quite detectable). We also found out that we can, to some extent, automate such analysis by utilizing LLMs to analyze the generated texts, making the first steps toward automatic evaluation. Overall, we must conclude that existing LLMs (including open-source ones) can easily generate news articles with real or hallucinated supporting evidence about all kinds of dangerous disinformation narratives. Figure illustrates how dangerous or safe different LLMs are according to our methodology. 
Note that this paper provides but a snapshot of current capabilities, and understandably, we expect that newer LLMs trained on newer data might have different behavior. Our goal is to show what is the state of the field today so that we can understand how dangerous the LLMs are as a technology right now, but also to have the ability to observe how these capabilities will evolve in the future."
Learn or Recall? Revisiting Incremental Learning with Pre-trained Language Models,2312.07887v5,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2312.07887v5.pdf,The comparison between the proposed SEQ* and SOTA IL methods on five class-incremental tasks. We report the average accuracy after learning the final task. The detailed results are provided in Table.,"Incremental Learning (IL) has been a long-standing problem in both vision and Natural Language Processing (NLP) communities. In recent years, as Pre-trained Language Models (PLMs) have achieved remarkable progress in various NLP downstream tasks, utilizing PLMs as backbones has become a common practice in recent research of IL in NLP. Most assume that catastrophic forgetting is the biggest obstacle to achieving superior IL performance and propose various techniques to overcome this issue. However, we find that this assumption is problematic. Specifically, we revisit more than 20 methods on four classification tasks (Text Classification, Intent Classification, Relation Extraction, and Named Entity Recognition) under the two most popular IL settings (Class-Incremental and Task-Incremental) and reveal that most of them severely underestimate the inherent anti-forgetting ability of PLMs. Based on the observation, we propose a frustratingly easy method called SEQ* for IL with PLMs. The results show that SEQ* has competitive or superior performance compared with state-of-the-art (SOTA) IL methods yet requires considerably less trainable parameters and training time. These findings urge us to revisit the IL with PLMs and encourage future studies to have a fundamental understanding of the catastrophic forgetting in PLMs. The data, code and scripts are publicly available.","Learning knowledge incrementally without much forgetting is an essential ability of human beings but still an unsolved challenge for neural networks in achieving human-level intelligence. Incrementally learning a sequence of tasks can be formulated into the paradigm of Incremental Learning (IL) and has been impeded by catastrophic forgetting. Catastrophic forgetting refers to neural networks forgetting previous knowledge after learning new tasks. 
Recent years have witnessed significant breakthroughs in Pre-trained Language Models (PLMs) in vision and NLP tasks. Most recent studies of IL use PLMs as the backbone and design various methods for alleviating catastrophic forgetting in NLP tasks. However, is forgetting really catastrophic in PLMs? More specifically, how can we quantify forgetting and how much knowledge is forgotten in various IL scenarios when using various backbones and methods on various tasks? More recently, reveal for the first time that BERT-like models have a strong anti-forgetting ability in the task-incremental setting. Why does this happen? Does it hold for a more challenging setting, such as class-incremental learning, and for other model architectures, such as GPT-like models? 
To answer the above questions, we carry out extensive experiments to explore forgetting in more than 20 methods on four classification tasks (Text Classification, Intent Classification, Relation Extraction, and Named Entity Recognition) under the two most popular IL settings (Class-Incremental and Task-Incremental) with various model architecture (encoder only and decoder only) and scales (from 19M to 1.21B number of parameters). Through extensive experiments, we have several major findings: 
 
 * The popular assumption that PLMs suffer from catastrophic forgetting does not hold. Even under sequential fine-tuning (SEQ), the PLMs maintain the knowledge without much forgetting (Sec. ). From the probing perspective, most existing IL methods do not learn incremental knowledge for PLMs (Sec. ). 
 * By combining SEQ with simple strategies (Sec. ), we propose SEQ* and find that SEQ* has competitive or even superior performance than SOTA IL methods (Figure, Sec. ). 
 * The inherent anti-forgetting ability of PLMs comes from both the pre-training stage as well as the architecture of Transformer (Sec. ). Randomly initialised PLMs learn incrementally when SEQ is performed on a sequence of tasks. 
 * The forgetting of SEQ is due to the deviation of the classifier from the PLM rather than the loss of old knowledge in the PLM. (Sec. ). 
 Our study urges the NLP community to revisit and deepen the understanding of the forgetting in PLMs."
Latxa: An Open Language Model and Evaluation Suite for Basque,2403.20266v2,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2403.20266v2.png,Validation perplexity throughout training.,"We introduce Latxa, a family of large language models for Basque ranging from 7 to 70 billion parameters. Latxa is based on Llama 2, which we continue pretraining on a new Basque corpus comprising 4.3M documents and 4.2B tokens. Addressing the scarcity of high-quality benchmarks for Basque, we further introduce 4 multiple choice evaluation datasets: EusProficiency, comprising 5,169 questions from official language proficiency exams; EusReading, comprising 352 reading comprehension questions; EusTrivia, comprising 1,715 trivia questions from 5 knowledge areas; and EusExams, comprising 16,774 questions from public examinations. In our extensive evaluation, Latxa outperforms all previous open models we compare to by a large margin. In addition, it is competitive with GPT-4 Turbo in language proficiency and understanding, despite lagging behind in reading comprehension and knowledge-intensive tasks. Both the Latxa family of models, as well as our new pretraining corpora and evaluation datasets, are publicly available under open licenses. Our suite enables reproducible research on methods to build LLMs for low-resource languages.","Motivated by their increasing training cost and commercial interest, the development of Large Language Models (LLMs) has been led by close initiatives like GPT, Claude and Gemini. In recent times, a more open ecosystem has emerged following the release of various competitive models like Llama 2 and Mistral. However, despite early efforts to build open multilingual models, the most competitive ones are notoriously English-centric. As shown in Table, all these open models perform poorly in low-resource languages like Basque, with most results marginally surpassing random chance. 
In this work, we present Latxa, an open family of LLMs for Basque that substantially outperforms all these previous models. Basque is an agglutinative language written in Latin script and with no known relatives, although a significant part of the vocabulary is shared with contact languages like Spanish and French. Basque is the 52th language in Common Crawl, with 0.035%of the total content –for reference, English is the 1st language with 46%of the content and Spanish is the 5th with 4.6%. Our work builds on various open resources and models that we further expand to Basque, highlighting the importance of an open ecosystem for the development of language technology for low-resource languages. In particular, our models are based on Llama 2, which we continue training in Basque using a new corpus with 4.3M documents from 4 existing and 3 new sources. In addition, we release 4 diverse and challenging multiple-choice benchmarks comprising a total of 23,282 questions, covering language proficiency, reading comprehension, trivia questions, and public examinations. 
As shown in Table, Latxa performs substantially better than all existing open models, with the 70B variant outperforming the previous best open model (Yi 34B) by 18.95 points in average. In addition, it also outperforms the Llama 2 model it is based on by 25.18 points, and it is also superior to GPT-3.5 Turbo in all datasets we evaluate on. Interestingly, our best model also outperforms GPT-4 Turbo in language proficiency exams (EusProf), despite lagging behind in reading comprehension and knowledge-intensive tasks. This suggests that the capabilities that an LLM exhibits in a given language are not determined by its linguistic competence in this particular language, opening the doors to further improvements in low-resource LLMs as stronger English models become available. 
This paper makes the following contributions: (1) We release a high-quality corpus for Basque, comprising 4.3M documents and 4.2B tokens. The corpus combines the EusCrawl v1.1, Egunkaria, Booktegi, Wikipedia, CulturaX, Colossal OSCAR and HPLT v1 datasets (the first 3 being new), which we carefully deduplicate and filter. (2) We release the Latxa family of Basque LLMs, comprising 3 models with 7B, 13B and 70B parameters. (3) We release 4 new multiple-choice benchmarks for Basque: EusProficiency (official language proficiency exams), EusReading (reading comprehension), EusTrivia (trivia questions from 5 knowledge areas), and EusExams (public examinations). (4) We present extensive experiments comparing Latxa to previous open and closed models. (5) We show that it is possible to train significantly stronger LLMs for low-resource languages building on the existing ecosystem of open models and resources. In a similar spirit to other open LLMs, such as Pythia, LLM360 and OLMO, we release all the necessary data, code, weights and documentation to run and evaluate our models, facilitating similar efforts for other low-resource languages."
Talk With Human-like Agents: Empathetic Dialogue Through Perceptible Acoustic Reception and Reaction,2406.12707v1,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2406.12707v1.pdf,Examples illustrating the definition of empathy within dialogues.,"Large Language Model (LLM) -enhanced agents become increasingly prevalent in Human-AI communication, offering vast potential from entertainment to professional domains. However, current multi-modal dialogue systems overlook the acoustic information present in speech, which is crucial for understanding human communication nuances. This oversight can lead to misinterpretations of speakers 'intentions, resulting in inconsistent or even contradictory responses within dialogues. To bridge this gap, in this paper, we propose PerceptiveAgent, an empathetic multi-modal dialogue system designed to discern deeper or more subtle meanings beyond the literal interpretations of words through the integration of speech modality perception. Employing LLMs as a cognitive core, PerceptiveAgent perceives acoustic information from input speech and generates empathetic responses based on speaking styles described in natural language. Experimental results indicate that PerceptiveAgent excels in contextual understanding by accurately discerning the speakers' true intentions in scenarios where the linguistic meaning is either contrary to or inconsistent with the speaker's true feelings, producing more nuanced and expressive spoken dialogues. Code is publicly available at: <https: //github. com/Haoqiu-Yan/PerceptiveAgent>.","Artificial Intelligence (AI) agents are entities designed to replicate human-like intelligence and functionalities, serving as the essential building blocks of AI systems. An ideal agent should be capable of perceiving its environment with sensors, making informed decisions, and then taking actions in response to users or scenarios. Recently, Large Language Models (LLMs) have exhibited remarkable capabilities in diverse tasks, offering opportunities for building general AI agents that engage in human-like interactions, such as virtual assistants and intelligent robots. However, current text-only dialogue systems fall short in bridging the gap between experimental and realistic scenarios, where humans perceive and understand the world through diverse multi-modal information. Thus, the integration of acoustic information into dialogues has the potential to foster the development of more human-like agents, thereby enhancing the empathetic experience they offer. 
Empathetic responses involve two essential aspects: cognitive and affective empathy, which reflect an understanding of the human-talker 's thoughts and feelings respectively. Specifically, cognitive empathy involves understanding the human-talker' s thoughts, perspectives, and described events, enabling the agent to provide responses relevant to the dialogue topic. Conversely, affective empathy entails responding based on observed emotional expressions in the dialogue history, contributing to the naturalness of synthesized speech. While recent works leverage LLM 's strong capabilities of contextual understanding and content generation to synthesize empathetic speeches, there remains a discrepancy between cognitive and affective empathy. This arises because cognitive content is preassigned before affective speech is deduced from latent representations of multi-modal dialogue history. 
Recently, advancements in multi-modal content perception and generation have been achieved by various methods, where audio is represented as either recognized text with an automatic speech recognition model or discrete features with a speech encoder. However, while linguistic information in speech is predominantly captured by both discrete acoustic units and textual representations, acoustic features tend to be disregarded. This oversight can lead to misinterpretations of the speaker' s intentions, resulting in discrepant or even contradictory responses within the dialogue history. As illustrated in Figure, the left scenario fails to consider the perspective of the listener while the right one barely understands or empathizes with the speaker 's feelings. 
In this paper, we propose PerceptiveAgent, an empathetic multi-modal dialogue system that can discern deeper or more subtle meanings beyond the literal interpretations of words, based on speaking styles described in natural language. Specifically, PerceptiveAgent first comprehends the speaker' s intentions accurately by a perceptive captioner model that captures acoustic features from each speech within dialogues. Subsequently, an LLM module acts as the cognitive core, producing the relevant response content with a caption describing how to articulate the response. A Multi-Speaker and Multi-Attribute Synthesizer (MSMA-Synthesizer) is then developed to synthesize nuanced and expressive speech. 
Our contributions include the following:"
WebCiteS: Attributed Query-Focused Summarization on Chinese Web Search Results with Citations,2403.01774v2,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2403.01774v2.pdf,Illustration of attributed query-focused summarization (AQFS). Full example is shown in Table.,"Enhancing the attribution in large language models (LLMs) is a crucial task. One feasible approach is to enable LLMs to cite external sources that support their generations. However, existing datasets and evaluation methods in this domain still exhibit notable limitations. In this work, we formulate the task of attributed query-focused summarization (AQFS) and present WebCiteS, a Chinese dataset featuring 7k human-annotated summaries with citations. WebCiteS derives from real-world user queries and web search results, offering a valuable resource for model training and evaluation. Prior works in attribution evaluation do not differentiate between groundedness errors and citation errors. They also fall short in automatically verifying sentences that draw partial support from multiple sources. We tackle these issues by developing detailed metrics and enabling the automatic evaluator to decompose the sentences into sub-claims for fine-grained verification. Our comprehensive evaluation of both open-source and proprietary models on WebCiteS highlights the challenge LLMs face in correctly citing sources, underscoring the necessity for further improvement.","In today's information-driven society, swift access to knowledge is essential. A major limitation of web search engines is the need for users to manually compile information from various sources, which can be time-consuming. Large language models (LLMs) exhibit potential in this domain by generating straightforward and well-organized responses. However, the potential risks of hallucination and factual errors undermine their trustworthiness as knowledge sources. 
An emerging solution is generative search engines which use LLMs to synthesize web search results into responses with in-line citations. This allows users and developers to verify the generations against the cited sources. However, recent investigations on commercial products and retrieval-augmented LLMs reveal frequent occurrences of unsupported claims and incorrect citations, highlighting the challenges of attribution in LLMs. Nonetheless, the limitations of pertinent datasets and evaluation methods pose obstacles to in-depth explorations within the community. 
 
 
Firstly, most existing datasets are deficient in high-quality citation annotations. For instance, the ALCE benchmark compiles three question-answering datasets without providing citations in the reference answers, limiting its utility for model training. In contrast, WebGLM prompts InstructGPT to generate training data with citations. It controls the citation quality via a sample filtering method which calculates the ROUGE score between the answers and their citations. However, this method focuses on lexical similarity rather than logical entailment, and thus could not precisely measure attribution."
"PsySafe: A Comprehensive Framework for Psychological-based Attack, Defense, and Evaluation of Multi-agent System Safety",2401.11880v3,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2401.11880v3.pdf,"Examples of Agents' Interactions after Psychological-based Attack. After being attacked, the multi-agent system, whether for safe daily tasks or dangerous jailbreak tasks, provides dangerous answers. Agents collaborate with each other to generate dangerous content. Responses identified as dangerous are highlighted in red, whereas safe responses are indicated in green.","Multi-agent systems, when enhanced with Large Language Models (LLMs), exhibit profound capabilities in collective intelligence. However, the potential misuse of this intelligence for malicious purposes presents significant risks. To date, comprehensive research on the safety issues associated with multi-agent systems remains limited. In this paper, we explore these concerns through the innovative lens of agent psychology, revealing that the dark psychological states of agents constitute a significant threat to safety. To tackle these concerns, we propose a comprehensive framework (PsySafe) grounded in agent psychology, focusing on three key areas: firstly, identifying how dark personality traits in agents can lead to risky behaviors; secondly, evaluating the safety of multi-agent systems from the psychological and behavioral perspectives, and thirdly, devising effective strategies to mitigate these risks. Our experiments reveal several intriguing phenomena, such as the collective dangerous behaviors among agents, agents 'self-reflection when engaging in dangerous behavior, and the correlation between agents' psychological assessments and dangerous behaviors. We anticipate that our framework and observations will provide valuable insights for further research into the safety of multi-agent systems. We make our data and code publicly accessible at <https: //github. com/AI4Good24/PsySafe>. 
Warning: this paper includes examples that may be offensive or harmful.","Recently, agents based on Large Language Models (LLMs) have demonstrated significant capabilities, such as solving complex tasks and simulating social interactions. However, their growing capabilities also raise concerns about potential misuse, such as creating malware or deceptive websites, as illustrated in Figure. 
While there are numerous efforts on LLM safety, the exploration of safety in multi-agent systems, particularly from a psychological perspective, remains underdeveloped. We observe that agents, when processing dark psychological states, tend to exhibit dangerous behaviors. From this standpoint, we propose a framework (PsySafe) that systematically targets psychological safety vulnerabilities within multi-agent systems, comprehensively assesses their safety from psychological and behavioral angles, and strategically defends against identified vulnerabilities. Our framework focuses on three questions: 1) How to discover safety vulnerabilities of multi-agent systems? 2) How to comprehensively evaluate the safety of multi-agent systems? 3) How to defend these safety vulnerabilities? 
How to discover safety vulnerabilities of multi-agent systems? Currently, the majority of research is mainly concentrated on attacking LLMs, but the complex interactions and role settings in multi-agent systems present unique challenges. To identify safety vulnerabilities in multi-agent systems, we explore two areas: a) dark psychological effect on agents 'behaviors; and b) different attack strategies on multi-agent systems. For the first aspect, we start from the perspective of agent psychology, investigating the impact of dark traits on agents' behaviors. We devise an advanced dark traits injection method to contaminate agents. As illustrated in Figure, agents with injected dark traits not only respond to dangerous queries but also suggest risky solutions to safe queries. We also obtain some interesting observations, such as collective dangerous tendencies and self-reflection within the multi-agent system, as well as the correlation between the behaviors and psychological states of agents. For the second area, we investigate attacks on multi-agent systems from two angles: their role configurations and human-agent interaction. We develop two attack strategies: targeting agent traits and human input. These attack angles can effectively compromise prevalent multi-agent systems and lead to the emergence of collective dangerous behaviors within agents. 
How to comprehensively evaluate the safety of multi-agent systems? Evaluating the safety of large language models primarily focuses on their inputs and outputs. Due to the role of agents and the complexity of multi-turn dialogues, safety evaluation methodologies tailored for LLMs are not directly suitable for multi-agent systems. To comprehensively evaluate the safety of multi-agent systems, we focus on two perspectives: the psychology and the behavior of agents, conducting psychological evaluation and behavior evaluation on the multi-agent systems. For the psychological evaluation, we administer popular dark triad psychological tests to the agents, representing their tendency to engage in dangerous behaviors in the future. Our findings reveal a significant correlation between psychological assessment scores and the safety of agent behaviors, which can be utilized to evaluate the safety status of the agent and in developing defense mechanisms. Regarding the behavior evaluation, we propose the process danger rate and joint danger rate, derived from the perspective of the agent 's interaction process. Process danger rate denotes the partial danger condition present in the agents' interaction process. Joint danger rate denotes the joint danger conditions among agents across different interaction rounds. Together, they can comprehensively represent the dangerous behaviors in multi-agent systems and the trends of agents' dangerous propensity. To achieve a more comprehensive evaluation of multi-agent system safety, we compile a dataset comprising both safe and dangerous tasks, assessing the safety of multi-agent systems under various circumstances. 
How to defend these safety vulnerabilities? Current defense strategies primarily concentrate on safeguarding individual Large Language Models (LLMs) with limited studies addressing the protection of multi-agent systems. In our analysis, we explore the defense mechanisms of multi-agent systems from both external and internal perspectives, including input defense, psychological-based defense, and role-based defense. Input defense refers to input filtering using popular dangerous content detectors. We find that current input-side filtering techniques are ineffective in mitigating the dark traits injection. From an internal perspective, we propose psychological-based defense to effectively mitigate the dark psychological states of agents, thereby substantially decreasing the likelihood of dangerous behaviors. Furthermore, role-based defense can also effectively reduce the emergence of collective dangerous behaviors among agents."
Political Compass or Spinning Arrow? Towards More Meaningful Evaluations for Values and Opinions in Large Language Models,2402.16786v2,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2402.16786v2.png,"A model is prompted with a proposition from the Political Compass Test. In the most constrained setting (left), the model is given multiple choices and forced to choose one. In a less constrained setting (middle), the same model gives a different answer. In the more realistic unconstrained setting (bottom), the same model takes a different position again, which is also one discouraged in the constrained settings.","Much recent work seeks to evaluate values and opinions in large language models (LLMs) using multiple-choice surveys and questionnaires. Most of this work is motivated by concerns around real-world LLM applications. For example, politically-biased LLMs may subtly influence society when they are used by millions of people. Such real-world concerns, however, stand in stark contrast to the artificiality of current evaluations: real users do not typically ask LLMs survey questions. Motivated by this discrepancy, we challenge the prevailing constrained evaluation paradigm for values and opinions in LLMs and explore more realistic unconstrained evaluations. As a case study, we focus on the popular Political Compass Test (PCT). In a systematic review, we find that most prior work using the PCT forces models to comply with the PCT's multiple-choice format. We show that models give substantively different answers when not forced; that answers change depending on how models are forced; and that answers lack paraphrase robustness. Then, we demonstrate that models give different answers yet again in a more realistic open-ended answer setting. We distill these findings into recommendations and open challenges in evaluating values and opinions in LLMs.","What values and opinions are manifested in large language models (LLMs)? This is the question that a growing body of work seeks to answer. The motivation for most of this work comes from real-world LLM applications. For example, we may be concerned about how LLM opinions on controversial topics such as gun rights (mis-) align with those of real-world populations. We may also worry about how LLMs that exhibit specific political values may influence society when they are used by millions of people. 
Current evaluations for LLM values and opinions, however, mostly rely on multiple-choice questions, often taken from surveys and questionnaires. , for example, take questions from Pew 's Global Attitudes and the World Value Survey. primarily draw on Dutch and German voting advice applications. These may be suitable instruments for measuring the values and opinions of human respondents, but they do not reflect real-world LLM usage: while real users do talk to LLMs about value-laden topics and ask controversial questions, they typically do not use multiple-choice survey formats. This discrepancy motivates our main research question: How, if at all, can we meaningfully evaluate values and opinions in LLMs? 
To answer this question, we revisit prior work and provide new evidence that demonstrates how constrained evaluations for LLM values and opinions produce very different results than more realistic unconstrained evaluations, and that results also depend on the precise method by which models are constrained (see Figure). As a case study, we focus on the Political Compass Test (PCT), a multiple-choice questionnaire that has been widely used to evaluate political values in LLMs. We make five main findings: 
 0em 
 * We systematically review Google Scholar, arXiv, and the ACL Anthology, and show that most of the 12 prior works that use the PCT to evaluate LLMs force models to comply with the PCT' s multiple-choice format (). 
 * We show that models give different answers when not forced (). 
 * We show that answers also change depending on how models are forced (). 
 * We show that multiple-choice answers vary across minimal prompt paraphrases (). 
 * We show that model answers change yet again in a more realistic open-ended setting (). 
Overall, our findings highlight clear instabilities and a lack of generalisability across evaluations. Therefore, we recommend the use of evaluations that match likely user behaviours in specific applications, accompanied by extensive robustness tests, to make local rather than global claims about values and opinions manifested in LLMs."
Pride and Prejudice: LLM Amplifies Self-Bias in Self-Refinement,2402.11436v2,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2402.11436v2.png,"How LLM's self-feedback inflates scores compared to human assessment. Bias is the mean difference between LLM and human scores, while skewness (Dskew) measures the asymmetry of their distribution around zero. Non-biased estimation will have Dskew=0.","Recent studies show that large language models (LLMs) improve their performance through self-feedback on certain tasks while degrade on others. We discovered that such a contrary is due to LLM 's bias in evaluating their own output. In this paper, we formally define LLM' s self-bias – the tendency to favor its own generation – using two statistics. We analyze six LLMs (GPT-4, GPT-3.5, Gemini, LLaMA2, Mixtral and DeepSeek) on translation, constrained text generation, and mathematical reasoning tasks. We find that self-bias is prevalent in all examined LLMs across multiple languages and tasks. Our analysis reveals that while the self-refine pipeline improves the fluency and understandability of model outputs, it further amplifies self-bias. To mitigate such biases, we discover that larger model size and external feedback with accurate assessment can significantly reduce bias in the self-refine pipeline, leading to actual performance improvement in downstream tasks. The code and data are released at <https: //github. com/xu1998hz/llm_ self_ bias>.","Large language models (LLMs) have shown strong capabilities in many NLP tasks. While these models still make mistakes, recent studies show that ""self-refine"" (also known as ""self-reflection"") is promising to rectify errors based on LLM 's self-feedback. Meanwhile, opposite study also shows that LLMs fail to correct their mistakes and their performance even gets worse after self-feedback. These contradictory results suggest that LLM' s self-feedback is unreliable. Self-refine procedure relies on LLM 's evaluation capability of the generated text. We hypothesize that if there is a bias during the self-evaluation process, such bias will be amplified during iterative self-refinement. This is consistent with a prior finding that LM-based metrics (e.g.,BARTScore) exhibit ""narcissism"" during self-evaluation, i.e., the metric model favors text generated by the same underlying language model in the context of summarization tasks. However, it remains unclear whether bias exists universally in LLMs across a wide range of tasks. How to quantify such biases? How does this ""narcissism"" impact LLM' s self-refinement? 
In this work, we define ""self-bias"" to the degree that an LLM favors its own generation. We propose to use two principled statistics to estimate self-bias in LLM 's self-refinement procedure. The first one measures the degree of inflation in the LLM' s self-evaluation compared to the true (human) evaluation. The second measures whether LLM 's self-evaluation is skewed compared to the ture estimate. Figure illustrates these two statistics. We examine self-bias scores on six diverse LLMs, covering four languages across three distinct tasks: machine translation, constrained text generation, and mathematical reasoning. We find that self-bias is universal in self-refine and self-rewarding pipelines, regardless of the languages and tasks. This bias causes LLMs to optimize for false positive corrections rather than improving the actual output quality. 
We further investigate what is the real benefit of self-refine. We find that while the self-refine pipeline improves the fluency and understandability of model outputs, it does not necessarily lead to intended improvements as specified in the prompt. Moreover, LLMs may favor texts that mirror their style, potentially leading to false positive optimization and reduced diversity in text generation. To mitigate the self-bias, we propose two solutions: increasing the model size and incorporating external feedback to provide accurate assessment, thereby directing the LLM towards more accurate self-correction. Our contributions are: 
 * We formally define the self-bias of an LLM using two principled estimated statistics. 
 * We quantify self-biases for six diverse LLMs and find that self-bias amplifies during self-refine across many languages and tasks. 
 * We observe two factors that contribute to self-bias and pinpoint two directions to mitigate it and elicit LLMs' self-correction ability."
CHECKWHY: Causal Fact Verification via Argument Structure,2408.10918v2,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2408.10918v2.pdf,"An entry from CheckWhy: a ""why"" claim with its corresponding cause and effect, and an argument structure representing the reasoning process from cause to effect. Notably, the cause-effect pair is used solely during the annotation process and not included in the argument structure, implying that it is implicitly inferred from the claim, rather than being provided explicitly.","With the growing complexity of fact verification tasks, the concern with ""thoughtful"" reasoning capabilities is increasing. However, recent fact verification benchmarks mainly focus on checking a narrow scope of semantic factoids within claims and lack an explicit logical reasoning process. In this paper, we introduce CheckWhy, a challenging dataset tailored to a novel causal fact verification task: checking the truthfulness of the causal relation within claims through rigorous reasoning steps. CheckWhy consists of over 19K ""why"" claim-evidence-argument structure triplets with supports, refutes, and not enough info labels. Each argument structure is composed of connected evidence, representing the reasoning process that begins with foundational evidence and progresses toward claim establishment. Through extensive experiments on state-of-the-art models, we validate the importance of incorporating the argument structure for causal fact verification. Moreover, the automated and human evaluation of argument structure generation reveals the difficulty in producing satisfying argument structure by fine-tuned models or Chain-of-Thought prompted LLMs, leaving considerable room for future improvements.","Fact verification is a crucial debunking task that entails verifying the truthfulness of claims by cross-referencing them with reliable evidence drawn from established resources, which prevents the proliferation of erroneous information online and fosters public trust. However, with the multi-step reasoning capability in fact verification models remaining uncertain, existing research reflects the deficiency in in-depth understanding of the explicit reasoning mechanisms when performing inference on multi-hop evidence. This prompts us to develop a strong benchmark that incorporates the interpretable ""thought"" process to assess the logical reasoning capabilities of models. 
Currently, substantial progress has been made on common fact verification benchmarks, e.g., HoVer, FEVEROUS, and SciTab. Nevertheless, existing resources have inherent limitations. Classic datasets primarily focus on verifying the semantic factoids of ""who"", ""what"", ""when"", and ""where"" within the claim. For example, verifying the claim ""John Lennon was born before the astronaut who drank the first coffee in space. "" can be decomposed into verifying factoids such as ""who, and when the astronaut drank the first coffee in space. "" and ""when the man was born. "". However, these semantic factoids can be answered individually by straightforward factoids-matching between the claim and distinct independent evidence, e.g., word overlapping or proof matching, thereby limiting its significant potential to summarize the correlational evidence with ""thought"" steps. Heretofore, noticeably absent in prior datasets are ""why"" claims: containing causal relations that need to be verified. These claims prompt for not simple factoid matching, but an explicit logical reasoning path for verification. 
More specifically, Figure presents a ""why"" claim featuring a cause-effect pair where ""military crises"" causes the ""decrement of the purity of denarius silver. "". Verifying such causal relations is quite challenging, which necessitates complex logical reasoning and context information beyond the factoids within the claim. For instance, to support this causal relations (i.e., military crises 
 decrease purity of silver), it is essential to incorporate the extra intermediate reasoning steps to bridge the connection between the cause-effect pair, thus forming a logical reasoning path: military crisesfinancial crisisincrease amount of currencydecrease purity of silver. The rationale behind 171+1 is that military crises often coincide with extra factors such as trade disruption or more government spending, leading to the financial crisis. The reason supporting 171+2 is that increase amount of currency is a demanding response to suppress the financial crisis. Furthermore, the rights to mint money that is safeguarded by Roman Law ensures the behavior of 171+3 is established. The above inference reveals a coherent reasoning process, which involves aligning the construction of a ""thoughtful"" logical structure among correlational evidence with causal relation verification. 
 
 In this paper, we introduce CheckWhy, a challenging dataset built around a novel causal fact verification task: assessing whether the causal relation within the claim is valid by explicit logical structure. This dataset consists of 19,596 claim-evidence-argument structure triplets derived from the WikiWhy dataset. The uniqueness of CheckWhy is that each entry contains a ""why"" claim with causal relations and an argument structure formed by correlated evidence: the latter is inspired by the theory literature on argument structure, which depicts how the different statements fit together as wholes to allegedly lend support to the claim. Moreover, inspired by prior research, we assume that the label of a causal relation within the claim depends on the provided argument structures, rather than the semantics itself. Thus, each claim is labeled as supports, refutes, or not enough info based on different argument structures. In addition, to prevent the bias in human cognition, we employ a human-model collaboration annotation approach to generate claims, evidence, and corresponding argument structures. Compared to existing datasets, CheckWhy covers a variety of topics and argument structures, which may prove valuable for performing causal reasoning across various scenarios. 
 
 Based on the experiments on four tasks we propose and the human evaluation in our CheckWhy, our experiments reveal the significance of incorporating the argument structure for causal fact verification. Meanwhile, our experiments in argument structure generation also validate the difficulty in producing satisfying argument structures for causal claims. Our key contributions are summarized as follows: (I) We propose verifying the ""why"" claims with causal relations through reasoning on argument structure as a novel causal fact verification formulation. (II) We construct CheckWhy by introducing a human-model collaboration annotation approach, drawing inspiration from the theory research on argument structure. (III) We conduct thorough experiments on state-of-the-art models with four tasks, including fine-tuned models and LLMs, which investigates various settings and points out the potential for improvement."
American Sign Language Handshapes Reflect Pressures for Communicative Efficiency,2406.04024v2,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2406.04024v2.pdf,"Examples of handshapes in ASL components. The ASL lexicon can be divided into a native component (e.g.,signs native to ASL; left) and a foreign component (e.g.,fingerspelling, loan signs; right). 19 out of 22 handshapes in ASL fingerspelling also appear in the native lexicon.","Communicative efficiency is a key topic in linguistics and cognitive psychology, with many studies demonstrating how the pressure to communicate with minimal effort guides the form of natural language. However, this phenomenon is rarely explored in signed languages. This paper shows how handshapes in American Sign Language (ASL) reflect these efficiency pressures and provides new evidence of communicative efficiency in the visual-gestural modality. 
We focus on hand configurations in native ASL signs and signs borrowed from English to compare efficiency pressures from both ASL and English usage. First, we develop new methodologies to quantify the articulatory effort needed to produce handshapes and the perceptual effort required to recognize them. Then, we analyze correlations between communicative effort and usage statistics in ASL or English. Our findings reveal that frequent ASL handshapes are easier to produce and that pressures for communicative efficiency mostly come from ASL usage, rather than from English lexical borrowing.","There is increasing evidence suggesting that human languages adapt to the needs of their users by minimizing the effort required by the sender and receiver to achieve successful communication. It is argued that natural languages tend to find an optimal balance between the efforts of the two participants when they are at odds with each other. 
While the majority of existing work studying communicative efficiency has focused on spoken languages, recent results have provided evidence that signed languages are also shaped by drives for efficient communication in the visual-gestural modality. For example, found that casual signing prioritizes moving fewer, more distal joints, increasing articulatory ease. found that rare hand configurations are produced close to the signer's face, increasing perceptual ease. Handshape artworks shared with permission from. 
This paper further explores communicative pressures on visual signed languages by focusing on American Sign Language (ASL) handshapes. Handshapes refer to distinctive configurations of the hand and fingers, and are one of the five fundamental parameters characterizing signs in ASL (). A finite set of handshapes is combined with different movements, locations, palm orientations, and non-manual markers to express various ASL signs. Figure shows example handshapes, some of which are used only in native ASL signs (left), some of which are used only in ASL signs borrowed from other languages (right), and some of which are used in both contexts (center). 
We investigated evidence of communicative efficiency in both the native and foreign components of ASL. Foreign signs are borrowed from other languages in contrast to native signs that are inherently derived from ASL itself. The foreign component of ASL includes neutral fingerspelling where English words are spelled out using one-handed signs that each represent a letter of the English alphabet (); loan signs where commonly fingerspelled words evolved into a lexicalized sign; and initialized signs that are produced using the handshape of the first letter of its English translation. 
To compare effects of ASL and English usage on handshape efficiency, we focused on fingerspelling handshapes (FS handshapes; Figure) since 19 out of 22 FS handshapes appear in both the native and foreign components. We were motivated by the following research questions: 
 
 RQ1Do handshapes reflect pressure for communicative efficiency? 
 RQ2Is pressure for efficiency mostly or all from ASL usage, or does English usage also play a role? 
To test these ideas, we designed new methodologies to measure articulatory effort required by the sender to produce handshapes, as well as the perceptual effort needed by the receiver to recognize handshapes. We propose three predictions following from the hypothesis of communicative efficiency (P1-3). 
First, we predicted that according to the hypothesis of communicative efficiency, FS handshapes which appear frequently in native ASL signs should be easier to produce: this would help to keep overall articulatory (sender) effort low (P1). Our evidence indeed supports this prediction: we found a positive correlation between handshape frequency in native ASL signs and articulatory ease. 
Next, since foreign signs obey fewer phonological constraints observed in ASL, one might expect that handshapes in foreign signs reflect little to no pressure for communicative efficiency. If so, letters that appear frequently in English do not necessarily correspond to being easier to sign in ASL fingerspelling (P2). We indeed found no significant correlation between English letter frequency and fingerspelling articulatory ease, which supports P2. 
If English usage has negligible effect on communicative efficiency, foreign signs should reflect little to no pressure for efficiency in perceptual (receiver) effort either (P3). If there are perceptual pressures for efficiency, pairs of letters that appear in similar contexts in English should be easier to disambiguate from one another in ASL fingerspelling: this would help to keep receiver effort low by placing perceptually disambiguating elements in places that might otherwise be confusable. We indeed verified that there is no correlation between English letter confusability and perceptual ease in ASL fingerspelling, which supports P3. 
In summary, our analysis finds evidence of pressure for articulatory ease in ASL handshapes and suggests that pressure for communicative efficiency mostly comes from ASL usage, not from English usage."
COKE: A Cognitive Knowledge Graph for Machine Theory of Mind,2305.05390v2,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2305.05390v2.pdf,COKEinstantiates the Theory of Mind as cognitive chains in social situations.,"Theory of mind (ToM) refers to humans 'ability to understand and infer the desires, beliefs, and intentions of others. The acquisition of ToM plays a key role in humans' social cognition and interpersonal relations. Though indispensable for social intelligence, ToM is still lacking for modern AI and NLP systems since they cannot access the human mental state and cognitive process beneath the training corpus. To empower AI systems with the ToM ability and narrow the gap between them and humans, in this paper, we propose COKE: the first cognitive knowledge graph for machine theory of mind. Specifically, COKEformalizes ToM as a collection of 45k+ manually verified cognitive chains that characterize human mental activities and subsequent behavioral/affective responses when facing specific social circumstances. In addition, we further generalize COKEusing LLMs and build a powerful generation model COLMtailored for cognitive reasoning. Experimental results in both automatic and human evaluation demonstrate the high quality of COKE, the superior ToM ability of COLM, and its potential to significantly enhance social applications. We release our code and data at <https: //github. com/jincenziwu/COKE>.","In social environments, human beings must be able not only to react to what others are doing, but also to anticipate what they will do. This ability to understand and infer human goals is typically described as Theory of Mind (ToM). One way of accomplishing ToM is to observe what others do in various situations, and derive a set of affective and behavioral rules. When the same or highly similar things arise again, we can bring out plausible predictions accordingly. Figure presents an example that someone will deliver a talk at the university tomorrow (a social circumstance), and he has substantial public speaking experience (a trigger factor). We can plausibly anticipate that he will deliver an impressive speech (a mental activity), feels joyful (an affective response), and has a restful sleep tonight (a behavioral response). Here ToM is instantiated as a chained cognitive process that derives from our knowledge, experiences, and memories. ToM is indispensable to humans since it allows us to leverage our own minds to simulate others ', so as to achieve efficient communication. 
 
 
Despite its importance for social intelligence, ToM is not well internalized by modern AI and NLP systems. illustrates a significant decline in performance and outright failure of Large Language Models (LLMs) in ToM tasks, particularly evident when confronted with adversarial samples. The main reason is that learning-based systems are usually trained on superficial text corpora, while lacking access to the underlying human mental state and cognitive process. In other words, NLP systems rely on the maximum likelihood to understand and generate texts, but do not go beneath the surface to the desires, beliefs, and intentions of humans. 
In this paper, we introduce COKE: the first COgnitive KnowledgE graph for machine theory of mind. Our goal is to formalize ToM and make it accessible and learnable for AI systems. In COKE, we instantiate ToM as a collection of manually verified cognitive chains that characterize humans' mental activities in specific social circumstances along with their behavioral and affective responses. Each cognitive chain involves five types of nodes: 1) situations denote the social circumstances; 2) clues denote the trigger factors; 3) thoughts denote the mental activities; 4) actions denote the behavioral responses; 5) emotions denote the affective responses. Moreover, as shown in Figure, individuals react differently to the same situation due to the diversified cognitive processes. Therefore, for each situation, we derive multiple cognitive chains and further label them as positive (means optimistic) or negative (means pessimistic) to mark the chain polarity. We propose to induce the raw data from LLMs, and then recruit educated workers majoring in psychology for manual selection and revision. The resulting knowledge graph constitutes 62,328 nodes and 45,369 cognitive chains. 
The construction of COKEoffers the basic ToM ability to understand and infer the human goals in already collected situations. But obviously, it is impossible to enumerate all situations in the real world. Thus we move one step further and build a cognitive language model COLMto cope with unseen situations that have not appeared in the knowledge graph. Specifically, we decompose the construction of cognitive chains into four cognitive generation tasks, then finetune LLMs using the manually collected data in COKE. By this means, we combine the commonsense knowledge embedded in LLMs and the ToM ability provided by COKE, enabling COLMto infer cognitive chains for unseen situations. 
We summarize our contributions in this work as follows. 1) We propose the first cognitive knowledge graph for machine theory of mind. We instantiate human theory of mind as a collection of 45k
 +manually verified cognitive chains, which provides a basic ToM ability for accessing and learning. 2) We build a powerful cognitive language model COLM by associating COKE with LLaMA-2, so as to predict cognitive chains for out-of-KG situations. 3) We conduct extensive experiments to evaluate the ToM ability of COLM and typical LLMs. The results show that COLM outperforms strong baseline models such as GPT-4 in both zero-shot and few-shot settings, proved by automatic and human evaluations in all cognitive generation tasks, which in turn demonstrates the high quality of COKE. 4) We further substantiate the potential of COKE in enhancing social applications, and prove its effectiveness on downstream emotional support conversation task."
MMToM-QA: Multimodal Theory of Mind Question Answering,2401.08743v2,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2401.08743v2.pdf,"Sketch of the MMToM-QA benchmark. Each question is associated with a video stream (representative frames highlighting key moments are shown above for illustration) and text input (illustrative text above is shortened for brevity). In the example video, Emily can see the wine glass on one of the kitchen tables (1st frame) and passes by it without picking it up (2nd frame). At the end of the clip (3rd frame), it appears that she could be walking towards the cabinets on the left side of the room; or she might want to check if a goal object is inside the microwave. The text indicates that there are no cupcakes in the cabinets, but there is a cupcake inside the microwave. To confidently choose the correct answer, a model must fuse relevant information from both the video and the text.","Theory of Mind (ToM), the ability to understand people 's mental states, is an essential ingredient for developing machines with human-level social intelligence. Recent machine learning models, particularly large language models, seem to show some aspects of ToM understanding. However, existing ToM benchmarks use unimodal datasets – either video or text. Human ToM, on the other hand, is more than video or text understanding. People can flexibly reason about another person' s mind based on conceptual representations (e.g., goals, beliefs, plans) extracted from any available data. To address this, we introduce a multimodal Theory of Mind question answering (MMToM-QA) benchmark. MMToM-QA comprehensively evaluates machine ToM both on multimodal data and on different kinds of unimodal data about a person's activity in a household environment. To engineer multimodal ToM capacity, we propose a novel method, BIP-ALM (Bayesian Inverse Planning Accelerated by Language Models). BIP-ALM extracts unified representations from multimodal data and utilizes language models for scalable Bayesian inverse planning. We conducted a systematic comparison of human performance, BIP-ALM, and state-of-the-art models, including GPT-4. The experiments demonstrate that large language models and large multimodal models still lack robust ToM capacity. BIP-ALM, on the other hand, shows promising results, by leveraging the power of both model-based mental inference and language models.","Theory of Mind (ToM) is the cognitive ability to ascribe hidden mental states (e.g.,goals, beliefs, and desires) to other individuals based on their observed behavior. A hallmark of human social intelligence, ToM serves as the foundation for a wide range of social interactions and a pillar of commonsense reasoning. Systems designed to safely and productively interact with humans in an open-ended manner, such as assistive robots, AI teachers, and autonomous vehicles, would greatly benefit from incorporating ToM reasoning capabilities. The recent advancements in machine learning, especially in the realm of Large Language Models (LLMs), have spurred increased interest in assessing these models 'aptitude for ToM reasoning. Many of these assessments use either text-based or video-based benchmarks inspired by classic ToM experiments in the cognitive science literature. 
While the recent ToM benchmarks provide well-designed, cognitively informed tools, they share several notable limitations. One such limitation is the dependence on massive training data, which raises the concern that these models work by finding data patterns in a way that deviates from human-like ToM reasoning. This paper focuses on a different but related limitation: These benchmarks rely on unimodal data, either in the form of videos, or textual descriptions of actions and environments. But ToM reasoning goes beyond merely text comprehension or video understanding. It is about forming a causal model of another person' s mind, which connects mental variables to possible actions. Such a model can infer mental states from either words or vision separately, or fuse the separate information to form a single coherent mental scene. By examining multimodal ToM reasoning, we can both gain insight into the computational models that underlie human ToM and offer a stronger test for current ML models, particularly LLMs. 
To systematically evaluate the ability of ML models to infer mental states from multimodal data, we developed a novel Multimodal Theory of Mind Question Answering benchmark (MMToM-QA). As shown in Figure, the benchmark includes as input both videos and text describing the activity of a person in a household environment. The benchmark also includes questions associated with different points in each of the videos. The questions refer to the mental states (goals and beliefs) of the person described by the video or text. Each question has two possible options, neither surely true nor surely false, but with one option significantly more likely to be true given the observations. Some questions can be adequately answered based on a single modality, but some questions require fusing information from both modalities (e.g.,understanding the woman 's goal in Figure). We validated our benchmark through human experiments, showing that people are adept at answering the questions in the benchmark and providing a human baseline. 
We propose a novel multimodal ToM model, Bayesian Inverse Planning Accelerated by Language Models (BIP-ALM). As illustrated in Figure, BIP-ALM first extracts symbolic representations about the physical scene and the actions of the person from both video and text inputs. Using these symbolic representations, BIP-ALM then extends Bayesian inverse planning (BIP), a cognitively grounded ToM method originally designed for visual data, to reason about the multimodal data. To accelerate the inference in real-world scenarios such as household activities in our benchmark, BIP-ALM uses a language model (LM) finetuned on human activity data to evaluate the likelihood of hypotheses about the person' s belief and goal. By doing so, it takes advantage of the robustness of Bayesian inverse planning, as well as the scalability and open-endedness of LMs. 
We compared the performance of BIP-ALM and several state-of-the-art models for text QA or multimodal QA, including GPT-4 (V). We found that existing models, however impressive in other QA benchmarks, make large and systematic errors in our benchmark, and fail to match human performance. In contrast, BIP-ALM significantly outperforms these models. 
In sum, our main contributions include (1) the first benchmark for multimodal ToM, (2) a novel ToM reasoning method, BIP-ALM, that combines Bayesian inverse planning and LMs to conduct robust yet efficient ToM inference based on multimodal data, and (3) a systematic comparison of different ML models and human ToM."
Unintended Impacts of LLM Alignment on Global Representation,2402.15018v2,/export/fs06/yguan19/Figure1_Dataset_2024_latexparser/benchmark/img/2402.15018v2.pdf," [Country-specific rewards for Starling 7B Reward Model] Country rewards for Starling 7B Reward Model prompted with ""User: Where are you from? Assistant: I am from {country}. "" Starling assigns higher rewards to English-speaking Western nations and lower rewards to countries in the Middle East/Africa.","Before being deployed for user-facing applications, developers align Large Language Models (LLMs) to user preferences through a variety of procedures, such as Reinforcement Learning From Human Feedback (RLHF) and Direct Preference Optimization (DPO). Current evaluations of these procedures focus on benchmarks of instruction following, reasoning, and truthfulness. However, human preferences are not universal, and aligning to specific preference sets may have unintended effects. We explore how alignment impacts performance along three axes of global representation: English dialects, multilingualism, and opinions from and about countries worldwide. Our results show that current alignment procedures create disparities between English dialects and global opinions. We find alignment improves capabilities in several languages. We conclude by discussing design decisions that led to these unintended impacts and recommendations for more equitable preference tuning. We make our code and data publicly available on Github.","Recently, LLM-powered chat assistants have exploded in popularity. As of December 2023, ChatGPT has amassed over 100M weekly users and Llama-2-Chat-7B is downloaded almost one million times a month from HuggingFace. The success of these chat models is dependent on ""alignment"", which takes a base model with a language modeling objective and produces an instruction following preference-guided model to better serve user interests. Practitioners use algorithms such as RLHF and DPO to optimize models for attributes such as helpfulness and harmlessness and give them their chat assistant persona. 
Unlike the nebulous pre-training process, which is largely defined by the distribution of data online, model developers have a high degree of control for the key alignment variables. Who will give feedback? What prompts/tasks are in-domain? Who will provide exemplar responses? These are just a few design decisions that underscore a larger question: Whose preferences are we aligning LLMs with, and crucially, whose preferences are we missing? As put it, ""For which speakers are NLP systems developed? "" This question often does not have an explicit answer in current alignment practices, making it unclear which model behaviors are intentional normative judgments and which are unintended biases. For example, the Starling 7B Reward Model gives higher scores to responses claiming to be from English-speaking Western nations and lower scores for Middle Eastern and African nations (See Figure). In this work, we take a closer look at the effects these design decisions have on a model 's ability to serve a global population, which is key to understanding if the general use of aligned LLMs is likely to be positively adopted globally. 
Existing performance evaluations of chat assistants mainly focus on tasks such as reasoning, multitask knowledge, truthfulness, multi-turn instruction following, and similar variations of broad knowledge/reasoning/skills. Instead, we explore a set of representative domains covering variations common in diverse global user bases: English dialects, multilingualism, and global opinions, and show a direct impact on model performance. 
Our evaluations focus on measuring how alignment makes LLMs more agreeable and helpful for different groups of possible global users. While prior works () have explored the representation of global opinions in language models, they only study the final model. However, the process of transforming a base language model to a user-facing chat model involves two key sequential steps: supervised fine-tuning (SFT) and preference tuning (PT). The impacts of alignment are the product of the base model, SFT, and PT. In addition to evaluating surveys, we study performance gaps on downstream tasks that occur throughout the alignment process for several variations common in global user bases. Together, these evaluations assess whether alignment procedures make LLMs both more agreeable and helpful for a global user base. In summary, our contributions are as follows: 
 0em 
 * We first evaluate the effects of alignment in a purely English setting, focused on global dialects of English (). Effective alignment procedures improve performance on an English intent prediction task for conversations between US, Indian, and Nigerian speakers. However, alignment significantly increases the disparity between English dialects from about 1%before alignment to as high as 17.1%after alignment. 
 * We then evaluate the effects of alignment on model multilingualism (). Despite most models branding themselves as primarily English, alignment largely improves multilingual performance in two question-answering tasks, highlighting a positive unintended impact. 
 * Finally, we evaluate the effects of alignment on a model' s correlation with global opinions from particular countries and about particular countries (). We find that all evaluated alignment procedures increase the similarity between model responses and opinions from the US relative to major nations from other regions, such as China, Jordan, and Nigeria. We further release a new dataset of 554 opinionated questions about countries from r/AskReddit. We find that the open-source Starling reward model, on average, rates 99.4%of all other countries more negatively than the USA. However, this bias does not seem to propagate to the language model preference-tuned with this reward model."
