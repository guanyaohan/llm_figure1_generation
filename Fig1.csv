paper_title,arxiv_id,fig1_file_path,abstract,fig1_caption,introduction
Quantized Side Tuning: Fast and Memory-Efficient Tuning of Quantized Large Language Models,2401.07159v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2401.07159v1_0.pdf;/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2401.07159v1_1.pdf,"Finetuning large language models (LLMs) has been empirically effective on a variety of downstream tasks. Existing approaches to finetuning an LLM either focus on parameter-efficient finetuning, which only updates a small number of trainable parameters, or attempt to reduce the memory footprint during the training phase of the finetuning. Typically, the memory footprint during finetuning stems from three contributors: model weights, optimizer states, and intermediate activations. However, existing works still require considerable memory and none can simultaneously mitigate memory footprint for all three sources. In this paper, we present Quantized Side Tuing (QST), which enables memory-efficient and fast finetuning of LLMs by operating through a dual-stage process. First, QST quantizes an LLM's model weights into 4-bit to reduce the memory footprint of the LLM's original weights; QST also introduces a side network separated from the LLM, which utilizes the hidden states of the LLM to make task-specific predictions. Using a separate side network avoids performing backpropagation through the LLM, thus reducing the memory requirement of the intermediate activations. Furthermore, QST leverages several low-rank adaptors and gradient-free downsample modules to significantly reduce the trainable parameters, so as to save the memory footprint of the optimizer states. Experiments show that QST can reduce the total memory footprint by up to 2.3 $$ and speed up the finetuning process by up to 3 $$ while achieving competent performance compared with the state-of-the-art. When it comes to full finetuning, QST can reduce the total memory footprint up to 7 $$.","Figure \ref{fig:mem_comp_70b} shows the memory footprint of different methods of fintuning LLaMA-2-70b. Figure \ref{fig:acc_comp_7ob} shows the MMLU 5-shot accuracy of different methods when tuning LLaMA-2-7B, LLaMA-2-13B, and LLaMA-2-70B. Note that we set the batch size to 16 and the sequence length to 384. Larger markers represent larger models.","Recent advancements in large language models (LLMs), including GPT, PaLM , OPT , and LLaMA , have showcased remarkable task-generalization capabilities across diverse applications . The ongoing evolution of LLMs' capabilities is accompanied by exponential increases in LLMs' sizes, with some models encompassing 100 billion parameters . Finetuning pre-trained LLMs for customized downstream tasks provides an effective approach to introducing desired behaviors, mitigating undesired ones, and thus boosting the LLMs' performance. Nevertheless, the process of LLM finetuning is characterized by its substantial memory demands. For instance, finetuning a 16-bit LLaMA model with 65 billion parameters requires more than 780GB of memory. To reduce the computational requirement of LLM finetuning, recent work introduces { parameter-efficient finetuning} (PEFT), which updates a subset of trainable parameters from an LLM or introduces a small number of new parameters into the LLM while keeping the vast majority of the original LLM parameters frozen. PEFT methods achieve comparable performance as full finetuning while enabling fast adaption to new tasks without suffering from catastrophic forgetting . However, PEFT methods necessitate caching intermediate activations during forward processing, since these activations are needed to update trainable parameters during backward propagation. As a result, PEFT methods require saving more than 70\ Concisely, existing PEFT techniques cannot effectively reduce the memory footprint of LLM finetuning, restricting their applications in numerous real-world memory-constrained scenarios. Recent work has also introduced approaches to combining PEFT and quantization. For example, QLoRA quantizes an LLM's weights to 4-bit and leverages low-rank adaption (LoRA) to finetune the quantized LLM. QLoRA reduces the memory footprint of an LLM's weights and optimizer states, and as a result, finetuning a 65B LLM requires less than 48 GB of memory. However, QLoRA does not consider the memory footprint of intermediate activations, which can be particularly large when using a large batch size for finetuning. As a result, QLoRA only supports small-batch training (e.g. a batch size of $1$), and finetuning a 65B LLM requires checkpointing gradients to fit the LLM on a single 48GB GPU, resulting in long training time. Besides, our evaluation also reveals that the performance of QLoRA becomes unstable when using 16-bit floating points. and propose to use a { side network} to reduce the memory footprint of intermediate activations by { avoiding backpropagation of the LLM} on natural language processing (NLP) and computer vision (CV) tasks, respectively. Even with the adoption of a side network, the inherent model size of the LLM remains a challenge. Meanwhile, these approaches focus on small models (i.e., less than 3 billion parameters), and their applicability and efficacy for larger models remain unexplored. In this paper, we propose a fast, memory-efficient LLM finetuning framework, called {Q}uantized {S}ide-{T}uning (QST), which operates through a dual-stage process as shown in Figure~. First, QST quantizes an LLM into 4-bit to reduce the memory footprint of its model weights. Second, QST introduces a side network separating from the quantized LLM to avoid performing backward propagation for the quantized LLM, thus saving the memory footprint of intermediate activations. During the training phase of QST, the input to each layer of the side network is formed by combining (1) the downsampled output of the corresponding quantized LLM layer and (2) the output of the previous layer of the side network. A larger LLM usually has a larger model depth (i.e., the number of layers) and width (the hidden size of each layer), which in turn requires more trainable parameters for the downsampling layers. Unlike that leverages linear layer to perform downsampling, QST uses several low-rank adapter methods such as MaxPooling and AvgPooling, significantly reducing the required trainable parameters and the memory footprint for the optimizer states. After that, we use a learnable parameter to assign weights and subsequently aggregate the hidden states of the quantized LLM and the side network. Finally, we reuse the LLM head or classifier to predict. Combined with 4-bit quantization and side tuning, QST significantly reduces all three main contributors of the memory footprint and training time during the training phase. Besides, QST does not increase inference latency since the LLM and side network can be computed in parallel. Figure compares the memory footprint of QST and existing parameter-efficient fine-tuning methods, including QLoRA and LST. To validate the effectiveness of our QST, we conduct extensive evaluations for different types of LLMs (e.g., OPT, LLaMA 2), with 1.3B to 70B parameters, on various benchmarks. Experiment results show that QST can reduce the total memory footprint by up to 2.3 $$ and speed up the finetuning process by up to 3 $$ while achieving competent performance compared with the state-of-the-art. Our codes are released to the GitHub {{https://github.com/YouAreSpecialToMe/QST}{https://github.com/YouAreSpecialToMe/QST"
Unsupervised Multimodal Clustering for Semantics Discovery in Multimodal Utterances,2405.12775v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2405.12775v1_0.pdf,"Discovering the semantics of multimodal utterances is essential for understanding human language and enhancing human-machine interactions. Existing methods manifest limitations in leveraging nonverbal information for discerning complex semantics in unsupervised scenarios. This paper introduces a novel unsupervised multimodal clustering method (UMC), making a pioneering contribution to this field. UMC introduces a unique approach to constructing augmentation views for multimodal data, which are then used to perform pre-training to establish well-initialized representations for subsequent clustering. An innovative strategy is proposed to dynamically select high-quality samples as guidance for representation learning, gauged by the density of each sample's nearest neighbors. Besides, it is equipped to automatically determine the optimal value for the top-$K$ parameter in each cluster to refine sample selection. Finally, both high- and low-quality samples are used to learn representations conducive to effective clustering. We build baselines on benchmark multimodal intent and dialogue act datasets. UMC shows remarkable improvements of 2-6\% scores in clustering metrics over state-of-the-art methods, marking the first successful endeavor in this domain. The complete code and data are available at \url{https://github.com/thuiar/UMC}.","{example} Text-only clustering deviates from real multimodal utterance semantics, highlighting the need of multimodal information in semantics discovery.","Discovering the semantics of dialogue utterances in unsupervised multimodal data requires integrating various modalities (i.e., text, video, and audio) to effectively mine the complicated semantics inherent in multimodal language. Conventional methods for semantics discovery typically focus solely on the text modality with clustering algorithms, failing to leverage the rich multimodal information in the real world (e.g., body language, facial expressions, and tones). However, we argue that non-verbal modalities (i.e., video and audio) also play a critical role when performing unsupervised clustering. Taking Figure~ as an example, relying solely on textual information yields clustering results that differ from the ground truth of multimodal cluster allocations (a detailed analysis on real-world examples is available in Appendix~), suggesting that non-verbal modalities can provide useful cues for semantics discovery. Moreover, effectively capturing multimodal interactions can yield more powerful and robust representations, thereby better addressing the challenges of ambiguous intent-cluster boundaries found in text-based clustering (see Section~ and Appendix~). Discovering multimodal utterance semantics holds significant promise for a variety of applications, including video content recommendation, efficient multimodal data annotation, and virtual human technologies (detailed in Appendix~). Understanding semantics in multimodal utterances has attracted much attention with the boom in multimodal language analysis. For example, annotated multimodal dialogue act (DA) labels on two popular multimodal multi-party conversational datasets and performed DA recognition using attention sub-networks build upon modality encoders. pioneered multimodal intent analysis, introducing a new dataset with multimodal intent labels and establishing baselines with three multimodal fusion methods. However, these works remain restricted within supervised tasks, i.e., the training target for each piece of data is known, which is not applicable in unsupervised scenarios. In contrast, semantics discovery is an emerging field in NLP. It fundamentally operates as a clustering task and has seen the development of many unsupervised and semi-supervised methods. However, these methods are primarily designed for the text-only modality and lack proficiency in handling the diverse modalities encountered in real-world scenarios. Thus, there is a lack of multimodal clustering methods for discovering utterance semantics, posing two challenges: (1) determining how to leverage information from nonverbal modalities to complement the text modality in clustering and (2) devising ways to fully exploit multimodal unlabeled data to learn clustering-friendly representations. To address these challenges, we introduce UMC, a novel unsupervised multimodal clustering algorithm for semantics discovery, as shown in Figure~. We utilize the capabilities of the pre-trained language model to process text data. For the video and audio modalities, deep features are initially extracted using powerful backbones from computer vision and speech signal processing. Two transformer encoders are then employed to capture the deep semantics of these features. The text modality is designated as the anchor, guiding the learning of the other modalities. For this purpose, we concatenate features from all three modalities and mask the video or audio features with zero vectors, creating two sets of positive augmentation views. These multimodal representations and their augmentations are applied to an unsupervised contrastive loss, yielding well-initialized representations for subsequent process. To fully mine the semantic similarities among unsupervised multimodal data, we introduce a novel strategy that initially selects high-quality samples. This strategy employs a dynamic sample selection threshold $t$, aiming to select the highest-quality $t$ percent of samples in each iteration for training. This selection is based on a unique mechanism that calculates the density of each sample within its respective cluster and ranks them accordingly. Besides, an evaluation process is designed to automatically determine the optimal parameters for the top-$K$ nearest neighbors from a set of candidates. After selecting high-quality samples, we propose an iterative process to perform multimodal representation learning. This process begins by learning from high-quality samples using supervised contrastive loss and then refines the remaining low-quality samples using unsupervised contrastive loss. This dual approach promotes beneficial intra-class and inter-class relations among high-quality samples while pushing apart low-quality samples, thereby generating representations conducive to clustering. The entire process is repeated until the sample selection threshold $t$ is met. We summarize our contributions as follows: (1) To the best of our knowledge, we make the first exploration in the task of multimodal semantics discovery, effectively leveraging non-verbal modalities for unsupervised clustering. This is achieved by introducing a novel data augmentation method. (2) We propose a new clustering algorithm, UMC, which features an innovative high-quality sample selection strategy and an iterative representation learning method between high- and low-quality samples, resulting in excellent performance across both single and multimodal modalities. (3) We establish baselines using benchmark multimodal intent and dialogue datasets for this task. Extensive experiments show that the proposed UMC outperforms state-of-the-art clustering algorithms by a notable margin of 2-6\"
GenTranslate: Large Language Models are Generative Multilingual Speech and Machine Translators,2402.06894v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.06894v2_0.pdf,"Recent advances in large language models (LLMs) have stepped forward the development of multilingual speech and machine translation by its reduced representation errors and incorporated external knowledge. However, both translation tasks typically utilize beam search decoding and top-1 hypothesis selection for inference. These techniques struggle to fully exploit the rich information in the diverse $N$-best hypotheses, making them less optimal for translation tasks that require a single, high-quality output sequence. In this paper, we propose a new generative paradigm for translation tasks, namely ``GenTranslate'', which builds upon LLMs to generate better results from the diverse translation versions in $N$-best list. Leveraging the rich linguistic knowledge and strong reasoning abilities of LLMs, our new paradigm can integrate the rich information in $N$-best candidates to generate a higher-quality translation result. Furthermore, to support LLM finetuning, we build and release a HypoTranslate dataset that contains over 592K hypotheses-translation pairs in 11 languages. Experiments on various speech and machine translation benchmarks ({e.g.}, FLEURS, CoVoST-2, WMT) demonstrate that our GenTranslate significantly outperforms the state-of-the-art model}.","Illustration of (a) Typical seq2seq translation with beam search decoding and top-1 hypothesis selection, (b) our ``GenTranslate'' with LLM integration.","Recent advances in large language models (LLMs) have attracted a surge of research interest due to their strong abilities in logical reasoning and language generation . These models have achieved surprisingly wide-ranging success across various natural language processing (NLP) tasks. In the realm of NLP, the translation tasks, which encompasses speech and machine translation (ST \& MT), hold significant practical importance for global communication. Similar to other NLP tasks, translation tasks also gain a notable progress thanks to the recent advancement of LLMs. In the domain of speech translation, Whisper demonstrates superior performance by collecting 680K-hour data for web-scale model training. AudioPaLM2 integrates both text- and speech-based language models into a unified architecture to process and generate text and speech, thereby augmenting speech translation performance to a great extent. On the other hand, LLMs also show remarkable ability in machine translation. NLLB is the first to extend LLMs' linguistic capability to over 200 languages. BigTranslate is finetuned on LLaMA with multilingual instruction tuning, which achieves comparable performance to ChatGPT and Google Translate. Most recent work proposes SeamlessM4T, a foundational multilingual and multitask model that can translate across speech and text, which achieves the state-of-the-art on both ST and MT tasks on various public datasets. Despite the superior performance, most existing translation models employ the typical beam search algorithm for inference and select the top-1 hypothesis as final output (see Fig.~ (a)), following that in automatic speech recognition (ASR). However, this strategy discards the 2 to $N$-best hypotheses that could be advantageous to the generation of ground-truth translation. As illustrated in Fig.~, the discarded 2 to $N$-best hypotheses contain abundant semantic information that is the key to composite the ground-truth utterance, while the 1-best hypothesis lacks this part of information. As a result, the typical top-1 hypothesis selection is sub-optimal to the translation tasks that require a single informative and high-quality output sequence. Inspired by the recent works on LLMs-enhanced ASR, we propose a new generative paradigm for translation tasks, namely GenTranslate (see Fig.~ (b)). Leveraging the rich linguistic knowledge and strong reasoning ability of LLMs, our paradigm integrates the diverse translation versions in the $N$-best list from foundation model to generate a higher-quality translation result. Furthermore, in order to support LLM finetuning, we also build and release a HypoTranslate dataset that contains over 592K pairs of $N$-best hypotheses and ground-truth translation in 11 languages. Experimental evidence on various ST and MT benchmarks ({e.g.}, FLEURS, CoVoST-2, WMT) demonstrate that our proposed GenTranslate significantly outperforms the state-of-the-art model with efficient LLM finetuning. Our contributions are summarized as follows: {itemize} We propose GenTranslate, a new generative paradigm for translation tasks that leverages LLMs to generate higher-quality translation results from the diverse $N$-best hypotheses decoded from foundation translation model. We release a HypoTranslate dataset to support LLM finetuning, which contains over 592K pairs of $N$-best hypotheses and ground-truth translation in 11 languages. Experiments on various ST and MT benchmarks show that our GenTranslate significantly outperforms the state-of-the-art model. {itemize"
A Unified Temporal Knowledge Graph Reasoning Model Towards Interpolation and Extrapolation,2405.18106v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2405.18106v1_0.png,"Temporal knowledge graph (TKG) reasoning has two settings: interpolation reasoning and extrapolation reasoning. Both of them draw plenty of research interest and have great significance. Methods of the former de-emphasize the temporal correlations among facts sequences, while methods of the latter require strict chronological order of knowledge and ignore inferring clues provided by missing facts of the past. These limit the practicability of TKG applications as almost all of the existing TKG reasoning methods are designed specifically to address either one setting. To this end, this paper proposes an original Temporal PAth-based Reasoning (TPAR) model for both the interpolation and extrapolation reasoning. TPAR performs a neural-driven symbolic reasoning fashion that is robust to ambiguous and noisy temporal data and with fine interpretability as well. Comprehensive experiments show that TPAR outperforms SOTA methods on the link prediction task for both the interpolation and the extrapolation settings. A novel pipeline experimental setting is designed to evaluate the performances of SOTA combinations and the proposed TPAR towards interpolation and extrapolation reasoning. More diverse experiments are conducted to show the robustness and interpretability of TPAR.",An illustration of Bellman-Ford-based recursive encoding.,"Knowledge graph (KG) is a semantic network that represents real-world facts in a structured way using entities and relations . Typically, a fact is represented by a triple $(s,r,o)$ in KG, consisting of a subject entity $s$, an object entity $o$, and a relation $r$ between $s$ and $o$. In the real world, many facts are closely associated with a particular time interval. For example, the fact {``Barack Obama is the president of USA''} is valid for the time period of {2009 January 20th - 2017 January 20th } and the fact {``Donald Trump is the president of USA''} is only valid for the following four years. To represent such time-sensitive facts, Temporal Knowledge Graphs (TKGs) have recently gained significant attention from both academic and industrial communities. Specifically, TKGs extend static KGs by incorporating the temporal information $t$ into fact triples, represented as a quadruple $(s,r,o,t)$, which allows for modelling the temporal dependencies and evolution of knowledge over time, being crucial for reasoning time-evolving facts in applications such as financial forecasting, social networks, and healthcare. TKG reasoning infers new knowledge with time-sensitive facts in existing TKGs, which generally has two settings: the interpolation reasoning and the extrapolation reasoning . Given a temporal knowledge graph with facts from time $t_0$ to time $t_T$, the interpolation reasoning infers missing facts at any time in history ($t_0 t t_T$) and the extrapolation reasoning attempt to predict unknown facts that may occur in the future ($t>t_T$). Many approaches have been proposed to tackle the TKG reasoning problem, however, these two reasoning tasks are tackled in totally different and incompatible manners. On the one hand, interpolation methods de-emphasize the temporal correlations among fact sequences while training, thus it's difficult to cope with the challenges of invisible timestamps and invisible entities in extrapolation due to their poor inductive reasoning ability . On the other hand, most state-of-the-art (SOTA) extrapolation solutions require a strict chronological order of data during training. As a result, they can only predict unknown future facts, but they could hardly infer missing historical facts which are crucial for completing the overall knowledge landscape and providing more clues for predicting accurate future events. These limit the practicability of TKG applications as almost all of the existing TKG reasoning methods are designed specifically to address either one setting. Experiments with a novel pipeline setting intuitively reveal that even with the SOTA methods from both settings, the composed methods show a frustrating decrease in reasoning performance. More in-depth analysis can be found in Section and Appendix . Therefore, the motivation of this work is to propose a unified method that can accommodate two types of reasoning settings, enabling temporal knowledge graph reasoning to be conducted simultaneously for both the interpolation and the extrapolation. To this end, we take inspiration from recent neural and symbolic TKG reasoning approaches. Neural network approaches can perform effective reasoning but lack interpretation as they cannot provide explicit rules to explain the reasoning results, while symbolic reasoning approaches use logical symbols and rules to perform reasoning tasks but are not suitable for handling ambiguous and noisy data due to their strict matching and discrete logic operations used during rule searching . In this paper, we propose a {T}emporal {PA}th based {R}easoning (TPAR) model with a neural-symbolic fashion applicable to both the interpolation and the extrapolation TKG Reasoning. Specifically, we utilize the Bellman-Ford Shortest Path Algorithm and introduce a recursive encoding method to score the destination entities of various temporal paths, and then our TPAR performs symbolic reasoning with the help of the obtained scores. It is noticeable that the neural-driven symbolic reasoning fashion we adopted is more robust to the uncertainty data compared to traditional pure symbolic reasoning methods, and comprehensible temporal paths with fine interpretability as well. We summarize our main contributions as follows: {enumerate} We propose an original unified Temporal PAth-based Reasoning (TPAR) model for both the interpolation and extrapolation reasoning settings. To the best of our knowledge, this is the first work to achieve the best of both worlds. We develop a novel neural-driven symbolic reasoning fashion on various temporal paths to enhance both the robustness and interpretability of temporal knowledge reasoning. Comprehensive experiments show that TPAR outperforms SOTA methods on the link prediction task for both the interpolation and the extrapolation settings with decent interpretability and robustness. An intriguing pipeline experiment is meticulously designed to demonstrate the strengths of TPAR in addressing the unified prediction through both settings. {enumerate"
Unsupervised Information Refinement Training of Large Language Models for Retrieval-Augmented Generation,2402.18150v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.18150v2_0.pdf,"Retrieval-augmented generation (RAG) enhances large language models (LLMs) by incorporating additional information from retrieval. However, studies have shown that LLMs still face challenges in effectively using the retrieved information, even ignoring it or being misled by it. The key reason is that the training of LLMs does not clearly make LLMs learn how to utilize input retrieved texts with varied quality. In this paper, we propose a novel perspective that considers the role of LLMs in RAG as ``Information Refiner'', which means that regardless of correctness, completeness, or usefulness of retrieved texts, LLMs can consistently integrate knowledge within the retrieved texts and model parameters to generate the texts that are more concise, accurate, and complete than the retrieved texts. To this end, we propose an information refinement training method named {-RAG} that optimizes LLMs for RAG in an unsupervised manner. is low-cost and general across various tasks. Extensive experiments on zero-shot prediction of 11 datasets in diverse tasks including Question Answering, Slot-Filling, Language Modeling, Dialogue, and Code Generation show that improves the performance of LLaMA2 by an average of 9.39\% relative points. also shows advantages in in-context learning and robustness of RAG.","We consider the role of LLMs in RAG as ``Information Refiner'' that can generate more concise, accurate, and complete texts than the input retrieved texts. In this way, LLM can consistently make RAG system produce positive information gain.","Retrieval-augmented generation (RAG) is a popular framework in modern NLP systems that equips neural with retrieved information for text generation like open-domain question answering, dialogue etc. Recently, RAG has been applied to large language models (LLMs) to provide additional knowledge and mitigate issues such as hallucination. Despite the improved performance of retrieval models, the internet continues to be inundated with fake news, rumors, and fragmented, noisy information, posing challenges for retrieval models to reliably identify and shield against such content. Consequently, not all retrieved texts are beneficial, necessitating that LLMs determine how to judiciously utilize them. However, pre-training tasks do not explicitly enable LLMs to learn how to utilize the retrieved texts with varied quality for generation. For a question and its retrieved texts as input sequence, RAG aims to minimize the negative log-likelihood (NLL) of sub-sequence (question and generated answer) by referring to the retrieved texts. However, mainstream pre-training for LLMs with decoder-only architecture is language modeling based on the prefix, the training objective aims to minimize the negative log-likelihood (NLL) of the entire input sequence (retrieved texts, question, and generated answer). This gap causes LLMs to only regard the input retrieved texts as a part of the prefix for language modeling rather than additional reference, which leads to the following problems. Firstly, for the long and complex retrieved texts, LLMs struggle to extract the correct answers accurately. Secondly, in situations where the retrieved texts cannot address the task, LLMs lack the capability to integrate the knowledge within model parameters with the retrieved texts to generate improved texts. Thirdly, LLMs are susceptible to incorrect and noisy information in retrieved texts, posing a risk of being misled. To solve above problems, some previous methods explore strategies for how or when to perform retrieval for LLMs by prompt techniques. However, prompt cannot materially change the ability of LLMs to utilize retrieved texts because model parameters are not updated for this ability. Some methods fine-tune LLMs on the constructed RAG data for a specific task such as QA. However, under the trend that LLMs are regarded as foundation models for various tasks in zero-shot setting, fine-tuning LLMs only on a few tasks make LLMs limited to the RAG of training tasks and lose their generalizability. Because catastrophic forgetting still exists in supervised fine-tuning of LLMs. Although constructing data for a large number of tasks can alleviate this, it is hard to design the data in various RAG tasks and requires high data annotation costs. Our paper aims to fundamentally improve the ability of LLMs to utilize retrieved texts while preserving the generalizability of LLMs for various RAG tasks in zero-shot setting, which is orthogonal to prompt techniques and can be combined with them to get better performance. In this paper, considering that LLMs have a certain ability to use their own knowledge to examine information, we introduce a novel perspective to reassess the role of LLMs in RAG. Specifically, we propose considering LLMs as {``Information Refiner''}. The key idea behind this is to continue training the pre-trained LLMs with an Information Refinement objective that regardless of the correctness, completeness, or usefulness of the input retrieved texts, LLMs can consistently integrate knowledge within the retrieved texts and model parameters to generate the texts that are more concise, accurate, and complete than the retrieved texts (Figure~). We term this process ``Positive Information Gain''. This enables LLMs to extract correct information from complex texts as well as resist and rectify retrieved erroneous information and noise, thereby improving the information bottleneck of the RAG and allowing the knowledge capacity of RAG to approximate the combined knowledge of IR and LLMs. We make the information refinement training work in a completely unsupervised manner, such that it is easy to obtain large-scale training data and maintain the generalizability of the trained LLMs that can be used in various RAG tasks in zero-shot setting. Specifically, we propose an unsupervised training method named . classifies the retrieved texts into three scenarios (shown in Figure~) and proposes the unsupervised training task for each scenario. For the first scenario that all knowledge for the question is already in the retrieved texts, LLMs need to accurately extract relevant knowledge from complex retrieved texts and generate more concise texts. For the second scenario that retrieved texts are incomplete or incorrect for the question, LLMs need to combine the knowledge within model parameters to verify the retrieved texts, correct the wrong knowledge, and complete the missing knowledge. For the third scenario that retrieved texts are relevant but do not have any answer, LLMs need to find the knowledge within model parameters based on relevant context to generate correct answers. We mix the above three tasks to train unsupervisedly. Main contributions of this paper are as follows: (1) We introduce a novel perspective to reassess the role of LLMs in the RAG system that considers LLMs as {``Information Refiner''} that can produce positive information gain in RAG scenarios. (2) We propose an unsupervised training method named that enables LLMs to perform information refinement in RAG. is low-cost and general for various RAG tasks. (3) Extensive experiments show enhances the zero-shot RAG of LLaMA2 across Question Answering, Slot-Filling, Language Modeling, Dialog, and Code Generation. also shows advantages in in-context learning and robustness of RAG. Code is released at {https://github.com/xsc1234/INFO-RAG/"
CSCD-NS: a Chinese Spelling Check Dataset for Native Speakers,2211.08788v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2211.08788v3_0.jpg,"In this paper, we present CSCD-NS, the first Chinese spelling check (CSC) dataset designed for native speakers, containing 40,000 samples from a Chinese social platform. Compared with existing CSC datasets aimed at Chinese learners, CSCD-NS is ten times larger in scale and exhibits a distinct error distribution, with a significantly higher proportion of word-level errors. To further enhance the data resource, we propose a novel method that simulates the input process through an input method, generating large-scale and high-quality pseudo data that closely resembles the actual error distribution and outperforms existing methods. Moreover, we investigate the performance of various models in this scenario, including large language models (LLMs), such as ChatGPT. The result indicates that generative models underperform BERT-like classification models due to strict length and pronunciation constraints. The high prevalence of word-level errors also makes CSC for native speakers challenging enough, leaving substantial room for improvement.","An error from SIGHAN: misspelling “错误” as “错勿”. Despite having the same pronunciation, it's hard to reproduce this error in the given context through a Chinese IME, no matter what input form is used.","Chinese spelling check (CSC) is a task to detect and correct spelling errors in Chinese texts. There are two primary user groups for CSC: (1) Chinese learners, including teenage students and individuals who use Chinese as a second language, and (2) Chinese native speakers. It is obvious that the latter user group has a larger population and more diverse applications, therefore, this paper concentrates on CSC for native speakers. {CJK*}{UTF8}{gbsn} {CJK*} However, there is still no CSC dataset specifically designed for native speakers. Existing CSC datasets, such as SIGHAN13, 14, and 15 , are all sourced from Chinese learners. Spelling errors made by Chinese learners differ greatly from those made by native speakers. This is because Chinese input relies on Chinese input methods (IME), and modern Chinese IMEs always have powerful language models, making it difficult to recommend candidates that clearly do not fit the context. As shown in Figure , native speakers using Chinese IMEs are unlikely to make such an unusual error. Furthermore, the size of existing datasets is limited. As shown in Table , for three SIGHAN datasets, the training set contains an average of merely 2158 samples, while the test set comprises an average of only 1054 samples, and no development set is provided. When using such small-scale datasets, it is difficult for models to be trained sufficiently and for evaluation results to be reliable. To address the aforementioned issues, we introduce CSCD-NS, a Chinese spelling check dataset designed for native speakers. The dataset is sourced from real Weibo (a Chinese social media platform) posts, which contain genuine spelling errors made by native speakers during their input process. Moreover, the dataset comprises 40,000 samples, which is ten times larger than previous datasets and this is also the largest dataset for the CSC task. To conduct an in-depth investigation into the distribution of spelling errors, we develop a tagging system that operates at phonetic and semantic levels. The analysis indicates that native speakers make a higher proportion of homophonic and word-level errors compared to Chinese learners, with the proportion of word-level errors doubling. {CJK*}{UTF8}{gbsn} {CJK*} Due to the lack of labeled data, previous studies always build additional pseudo data to improve the performance of models. However, these methods, which rely on confusion sets or ASR transcriptions , do not align with the real-world input scenario. Therefore, we propose a novel method that directly simulates the input process through the Chinese IME and adds sampled noises to construct high-quality pseudo data. Experimental results show that our method can better fit the real error distribution and bring greater improvements. We conduct comprehensive experiments on CSCD-NS, with different model sizes (from 0.1B to 13B parameters), architectures (encoder-only, encoder-decoder, and decoder-only), and learning approaches (fine-tuning and in-context learning). We also evaluate the performance of ChatGPT and GPT4. The results demonstrate that BERT-like classification models outperform generative models, as the latter struggle with the simultaneous constraints of text length and pronunciation. Concurrently, the CSC task for native speakers is challenging due to the high proportion of word-level errors, leaving substantial room for improvement. In summary, our contributions are as follows: {itemize} We introduce the first Chinese spelling check dataset for native speakers which is also the largest dataset for the CSC task. Through quantitative analyses, we further unveil the specific error distribution for this scenario. We propose a novel method for constructing high-quality and large-scale pseudo data through a Chinese IME. Experimental results show that our method can bring greater improvements than existing methods. We explore the performance of different types of models in this scenario and analyze the challenges. To the best of our knowledge, we are the first to investigate the effectiveness and limitations of large language models (LLMs), such as ChatGPT, in addressing the CSC task. {itemize"
Evaluating Dynamic Topic Models,2309.08627v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2309.08627v1_0.pdf,"There is a lack of quantitative measures to evaluate the progression of topics through time in dynamic topic models (DTMs). Filling this gap, we propose a novel evaluation measure for DTMs that analyzes the changes in the quality of each topic over time. Additionally, we propose an extension combining topic quality with the model's temporal consistency. We demonstrate the utility of the proposed measure by applying it to synthetic data and data from existing DTMs. We also conducted a human evaluation, which indicates that the proposed measure correlates well with human judgment. Our findings may help in identifying changing topics, evaluating different DTMs, and guiding future research in this area.","This figure illustrates the core concept presented in this paper. It illustrates the topic structure within DTMs. The vertical box highlights the set of topics for the first year, and the horizontal box shows the evolution of Topic~1 over time. Topic Quality (TQ) evaluates the topics for each year vertically, whereas Temporal Topic Quality (TTQ) evaluates each topic horizontally, capturing both the topic's evolution over time and the smoothness of topic progression.","Dynamic Topic Models (DTMs) learn topics and their evolution over time from a time-indexed collection of documents. DTMs have been proven to be useful in various domains, including text mining , computer vision , and computational biology . DTMs enable summarization, browsing, and searching of large document collections by capturing the changes of topics over time. However, evaluating DTMs can be challenging due to their unsupervised nature, although it is crucial for effectively detecting trends in time-indexed documents. Currently, the development of evaluation measures is not keeping pace with the advancements in new models. While traditional evaluation measures can assess the quality and diversity of topics, they fail to capture the smoothness of topic changes over time. This limitation becomes problematic when a DTM exhibits high topic quality but lacks temporal smoothness. In such cases, existing evaluation measures may erroneously assign a high score to the model, even if there are rapid and abrupt transitions between topics. For instance, if a topic quickly shifts from ``politics'' to ``sports'', conventional evaluation measures might still rate the model positively. To accurately assess the quality of a DTM, it is crucial to consider the smoothness of topic changes over time, which can help identify gradual topic drifts or sudden shifts. Unfortunately, existing evaluation measures lack the capability to effectively track topic changes over time. To bridge this gap, we propose Temporal Topic Quality (TTQ)---a novel evaluation measure specifically designed for DTMs. TTQ incorporates the changes in topic quality into its assessment, thereby capturing the temporal characteristics of topics in DTMs. We provide empirical evidence of the effectiveness of the proposed measure by evaluating it on both synthetic and real topics. The results demonstrate a positive correlation between human ratings and the individual components of the TTQ measure. To provide an overall evaluation of DTMs, we propose the Dynamic Topic Quality (DTQ). The DTQ measure aggregates the TTQ measure with the static topic quality assessment. This aggregation is performed for both year-wise evaluations and temporal topic assessments, as illustrated in Figure~. In our experiments, we compare the results obtained using the DTQ measure with those obtained using previously employed measures for different topic models. The findings indicate that the DTQ measure provides a superior indication of the smoothness of topics in trained DTMs compared to the measures used in the past. We anticipate that the introduction of the new measure will contribute to improved comparisons between DTMs in future research endeavors. Our contributions can be summarized as follows: {itemize} We present a novel evaluation measure for DTMs that integrates both the vertical (year-wise) and the horizontal (temporal) dimension in the quality estimate (See Figure ). We conduct a meta-analysis of prominent (statistical and neural) DTMs with our novel evaluation measures and present our findings. We show a positive correlation between human evaluations and the new evaluation measures, confirming their soundness. {itemize"
How Abilities in Large Language Models are Affected by Supervised Fine-tuning Data Composition,2310.05492v4,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2310.05492v4_0.pdf,"Large language models (LLMs) with enormous pre-training tokens and parameters emerge diverse abilities, including math reasoning, code generation, and instruction following. These abilities are further enhanced by supervised fine-tuning (SFT). While the open-source community has explored ad-hoc SFT for enhancing individual capabilities, proprietary LLMs exhibit versatility across various skills. Therefore, understanding the facilitation of multiple abilities via SFT is paramount. In this study, we specificially focuses on the interplay of data composition between mathematical reasoning, code generation, and general human-aligning abilities during SFT. We propose four intriguing research questions to explore the association between model performance and various factors including data amount, composition ratio, model size and SFT strategies. Our experiments reveal that distinct capabilities scale differently and larger models generally show superior performance with same amount of data. Mathematical reasoning and code generation consistently improve with increasing data amount, whereas general abilities plateau after roughly a thousand samples. Moreover, we observe data composition appears to enhance various abilities under limited data conditions, yet can lead to performance conflicts when data is plentiful. Our findings also suggest the amount of composition data influences performance more than the composition ratio. In analysis of SFT strategies, we find that sequentially learning multiple skills risks catastrophic forgetting. Our proposed {Dual-stage Mixed Fine-tuning (DMT) strategy} offers a promising solution to learn multiple abilities with different scaling patterns.",The illustration of four different training strategies in this paper.,"Recent research has demonstrated the remarkable and versatile proficiency of large language models (LLMs) in dealing with a variety of real-world tasks expressed in natural languages , especially Information Extraction (IE) , Information Retrieval (IR) and Spoken Language Understanding (SLU) . Among the tasks, LLMs especially emerge with three outstanding abilities in reasoning , coding , and aligning general human intentions , which have drawn much attention from the LLM research community. In order to further incentivize such abilities, it necessitates supervised fine-tuning (SFT) stages on annotated task data. However, existing research has mostly conducted separate SFT investigations on each of the three tasks, where reasoning and coding abilities require SFT on in-domain human-annotated or augmented data while diverse and complex human instructions are applauded for aligning human intentions . As shown by the strong performance of proprietary LLMs such as GPT-4 and Claude, LLMs have the potential to master all the tasks in one model. Therefore, it is of paramount importance to investigate the versatile performance of SFT with composite task data, and understanding and addressing the challenges posed by the data composition problem in the SFT stage is crucial for further enhancing the capabilities of LLMs in a comprehensive manner. In essence, the tasks of reasoning, coding, and aligning human intentions are of different characteristics. Reasoning and coding tasks require ad-hoc abilities of complex and detailed logic in decomposing task instructions and dealing with non-linguistic and symbolic features , whereas aligning human intentions requires versatility and understanding obscure intentions expressed in human instructions . Given the fundamental difference among the tasks, multi-task learning with composite data fine-tuning for small-scaled pre-trained language models is prone to catastrophic forgetting , hindering the fine-tuned performance of one model on separate tasks. Many efforts have been made to compensate for the phenomenon . There has also been research discovering that scaling up the pre-trained language model scale and the fine-tuning data scale are beneficial for zero-shot out-of-domain generalization on various linguistic tasks while leaving out the assessment of in-domain performance . Given the increased capacity of LLMs, the multi-task performance by SFT on composite data of essentially different downstream tasks is less studied. Understanding the SFT performance with composite data and corresponding scaling patterns is of great utility in practice. In this study, we focus on the data composition problem among {mathematical reasoning}, {code generation}, and {general human-aligning abilities} in SFT. We aim to comprehensively investigate the relationship between model performance and different factors including data amount, data composition ratio, model scales, and SFT training strategies. We also investigate how the relationship varies under different scales. Specifically, we focus on the following four research questions: {1.}~ {How do math reasoning, coding, and general abilities scale with SFT data amounts?} {2.}~ {Are there performance conflicts when combining these three abilities in SFT?} {3.}~ {What are the key factors that induce the performance conflicts?} {4.}~ {What are the impacts of different SFT strategies for composite data?} To answer these questions, we conduct experiments on three benchmarks, which are GSM8K for mathematical reasoning, HumanEval for coding, and MT-Bench for general human alignment. We fine-tune LLMs on the related training data to activate these abilities. Furthermore, we conduct extensive analysis regarding model parameter scales ranging from LLaMA 7B to 33B and explore four different SFT strategies shown in Figure~: multi-task learning, sequential training, mixed sequential training, and dual-stage mixing fine-tuning (DMT), providing empirical guidance for learning a versatile LLM with composite SFT. The key findings of this paper can be summarized as follows: {itemize} Different SFT abilities exhibit distinct scaling patterns, while larger models show better performances with the same data amount generally. Compared to single ability learning, multi-task learning multiple abilities exhibits improvement in low-resource and decline in high-resource. Additionally, as the model size increases, there is a greater performance gain in low-resource settings for math and general abilities. Data amounts directly influence each ability, while the data ratio is insignificant. Multi-task learning lead to conflicts, while sequential training results in catastrophic forgetting. Our proposed DMT effectively alleviates both performance conflicts and catastrophic forgetting in the SFT phrase, achieving a balance between general and specialized abilities. {itemize"
Inference to the Best Explanation in Large Language Models,2402.10767v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.10767v2_0.pdf,"While Large Language Models (LLMs) have found success in real-world applications, their underlying explanatory process is still poorly understood. This paper proposes {IBE-Eval}, a framework inspired by philosophical accounts on {Inference to the Best Explanation (IBE)} to advance the interpretation and evaluation of LLM explanations. {IBE-Eval} estimates the plausibility of natural language explanations through a combination of explicit logical and linguistic features including: {consistency}, {parsimony}, {coherence}, and {uncertainty}. Extensive experiments are conducted on {Causal Question Answering (CQA)}, where {IBE-Eval} is tasked to select the most plausible causal explanation amongst competing ones generated by the LLM (e.g. GPT 3.5 or LLaMA 2). The experiments reveal that {IBE-Eval} can successfully identify the best explanation with up to 77\% accuracy ($ 27\%$ above random), improving upon a GPT 3.5-as-a-judge baseline ($+17\%$) while being intrinsically more efficient and interpretable. Additional analysis suggests that, despite LLM-specific variances, generated explanations tend to conform to IBE criteria and that {IBE-Eval} is significantly correlated with human judgment, opening up opportunities for future development of automated explanation verification tools.","{IBE-Eval} qualifies LLM-generated explanations with a set of logical and linguistic selection criteria to identify the most plausible hypothesis. The corresponding explanation for each hypothesis is evaluated across the IBE criteria of logical consistency, parsimony, internal coherence, and linguistic uncertainty. A final plausibility score is computed across those features and the hypothesis with highest score is identified as the best explanation.","Large Language Models (LLMs) such as OpenAI's GPT and LLaMA have been highly effective across a diverse range of language understanding and reasoning tasks . While LLM performances have been thoroughly investigated across various benchmarks , the principles and properties behind their step-wise reasoning process are still poorly understood . LLMs are notoriously black-box and can be difficult to interpret . Moreover, the commercialization of LLMs has led to strategic secrecy around model architectures and training details . Finally, LLMs are susceptible to hallucinations and adversarial perturbations , often producing plausible but factually incorrect answers . As the size and complexity of LLM architectures increase, the systematic study of generated explanations becomes crucial to better interpret and validate the LLM's internal inference and reasoning processes . The automatic evaluation of natural language explanations presents several challenges . Without resource-intensive annotation , explanation quality methods tend to rely on either weak supervision, where the identification of the correct answer is taken as evidence of explanation quality, or require the injection of domain-specific knowledge . In this paper, we seek to better understand the LLM explanatory process through the investigation of explicit linguistic and logical properties. While explanations are hard to formalize due to their open-ended nature, we hypothesize that they can be analyzed as linguistic objects, with measurable features that can serve to define criteria for assessing their quality. Specifically, this paper investigates the following overarching research question: {``Can the linguistic and logical properties associated with LLM-generated explanations be used to qualify the models' reasoning process?''}. To this end, we propose an interpretable framework inspired by philosophical accounts of abductive inference, also known as {Inference to the Best Explanation (IBE)} - i.e. the process of selecting among competing explanatory theories . In particular, we aim to measure the extent to which LLM-generated explanations conform to IBE expectations when attempting to identify the most plausible explanation. To this end, we present {IBE-Eval}, a framework designed to estimate the plausibility of natural language explanations through a set of explicit logical and linguistic features, namely: {logical consistency}, {parsimony}, {coherence}, and {linguistic uncertainty}. To evaluate the efficacy of {IBE-Eval}, we conduct extensive experiments in the multiple-choice Causal Question Answering (CQA) setting. The overall results and contributions of the paper can be summarized as follows: {enumerate} To the best of our knowledge, we are the first to propose an interpretable framework inspired by philosophical accounts on Inference to the Best Explanation (IBE) to automatically assess the quality of natural language explanations. We propose {IBE-Eval}, a framework that can be instantiated with external tools for the automatic evaluation of LLM-generated explanations and the identification of the best explanation in a multiple-choice CQA setting. We provide empirical evidence that LLM-generated explanations tend to conform to IBE expectations with varying levels of statistical significance correlated to the LLM's size. We additionally find that uncertainty, parsimony, and coherence are the best predictors of plausibility and explanation quality across all LLMs. However, we also find that the LLMs tend to be strong rationalizers and can produce logically consistent explanations even for less plausible candidates, making the consistency metric less effective in practice. {IBE-Eval} can successfully identify the best explanation supporting the correct answers with up to 77\ {IBE-Eval} is significantly correlated with human judgment, outperforming a GPT3.5-as-a-Judge baseline in terms of alignment with human preferences. {enumerate} For reproducibility, our code is made available on Github{{https://github.com/dhairyadalal/IBE-eval}} to encourage future research in the field"
A Novel Cartography-Based Curriculum Learning Method Applied on RoNLI: The First Romanian Natural Language Inference Corpus,2405.11877v5,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2405.11877v5_0.pdf,"Natural language inference (NLI), the task of recognizing the entailment relationship in sentence pairs, is an actively studied topic serving as a proxy for natural language understanding. Despite the relevance of the task in building conversational agents and improving text classification, machine translation and other NLP tasks, to the best of our knowledge, there is no publicly available NLI corpus for the Romanian language. To this end, we introduce the first Romanian NLI corpus (RoNLI) comprising 58K training sentence pairs, which are obtained via distant supervision, and 6K validation and test sentence pairs, which are manually annotated with the correct labels. We conduct experiments with multiple machine learning methods based on distant learning, ranging from shallow models based on word embeddings to transformer-based neural networks, to establish a set of competitive baselines. Furthermore, we improve on the best model by employing a new curriculum learning strategy based on data cartography. Our dataset and code to reproduce the baselines are available at \url{https://github.com/Eduard6421/RONLI}.","Data cartography visualization of the RoNLI dataset based on fine-tuning the Ro-BERT model \cite{Dumitrescu-EMNLP-2020}. In the left-hand side plot, the $y$-axis corresponds to the level of confidence exhibited by the model during training, while the $x$-axis represents the variability of the confidence level. Adjacent to the primary plot, three histograms are displayed on the right-hand side, each representing a different metric: the confidence, the variability of confidence, and the correctness. The visualization offers a comprehensive overview of our dataset characteristics and the behavior of Ro-BERT during training. Best viewed in color.\vspace{-2mm}","Given a sentence pair composed of a premise and a hypothesis, natural language inference (NLI) , a.k.a.~textual entailment recognition, is the task of determining if the premise entails, contradicts, or is neutral to the hypothesis. NLI is an actively studied problem , being an essential task to be solved before addressing natural language understanding (NLU). Its complexity stems from the fact that NLU is generally considered an AI-hard problem . Notably, NLI forms the foundation of numerous advanced natural language processing systems , providing a backbone for multiple study areas such as language modeling , conversational agents , zero-shot text classification , image captioning , text summarization , discourse parsing , and many others . The importance of NLI is well recognized, being included as a reference task in benchmarks such as GLUE and SuperGLUE . {-2mm} To date, the NLI task has been intensively studied in the English language and other languages, such as Chinese , Turkish , Portugese and Indonesian , as well as in multi-lingual scenarios . However, the sparsity of resources led researchers to overlook the development of NLI models for low-resource languages, such as Romanian . In this context, we introduce a novel {Ro}manian {N}atural {L}anguage {I}nference corpus (RoNLI) composed of 64K sentence pairs. The samples are divided into 58K for training, 3K for validation, and 3K for testing. We collected the sentence pairs from the Romanian version of Wikipedia, while searching for certain linking phrases between contiguous sentences to automatically label the sentence pairs. The training pairs are obtained via an automatic rule-based annotation process known as {distant supervision} , while the validation and test sentence pairs are manually annotated with the correct labels. To the best of our knowledge, RoNLI is the first publicly available corpus for Romanian natural language inference. RoNLI is meant to attract the attention of researchers in studying and developing NLI models for under-studied languages, where NLU systems often produce subpar results, due to data scarcity and cross-lingual settings. To this end, we believe that RoNLI is a valuable resource. We publish our data and source code under the open-source CC BY-NC-SA 4.0 license at: {https://github.com/Eduard6421/RONLI}. We carry out experiments with multiple machine learning methods, ranging from shallow models based on word embeddings to transformer-based neural networks, to establish a set of competitive baselines. Moreover, we employ data cartography to characterize our training samples as easy-to-learn (E2L), ambiguous (A), and hard-to-learn (H2L), from the perspective of the best baseline model. Further, we study several approaches that harness the resulting data characterization to mitigate the inherent labeling noise caused by the automatic labeling process. We manage to improve the top model via a novel curriculum learning method based on data cartography"
MinPrompt: Graph-based Minimal Prompt Data Augmentation for Few-shot Question Answering,2310.05007v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2310.05007v3_0.pdf,"Recent advances in few-shot question answering (QA) mostly rely on the power of pre-trained large language models (LLMs) and fine-tuning in specific settings. Although the pre-training stage has already equipped LLMs with powerful reasoning capabilities, LLMs still need to be fine-tuned to adapt to specific domains to achieve the best results. In this paper, we propose to select the most informative data for fine-tuning, thereby improving the efficiency of the fine-tuning process with comparative or even better accuracy on the open-domain QA task. We present , a minimal data augmentation framework for open-domain QA based on an approximate graph algorithm and unsupervised question generation. We transform the raw text into a graph structure to build connections between different factual sentences, then apply graph algorithms to identify the minimal set of sentences needed to cover the most information in the raw text. We then generate QA pairs based on the identified sentence subset and train the model on the selected sentences to obtain the final model. Empirical results on several benchmark datasets and theoretical analysis show that is able to achieve comparable or better results than baselines with a high degree of efficiency, bringing consistent improvements in F-1 scores.",Framework overview for \ours.,"Question answering (QA) provides accurate responses to a series of questions based on given narrative contexts. Its diverse applications extend to areas such as chatbots, dialogue systems, and instant information retrieval, making it a key pursuit in the field of natural language processing (NLP). Supervised learning has traditionally been the approach for developing efficient QA systems that deliver commendable results. However, this method is intrinsically restricted by its reliance on a large set of annotated QA training examples, which becomes problematic due to the substantial cost associated with acquiring expert-level annotations. Our research focuses on the few-shot QA task, an effort to address the QA challenge with the presence of only a limited number of training examples. The prevalent approaches under the few-shot setting either introduce a new task and pre-train an extensive language model from scratch, or they fine-tune an already pre-trained model on the given training examples. The fine-tuning stage is crucial in the sense that it stimulates the power of the LLMs obtained during the pre-training stage and makes the model align with the input/output distribution of a certain domain or dataset. However, with an increasing data size for fine-tuning, the training duration increases accordingly, which is undesirable, especially when the model size is also large. As such, the importance of minimal data augmentation cannot be understated. The fine-tuning data, often a limited resource in our consideration (up to 128 shots), is directly used to adjust the parameters of a pre-trained model to enhance performance on the downstream task. The data is usually labeled by domain experts and thus could be time-consuming to obtain in large quantities. On the other hand, augmented data represents a broader dataset, generated in an unsupervised manner by converting statements into question-answer pairs. In QA tasks, it is vital for a model to be exposed to a diverse range of questions, answers, and contexts to develop a robust understanding of the language and the task at hand. However, not all parts of the training data hold equal relevance or significance for the model's learning process. Some parts may contain more valuable information or more complex language structures that the model needs to understand to improve its performance. Consequently, identifying and augmenting these critical portions of the training data could substantially enhance the model's capacity to answer questions accurately and comprehensively. To address the above challenges, we present , which consists of the following three modules: (1) A {sentence graph construction module} that leverages sentence graph representation to structurize the raw text. Each node in the graph symbolizes a sentence, while edges illustrate the shared entities between sentences. This sentence graph effectively encapsulates the complex interconnections between various textual elements; (2) A {data selection module} that features an approximate minimal dominating set algorithm. The algorithm is applied to the sentence graph to identify the smallest set of sentences to cover all shared entities. This module ensures efficient use of computational resources, reduces the risk of overfitting, and enhances the model's generalization ability, resulting in an overall improvement in QA performance; and (3) A {question generation module} that transforms the selected plain factual sentences into QA pairs. The synthesized QA pairs are further turned into prompts, providing a condensed, yet comprehensive representation of the text. The generated prompts serve as high-quality, information-rich training instances for the QA model. This model trained on the compact and meaningful prompts is then capable of generating accurate answers to the posed questions, all without requiring any additional explicit supervision. In summary, our contributions are as follows: {itemize}[leftmargin=*] We propose to study minimal data augmentation for effective and efficient few-shot QA We introduce , a minimal data augmentation framework that uses a graph-based algorithm and unsupervised question generation to synthesize the most informative QA training samples out of the raw text. We conduct extensive experiments on publicly accessible benchmarks to validate the effectiveness of , and observe a solid improvement over competitive compared methods. Beyond that, we also study the necessity of different parts of the model. {itemize"
SportsMetrics: Blending Text and Numerical Data to Understand Information Fusion in LLMs,2402.10979v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.10979v2_0.pdf,"Large language models hold significant potential for integrating various data types, such as text documents and database records, for advanced analytics. However, blending text and numerical data presents substantial challenges. LLMs need to process and cross-reference entities and numbers, handle data inconsistencies and redundancies, and develop planning capabilities such as building a working memory for managing complex data queries. In this paper, we introduce four novel tasks centered around sports data analytics to evaluate the numerical reasoning and information fusion capabilities of LLMs. These tasks involve providing LLMs with detailed, play-by-play sports game descriptions, then challenging them with adversarial scenarios such as new game rules, longer durations, scrambled narratives, and analyzing key statistics in game summaries. We conduct extensive experiments on NBA and NFL games to assess the performance of LLMs on these tasks. Our benchmark, , introduces a new mechanism for assessing LLMs' numerical reasoning and fusion skills.","Play-by-plays of an NBA game. We include timestamps, player actions, team affiliations and a game recap. Total points for both teams are indicated in dotted circles and are withheld from LLMs.","Large language models (LLMs) are more powerful than ever. OpenAI's GPT-4 Turbo~{gpt4-turbo} features a 128k context window, allowing it to process over 300 pages of text in a single prompt. Claude v2.1~{claude-2.1} steps it up with a 200k token window, equivalent to roughly 150,000 words or more than 500 pages. Mistral AI~{mixtral-8x7b} has created a sparse mixture of experts model capable of processing up to 32k tokens. The developments suggest language models can now engage with vast amounts of text content and data, opening doors to exciting new applications in various domains. One of the most promising uses of LLMs is in handling a combination of unstructured texts and structured data. For example, determining if a patient can be discharged from the hospital may involve reviewing doctor notes, radiology and pathology reports, lab results, and other records that blend text and structured data; LLM Assistants for online shopping need to process product catalogs, sales transactions, and customer queries. Yet, summarizing key details from a mix of unstructured and structured sources remains a considerable challenge. An LLM must navigate text descriptions, link entities, aggregate numbers, handle discrepancies, and beyond. Information fusion focuses on synthesizing information from multiple textual sources to derive meaningful conclusions. Current approaches involve summarizing multiple text documents, providing concise answers to user queries, and integrating summarization with natural language inference to deduce information. The output is often a short text summary, the quality of which is difficult to evaluate. In contrast, our approach emphasizes the numerical aspect of information fusion. We enable the LLM to navigate through lengthy texts, gather crucial statistics, and develop a working memory to manage complex data queries. We introduce {SportsMetrics}, a benchmark designed to assess LLMs' abilities in numerical reasoning and data fusion. This benchmark provides LLMs with detailed, play-by-play descriptions of sports games, including timestamps, player actions, and team affiliations, as illustrated in Figure~. We focus on four novel tasks to evaluate LLMs in adversarial scenarios: (a) {adapting to new game rules}, (b) {handling lengthy game descriptions}, (c) {managing scrambled game narratives}, and (d) {analyzing critical statistics in game summaries}. E.g., an LLM might be asked to complete a basketball game recap by inserting missing key statistics, which requires the development of a working memory for game stats and reasoning skills. Our {SportsMetrics} benchmark presents three main benefits. First, it leverages sports data, including team-player affiliations and play-by-play details; they are dynamic narratives that LLMs cannot easily memorize. Second, it allows us to evaluate LLMs' ability to track key statistics such as team points, assists, blocks, steals, and more, while also offering an overall game efficiency score for direct LLM comparison. Lastly, its use of widely understood sports terminology makes it more accessible to researchers than specialized medical language, making it an ideal benchmarking tool. While our current focus is on English, {SportsMetrics} also holds promise for multilingual applications"
Expedited Training of Visual Conditioned Language Generation via Redundancy Reduction,2310.03291v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2310.03291v3_0.png,"In this paper, we introduce $_{}$, a streamlined framework designed for the pre-training of visually conditioned language generation models with high computational demands, utilizing frozen pre-trained large language models (LLMs). The conventional approach in vision-language pre-training (VLP) typically involves a two-stage optimization process: an initial resource-intensive phase dedicated to general-purpose vision-language representation learning, focused on extracting and consolidating relevant visual features. This is followed by a subsequent phase that emphasizes end-to-end alignment between visual and linguistic modalities. Our novel one-stage, single-loss framework bypasses the computationally demanding first training stage by gradually merging similar visual tokens during training, while avoiding model collapse caused by single-stage training of BLIP-2 type models. The gradual merging process effectively condenses visual information while preserving semantic richness, resulting in rapid convergence without compromising performance. Our experimental findings demonstrate that our approach accelerates the training of vision-language models by a factor of 5 without a noticeable impact on overall performance. Furthermore, we illustrate that our models significantly narrow the performance gap to current vision-language models using only 1/10 of the data. Finally, we showcase how our image-text models can seamlessly adapt to video-conditioned language generation tasks through novel soft attentive temporal token contextualizing modules. Code is available at \url{https://github.com/yiren-jian/EVLGen}.","Overview of our $\text{EVL}_{\text{Gen}}$. $\text{EVL}_{\text{Gen}}$ employs a streamlined, single-stage training mechanism with a unified loss. Here, visual tokens (in grey) are progressively aggregated based on their inherent similarities at each layer of the TomeFormer architecture. The final set of merged tokens (in orange) serves as semantically rich but computationally efficient soft prompts, guiding the LLM to generate a corresponding caption for the input image.","The landscape of vision-language modeling has undergone significant transformations in recent years, with CLIP serving as a landmark development. It distinguished itself through unparalleled zero-shot classification capabilities and efficiency in image-text retrieval tasks. Successive models like ALBEF, X-VLM, and VLMo further broadened the scope, addressing a myriad of tasks such as retrieval, visual entailment, and closed-set Visual Question Answering (VQA), among others. Recently, the field has been enriched by the advent of generative models designed for complex image-to-language tasks. Notable contributions include CoCa, SimVLM, Frozen, and Flamingo, targeting tasks like image and video captioning and open-set VQA. These models all rely on billion-scale datasets for training from scratch to bridge the substantial modality gap between vision and language. As a result, the resource-intensive requirements (i.e., thousands of TPUs) of these training-from-scratch Vision-Language Models (VLMs) led to the conceptualization of BLIP-2: this model alleviates computational costs (e.g., only requiring $16$ fewer GPUs) by integrating existing well-pretrained vision encoders (ViT) with language decoders (LLM), and then tuning their joint operation. A central innovation in aligning vision and language modules in BLIP-2 is {Q-former}, a multimodal connector equipped with learnable queries for enhancing cross-attention mechanisms. This architectural choice, however, prevents the full model from end-to-end training and therefore {still} demands an additional pre-training regimen for Q-former, referred to as {BLIP-2's Stage 1}. The stage involves three learning objectives—image-text contrastive, image-text matching, and language generation—and necessitates multiple forward passes for facilitating the Q-former's optimization. Despite its efficiency gains over CoCa, BLIP-2's training still imposes considerable computational costs. This poses challenges for research environments with limited computational resources, such as university labs. Our experiments indicate that the Stage-1 training of BLIP-2 took approximately eight days on eight A100-80G GPUs (See Appendix~ for training configurations). This computational burden has consequently restricted research to using the pre-trained Q-former, hindering the exploration of alternative ViTs in VLMs. This limitation is evident in subsequent works such as InstructBLIP, VideoChat, Video-LLaMA, X-LLM. The prospect of reducing BLIP-2's computational cost through end-to-end, single-stage training is compelling. Such an approach would remove the complexities associated with resource allocation and hyper-parameter tuning inherent in multi-stage training. Yet, direct end-to-end training with BLIP-2 poses substantial challenges, corroborated by both original findings from BLIP-2 and our own empirical analyses. We hypothesize that these challenges emanate from the intrinsic design of the Q-former. Specifically, the inclusion of randomly initialized learnable queries and cross-attention mechanisms complicates the optimization landscape, especially when the aim is to minimize the representational disparity between visual and linguistic modalities. In this paper, we propose a token merging Transformer (TomeFormer) as an efficient vision-language connector. TomeFormer employs a systematic token-merging strategy that is both intuitive and effective. By connecting a pre-trained ViT as the visual encoder and a frozen LLM as the language decoder, we introduce a new VLM ``{E}xpedited {V}isual {L}anguage {Gen}eration model'' (${EVL}_{{Gen}}$), facilitates a streamlined, single-stage training process. It requires only a singular learning objective and a single forward pass per optimization step. This stands in contrast to BLIP-2's multi-stage training, laden with multiple objectives and several forward passes. Further, we introduce a {soft attentive temporal} contextualization mechanism within the ViT for effective video-language modeling. This uncovers more shared semantic features across temporal frames, thereby improving the efficiency of the spatial token merging process. It eliminates the need for modality realignment, contrasting approaches such as the temporal Q-former, or the addition of new learnable temporal queries. Our strategy simplifies the optimization challenges tied to working with relatively smaller video-text datasets, compared to their image-text counterparts. Remarkably, we demonstrate that even without video pre-training, our temporal token contextualize approach can effectively train robust video-language models. This differs from recent work in video-language models that depend on pre-training models using vast million-scale video-text datasets. In summary, our contributions are: {itemize}[leftmargin=*,noitemsep,nolistsep] For reducing vision redundancy within the vision language connector, we adopt Token Merging, initially designed to enhance ViT inference speed without training. Concurrently, we present a novel temporal token contextualization scheme for video modeling. Our proposed VLM featuring TomeFormer competes effectively with BLIP-2, while requiring just a fraction of the computational resources. Given the reliance on BLIP-2's pre-trained model in contemporary studies, our approach widens the exploratory scope for various ViTs. We introduce a straightforward spatial attentive temporal modeling technique that allows for the seamless adaptation of pre-trained image-text models to video tasks. This approach eliminates the need for complex modality re-alignment, a common requirement in alternative methods. {itemize"
Subtle Biases Need Subtler Measures: Dual Metrics for Evaluating Representative and Affinity Bias in Large Language Models,2405.14555v4,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2405.14555v4_0.png,"Research on Large Language Models (LLMs) has often neglected subtle biases that, although less apparent, can significantly influence the models' outputs toward particular social narratives. This study addresses two such biases within LLMs: {representative bias}, which denotes a tendency of LLMs to generate outputs that mirror the experiences of certain identity groups, and {affinity bias}, reflecting the models' evaluative preferences for specific narratives or viewpoints. We introduce two novel metrics to measure these biases: the Representative Bias Score (RBS) and the Affinity Bias Score (ABS), and present the Creativity-Oriented Generation Suite (CoGS), a collection of open-ended tasks such as short story writing and poetry composition, designed with customized rubrics to detect these subtle biases. Our analysis uncovers marked representative biases in prominent LLMs, with a preference for identities associated with being white, straight, and men. Furthermore, our investigation of affinity bias reveals distinctive evaluative patterns within each model, akin to `bias fingerprints'. This trend is also seen in human evaluators, highlighting a complex interplay between human and machine bias perceptions.{https://github.com/akkeshav/subtleBias}.}","Proportion of GPT-4's preferred responses for the short poem task in CoGS, categorized by identity-specific prompts, with highlighted sectors indicating a preference for outputs from those identities.","In recent years, the landscape of natural language processing has been transformed by the advent of Large Language Models (LLMs) such as GPT-4 , PaLM , LLaMA-2 , and Mixtral . These LLMs have expanded the boundaries of natural language generation and understanding beyond theoretical research, embedding themselves into critical decision-making processes with significant real-world implications, such as hiring practices, automated essay evaluations, and even judicial decision-making . The decision-making by humans is often subtly influenced by biases that, while less overt, significantly shape perceptions and judgments. Such subtle biases, although difficult to detect , can have far-reaching consequences . Among these, {representative bias} and {affinity bias} prominently affect decision-making processes. Representative bias stems from an unconscious presumption that dominant characteristics within a person's environment are universally normative, thus skewing what is considered `normal.' This bias is commonly seen in media representation, where prevalent cultural narratives disproportionately influence societal norms . Affinity bias is the unconscious preference for those who share similarities with oneself, such as cultural backgrounds, personal experiences, or gender identities. This type of bias is evident in scenarios like literary awards, where judges might favor narratives that resonate with their own experiences . As LLMs increasingly assume roles traditionally filled by humans, such as in creative writing and content moderation , they not only showcase their ability to replicate complex human tasks but also raise questions about their potential to perpetuate human biases. This study probes the extent to which LLMs exhibit representative and affinity biases, particularly in areas where they supplant human-generated content and its evaluation. We propose a comprehensive approach to quantify and analyze these biases in LLMs. Our methodology includes the `Creativity-Oriented Generation Suite' (CoGS), a novel benchmark suite designed to scrutinize subtle biases through a series of structured yet open-ended tasks. Figure~ offers a snapshot of our findings, depicting GPT-4's evaluation tendencies across different identity axes within the short poem task. Our contributions are threefold: {enumerate} {Creation of the `Creativity-Oriented Generation Suite,'} comprising 12 diverse open-ended tasks for content creation, ranging from short stories to haikus, complete with customized evaluation rubrics and a variety of themes for comprehensive analysis. {Development of two novel metrics,} the Representative Bias Score (RBS) and the Affinity Bias Score (ABS), tailored to measure biases in content generation and evaluation. {Extensive testing of recent LLMs,} such as LLaMA-2, GPT-4, and Mixtral, demonstrating prevalent representative biases towards identities typically associated with being {white, straight, and men}, and uncovering distinct patterns of affinity bias, with Mixtral displaying notably lowest ABS scores. {enumerate"
An Information-Theoretic Approach to Analyze NLP Classification Tasks,2402.00978v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.00978v1_0.png,"Understanding the importance of the inputs on the output is useful across many tasks. This work provides an information-theoretic framework to analyse the influence of inputs for text classification tasks. Natural language processing (NLP) tasks take either a single element input or multiple element inputs to predict an output variable, where an element is a block of text. Each text element has two components: an associated semantic meaning and a linguistic realization. Multiple-choice reading comprehension (MCRC) and sentiment classification (SC) are selected to showcase the framework. For MCRC, it is found that the context influence on the output compared to the question influence reduces on more challenging datasets. In particular, more challenging contexts allow a greater variation in complexity of questions. Hence, test creators need to carefully consider the choice of the context when designing multiple-choice questions for assessment. For SC, it is found the semantic meaning of the input text dominates (above 80\% for all datasets considered) compared to its linguistic realisation when determining the sentiment. The framework is made available at: \url{https://github.com/WangLuran/nlp-element-influence}.",Data generation for multiple-choice reading comprehension for the context (blue) and question (purple) respectively.,"Natural Language Processing (NLP) requires machines to understand human language to perform a specific task . NLP tasks typically take a single (such as summarization , sentiment classification , machine translation ) or multiple (such as reading comprehension , question generation ) text elements at the input and return a specific output. Each input text element can further be partitioned into its semantic content and the linguistic realization. The semantic meaning is the inherent meaning of the input element while the linguistic realization is the specific word choice to present the meaning in human language. Typically, there are several possible linguistic realizations for any semantic content. Therefore, for all NLP tasks, the output variable has contributions from at least two components: the semantic meaning of the element and the specific linguistic realization. Here, {element} refers to a specific input text in an NLP task that is formed of exactly two {component}s. In this work, we analyze the relative sensitivity of the output variable to each of the input elements as well as in terms of the breakdown between the elemental semantic content and its corresponding linguistic realization. A theoretical information-theoretic approach is applied to find the shared information content between each input component and the output variable. Here, the information-theoretic approach is framed for NLP classification tasks where the set of input components influence the output probability distribution over a discrete set of classes. We select multiple-choice reading comprehension (MCRC) and sentiment classification as case studies for the analysis. MCRC requires the correct answer option to be selected based on several inputs element: the context paragraph, the question and the set of answer options. Multiple-choice (MC) assessments are a widely employed method for evaluating the competencies of candidates across diverse settings and tasks on a global scale . Given their consequential impact on real-world decisions, the selection of appropriate MC questions tailored to specific scenarios is important for content creators. Consequently, there is a need to comprehend the underlying factors that contribute to the complexity of these assessments. Complexity of an MC question is best modelled by the distribution over the answer options by human test takers. Therefore, by understanding the influence of each input element on the output distribution, content creators can be better informed to what extent the complexity of an MC question can be controlled from changing each of the input elements. Moreover, analyzing the contribution of the semantic content vs the linguistic realization on the output human distribution informs the impact of the specific word choice in the element on the question complexity. However, it is not scalable to measure the variation in the output human distribution with variation in each of the input elements. demonstrated that the output distribution of machine reading comprehension systems is aligned (with minimal re-shaping parameters) to the human distribution. Therefore, the information-theoretic framework is applied to understand the influence of each input element as well as the semantic and linguistic components on the output probability distribution by an automated comprehension system. Sentiment classification (SC) is a common NLP classification task where the dominant sentiment class must be selected from a discrete set of sentiments based on a block of input text. This is an example of a single input text element NLP task. The information-theoretic approach is applied here to understand the role of the semantic content and the linguistic realization on the output distribution over the sentiment classes for common sentiment classification datasets. It is interesting to analyze SC as ideally the sentiment of a text block should be based on only its semantic meaning. Here, we determine whether this ideal is held in practice for popular SC corpuses. This work makes the following contributions: {itemize} Propose an information-theoretic framework for determining the contribution of each text element and further each elemental component on the output distribution for NLP classification tasks. Detailed analysis of the element and component breakdown according to the proposed framework for multiple-choice reading comprehension and sentiment classification datasets. {itemize} Despite the framework being applied to NLP classification tasks, it can be adapted to regression, sequence output and even vision tasks"
OPEx: A Component-Wise Analysis of LLM-Centric Agents in Embodied Instruction Following,2403.03017v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2403.03017v1_0.pdf,"Embodied Instruction Following (EIF) is a crucial task in embodied learning, requiring agents to interact with their environment through egocentric observations to fulfill natural language instructions. Recent advancements have seen a surge in employing large language models (LLMs) within a framework-centric approach to enhance performance in embodied learning tasks, including EIF. Despite these efforts, there exists a lack of a unified understanding regarding the impact of various components—ranging from visual perception to action execution—on task performance. To address this gap, we introduce OPEx, a comprehensive framework that delineates the core components essential for solving embodied learning tasks: Observer, Planner, and Executor. Through extensive evaluations, we provide a deep analysis of how each component influences EIF task performance. Furthermore, we innovate within this space by deploying a multi-agent dialogue strategy on a TextWorld counterpart, further enhancing task performance. Our findings reveal that LLM-centric design markedly improves EIF outcomes, identify visual perception and low-level action execution as critical bottlenecks, and demonstrate that augmenting LLMs with a multi-agent framework further elevates performance.",Overview of our OPEx framework. We will open-source the code after acceptance.,"Embodied learning, particularly through tasks like Embodied Instruction Following (EIF), stands at the forefront of artificial intelligence research. EIF, where agents must interpret natural language instructions to navigate and act within their environment using egocentric observations, epitomizes the challenge of integrating cognitive understanding with physical action. This intersection is crucial for developing autonomous agents capable of nuanced interaction with complex, real-world environments, marking a significant stride towards more advanced and versatile AI systems. As the research community harnesses advancements in deep learning, we edge closer to this ambition. Traditional approaches to Embodied Instruction Following (EIF) often rely on expert-generated annotations, a process that can be both expensive and challenging to scale for real-world applications. In contrast, Large Language Models (LLMs), such as those cited in recent studies, have emerged as a potent alternative, showcasing exceptional capabilities in natural language understanding and generation. These models, enriched by extensive textual datasets, demonstrate significant common-sense reasoning abilities. As a result, there's a growing trend towards leveraging LLM-centric architectures for embodied learning tasks including EIF, which promise to simplify planning and execution tasks through a few-shot learning paradigm. However, despite their potential, the implementations of EIF systems introduce a variety of designs and components across different studies. There remains a notable gap in systematically understanding how these disparate elements influence overall task performance, underscoring the need for a thorough analysis of LLM-centric methods within the context of EIF. In addressing the complexities of Embodied Instruction Following (EIF), we introduce OPEx, a novel framework designed to systematically outline the essential components for mastering embodied learning tasks. OPEx is segmented into three core parts: Observer, Planner, and Executor. The Observer component is tasked with processing and interpreting sensory inputs, primarily visual, to construct an actionable understanding of the agent's immediate environment. The Planner dynamically devises strategic plans as subtasks to complete the tasks based on perceptual inputs, effectively bridging the gap between perception and action. Lastly, the Executor is responsible for implementing these plans with a skill library, which translates several re-useable skills into precise, context-aware actions within the environment, ensuring the agent's interactions are both relevant and goal-oriented. This tripartite structure provides a clear delineation of roles within the system, facilitating a granular analysis of how each contributes to the overarching performance of EIF tasks. To understand the impact of each OPEx component on performance in EIF tasks, we conducted an in-depth analysis. By experimenting with different versions of the Observer, Planner, and Executor components, we assessed how each contributes to and influences overall success. This approach allowed us to identify the key attributes and design choices that enhance the system's ability to tackle complex embodied tasks, providing clear insights into optimizing embodied learning agents. To further unlock the potential of LLMs in embodied learning, we eliminate the influence of visual perception and low-level action execution of the system utilizing a pure-text counterpart environment and further adopt a multi-agent dialogue strategy, splitting the instruction-following challenge into distinct reasoning and grounding roles handled by a reasoner agent and an actor agent, respectively. This dialogue-driven approach simplifies the task into decision-making processes, where both agents utilize world knowledge obtained from an explorer. This explorer gathers insights either through direct interaction with the environment or from human contributions, thereby enriching the collaborative problem-solving capabilities of the reasoner and actor with more grounded and informed decision-making. Our experimental evaluation was conducted using the ALFRED and ALFWorld benchmarks, providing a comprehensive testing ground for our extensive evaluation. The core analysis of our experiments underscores significant advancements: the LLM-centric approach notably enhances performance in EIF tasks. We pinpoint visual perception and low-level action execution as pivotal bottlenecks. Moreover, our results affirm that incorporating a multi-agent dialogue strategy into an LLM-centric task solver significantly boosts overall task performance on AFLWorld, showcasing the effectiveness of our proposed methodology in addressing the complexities of embodied learning tasks"
Hyper-CL: Conditioning Sentence Representations with Hypernetworks,2403.09490v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2403.09490v2_0.pdf,"While the introduction of contrastive learning frameworks in sentence representation learning has significantly contributed to advancements in the field, it still remains unclear whether state-of-the-art sentence embeddings can capture the fine-grained semantics of sentences, particularly when conditioned on specific perspectives. In this paper, we introduce Hyper-CL, an efficient methodology that integrates hypernetworks with contrastive learning to compute conditioned sentence representations. In our proposed approach, the hypernetwork is responsible for transforming pre-computed condition embeddings into corresponding projection layers. This enables the same sentence embeddings to be projected differently according to various conditions. Evaluation of two representative conditioning benchmarks, namely conditional semantic text similarity and knowledge graph completion, demonstrates that Hyper-CL is effective in flexibly conditioning sentence representations, showcasing its computational efficiency at the same time. We also provide a comprehensive analysis of the inner workings of our approach, leading to a better interpretation of its mechanisms. Our code is available at {https://github.com/HYU-NLP/Hyper-CL}.","Illustration of our approach dubbed Hyper-CL. In the example, two sentences are provided along with two distinct conditions, $c_{high}$ and $c_{low}$. Specifically, $c_{high}$ (\textcolor{orange}{orange}) denotes a condition that results in the sentences being interpreted more similarly, whereas $c_{low}$ (\textcolor{blue}{blue}) leads to a perspective in which the two sentences are understood as being relatively more distinct. The identical pair of sentences are projected into different subspaces that reflect the provided conditions.","Building upon the established correlation between language model performance and computational capacity , there has emerged an undeniable trend towards the adoption of ever-larger language models across a diverse range of NLP applications. This trend is also evident in the computation of sentence or text representations. Despite the ongoing popularity of compact encoders such as BERT and RoBERTa , there is a growing inclination to leverage the capabilities of recent, larger language models, e.g., LLaMA-2 , even breaking from the conventional roles of encoders and decoders. Consequently, the enduring challenge of finding a balance between performance and computational cost---a persistent issue in sentence representation learning ---continues to be elusive. In recent years, there has been a marked improvement in the quality of sentence embeddings, a progress primarily driven by the advent of contrastive learning frameworks (; {inter alia}). However, since the performance of these embeddings is generally evaluated based on their ability to encapsulate the overall meaning of the corresponding sentences---as measured by benchmarks like STS-B and MTEB , it remains uncertain whether they adequately capture information relating to the various aspects of the source sentences. For instance, consider the sentences (1) {``A cyclist pedals along a scenic mountain trail, surrounded by lush greenery''} and (2) {``A hiker navigates through a dense forest on a winding path, enveloped by the tranquility of nature''}. In terms of {``The mode of transportation''}, these sentences should be perceived as similar since both depict individuals engaging in outdoor activities, traversing natural landscapes. However, regarding {``The speed of travel''}, they should be differentiated, as cycling generally entails a faster pace than hiking. reported that current models for sentence embeddings face challenges in recognizing the fine-grained semantics within sentences. In other words, the existing models struggle to accurately detect the subtle shifts in sentence nuances that occur when conditioned on specific criteria. In the literature, three prevalent approaches have been established for constructing {conditioned} representations , particularly in estimating their similarity (see Figure ). The first is the {cross-encoder} approach, which encodes the concatenation of a pair of sentences ($s_1$, $s_2$) with a condition ($c$), i.e., $[s_1; s_2; c]$.{$[; ]$ represents the concatenation operation.} The second method is the {bi-encoder} architecture, computing separate representations of sentences $s_1$ and $s_2$ with the condition $c$---$[s_1; c]$ and $[s_2; c]$. Despite their simplicity, both approaches share a notable limitation: the representation must be computed for every unique combination of sentences plus a condition. On the other hand, the {tri-encoder} architecture utilizes pre-computed embeddings of sentences $s_1$ and $s_2$ along with the condition $c$. It then employs a separate composition function responsible for merging the semantics of the sentence and condition. Considering that the embeddings for each component can be cached and reused, this approach offers enhanced long-term efficiency. The {tri-encoder} architecture, despite its potential, falls short in performance compared to the {bi-encoder}. This is primarily due to its inherent limitation, which is the inability to model explicit interactions between sentences and conditions during the representation construction process. Therefore, there is a need to propose a revised version of the {tri-encoder} architecture that improves its functionality without substantially sacrificing its efficiency. In this work, we present {Hyper-CL}, a method that integrates {Hyper}networks with {C}ontrastive {L}earning to efficiently compute conditioned sentence representations and their similarity. As illustrated in Figure , our proposed approach is derived from the {tri-encoder} architecture. It introduces an additional hypernetwork tasked with constructing a condition-sensitive network on the fly. This network projects the original sentence embeddings into a specific condition subspace. Figure illustrates the effectiveness of Hyper-CL in dynamically conditioning pre-computed sentence representations according to different perspectives. We demonstrate the effectiveness of Hyper-CL by significantly reducing the performance gap with the {bi-encoder} architecture in the Conditional Semantic Textual Similarity (C-STS) and Knowledge Graph Completion (KGC) tasks. In particular, for C-STS, Hyper-CL demonstrates an improvement of up to 7.25 points in Spearman correlation compared to the original {tri-encoder} architecture. Furthermore, compared to the {bi-encoder} approach, our method shows superior efficiency by reducing the running time by approximately 40\"
ABEX: Data Augmentation for Low-Resource NLU via Expanding Abstract Descriptions,2406.04286v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.04286v1_0.pdf,"We present {ABEX}, a novel and effective generative data augmentation methodology for low-resource Natural Language Understanding (NLU) tasks. ABEX is based on {AB}stract-and-{EX}pand, a novel paradigm for generating diverse forms of an input document -- we first convert a document into its concise, abstract description and then generate new documents based on expanding the resultant abstraction. To learn the task of expanding abstract descriptions, we first train BART on a large-scale synthetic dataset with abstract-document pairs. Next, to generate abstract descriptions for a document, we propose a simple, controllable, and training-free method based on editing AMR graphs. ABEX brings the best of both worlds: by expanding from abstract representations, it preserves the original semantic properties of the documents, like style and meaning, thereby maintaining alignment with the original label and data distribution. At the same time, the fundamental process of elaborating on abstract descriptions facilitates diverse generations. We demonstrate the effectiveness of ABEX on 4 NLU tasks spanning 12 datasets and 4 low-resource settings. ABEX outperforms all our baselines qualitatively with improvements of 0.04\% - 38.8\%. Qualitatively, ABEX outperforms all prior methods from literature in terms of context and length diversity~. $Equal Technical Contribution.}","Illustration of our proposed augmentation methodology. {Top: Learning to Expand Abstract Descriptions.} \textcircled{\raisebox{-0.9pt}{1}} We synthesize a large-scale synthetic dataset $\mathcal{D}_{ab}$ with abstract-document pairs by prompting LLMs with unlabeled documents from $\mathcal{D}_{ab}$. \textcircled{\raisebox{-0.9pt}{2}} We pre-train BART on this dataset with abstract as input and document as the target for learning to expand abstract descriptions. {Bottom: Data Augmentation.} \textcircled{\raisebox{-0.9pt}{1}} We convert the document into its AMR graph representation $\mathcal{G}_{i}$ using a Text-to-AMR Parser. \textcircled{\raisebox{-0.9pt}{2}} $\mathcal{G}_{i}$ then goes through multiple steps of {deletion} to obtain $\hat{\mathcal{G}}_{i}$ \textcircled{\raisebox{-0.9pt}{3}} We optionally retrieve a semantically similar document from $\mathcal{D}_{down}$, obtain its AMR graph $\mathcal{G}_{k}$, and replace subtrees in $\hat{\mathcal{G}}_{i}$ with {similar} subtrees in $\hat{\mathcal{G}}_{i}$. \textcircled{\raisebox{-0.9pt}{4}} $\hat{\mathcal{G}}_{i}$ is then converted back to text (which is now an abstract description) using an AMR-to-Text generator. \textcircled{\raisebox{-0.9pt}{5}} This abstract description is then passed to the fine-tuned BART for generating augmentations. \textcircled{\raisebox{-0.9pt}{6}} We optionally fine-tune the fine-tuned BART (from the 1st step) on abstract-document pairs from $\mathcal{D}_{down}$.","Improving the performance of deep learning models on downstream Natural Language Understanding (NLU) tasks requires sufficient good-quality training data. However, data annotation is an expensive, time-consuming, and noisy task. Data augmentation has proven to be an effective approach for overcoming the data scarcity issue in low-resource NLU tasks with limited training samples. The two major categories of study in data augmentation include online data augmentation by interpolation in the latent space and offline data augmentation that expands an existing small-scale dataset by generating additional synthetic data . Owing to advancements in generative models that facilitate the creation of high-quality synthetic data, the latter is gaining traction. However, generative data augmentation faces two major challenges: {diversity} in generated augmentations and {consistency} with the underlying data distribution . It is crucial to strike a balance between these two aspects, as overemphasizing one at the expense of the other can lead to poor downstream performance. Current augmentation methods based on text-infilling, where the primary task is to generate a new sentence constrained with keywords, are prone to replicate biases and overfit specific linguistic patterns in the low-resource training data, thereby hurting diversity. Additionally, we show that keyword-constrained free-form generation is unable to maintain the core semantic properties of the document, like style, which proves to be critical for specific tasks (e.g., {question} style document for intent classification. See example in Table~). Diversity also proves to be an issue with token-level editing methods that rarely introduce novel entities or contexts and often randomly edits important tokens. Finally, prompt-based methods that employ Large Language Models (LLMs) require well-curated attributes selected from the data to control the distribution of the generated data. {0.5mm} { {Main Contributions. }} In this paper, we propose {ABEX}, a novel data augmentation methodology based on a novel paradigm - Abstract-and-Expand. We first convert an input document into a concise, abstract description of itself and then generate augmentations by expanding the resultant abstraction. The task emulates human language perception and processing: the abstraction phase mirrors how humans distill core ideas from text, focusing on essential meanings, while the expansion phase reflects human creativity in generating varied narratives from a single abstract concept, akin to human extrapolation of ideas into diverse discussions. Our proposed Abstract-and-Expand task, which differs from all tasks proposed in prior art, generates augmentations that are both more consistent and diverse. To learn the task of expanding abstract descriptions, we first synthesize a large-scale synthetic dataset by prompting LLMs and then train an Encoder-Decoder Pre-trained Language Model (BART) on the dataset. Next, we propose a simple and controllable algorithm to generate abstract descriptions for training instances in any given downstream low-resource dataset. Our proposed algorithm leverages AMR-to-Text and Text-to-AMR and generates abstract descriptions by editing Abstract Meaning Representation (AMR) graphs. Inspired by the success of mixup in data augmentation, we also optionally mix AMR graphs of two sentences to boost the diversity of abstract descriptions. Finally, we synthesize diverse augmentations using the fine-tuned model and synthesized abstract descriptions. To summarize, our main contributions are: {enumerate} We propose ABEX, a novel and effective generative data augmentation methodology for low-resource NLP. We employ a novel Abstract-and-Expand task and fine-tune an Enc-Dec PLM to learn the task. ABEX differs from all prior work in its motivation and methodology and closely mimics the human perception and processing of language. We propose a simple, controllable, and training-free method for generating abstract descriptions of source documents from downstream NLU datasets. Our proposed methodology provides explicit control in the document-to-abstract generation process and overcomes the contained generation issue that LLMs face in abstract generation. To evaluate the efficacy of ABEX augmentations, we experiment on 12 datasets across 4 NLU tasks under 4 low-resource settings and show that ABEX outperforms most prior works quantitatively by 0.04\ We also contribute the large-scale synthetic dataset with $$0.2 million abstract-expansion pairs to promote further research in this space. {enumerate"
Token-wise Influential Training Data Retrieval for Large Language Models,2405.11724v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2405.11724v2_0.pdf,"Given a Large Language Model (LLM) generation, how can we identify which training data led to this generation?} In this paper, we proposed , a scalable framework adapting to LLMs for estimating the influence of each training data. The proposed framework consists of two stages: caching and retrieval. First, we compress the gradient vectors by over 200,000x, allowing them to be cached on disk or in GPU/CPU memory. Then, given a generation, efficiently traverses the cached gradients to estimate the influence within minutes, achieving over a 6,326x speedup. Moreover, supports multi-GPU parallelization to substantially accelerate caching and retrieval. Our empirical result confirms the efficiency and effectiveness of \texttt{RapidIn}.",Influence estimation for a given generation.,"Large language models (LLMs) have been widely used in various applications across different industries, such as text generation, translation, summarization, and scientific applications, due to their unprecedented scale and the impressive capabilities derived from the massive training dataset . E.g., llama-2 has up to 70 billion parameters and is trained on 2 trillion tokens of online data. {Given a model generation, can we determine which training data have the most influence for this generation?} Understanding how training data influence the content they generate is particularly crucial. For example, when a risky generation is identified, tracing it back to the most influential training data can help developers filter out risky data and retrain the model. In addition, knowing the influence of training data for a target generation is highly valuable for machine unlearning, explainablity, detoxification, data cleansing and poisoning, privacy and security preserving. However, estimating influence of training data on LLMs of this unprecedented scale, trained on massive data containing over trillions of tokens, remains a challenge. {Influence Estimation} estimates the influence and traces generation back to training data (Figure~). Although many studies explored influence estimation on deep learning , these methods cannot be scaled up to LLMs due to lacking of scalability and efficiency: e.g., proposed influence function using Hessian-vector products, but computing second-order gradients is prohibitively expensive for LLMs. To reduce computation, presented TracIn which only requires first-order gradient. However, even first-order gradients scale poorly---the gradients of a full-precision llama-2 7b model is $$26GB in size; and $$260GB for llama-2 70b. The massive gradient storage and processing make them impractical for LLMs. Although these studies have shown remarkable performance on influence estimation, they primarily focus on general deep learning models, and require first or second-order gradients. The extreme memory and computation of calculating full gradients presents substantial challenges in applying them to LLMs, particularly in the context of token-wise cases. {Challenges.} (1) Compared to general models, LLMs like llama-2, which has up to 70 billion parameters, present exceptional scalability challenges for influence estimation methods due to their vast number of parameters. (2) In addition to the scalability issues of model size, LLMs are trained on massive datasets (e.g., 2 trillion tokens for llama-2). Estimating the influence of each training data from such massive datasets presents another substantial challenge. (3) Almost all studies of influence function are based on the classification task and assign influence scores to each training sample. However, in LLM datasets, a single data sample consists of numerous tokens, and it is very challenging to assign an influence score to each token. In this paper, we propose {RapidIn}, a rapid influence estimating framework for LLMs, to estimate the influence of each training data for a given generation. RapidIn is designed to efficiently scale to large models and massive datasets. The framework includes two stages: caching and retrieval. Caching: {RapidIn} compresses the gradient vector of each training data into a low-dimensional representation called {RapidGrad}, reducing the size to MBs or even KBs. These compact RapidGrad representations are then cached to disk or memory. Subsequently, in retrieval, {RapidIn} can estimate the influence using the cached {RapidGrad} for the entire training data in minutes for any generation. {Contributions.} Our main contributions are: {itemize}[leftmargin=*,noitemsep,topsep=0pt] We present {RapidIn} that estimates the influence of each training data for a given LLM generation. We apply a collection of techniques to cache the gradients of LLMs by compressing gradient vectors by over $200,000$x in the caching stage, and achieve a $6,326$x speedup in the retrieval stage, enabling estimating the influence of the entire dataset for any test generation within minutes. We utilize multi-GPU parallelization to substantially accelerate the caching and retrieval. We release an open-source and easy-to-run implementation of {RapidIn}{{https://github.com/huawei-lin/RapidIn}} in PyTorch. {itemize"
AbsInstruct: Eliciting Abstraction Ability from LLMs through Explanation Tuning with Plausibility Estimation,2402.10646v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.10646v2_0.pdf,"Abstraction ability is crucial in human intelligence, which can also benefit various tasks in NLP study. Existing work shows that LLMs are deficient in abstract ability, and how to improve it remains unexplored. In this work, we design the framework to enhance LLMs' abstraction ability through instruction tuning. The framework builds instructions with in-depth explanations to assist LLMs in capturing the underlying rationale of abstraction. Meanwhile, we introduce a plausibility estimator to select instructions that are more consistent with the abstraction knowledge of LLMs to be aligned. Then, our framework combines abstraction instructions with general-purpose ones to build a hybrid dataset. Extensive experiments and analyses} demonstrate that our framework can considerably enhance LLMs' abstraction ability with strong generalization performance while maintaining their general instruction-following abilities.",An illustration of our \papertitle framework. We collect explanation traces for each example and design a plausibility estimator to select data that match the knowledge of an LLM to be aligned.,"Abstraction ability is central to human cognition, which is identifying shared traits among items to build a broader concept, like deriving the concept of ``beverage'' from ``coffee'' and ``tea.'' With this ability, we can derive general rules and principles from past experiences, which enables us to adeptly navigate new situations in our daily life. In NLP, building abstraction resources has long been a vital challenge to which the community has devoted many efforts. Among them, built the first comprehensive benchmark, , of abstract concepts for nouns, verbs, and events. In this benchmark, models are asked to detect the validity of an abstract concept, as shown in {fig:intro_illustration}. Their evaluations on the benchmark reveal that abstraction remains challenging even for state-of-the-art LLMs. For example, ChatGPT only modestly exceeds majority voting and substantially trails behind fine-tuned smaller models. While prior works have explored ways for general-domain LLM alignment, how to elicit the abstraction knowledge of LLMs remains unexplored. Nonetheless, enhancing LLMs' abstraction ability is a non-trivial task. We only observe slight improvements when gathering vanilla instructions from randomly sampled data for detecting abstract concepts. First, the responses of vanilla instructions only express the validity of abstract concepts as ``Yes/No.'' As a result, LLMs might only grasp the surface-level styles but miss underlying rationales in deciding the validity of abstract concepts. Moreover, existing studies show that LLMs acquire most of the knowledge and abilities during pre-training. Thus, instructions from randomly sampled data might not be consistent with the abstraction knowledge of pre-trained models for better elicitation. To tackle those issues, we propose the framework to build instructions with detailed explanation traces and well-crafted data selection, as shown in {fig:intro_illustration}. The framework forms explanation traces by collecting meanings of each given instance and abstract concept. These traces can help LLMs better comprehend the underlying reasoning process of detecting abstract concepts. Moreover, we introduce a plausibility estimator to select instruction data consistent with the abstraction knowledge of a pre-trained model to be aligned. The estimator assesses the plausibility score of each example based on the probability computed by the pre-trained model. Then, we only retain examples with higher plausibility scores, which align better with the model's knowledge. We also introduce a collection of filters based on lexical overlap, keywords, and predicted labels to ensure diversity and quality further. Ultimately, a hybrid dataset is constructed by combining instructions for abstraction detection with those in the general domain. For evaluation, the framework first builds instructions for abstraction detection based on and combines them with instructions from Alpaca. Next, we conduct extensive experiments and analyses of several popular LLMs instruction-tuned with our framework. The evaluation results show that applying can effectively unlock LLMs' abstraction ability, with the performance surpassing existing alignment methods by a large margin of 6-10\"
An Information Bottleneck Perspective for Effective Noise Filtering on Retrieval-Augmented Generation,2406.01549v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.01549v2_0.pdf,"Retrieval-augmented generation integrates the capabilities of large language models with relevant information retrieved from an extensive corpus, yet encounters challenges when confronted with real-world noisy data. One recent solution is to train a filter module to find relevant content but only achieve suboptimal noise compression. In this paper, we propose to introduce the information bottleneck theory into retrieval-augmented generation. Our approach involves the filtration of noise by simultaneously maximizing the mutual information between compression and ground output, while minimizing the mutual information between compression and retrieved passage. In addition, we derive the formula of information bottleneck to facilitate its application in novel comprehensive evaluations, the selection of supervised fine-tuning data, and the construction of reinforcement learning rewards. Experimental results demonstrate that our approach achieves significant improvements across various question answering datasets, not only in terms of the correctness of answer generation but also in the conciseness with $2.5\%$ compression rate.","In retrieval-augmented generation, passages $X$ are retrieved to enhance the generation of output $Y$. (A) Recent Noise filtering approaches obtain the compression $\tilde{X}\subseteq X$ with log likelihood objective to outputs $Y$. Our information bottleneck objective enables a precise delineation of the intersection $\tilde{X}_{\exper{IB}}=X\cap Y$. (B) Information bottleneck explicitly compresses $\tilde{X}_{\exper{IB}}=\phi$, when retrieved passages are irrelevant to outputs.","Large language models represent a significant advancement in natural language understanding and generation, with the capability to process and produce human-like language at an unprecedented scale and complexity . Nonetheless, large language models have several drawbacks, such as hallucination and lacking knowledge for specific domains or highly specialized queries . Retrieval-augmented generation has gained attention for its ability to incorporate information from external knowledge sources during the inference stage. By combining retrieval-based methods with generative models, this approach can improve the relevance, coherence, and factual accuracy of text generation . Retrieval-augmented generation also presents problems. On the one hand, the retriever's efficacy may be suboptimal in practical use . On the other hand, the internet data is often of low quality, with redundancy and noise. Indeed, the retrieved content can be completely irrelevant to the query, leading to the model producing incorrect results . Recent solutions to mitigate noise in retrieval evidence often involve the adoption of a filtering module . However, these methods encounter several issues: (1) The inability to ensure that the annotated filtering results can effectively support the generation model in accurately answering questions. (2) The difficulty in directing the filter to refrain from answering when confronted with retrieval evidence that does not support question resolution. (3) The lack of adaptation to the compression extent of the filtering results, impeding the achievement of an optimal solution in terms of cost performance. We observe that issues above originate from sub-optimal objectives. As shown in Figure A, the intersection between retrieved passages $X$ and outputs $Y$ denotes the precise information in $X$ which is useful for $Y$. The noise filter extracts compression ${X}$ from retrieved passages $X$, where the filter is optimized with log likelihood objective to output $Y$. The noise filter trained with this objective can obtain a compression ${X}$ containing the intersection $X Y$, but is incapable of realizing its exact area, which means the filter cannot in principle eliminate the interference of noise for subsequent generation. Therefore, we propose to utilize the information bottleneck theory to optimize the noise filter from a comprehensive perspective, via simultaneously maximizing the useful information while minimizing the noise, thus facilitating a precise delineation of the intersection ${X}_{{IB}}=X Y$. Furthermore, in cases (Figure B) where retrieval is not necessitated for content generation or exhibits limited efficacy, the information bottleneck objective enables noise filters to compress the retrieved passages into empty ${X}_{{IB}}=$. Specifically, we consider information bottleneck as a principle for retrieval augmentation. We first theoretically derive the formula of information bottleneck for retrieval-augmented generation, which integrates large language models. Then we introduce information bottleneck as a new comprehensive evaluation metric for noise filtering, assessing both conciseness and correctness of compressed contents. Next we derive information bottleneck version of supervised fine-tuning and reinforcement learning objectives to train the noise filter. We conduct experiments on the open-domain question answering datasets: Natural Questions ({NQ}) , {TriviaQA} , and a more complex multi-hop {HotpotQA} . Using {Llama2} as filtering and generation model, our approach is proved to be effective compared with strong baseline models, including {RankGPT}, {LongLLMLingua} and {Lllama2} on all three datasets. We achieve a $2.5\ Our paper presents three main innovations: {itemize} We are the first, to the best of our knowledge, to introduce the information bottleneck theory into retrieval-augmented generation, which illustrates the optimum of filtration. We propose to apply the information bottleneck on evaluation metrics, supervised fine-tuning objectives, and reinforcement learning rewards for retrieval-augmented generation. Experimental results reveal the effectiveness of our approach in terms of generation correctness and compression conciseness. {itemize"
RORA: Robust Free-Text Rationale Evaluation,2402.18678v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.18678v3_0.pdf,"Free-text rationales play a pivotal role in explainable NLP, bridging the knowledge and reasoning gaps behind a model's decision-making. However, due to the diversity of potential reasoning paths and a corresponding lack of definitive ground truth, their evaluation remains a challenge. Existing evaluation metrics rely on the degree to which a rationale {supports} a target label, but we find these fall short in evaluating rationales that inadvertently {leak the labels}. To address this problem, we propose , a bust free-text tionale evaluation against label leakage.} quantifies the new information supplied by a rationale to justify the label. This is achieved by assessing the conditional $$-information with a predictive family robust against leaky features that can be exploited by a small model. consistently outperforms existing approaches in evaluating human-written, synthetic, or model-generated rationales, particularly demonstrating robustness against label leakage. We also show that aligns well with human judgment, providing a more reliable and accurate measurement across diverse free-text rationales.","\name{} framework for evaluating rationales $R_1^{\text{True}}$, $R_2^{\text{True}}$, $R_3^{\text{True}}$. Existing baselines are highly sensitive to rationales that \textcolor{orange}{simply restate the label} or \textcolor{Green}{paraphrase the given question and label}, leading to inflated scores compared to the \textcolor{blue}{human-annotated} rationale. In contrast, \name{} provides an informativeness score that better characterizes rationale quality. It is achieved by \Circled{1} detecting potential leakage tokens in the rationale (\S\ref{subsec:leakage-detection}) and \Circled{2} generate additional training data with counterfactual editing for data augmentation (\S\ref{subsec: data augmentation}), followed by \Circled{3} training an evaluation model invariant to label leakage (\S\ref{subsec: evaluation}).","The ability of large language models (LLMs) to generate free-text rationales that elaborate on their decision-making processes holds promise for explainable NLP, either in the form of a reasoning chain or post-hoc explanations . Previous works have also collected human-written rationales to enhance model reasoning and the generation of free-text rationales . However, evaluating these rationales remains an open problem because of the diversity of reasoning paths and the lack of definitive ground truth . As a result, existing metrics rely on measuring how much the rationale supports a given label. This is usually achieved by comparing predictions of models trained with and without rationales. For example, Leakage-Adjusted Simulatability (LAS) and Rationale Quality (RQ) measure rationale quality through the difference in accuracy. Alternatively, Rationale Evaluation with conditional-${V}$-information (REV) evaluates the reduction in model predictive uncertainty upon conditioning on the rationale. Yet all these methods are vulnerable to {label leakage} : the rationale inadvertently {paraphrasing} or {restating} labels, creating a spurious shortcut for the evaluation model to infer the label. The critical issue stems from the mismatch in objectives: existing methods evaluate how easy it is to utilize information in the rationale, but rationales are {explanations}, whose quality does not always come with simplicity. The best explanation has to support the answer through some sense of mechanisms, such as methodically considering a set of axioms and running through a deductive chain , without which they are mere ``effects'' . {figure: pipeline} shows an example where existing evaluation methods are highly sensitive to label leakages in paraphrased or restated rationales, while in fact, these label leakages merely increase the predictive probability without providing any meaningful explanations.{Note that scores between different evaluation metrics are not directly comparable because of different scales and criteria. In this paper, our analysis mainly focuses on the ranking and relative differences within each metric.} With this objective in mind, we introduce {}, a novel approach to evaluate rationales robust to label leakage. {}'s construction consists of three stages as illustrated in {figure: pipeline}. First, we fit a small model and identify label-leaking tokens via its gradient-based attributions (). After that, we generate additional training data with counterfactual editing (). Finally, we force the evaluation model to ignore these label-leaking tokens through invariant learning (). Our approach aligns with the human perception that explanations should apparently increase the understanding of a given phenomenon by helping to create knowledge and to develop better theories . On the contrary, label leakage tends to be repetitive and tautological , dominating the insightful parts of the explanation. We compare {} with baseline metrics in evaluating various synthetic and human-annotated rationales, with or without label leakage, on three QA datasets. {} consistently outperforms baseline metrics by providing robust evaluations against label leakages. We also compare {} against model-generated rationale evaluation and demonstrate its better agreement with human evaluation"
DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models,2401.06066v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2401.06066v1_0.pdf,"In the era of large language models, Mixture-of-Experts (MoE) is a promising architecture for managing computational costs when scaling up model parameters. However, conventional MoE architectures like GShard, which activate the top-$K$ out of $N$ experts, face challenges in ensuring expert specialization, i.e. each expert acquires non-overlapping and focused knowledge. In response, we propose the {} architecture towards ultimate expert specialization. It involves two principal strategies: (1) finely segmenting the experts into $mN$ ones and activating $mK$ from them, allowing for a more flexible combination of activated experts; (2) isolating $K_s$ experts as shared ones, aiming at capturing common knowledge and mitigating redundancy in routed experts. Starting from a modest scale with 2B parameters, we demonstrate that 2B achieves comparable performance with GShard 2.9B, which has 1.5$$ expert parameters and computation. In addition, 2B nearly approaches the performance of its dense counterpart with the same number of total parameters, which set the upper bound of MoE models. Subsequently, we scale up to 16B parameters and show that it achieves comparable performance with LLaMA2 7B, with only about 40\% of computations. Further, our preliminary efforts to scale up to 145B parameters consistently validate its substantial advantages over the GShard architecture, and show its performance comparable with DeepSeek 67B, using only 28.5\% (maybe even 18.2\%) of computations.","Comparison between \spmoe{} 16B and open source models on the Open LLM Leaderboard. The red dashed line is linearly fitted from data points of all models except \spmoe{} 16B. \spmoe{} 16B consistently outperforms models with a similar number of activated parameters by a large margin, and achieves comparable performance with LLaMA2 7B, which has approximately 2.5 times the activated parameters.","Recent research and practices have empirically demonstrated that, with sufficient training data available, scaling language models with increased parameters and computational budgets can yield remarkably stronger models. It is imperative to acknowledge, however, that the endeavor to scale models to an extremely large scale is also associated with exceedingly high computational costs. Considering the substantial costs, the Mixture-of-Experts~(MoE) architecture has emerged as a popular solution. It can enable parameter scaling, while concurrently keeping computational costs at a modest level. Recent applications of MoE architectures in Transformers have yielded successful attempts at scaling language models to a substantial size, accompanied with remarkable performance. These achievements underscore the considerable potential and promise of MoE language models. Despite the promising potential of MoE architectures, existing MoE architectures potentially suffer from issues of knowledge hybridity and knowledge redundancy, which limit the expert specialization, i.e., each expert acquires non-overlapping and focused knowledge. Conventional MoE architectures substitute the Feed-Forward Networks (FFNs) in a Transformer with MoE layers. Each MoE layer consists of multiple experts, with each structurally identical to a standard FFN, and each token is assigned to one or two experts. This architecture manifests two potential issues: (1) {Knowledge Hybridity}: existing MoE practices often employ a limited number of experts (e.g., 8 or 16), and thus tokens assigned to a specific expert will be likely to cover diverse knowledge. Consequently, the designated expert will intend to assemble vastly different types of knowledge in its parameters, which are hard to utilize simultaneously. (2) {Knowledge Redundancy}: tokens assigned to different experts may require common knowledge. As a result, multiple experts may converge in acquiring shared knowledge in their respective parameters, thereby leading to redundancy in expert parameters. These issues collectively hinder the expert specialization in existing MoE practices, preventing them from reaching the theoretical upper-bound performance of MoE models. In response to the aforementioned issues, we introduce {{}}, an innovative MoE architecture specifically designed towards ultimate expert specialization. Our architecture involves two principal strategies: (1) {Fine-Grained Expert Segmentation:} while maintaining the number of parameters constant, we segment the experts into a finer grain by splitting the FFN intermediate hidden dimension. Correspondingly, keeping a constant computational cost, we also activate more fine-grained experts to enable a more flexible and adaptable combination of activated experts. Fine-grained expert segmentation allows diverse knowledge to be decomposed more finely and be learned more precisely into different experts, where each expert will retain a higher level of specialization. In addition, the increased flexibility in combining activated experts also contributes to a more accurate and targeted knowledge acquisition. (2) {Shared Expert Isolation:} we isolate certain experts to serve as shared experts that are always activated, aiming at capturing and consolidating common knowledge across varying contexts. Through compressing common knowledge into these shared experts, redundancy among other routed experts will be mitigated. This can enhance the parameter efficiency and ensure that each routed expert retains specialized by focusing on distinctive aspects. These architectural innovations in {} offer opportunities to train a parameter-efficient MoE language model where each expert is highly specialized. Starting from a modest scale with 2B parameters, we validate the advantages of the {} architecture. We conduct evaluations on 12 zero-shot or few-shot benchmarks spanning diverse tasks. Empirical results indicate that {} 2B surpasses GShard 2B by a substantial margin, and even matches GShard 2.9B, a larger MoE model with 1.5$$ expert parameters and computation. Remarkably, we find that {} 2B nearly approaches the performance of its dense counterpart with an equivalent number of parameters, which sets the strict upper bound of MoE language models. In pursuit of deeper insights, we conduct elaborate ablation studies and analysis on the expert specialization for {}. These studies validate the effectiveness of fine-grained expert segmentation and shared expert isolation, and provide empirical evidence supporting the assertion that {} can achieve a high level of expert specialization. Leveraging our architecture, we subsequently scale up the model parameters to 16B and train {} 16B on a large-scale corpus with 2T tokens. Evaluation results reveal that with only about 40\ We also compare {} with open source models and the evaluations demonstrate that {} 16B consistently outperforms models with a similar number of activated parameters by a large margin, and achieves comparable performance with LLaMA2 7B, which has approximately 2.5 times the activated parameters. Figure~ demonstrates the evaluation results on the Open LLM Leaderboard{https://huggingface.co/spaces/HuggingFaceH4/open\_llm\_leaderboard}. Additionally, we conduct supervised fine-tuning~(SFT) for alignment, transforming the model into a chat model. Evaluation results show that {} Chat 16B also achieves comparable performance with DeepSeek Chat 7B and LLaMA2 SFT 7B in the chat setting. Encouraged by these results, we further undertake a preliminary endeavor to scale up {} to 145B. The experimental results still validate its substantial advantages over the GShard architecture consistently. In addition, it shows performance comparable with DeepSeek 67B, using only 28.5\ Our contributions are summarized as follows: {itemize} {Architectural Innovation.} We introduce {}, an innovative MoE architecture aiming at achieving ultimate expert specialization, which employs two principal strategies of fine-grained expert segmentation and shared expert isolation. {Empirical Validation.} We conduct extensive experiments to empirically validate the effectiveness of the {} architecture. Experimental results validate the high level of expert specialization in {} 2B, and indicate that {} 2B can nearly approach the upper bound performance for MoE models {Scalability.} We scale up {} to train a 16B model and show that with only about 40\ We also undertake a preliminary endeavor to scale up {} to 145B, highlighting its consistent advantages over the GShard architecture and showing a comparable performance with DeepSeek 67B. {Alignment for MoE.} We successfully perform supervised fine-tuning on {} 16B to create an aligned chat model, showcasing the adaptability and versatility of {} 16B. {Public Release.} In the spirit of open research, we release the model checkpoint of {} 16B to the public. Notably, this model can be deployed on a single GPU with 40GB of memory without the need for quantization. {itemize"
Rewriting the Code: A Simple Method for Large Language Model Augmented Code Search,2401.04514v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2401.04514v2_0.pdf,"In code search, the Generation-Augmented Retrieval (GAR) framework, which generates exemplar code snippets to augment queries, has emerged as a promising strategy to address the principal challenge of modality misalignment between code snippets and natural language queries, particularly with the demonstrated code generation capabilities of Large Language Models (LLMs). Nevertheless, our preliminary investigations indicate that the improvements conferred by such an LLM-augmented framework are somewhat constrained. This limitation could potentially be ascribed to the fact that the generated codes, albeit functionally accurate, frequently display a pronounced stylistic deviation from the ground truth code in the codebase. In this paper, we extend the foundational GAR framework and propose a simple yet effective method that additionally writes the de (ReCo) within the codebase for style normalization. Experimental results demonstrate that ReCo significantly boosts retrieval accuracy across sparse (up to 35.7\%), zero-shot dense (up to 27.6\%), and fine-tuned dense (up to 23.6\%) retrieval settings in diverse search scenarios. To further elucidate the advantages of ReCo and stimulate research in code style normalization, we introduce Code Style Similarity, the first metric tailored to quantify stylistic similarities in code. Notably, our empirical findings reveal the inadequacy of existing metrics in capturing stylistic nuances. The source code and data are available at .","Comparison of GAR between passage retrieval and code search. In passage retrieval, the truth (yellow) is included in the generated content. In code search, despite the generated exemplar code satisfies the description of the query, it exhibits noticeable dissimilarity to the true code.","Code search, aimed at retrieving the most semantically relevant code snippets from a codebase according to a specified natural language query, is a common activity that plays an important role in software development . Retrieving and reusing analogous code fragments from large-scale codebases like GitHub can enhance productivity significantly. Despite both being sequences of words, matching code queries and natural language queries is challenging as they share few grammatical rules, causing them to fall into two distinct modalities. This grammatical distinction results in limited word overlap, significantly hampering the application of sparse retrieval systems in code search. On the other hand, in dense retrieval systems, the alignment of query and code representations during the training phase assists in alleviating the challenge . As a result, these systems are capable of encapsulating potential semantic correlations between terminologies employed in programming languages and those in natural languages. However, this potential association becomes challenging to capture if two terminologies rarely manifest together within a query-code pair. To bridge this gap, one possible solution is to transform the data from one modality to the other. This could involve either generating exemplar codes based on the query or summarizing the functionality of codes in the codebase. Given that natural language queries in code search are often short and ambiguous , our research concentrates on the former solution, referred as Generation-Augmented Retrieval (GAR) . GAR has demonstrated competitive performance in question answering and passage retrieval. In these NLP tasks, a language model is adopted to generate references based on the query to augment it. Similarly, we could use a language model to generate exemplar code snippets that realize the functionalities described in the query. Then the query and exemplar codes are combined to be fed into the retrieval system. With many LLMs demonstrating great intelligence in precisely writing codes , performing GAR with LLMs becomes a promising approach for code search. However, from our preliminary studies, the improvement in performance brought by GAR using LLMs is limited, especially with the high computational cost of LLMs. We argue that answer format influences the performance of GAR on question answering and code search. In question answering, the correct answer to the question is often unique and can be expressed in limited forms. The generated contents from LLMs, if correct, are usually in the exact same form as the answer. As highlighted in Fig.~, the matching word ``depressive'' appears in the reference. On the other hand, code snippets with the same functionality can have diverse formulations, which lowers the chance of matching the code in the codebase, and thus leads to minor improvement of GAR in code search. As shown in Fig.~, the true code uses Python built-in function {Counter} to count the number of elements in a list, while the exemplar code snippet does it manually. To address the mismatch of the generated and ground truth code snippets, we build upon GAR and propose a simple yet effective framework that additionally {Re}writes and the {Co}de (ReCo) in the codebase. As shown in Fig.~, after rewriting, the style of codes in the codebase are normalized by LLMs to align with the exemplar code, thereby facilitating the retrieval. We evaluate ReCo on several code search models across various search scenarios, including coding challenge competence, online programming community, and general programming problems in Python and Java. Experimental results show that ReCo could significantly boost the performance of sparse retrieval systems (up to 35.7\ Furthermore, we propose a novel evaluation metric, dubbed Code Style Similarity, to quantitatively measure the disparity in code style. Our metric validates ReCo's capability in aligning the style of code within the codebase with that of code generated by LLMs. Conventional metrics like BLEU and CodeBLEU are deemed less appropriate as they calculate similarity based on exact-matched tokens of the given two code snippets. In contrast, Code Style Similarity evaluates style from three distinct perspectives: variable naming, API invocation, and code structure, based on edit distance . Our experiments show that Code Style Similarity exhibits superior explanatory power than existing metrics in measuring the style deviation of code from the dataset and that generated from LLM"
A Multidimensional Framework for Evaluating Lexical Semantic Change with Social Science Applications,2406.06052v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.06052v1_0.png,"Historical linguists have identified multiple forms of lexical semantic change. We present a three-dimensional framework for integrating these forms and a unified computational methodology for evaluating them concurrently. The dimensions represent increases or decreases in semantic 1) sentiment (valence of a target word’s collocates), 2) breadth (diversity of contexts in which the target word appears), and 3) intensity (emotional arousal of collocates or the frequency of intensifiers). These dimensions can be complemented by the evaluation of shifts in the frequency of the target words and the thematic content of its collocates. This framework enables lexical semantic change to be mapped economically and systematically and has applications in computational social science. We present an illustrative analysis of semantic shifts in {mental health} and {mental illness} in two corpora, demonstrating patterns of semantic change that illuminate contemporary concerns about pathologization, stigma, and concept creep.",Three Major Dimensions of Semantic Change.,"Lexical semantic change is defined by historical linguists as innovations that alter the meaning, but not the grammatical function, of a form . For instance, ``awesome'' once denoted the capacity to inspire awe, but its meaning has since been bleached to a general expression of approval. Computational linguists have made strides in developing distributional semantic methods to detect semantic change and its laws as distinct from cultural shifts . Advances in deep learning since 2018 afford new ways to model semantic change processes. These innovations have facilitated the development of language models with sophisticated word embeddings or vector representations. As a result, word embeddings have evolved from count-based models , where words are represented by their co-occurrence frequency with other words, to prediction-based representations , where word vectors are iteratively learned as part of a language modelling task objective. The granularity of these representations shifted from {type-level}, where each word has a single vector despite its usages, to {token-based}, or contextualized representations , where each word instance (token) has a vector, dynamically capturing shifts in meaning based on context. Lexical semantic relations can be detected by type- and token-level embeddings. Other work has started addressing the challenge of formalizing and understanding kinds of semantic change . Processes such as broadening , metaphorization , and pejoration/amelioration have been modelled. Researchers have created methods to automatically disambiguate a word's pejorative usage from its non-pejorative use . Attempts have also been made to evaluate understudied classes of semantic change. Sentence representations from neural language models were used for hyperbole detection . Exaggerated language can be generated and detected , alongside metaphor . Researchers have also evaluated semantic bleaching, whereby words lose elements of their meaning , and found it to be triggered in contexts where an adverb premodifies a semantically similar adjective (e.g., “insanely jealous”). Nevertheless, there are a dearth of diachronic methods for evaluating lexical semantic change . Despite advances in detecting and modelling lexical semantic change, there is a need for a unifying framework to integrate multiple dimensions of change. The present study addresses this gap by proposing a framework which synthesizes the theoretical insights of historical linguists about the many distinct forms of diachronic lexical semantic change [e.g.,][]{bloomfield1933} and aligns them with the methodological sophistication of natural language processing. The comprehensive computational framework for evaluating lexical semantic change that emerges should be valuable for computational social scientists seeking to understand and model social and cultural change"
Mitigating Catastrophic Forgetting in Large Language Models with Self-Synthesized Rehearsal,2403.01244v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2403.01244v2_0.pdf,"Large language models (LLMs) suffer from catastrophic forgetting during continual learning. Conventional rehearsal-based methods rely on previous training data to retain the model's ability, which may not be feasible in real-world applications. When conducting continual learning based on a publicly-released LLM checkpoint, the availability of the original training data may be non-existent. To address this challenge, we propose a framework called Self-Synthesized Rehearsal (SSR) that uses the LLM to generate synthetic instances for rehearsal. Concretely, we first employ the base LLM for in-context learning to generate synthetic instances. Subsequently, we utilize the latest LLM to refine the instance outputs based on the synthetic inputs, preserving its acquired ability. Finally, we select diverse high-quality synthetic instances for rehearsal in future stages. Experimental results demonstrate that SSR achieves superior or comparable performance compared to conventional rehearsal-based approaches while being more data-efficient. Besides, SSR effectively preserves the generalization capabilities of LLMs in general domains.",Comparison of standard rehearsal and our proposed Self-Synthesized Rehearsal (SSR).,"Large language models (LLMs) have demonstrated remarkable performance across various natural language processing (NLP) tasks . In real-world applications, LLMs are often updated in a continual learning (CL) manner , where new instruction tuning data is incrementally introduced over time. However, a significant issue that limits the effectiveness of LLMs is catastrophic forgetting, which refers to the LLM's tendency to forget previously acquired knowledge when learning new instances . To mitigate catastrophic forgetting, a line of work focuses on rehearsing previous training instances . These rehearsal-based methods maintain the model's ability by training on real data from previous training stages. However, the real data may not always be desirable in practical applications. For instance, when conducting continual learning based on a publicly-released LLM checkpoint (e.g. Llama-2-chat), the availability of the original training data may be non-existent. This raises an interesting research question: {Can we maintain the LLM's ability during continual learning without using real data in previous training stages?} We propose the {S}elf-{S}ynthesized {R}ehearsal ({SSR}) framework to mitigate catastrophic forgetting in continual learning. As shown in Figure~, unlike standard rehearsal-based continual learning that samples training instances from previous stages as rehearsal data, SSR framework uses the LLM to generate synthetic instances for rehearsal. Specifically, we first use the base LLM to generate synthetic instances, conducting in-context learning (ICL) with few-shot demonstrations. These demonstrations can be collected from the previous data or human-constructed containing similar knowledge to the previous data. Then, the latest LLM is used to refine the outputs of synthetic instances to retain the latest LLM's ability. Finally, we select diverse high-quality synthetic instances for rehearsal in the future stages. Extensive experiments on the task sequences derived from the SuperNI dataset demonstrate that SSR has superior or comparable performance compared to the conventional rehearsal-based approaches, with higher data utilization efficiency. Besides, experiments on AlpacaEval and MMLU show that SSR can also effectively preserve the generalization capabilities of LLMs in general domains. We release our code and data at {https://github.com/DeepLearnXMU/SSR"
WaterBench: Towards Holistic Evaluation of Watermarks for Large Language Models,2311.07138v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2311.07138v2_0.pdf,"To mitigate the potential misuse of large language models (LLMs), recent research has developed watermarking algorithms, which restrict the generation process to leave an invisible trace for watermark detection. Due to the two-stage nature of the task, most studies evaluate the generation and detection separately, thereby presenting a challenge in unbiased, thorough, and applicable evaluations. In this paper, we introduce WaterBench, the first comprehensive benchmark for LLM watermarks, in which we design three crucial factors: (1) For {benchmarking procedure}, to ensure an apples-to-apples comparison, we first adjust each watermarking method's hyper-parameter to reach the same watermarking strength, then jointly evaluate their generation and detection performance. (2) For {task selection}, we diversify the input and output length to form a five-category taxonomy, covering $9$ tasks. (3) For {evaluation metric}, we adopt the GPT4-Judge for automatically evaluating the decline of instruction-following abilities after watermarking. We evaluate $4$ open-source watermarks on $2$ LLMs under $2$ watermarking strengths and observe the common struggles for current methods on maintaining the generation quality. The code and data are available at \url{https://github.com/THU-KEG/WaterBench}.","The generated texts without and with watermark \citep{kirchenbauer2023watermark} on a test example from AlpacaFarm~\cite{dubois2023alpacafarm}, an instruction-following benchmark. LLM equipped with watermark will be more inclined to generate tokens in the \colorbox{mygreen}{green list}, which can then be detected by a higher z-score measurement ($z>4$). We utilize TP, TN, and GM to jointly evaluate the watermarking performance.","LLM has achieved significant success in generating human-like texts. However, the potential misuse of LLM has also raised concerns. For example, ChatGPT can be used to generate fake news, which may manipulate the public opinion. To mitigate this kind of risk, it is necessary to develop a watermarking algorithm to detect whether a text is generated by LLM. As shown in Figure~, the watermarked texts are generated with a biased distribution of tokens, which distinguishes it from unwatermarked texts. We believe the goal of watermarking is to achieve high detection accuracy while maintaining the generation quality. So we utilize the commonly used TP (True Positive), TN (True Negative), and GM (Generation Metric) to evaluate watermarks. Due to the two-stage nature of this task, most studies evaluate the generation and detection separately and they do not conduct a unified hyper-parameter search for each watermarking method, which may lead to unfair comparisons. Since, there is usually a trade-off between the detection performance and the generation quality. Besides, previous evaluations are often conducted via text completion on a single dataset, such as C4 RealNewsLike dataset, which cannot comprehensively measure the generation quality of LLMs. Furthermore, most evaluations only calculate the perplexity, which is not aligned with human preference and thus not practical in the era of LLMs. To address these issues, we propose WaterBench, the first comprehensive benchmark for LLM watermarks, which has three crucial factors: (1) {Benchmarking Procedure}: We first introduce the concept of watermarking strength, i.e. the detection robustness to disturbance, to quantify the LLM watermarks' trade-off controlled by hyper-parameters. We present a reasonable hyper-parameter search procedure: Given a dataset and an LLM, we adjust the hyper-parameters of each watermarking method to unify the watermarking strength and then freeze the parameters to jointly evaluate the detection and generation performance. (2) {Task Selection}: To add disturbance on watermarks, we differentiate the task settings based on the length of input and output, which decides how much information the watermark can embed. Therefore, we form a new taxonomy with five task categories and nine sub-tasks, which are selected from existing datasets with various length settings. (3) {Evaluation Metric}: We adopt the GPT4-Judge for automatically evaluating the instruction-following performance decline after watermarking. Then we conduct a human evaluation to verify the agreement between the human and GPT4. Based on the WaterBench dataset, we conduct an experiment of 4 reproducible watermarks on 2 LLMs (Llama2-chat and InternLM), leading to some interesting findings: (1) We adjust two different watermarking strengths, 0.7 and 0.95, and observe that the detection and generation performance are significantly different. In other words, if we compare two watermark strategies without aligning their watermarking strengths, it is easy to let one ``surpass'' another in some aspects. (2) The tasks with short output length are generally more difficult to detect, with lower TP. The V2 watermark is the best watermarking method in terms of GM. (3) On the open-ended task, if we use GPT4-judge to evaluate, the watermarked LLM will decrease over $96\ To summarize, our contributions are three-fold: (1) We propose a new benchmarking procedure that first search hyper-parameters for watermarks then jointly evaluate detection and generation performance to eliminate the unfair comparison between different watermarking strengths. (2) We construct a multi-task benchmark to facilitate future research. (3) We incorporate GPT4-Judge to evaluate the watermarked LLMs, which effectively reflects the decline of generation quality"
Dependency Transformer Grammars: Integrating Dependency Structures into Transformer Language Models,2407.17406v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2407.17406v1_0.pdf,"Syntactic Transformer language models aim to achieve better generalization through simultaneously modeling syntax trees and sentences. While prior work has been focusing on adding constituency-based structures to Transformers, we introduce Dependency Transformer Grammars ( s), a new class of Transformer language model with explicit dependency-based inductive bias. simulate dependency transition systems with constrained attention patterns by modifying attention masks, incorporate the stack information through relative positional encoding, and augment dependency arc representation with a combination of token embeddings and operation embeddings. When trained on a dataset of sentences annotated with dependency trees, achieve better generalization while maintaining comparable perplexity with Transformer language model baselines. also outperform recent constituency-based models, showing that dependency can better guide Transformer language models. Our code is released at {\url{https://github.com/zhaoyd1/Dep_Transformer_Grammars}}.",An example sentence with its dependency tree and transition sequence. Numbers in blue and red are indices of tokens and arcs respectively.,"Transformer language models have shown strong performance on language modeling tasks and a broad spectrum of downstream tasks. Despite the great power of the Transformer architecture, it lacks the inductive biases of syntactic structures, which has been hypothesized to improve generalization. A straightforward way to incorporate such biases into Transformers is explicit modeling of syntactic structures. Inspired by earlier work of generative parsing as language modeling that integrates syntactic structures into RNNs, recent studies have focused on adapting this method to Transformer architectures. The models proposed by these studies are categorized as syntactic language models because they jointly model the distribution of surface strings and their corresponding syntactic trees. Experiments show that these models achieve competitive perplexity in language modeling and gain better syntactic generalization, supporting the above hypothesis on the benefits of introducing inductive bias of syntactic structures. However, the structural supervision that has been used in all these models is based on constituency trees and it is unclear of the performance of dependency-based Transformer syntactic language models. Different from constituency structures, which model recursive syntactic compositions, dependency structures focus more on the relationship between tokens, which is similar to the self-attention mechanism in Transformer, hinting at potential synergy between the two. In this paper, we propose Dependency Transformer Grammars ( s), dependency-based syntactic language models that learn joint distributions of sentences and dependency trees. introduce an inductive bias of dependency structures to Transformers by (i) modeling transition sequences of transition-based dependency parsers instead of sentences, (ii) simulating the stack operations in transition-based dependency parsers through modification of attention masks, (iii) incorporating the stack information of transition-based systems through relative positional encoding of stack depth, and (iv) representing head-dependent relations through a combination of head token embeddings and transition operation embeddings. Following a line of previous work in generative dependency parsing, the generative formulation of our model is based on the {arc-standard} system, which builds a dependency tree in a bottom-up manner. We also explore models using other dependency transition systems for comparison. Our experiments show that achieve comparable perplexity in language modeling and improved syntactic generalization on both the BLiMP benchmark and the SG test suites over Transformer language model baselines. Furthermore, outperform constituency-based syntactic language models in both language modeling and syntactic generalization. In summary, our contributions are as follows. {itemize} We propose dependency-based syntactic language models, s, to incorporate dependency inductive bias into Transformers. We primarily build using the {arc-standard} transition system, while we also study the usage of other dependency transition systems. Experimental results on two syntactic generalization benchmarks show the benefits of introducing inductive bias of dependency structures. {itemize"
HealMe: Harnessing Cognitive Reframing in Large Language Models for Psychotherapy,2403.05574v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2403.05574v3_0.pdf,"Large Language Models (LLMs) can play a vital role in psychotherapy by adeptly handling the crucial task of cognitive reframing and overcoming challenges such as shame, distrust, therapist skill variability, and resource scarcity. Previous LLMs in cognitive reframing mainly converted negative emotions to positive ones, but these approaches have limited efficacy, often not promoting clients' self-discovery of alternative perspectives. In this paper, we unveil the Helping and Empowering through Adaptive Language in Mental Enhancement (HealMe) model. This novel cognitive reframing therapy method effectively addresses deep-rooted negative thoughts and fosters rational, balanced perspectives. Diverging from traditional LLM methods, HealMe employs empathetic dialogue based on psychotherapeutic frameworks. It systematically guides clients through distinguishing circumstances from feelings, brainstorming alternative viewpoints, and developing empathetic, actionable suggestions. Moreover, we adopt the first comprehensive and expertly crafted psychological evaluation metrics, specifically designed to rigorously assess the performance of cognitive reframing, in both AI-simulated dialogues and real-world therapeutic conversations. Experimental results show that our model outperforms others in terms of empathy, guidance, and logical coherence, demonstrating its effectiveness and potential positive impact on psychotherapy.","An example of how HealMe communicates with a client, and how we prompt both sides to generate expected conversations as training data.","Cognitive reframing , a key part of cognitive-behavior therapy (CBT), helps individuals detach from their thoughts and situations, effectively addressing issues from mild negative thinking to severe depression and anxiety. Due to the extensive dialogue and significant empathy required in psychotherapy, Large Language Models (LLMs) {black}{hold immense potential} whether as an adjunct to human-based mental health treatment or as a standalone therapeutic tool. LLMs can help overcome obstacles such as shame or distrust often associated with traditional therapy methods. Additionally, they address issues like the limited availability of psychotherapeutic resources and the variability in therapists' skill levels. Contrasting with previous methods that conceptualize cognitive reframing as a sentence rewriting task, where negative emotions are transformed into neutral or positive expressions emphasizing factors like specificity and actionability, our approach marks a significant shift. Since cognitive reframing emphasizes the importance of clients undergoing cognitive changes themselves, rather than directly receiving guidance or suggestions from therapists, our method employs a conversational model that directly engages with and actively transforms the client's own negative thoughts. In conclusion, despite the significant potential shown by LLMs in prior research, they encounter crucial obstacles when it comes to cognitive reframing. (1) If viewing cognitive reframing as a sentence rewriting task, clients might not spontaneously discover alternative perspectives and could perceive the reframing as preaching or imposition rather than self-realization. (2) LLMs cannot consistently generate concrete and specific empathetic responses, which are crucial in psychotherapy. For instance, a specific empathetic response might be, {I understand how upsetting it is that your friend forgot your birthday.} In contrast, a more general response would be, {I understand your feelings.} (3) While LLMs are commonly used for answering human queries, the role is reversed in psychotherapy: psychotherapists are required to guide humans . According to Westerners effect, excessive external motivation can undermine internal motivation. As such, therapists giving direct suggestions may hinder clients' self-discovery and the development of self-efficacy. Moreover, guidance fosters a more collaborative environment, allowing clients to explore and understand their thoughts and feelings, leading to more sustainable and self-directed change. To tackle these challenges, we propose a specialized model {H}elping and {E}mpowering through {A}daptive {L}anguage in {M}ental {E}nhancement (HealMe$^{1}$[1]), for cognitive reframing therapy. {$^{1}$ Our data and code are available at {https://github.com/elsa66666/HealMe}{HealMe}.} We emphasize the empowerment of the client rather than reliance on therapist-driven solutions. We leverage dialogue data imbued with empathy and guidance for instruction tuning, ensuring empathetic and directive responses. Grounded in professional psychological literature, our domain-expert co-authors distill and organize a structured cognitive reframing therapy process, effectively emulating a complete psychotherapeutic procedure. HealMe operates in three main stages, as depicted in the blue section of Figure : 1) distinguishing between situations and thoughts for a rational outlook, 2) brainstorming for alternative perspectives to mitigate negative thinking, and 3) offering suggestions that acknowledge the client's effort and encourage positive action. This streamlined process aids clients in understanding their issues more clearly, accepting new interpretations, and moving toward constructive solutions. To build dialogue data for psychotherapy to train our model, we design prompts based on the ({thinking trap}, {client's thought}) pairs from , prompting ChatGPT to simulate both client and psychology therapist roles. To test our model, we simulate interactions between the ChatGPT client and our model along with baselines (including ChatGLM3-6b and LLaMA2-7b-chat). We also conduct experiments to evaluate the models' effectiveness in practical scenarios. We create a detailed psychological evaluation metric for our experiments, incorporating a three-dimensional scoring system to evaluate AI therapists in AI-to-AI scenarios. For real-person client scenarios, we directly employ professional psychological metrics to evaluate AI therapists. The results show that HealMe excels in both AI-to-AI conversations and real-world dialogues. In AI-to-AI dialogues, HealMe demonstrates superior empathy, guidance, and logical coherence compared to other models. During real-person testing, some clients using HealMe experienced notable decreases in negative emotional attributes, with negative scores dropping from 5/5 to 1/5, highlighting HealMe's potential in real-world scenarios. Our contributions are as follows: (1) We introduce an AI psychotherapy model, HealMe, that effectively implements cognitive reframing therapy, overcoming the challenge of maintaining continuous high empathy and guidance with LLMs. (2) We propose a comprehensive set of professional AI psychotherapy evaluation metrics applicable to both public and non-public therapy dialogue scenarios. (3) We conduct extensive comparative analyses of our approach against other LLMs, both in AI-to-AI conversations and human interactions. These experimental results underscore the superiority of our method, paving the way for AI to develop more advanced and specialized psychotherapeutic strategies"
Multimodal Prompt Learning with Missing Modalities for Sentiment Analysis and Emotion Recognition,2407.05374v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2407.05374v1_0.pdf,"The development of multimodal models has significantly advanced multimodal sentiment analysis and emotion recognition. However, in real-world applications, the presence of various missing modality cases often leads to a degradation in the model's performance. In this work, we propose a novel multimodal Transformer framework using prompt learning to address the issue of missing modalities. Our method introduces three types of prompts: generative prompts, missing-signal prompts, and missing-type prompts. These prompts enable the generation of missing modality features and facilitate the learning of intra- and inter-modality information. Through prompt learning, we achieve a substantial reduction in the number of trainable parameters. Our proposed method outperforms other methods significantly across all evaluation metrics. Extensive experiments and ablation studies are conducted to demonstrate the effectiveness and robustness of our method, showcasing its ability to effectively handle missing modalities. Codes are available at \url{https://github.com/zrguo/MPLMM}.",The overall architecture of our proposed method. A batch of data that contains different missing modality cases is fed to the Missing Modality Generation Module (see Section~\ref{s32}) to obtain generated features. They are then passed to the pre-trained backbone with missing-signal prompts and missing-type prompts (see Section~\ref{s33}).,"Humans perceive the world in a multimodal way, such as sight, sound, touch and language. These multimodal features can provide comprehensive information to help us understand and explore the world. Thus, modeling and mining multimodal data is of great importance and has much potential. Recently, multimodal sentiment analysis has attracted much attention. However, there are two main challenges in many existing methods: 1) Different from common multimodal tasks which only have two modalities (image and text), multimodal sentiment analysis task often has more modalities (video, audio, text, etc.). Therefore, in real-world scenarios, missing modality conditions always occur due to equipment failure, data corruption, privacy issues and the like, especially in low-resource domains, which could lead to a degradation in the model's performance. Current multimodal models trained on complete data usually fail when tested on incomplete data. 2) With the success of large-scale multimodal models, lots of researchers tend to finetune these large pre-trained models to downstream tasks. However, this kind of finetuning is infeasible for many researchers because it requires large computational resources. Besides, finetuning such a pre-trained model on small datasets could lead to instability. Recently, prompt learning is proposed, which freezes all the parameters of a pre-trained model while only finetuning several prompts and it has achieved great success. Motivated by prompt learning, in this paper, we intend to exploit a high-resource dataset that contains relatively more complete modality data for pre-training and then leverage several trainable prompts to transfer the knowledge from high-resource domains to low-resource domains where missing modality cases often occur. Previous works mainly focus on introducing sophisticated architecture to address the issue of missing modalities. These methods do not use pre-trained models and usually require a lot of computational resources. However, our method is based on prompt learning, which only finetunes a few parameters of prompts. is a recent work which is similar to ours. However, its proposed missing-aware prompts increase exponentially with the number of modalities. In contrast, our proposed prompts increase linearly with the number of modalities which is more parameter-efficient. Specifically, we propose three types of prompts: generative prompts, missing-signal prompts, and missing-type prompts which can learn the representations of the missing modalities, cross-modal and fine-grained features. These three types of prompts play a combined role in improving the model's performance. We conduct extensive experiments on four datasets: CMU-MOSEI, CMU-MOSI, IEMOCAP and CH-SIMS. The proposed method outperforms the baselines significantly across all metrics on all datasets. We further study the roles of three types of prompts, the effect of missing rate of training data, and the effect of prompt length. We find that: 1) missing-signal prompts are modality-specific while missing-type prompts are modality-shared which represent intra-modality and inter-modality information respectively. 2) with short prompts, our model can achieve very good results which demonstrates our proposed method is parameter-efficient. 3) the missing rate is important for the performance of the model, with 70\ Our contributions can be summarized as follows: {itemize}[itemsep=0pt,parsep=0pt,topsep=0pt,partopsep=0pt] We present a novel framework via prompt learning for sentiment analysis and emotion recognition which is not only computationally efficient but also capable of handling missing modalities during both the training and testing stages. The number of parameters of our proposed prompts is linearly related to the number of modalities, which significantly reduces computational resources. We propose three types of prompts to address the issue of missing modalities. These three types of prompts can generate missing information, and learn intra- and inter-modality information respectively. Our proposed method outperforms all the baselines across all metrics significantly. Furthermore, we discover that applying modality dropout with a rate of 70\ {itemize"
Generative Pre-trained Speech Language Model with Efficient Hierarchical Transformer,2406.00976v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.00976v2_0.pdf,"While recent advancements in speech language models have achieved significant progress, they face remarkable challenges in modeling the long acoustic sequences of neural audio codecs. In this paper, we introduce {G}enerative {P}re-trained {S}peech {T}ransformer (GPST), a hierarchical transformer designed for efficient speech language modeling. GPST quantizes audio waveforms into two distinct types of discrete speech representations and integrates them within a hierarchical transformer architecture, allowing for a unified one-stage generation process and enhancing Hi-Res audio generation capabilities. By training on large corpora of speeches in an end-to-end unsupervised manner, GPST can generate syntactically consistent speech with diverse speaker identities. Given a brief 3-second prompt, GPST can produce natural and coherent personalized speech, demonstrating in-context learning abilities. Moreover, our approach can be easily extended to spoken cross-lingual speech generation by incorporating multi-lingual semantic tokens and universal acoustic tokens. Experimental results indicate that GPST significantly outperforms the existing speech language models in terms of word error rate, speech quality, and speaker similarity. The code is available at \url{https://github.com/youngsheen/GPST}.",The comparison of frameworks for generative speech pre-training. (a) AudioLM is a three-stage model. (b) VALL-E is a two-stage model. (c) GPST is a one-stage model.,"Speech quantization has emerged as a crucial technique for speech language models to generate controllable, high-quality speech waveforms . Specifically, a speech waveform can be quantized into two distinct types of discrete representations: semantic tokens and acoustic tokens . Semantic tokens are typically obtained by applying the K-means clustering algorithm to the continuous activation space of self-supervised speech models. Notably, GSLM finds that auto-regressive models trained on semantic tokens can capture high-level linguistic content, supporting language modeling and resynthesis . However, semantic tokens fail to retain acoustic details such as speaker identity, resulting in suboptimal reconstruction. In contrast, acoustic tokens generated by neural codec models effectively compress speech at low bitrates while capturing the nuances of speech waveforms. Consequently, a speech language model can maintain long-term consistency with semantic tokens and produce high-quality synthesis with acoustic tokens. However, neural codec models require an excessive number of codes for high-quality speech synthesis. For example, EnCodec generates codec embeddings at $75$ Hz for audio waveforms at $24$ kHz. Subsequently, these codec embeddings are modeled using residual vector quantization (RVQ), wherein high-quality synthesis typically requires eight or more hierarchical quantizers with $1024$ entries. Therefore, a mere $10$-second waveform results in at least $75 8 10 = 6000$ codes, which constitutes an excessively long sequence for language models due to the quadratic complexity with respect to the sequence length for calculating self-attention. Consequently, addressing the trade-off between the perceptual quality and computational complexity remains a core challenge for speech language models. Recently, some methods have been proposed to address the issue of lengthy acoustic sequences. Acoustic tokens inherently possess a hierarchical structure because of residual vector quantization: tokens from the preceding quantizers restore acoustic properties such as speaker identity, while the subsequent quantizers capture finer acoustic details. Each quantizer is trained to model the residuals from the previous quantizers. Recent approaches treat the acoustic token generation process as a multi-stage framework to avoid learning excessively long sequences simultaneously. In this work, we present {G}enerative {P}re-trained {S}peech {T}ransformer (GPST), a model that facilitates controllable, high-quality speech generation in {single stage}. Our approach combines speech quantization with the architecture of a hierarchical transformer . GPST initially models the semantic sequence with a next token prediction task, followed by modeling the acoustic sequence with the task of predicting the next $D$ stack codes. The semantic sequence serves as a prompt for the acoustic token as a condition. We design a specialized hierarchical architecture to model the underlying hierarchical structure of the acoustic sequence, which comprises of a large global transformer and a small local transformer. The global transformer learns the high-level relationships between the semantic tokens and the stacked acoustic tokens, while the local transformer models the hierarchical details in the stacked acoustic codes. By incorporating semantic and acoustic tokens within one hierarchical transformer, GPST can significantly reduce computational costs and effortlessly learn the long-term interactions of semantic tokens and local dependencies among residual codes. Furthermore, we propose a training technique called ``local-drop'' to further improve the training efficiency of Hi-Res speech generation, which is typically impractical in current speech language models because of a large number of residual quantizers. Consequently, our model can generate high-quality and semantically coherent speeches in one stage efficiently. Our main contributions are summarized as follows. {itemize} We propose a novel generative pre-trained speech language model GPST that enables controllable, high-quality speech generation in a single stage. By integrating semantic tokens and acoustic tokens within a hierarchical transformer, GPST significantly reduces computational costs while efficiently learning the long-term interactions of semantic tokens and local dependencies among residual codes simultaneously. We demonstrate GPST's capacity not only to generate coherent speech unconditionally but also to generate speech while preserving the speaker's identity with only a 3-second short prompt. Experimental results reveal its superiority over existing speech language models with only $33\ To the best of our knowledge, GPST is the first work that supports spoken multilingual speech generation and Hi-Res speech synthesis. {itemize"
Selene: Pioneering Automated Proof in Software Verification,2401.07663v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2401.07663v2_0.png,"Ensuring correctness is a pivotal aspect of software engineering. Among various strategies available, software verification offers a definitive assurance of correctness. Nevertheless, writing verification proofs is resource-intensive and manpower-consuming, and there is a great need to automate this process. We introduce in this paper, which is the first project-level automated proof benchmark constructed based on the real-world industrial-level operating system microkernel, seL4. provides a comprehensive framework for end-to-end proof generation and a lightweight verification environment. Our experimental results with advanced large language models (LLMs), such as GPT-3.5-turbo and GPT-4, highlight the capabilities of LLMs in the domain of automated proof generation. Additionally, our further proposed augmentations indicate that the challenges presented by can be mitigated in future research endeavors.","A demonstration of the \benchmark pipeline for automated proof generation (best viewed in color). \benchmark facilitates both the construction of proofs from scratch (indicated by the gray ``generate'' path) and the refinement of existing proofs augmented by error messages (highlighted by the red ``fixing'' path). To validate the correctness of the generated proofs, they are subjected to verification by the Isabelle prover within the authentic seL4 environment.","Confirming the correctness of the software, , checking whether it adheres to the properties specified in the requirements, is advantageous for software engineering (SE). In contrast to testing, which is incomplete, verification provides rigorous guarantee of software correctness or incorrectness . Specifically, during testing, an adequate number of test cases are created and tested against the subject program. If the program violates a testing oracle or encounters other errors (, runtime error), a bug is found. However, the opposite conclusion cannot be guaranteed otherwise. Verification often involves the usage of a formal language and the corresponding prover. {Please note that there are other verification techniques such as model checking. We refer to it as methods involving interactive proof assistants in this paper.} This process requires formal proofs to rigorously demonstrate that the program satisfies the required properties, which can be verified by the prover. In general, software verification involves two stages. {182} The prerequisite specification stage translates the required properties and the subject program into the formal language, creating a to-be-proved proposition stating that ``the program meets the properties'', , the specification. {183} The proof stage is supposed to generate proofs that prove the above specification and can be formally checked by the prover. Both stages consume significant resources and manpower, with the second stage being particularly demanding. , the seL4 operating system microkernel {{https://sel4.systems/}}, which has been formally verified against strong functionality and security properties, requires 7 person-months dedicated to the specification stage and 11 person-years to the proof stage for correctness verification, and the amount of proof code in seL4 is even ten times more than that of the microkernel implementation itself . Therefore, in order to promote provable software, automated software verification, particularly automated proof, is highly desirable. As an early exploratory effort, in this paper, we explore to automate the major overhead. Typically, automated proof in software verification is a conditional generation task from the specification to the proof, involving reasoning capabilities. Fortunately, large language models (LLMs) offer an opportunity, as they have demonstrated significant capacity in logic and reasoning at mathematical theorem proving . Only limited research has explored how to leverage LLM for code verification . And they only focus on function-level code verification, rather than a complete industrial-level software. A distinctive feature of industrial-level projects is the complex dependencies among lemmas and files, which makes automated proof even harder. In order to promote software verification, we propose a real-world industrial-level benchmark based on seL4 for automated proof, namely . SeL4 is a high-assurance operating system microkernel, and it is comprehensively formally verified. The verification of seL4 is mainly based on the formal language of Isabelle , containing over 100k lines of code in Isabelle and thousands of lemmas (specification + proof), among which we randomly extract 360 for benchmarking. In the major pipeline (as presented in Figure ), inputs the specification of the target lemma, extracted from seL4, into the subject LLM, and checks the generated proof via the prover within the seL4 environment. As seL4 is a complicated system, provides the complete dependency graph of lemmas, definitions and functions, along with a lightweight verification environment for each lemma to be evaluated. Due to the dependencies of the lemmas, almost the entire verification project needs to be rebuilt in order to check the generated proof, which can lead to a huge evaluation overhead (tens of minutes per lemma). Thence, creates an isolated verification environment for each lemma to avoid duplicate construction and verification of the dependent lemmas, which enables efficient evaluation (it usually takes only a few minutes or even seconds to verify a generation). We evaluate GPT-3.5-turbo and GPT-4 in the pipeline. The experimental results demonstrate the feasibility of LLMs for automated proof in software verification. Still, we have identified some further challenges in . {182} The dependency graph of seL4 is complicated, and extracting facts to be applied from it can be hard for LLMs. {183} The logic and reasoning process of a rather large proof may be beyond the capability of the subject LLMs. Even GPT-4 has difficulty in solving the rather difficult categories in . Therefore, to address the challenges, we introduce three distinct augmentations, , similar lemma augmentation, dependency augmentation and fixing augmentation. These augmentations yield varying improvements across the 's different categories. Despite the inherent difficulties, our experimental results with these augmentations offer promising indications that the challenges posed by are surmountable. The main contributions of this paper can be summarized as below. {itemize}[leftmargin=0pt,itemsep=0pt,parsep=0pt] We introduce the benchmark, tailored for project-level automated proof in software verification, grounded in the real-world industrial-level project of the seL4 operating system microkernel. We introduce the technique of lemma isolation, which facilitates a lightweight verification environment capable of handling the complexities inherent in systems such as seL4. Our experiments with GPT-3.5-turbo and GPT-4 demonstrate the potential of LLMs in automated proof generation in software verification. We incorporate augmentations into the framework, which mitigate some of the challenges encountered within and suggest promising avenues for future studies. {itemize"
Dissecting Human and LLM Preferences,2402.11296v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.11296v1_0.pdf,"As a relative quality comparison of model responses, human and Large Language Model (LLM) {preferences} serve as common alignment goals in model fine-tuning and criteria in evaluation. Yet, these preferences merely reflect broad tendencies, resulting in less explainable and controllable models with potential safety risks. In this work, we dissect the preferences of human and 32 different LLMs to understand their quantitative composition, using annotations from real-world user-model conversations for a fine-grained, scenario-wise analysis. We find that humans are less sensitive to errors, favor responses that support their stances, and show clear dislike when models admit their limits. On the contrary, advanced LLMs like GPT-4-Turbo emphasize correctness, clarity, and harmlessness more. Additionally, LLMs of similar sizes tend to exhibit similar preferences, regardless of their training methods, and fine-tuning for alignment does not significantly alter the preferences of pretrained-only LLMs. Finally, we show that preference-based evaluation can be intentionally manipulated. In both training-free and training-based settings, aligning a model with the preferences of judges boosts scores, while injecting the least preferred properties lowers them. This results in notable score shifts: up to 0.59 on MT-Bench (1-10 scale) and 31.94 on AlpacaEval 2.0 (0-100 scale), highlighting the significant impact of this strategic adaptation. {Interactive Demo}: {Dataset}: {Code}: [h] [width=0.95]{figs/main_vis.pdf} Scenario (including {chitchat} and {value judgment}), and we highlight the most preferred property for each in its corresponding color: {lengthy} for human, {no severe errors} for GPT-4-Turbo, and {contain rich info} for LLaMA-2-70B-Chat. The value is the probability of a response being preferred in a pair when it satisfies one property better than the other response, holding all else equal. This can be interpreted as how much human or an LLM favor a certain property. Values above and below the 50\% line indicate a preference or dislike, respectively.} {fig:decomposition-example}","The preference dissection of human, GPT-4-Turbo and LLaMA-2-70B-Chat on {Communication} Scenario (including {chitchat} and {value judgment}), and we highlight the most preferred property for each in its corresponding color: \textcolor{fig1blue}{lengthy} for human, \textcolor{fig1orange}{no severe errors} for GPT-4-Turbo, and \textcolor{fig1green}{contain rich info} for LLaMA-2-70B-Chat. The value is the probability of a response being preferred in a pair when it satisfies one property better than the other response, holding all else equal. This can be interpreted as how much human or an LLM favor a certain property. Values above and below the 50\% line indicate a preference or dislike, respectively.","Human and LLM preferences have played a crucial role in the development pipeline of recent advanced language models. Preference-based training, such as Reinforcement Learning from Human/AI Feedback (RLHF/RLAIF) and Direct Preference Optimization (DPO) , are widely used to fine-tune models to align with more practical needs. On the other hand, the preferences of human and LLMs, in the form of LLM-as-a-judge or Elo ratings, have become the de facto judging criteria for assessing the quality of model outputs as the tasks are becoming increasingly diverse and complex. Unlike the widely applied preference-based methods above, the preferences themselves lack thorough research. In most cases, they are only binary labels indicating which response is preferred as a vague form of expression, and we are unable to understand the preferences in an explainable and quantitative way. As a result, optimizing models towards such goals inevitably leads to certain issues . These include the trained models engaging in over-optimization and reward hacking , manifesting in undesired ways such as producing overly verbose answers or demonstrating sycophancy , which hinder the building of more reliable AI systems. In this work, we build a systematic framework to dissect the overall preferences into a quantitative combination of multiple clearly defined properties. To pursue understanding in realistic settings, we sample real-world user conversations with a balanced distribution of different scenarios from ChatBot Arena Conversations , where each sample is a pair of model responses to a query. We adopt an elaborate yet automated pipeline to annotate the data with regard to our pre-defined properties (e.g., {harmless} or {admit limits}). Based on the annotations, we determine how a pair of responses differ from each other on all properties. These distinctions are then used to fit Bayesian logistic regression models, which help us quantitatively decompose preferences based on different properties by examining their weights. Leveraging the above framework, we analyze the human preferences of real users and the preferences of numerous LLMs we collect. The analysis is conducted separately on different scenarios to avoid the mixing of preferences and achieve clearer conclusions (see Figure for an example). We summarize the key findings as follows: {enumerate}[leftmargin=0.2in, label=., itemsep=0pt, parsep=0mm, topsep=-0.00mm] Humans are less sensitive to errors, clearly dislike a model when it admits its limits, and prefer a response that supports their stances ( ). Advanced LLMs like GPT-4-Turbo prefer correctness, clarity, and harmlessness more ( ). LLMs of similar sizes exhibit similar preferences irrespective of training methods, and the preference of a pretrained-only LLM is largely unchanged after alignment ( ). {enumerate} Finally, we reveal that benchmarks with LLM-as-a-judge are easy to manipulate ( ). Our experiments on AlpacaEval 2.0 and MT-Bench show that aligning models with the judges' preferences increases scores, whereas diverge from these preferences leads to lower scores. This is achievable across both training-free and training-based methods, with score variations up to 0.59 on MT-Bench (1-10 scale) and 31.94 on AlpacaEval 2.0 (0-100 scale). The manipulation highlights the urgent need for more robust benchmarks and further underscores the importance of understanding preference"
UniCoder: Scaling Code Large Language Model via Universal Code,2406.16441v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.16441v1_0.pdf,"Intermediate reasoning or acting steps have successfully improved large language models (LLMs) for handling various downstream natural language processing (NLP) tasks. When applying LLMs for code generation, recent works mainly focus on directing the models to articulate intermediate natural-language reasoning steps, as in chain-of-thought (CoT) prompting, and then output code with the natural language or other structured intermediate steps. However, such output is not suitable for code translation or generation tasks since the standard CoT has different logical structures and forms of expression with the code. In this work, we introduce the universal code () as the intermediate representation. It is a description of algorithm steps using a mix of conventions of programming languages, such as assignment operator, conditional operator, and loop. Hence, we collect an instruction dataset to train our model on multi-task learning objectives. comprises natural-language questions, code solutions, and the corresponding universal code. The alignment between the intermediate universal code representation and the final code solution significantly improves the quality of the generated code. The experimental results demonstrate that with the universal code significantly outperforms the previous prompting methods by a large margin, showcasing the effectiveness of the structural clues in pseudo-code.}",An example of \ourmethod{}. The Code LLM solves the code generation question by ``translating'' the pseudocode description (Universal Code) into executable code of the target programming language.,"The field of code translation and generation has advanced significantly with the advent of code-specific large language models (LLMs). Code LLMs, such as StarCoder and Code-Llama, are capable of generating executable code by analyzing natural language prompts. Chain-of-thought (CoT) prompting has emerged as the leading technique in enhancing LLMs, where the intermediate steps provide a structured pathway from the problem statement to the solution, effectively mirroring the human problem-solving process. Considering the low accuracy of CoT in coder generation, structure CoT (SCoT) is proposed to minimize the gap between the intermediate steps and the generated code. More intuitively, using a universal code as the intermediate representation to handle multiple programming languages (PL) is promising. Here, universal code is a blueprint for implementing an algorithm, which helps to make the design of algorithms logically clear and readily comprehensible. Moreover, it is universal across different programming languages (PL-agnostic) since it typically does not follow specific syntax and omits execution details. Yet, {how the universal code is used for code translation and generation in multilingual scenarios remains underexplored.} In this work, we scale up the code LLMs to support multiple programming languages via the universal code ({}), which is used as an efficient and language-independent intermediate representation of the key algorithm principles. Specifically, we first define {} by specifying grammar rules and providing paradigms, followed by prompting GPT-4 to create an instruction dataset {} comprising natural-language questions, code solutions, and the corresponding universal code, as shown in Figure~. Then, the {} model is built by performing instruction tuning on multi-task learning objectives, including zero-shot question-answer generation (question$$code), question-universal-code generation (question$${}$$code), universal-code-solution translation ({}$$code), and Universal-code-of-Thought (UoT) objectives. In UoT, the model is required to generate the universal code before the executable code. {} is evaluated on the Python benchmark (Humaneval and MBPP) and the extended multilingual benchmark MultiPL-E. The results demonstrate that {} consistently achieves state-of-the-art performance across all languages, notably surpassing the previous baselines. Furthermore, the ablation study verifies the efficacy of the proposed method, and extra discussions provide insights into the effect of our method. The contributions are summarized as follows: {itemize} {0em} We introduce the universal code {}, which is agnostic to programming languages, allowing LLMs to grasp the essence of algorithms step by step. In addition, the instruction dataset {} is collected and provided for follow-up research. We propose {}, a code generation method that uses multi-task learning objectives to fine-tune the code LLMs with the help of {}. The objectives include question-answer generation (QA), question-universal-code generation (QP), universal-code-answer translation (PA), and Universal-code-of-Thought (UoT). As extensive experiments show, our method {} consistently outperforms the previous baselines on different benchmarks, including HumanEval, MBPP, and MultiPL-E. To further verify the effectiveness of the universal code, we propose {} to test the capabilities of code LLMs. {itemize"
Does DetectGPT Fully Utilize Perturbation? Bridging Selective Perturbation to Fine-tuned Contrastive Learning Detector would be Better,2402.00263v4,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.00263v4_0.pdf,"The burgeoning generative capabilities of large language models (LLMs) have raised growing concerns about abuse, demanding automatic machine-generated text detectors. DetectGPT , a zero-shot metric-based detector, first introduces perturbation and shows great performance improvement. However, in DetectGPT, the random perturbation strategy could introduce noise, and logit regression depends on the threshold, harming the generalizability and applicability of individual or small-batch inputs. Hence, we propose a novel fine-tuned detector, , bridging metric-based and fine-tuned methods by contrastive learning on selective perturbation. Selective strategy retains important tokens during perturbation and weights for multi-pair contrastive learning. The experiments show that outperforms the state-of-the-art (SOTA) by 1.20\% in accuracy on average on four public datasets. And we further analyze the effectiveness, robustness, and generalization of the method. .}","Example of the selective strategy perturbation of \modelname{}, which prevent modifying important tokens (in green). Orange tokens are the perturbed texts.","Machine-generated text (MGT) detection is to discriminate MGT from human-written texts (HWT), preventing abuse of large language models (LLMs), including academic misconduct , spam synthesis , untrustworthy news , {} Currently, existing MGT detection methods can be mainly classified into two categories , fine-tuned methods and zero-shot metric-based methods . In general terms, fine-tuned detector methods can achieve better accuracy than zero-shot metric-based methods, especially generalizable to black-box generators, but are more costly during data collection, fine-tuning, and running, in most cases. On the other hand, zero-shot metric-based methods show better interpretability than fine-tuned ones. DetectGPT , as an unsupervised zero-shot metric-based method, first introduces perturbation in MGT detection. Specifically, it applies random masking to the original input sample and uses T5 to fill in. It posits that minor perturbations of MGT tend to have lower log probability under the base model than the original sample. The introduction of perturbation in DetectGPT surpasses the vanilla log-probability-based method in white-box settings. However, DetectGPT still has three significant defects: ({i}) DetectGPT's reliance on the logit regression module's threshold compromises its generalization in zero-shot settings and limited to large batch input, failing on individual inputs. ({ii}) DetectGPT does not fully utilize the perturbation. As a metrics-based method, it only considers the probability difference caused by perturbation, which is overly simplified and slightly indistinguishable. Perturbation should indeed be a stronger augment that carries implicit language pattern information. ({iii}) DetectGPT perturbs the original sample randomly and unrestricted, which could introduce more noise and negatively impact the performance . For example, find entity-relationship plays a role in the detection, which might be destroyed in random perturbation of DetectGPT. In this paper, we thus propose a {Pe}rturbation-based {Co}ntrastive {L}e{a}rning model, , for MGT detection, toward the defects via two stages, Selective Strategy Perturbation ( ) and Token-Level Weighted Multi-Pairwise Contrastive Learning ( ). {Firstly}, Selective Strategy Perturbation is a token-level rewriting method with restrictions on modifying important texts to reduce noise. The motivation is to simulate the human behavior of modification . The perturbation strategy consists of token removal and substitution, as shown in {fig:perturb}. The experiments show that the Selective Strategy Perturbation method can improve the performance of both metrics-based ({} DetectGPT) and model-based methods. {Secondly}, we propose a Multi-Pairwise Contrastive Learning model to process the perturbed texts. Different from the logit regression module in DetectGPT, the trained model is generalizable without any threshold setting, and it can deal with individual inputs. Moreover, by utilizing multi-pairwise contrastive learning, the model could better utilize perturbation to focus on the language pattern gap between HWT and MGT. The importance weight from the perturbation stage is also reused as contrastive learning weight. Notably, by using contrastive learning, {} is a strong few-shot fine-tuning method, which effectively bridges and integrates metric-based and fine-tuned detector categories. {Finally}, extensive experiments show {} is significantly superior to baseline and SOTA methods on four datasets, {} improves by 1.20\ Further experiments show that {} is also better at generalization, robustness, and effectiveness. Our contributions are summarized as follows: {itemize} {Selective Perturbation}: Based on our analysis of various selective perturbation strategies, we propose a novel method considering token importance, which reduces the noise and benefits to both supervised and unsupervised approaches. {Bridge Metric and Model-based Detectors}: We utilize a novel fine-tuned contrastive learning module to replace the logit regression of DetectGPT (metric-based), which frees the detector from setting the threshold, enables it to deal with individual input, and can be generalizable and effective on the few-shot setting by contrasting perturbed texts with origin ones. {Outperformance}: Our detector {} outperforms all eight compared models on four public datasets. And {} is more robust to the choice of base model and filling model. Furthermore, we prove its generalization ability across domains and generators of data. {itemize"
AFaCTA: Assisting the Annotation of Factual Claim Detection with Reliable LLM Annotators,2402.11073v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.11073v3_0.pdf,"With the rise of generative AI, automated fact-checking methods to combat misinformation are becoming more and more important. However, factual claim detection, the first step in a fact-checking pipeline, suffers from two key issues that limit its scalability and generalizability: (1) inconsistency in definitions of the task and what a claim is, and (2) the high cost of manual annotation. To address (1), we review the definitions in related work and propose a unifying definition of factual claims that focuses on verifiability. To address (2), we introduce {AFaCTA} ({A}utomatic {Fa}ctual {C}laim de{T}ection {A}nnotator), a novel framework that assists in the annotation of factual claims with the help of large language models (LLMs). AFaCTA calibrates its annotation confidence with consistency along three predefined reasoning paths. %to mirror the reasoning process of human experts. Extensive evaluation and experiments in the domain of political speech reveal that AFaCTA can efficiently assist experts in annotating factual claims and training high-quality classifiers, and can work with or without expert supervision. Our analyses also result in PoliClaim, a comprehensive claim detection dataset spanning diverse political topics.{ . We will open-source our code, annotations, and LLM outputs. }%Evaluation on PoliClaim$_{test}$ reveals that AFaCTA outperforms human experts on samples with consistent annotations. Comprehensive fine-tuning experiments on PoliClaim$_{train}$ (AFaCTA-annotated) demonstrate that AFaCTA-annotated data can be a strong alternative to human-annotated data in training classifiers.","AFaCTA Pipeline. All steps that need LLM prompting are annotated with the brain icon. Besides the target statement, a short context (if available) is also provided to help the model understand the statement.","The explosion of mis- and disinformation is a growing public concern, with misinformation being widely shared . Manual fact-checking is an important counter-measure to misinformation . However, fact-checking is a time-consuming and expensive endeavor, and computational remedies are required . A first step to identify mis- and disinformation consists of factual claim detection, which filters out the claims with factual assertions that need checking . Considering the sheer amount of daily online content and LLMs' generative capability, we argue that a valid factual claim detection system should be efficient and easily deployable to monitor misinformation consistently. Therefore, we need a way to produce high-quality resources to build transparent, accurate and fair models to automatically detect such claims. However, there are two major challenges in the data collection process. {Discrepancies in task and claim definitions.} By now, arguably, several different claim definitions exist, which confuse practitioners. What is a {claim} is unclear, leading to various {claim detection tasks}, e.g., in automated fact-checking and argument mining. For example, dismiss all opinions from factual claims, but includes ``opinions with social impact'' as factual claims. Many studies aim at detecting ``check-worthy'' claims while argues the definition of ``check-worthiness'' is highly subjective and political. Such variances reflect a lack of clarity in conceptualizing critical distinctions, such as the overlap between opinions and verifiable facts (refer to {tab:examples} row 1), and the separate nature of verifiability and check-worthiness in the context of factual claim detection (see {tab:examples} rows 2 and 3). To address these inconsistencies, we propose a definition of factual claims based on verifiability: factual claims present verifiable facts; a fact is verifiable only if it provides enough specificity to guide evidence retrieval and fact-checking. We focus on verifiability to maximize the definition's objectivity and clearly delineate facts from opinions. {Manual annotations are expensive.} All existing datasets are manually annotated, which is time-consuming and expensive. Thus, most existing resources are inevitably restricted to certain topics for which it is feasible to annotate claims manually. Such examples include presidential debates , COVID-19 tweets , biomedical and environmental claims . This potentially limits models' ability to generalize to future topics. However, manually annotating datasets with new topics is too expensive. In light of this, we propose {AFaCTA}, a multi-step reasoning framework that leverages LLMs to assist in claim annotation, making annotation more scalable and generalizable while rigorously following our factual claim definition. In fact-checking, it is essential to have high annotation accuracy. However, LLM annotators are far from perfect . Thus, to ensure the reliability of LLM annotations, AFaCTA calibrates the correctness of the annotations based on the consistency of different paths. Our evaluation shows that AFaCTA outperforms experts by a large margin when all reasoning paths achieve perfect consistency but fails to achieve expert-level performance on inconsistent samples. Nevertheless, we argue that AFaCTA can be an efficient tool in assisting factual claim annotation: perfectly consistent samples can be labeled automatically by the tool, which roughly saves 50\ Using AFaCTA, we annotate {PoliClaim}, a high-quality claim detection dataset covering U.S. political speeches across 25 years, spanning various political topics. We split the 2022 speeches as the test set and the 1998 to 2021 speeches as the training set to imitate the real-world use case where a model learns from the past and predicts future claims. We evaluate hundreds of classifiers trained on various data combinations, finding that AFaCTA's annotated data with perfect consistency can be a strong substitute for data annotated by human experts. In summary, our contributions include: {enumerate}[itemsep=0pt,topsep=1pt] We review the regular misconceptions and confounders in claim definition, proposing a claim definition for fact-checking focusing on verifiability. We propose AFaCTA, an LLM-based framework that assists factual claim annotation and ensures its reliability by calibrating annotation quality with consistency along different reasoning paths. We annotate PoliClaim, a high-quality factual claim detection dataset covering political speeches of 25 years and various topics. {enumerate"
Towards Faithful and Robust LLM Specialists for Evidence-Based Question-Answering,2402.08277v5,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.08277v5_0.pdf,"Advances towards more faithful and traceable answers of Large Language Models (LLMs) are crucial for various research and practical endeavors. One avenue in reaching this goal is basing the answers on reliable sources. However, this Evidence-Based QA has proven to work insufficiently with LLMs in terms of citing the correct sources (source quality) and truthfully representing the information within sources (answer attributability). In this work, we systematically investigate how to robustly fine-tune LLMs for better source quality and answer attributability. Specifically, we introduce a data generation pipeline with automated data quality filters, which can synthesize diversified high-quality training and testing data at scale. We further introduce four test sets to benchmark the robustness of fine-tuned specialist models. Extensive evaluation shows that fine-tuning on synthetic data improves performance on both in- and out-of-distribution. %Evidence-Based QA cases. Furthermore, we show that data quality, which can be drastically improved by proposed quality filters, matters more than quantity in improving Evidence-Based QA.{ All our codes, LLM generations, and human annotations are accessible through . }",Synthetic data generation pipeline and Evaluation for Evidence-Based QA.,"Large Language Models (LLMs) have become the center of many cutting-edge applications due to their generalisability and information processing abilities. A typical application of LLMs is in {Evidence-Based Question Answering (QA)}, where LLMs are expected to answer questions based on provided sources and cite the sources accurately [e.g.,][]{ni-etal-2023-chatreport, vaghefi2023chatclimate, cuiChatLawOpenSourceLegal2023, liuEvaluatingVerifiabilityGenerative2023}. By providing these additional sources, multiple shortcomings of standalone LLMs, such as hallucination and limited knowledge capacity , can be addressed, thereby enhancing answer traceability . However, the performance of existing LLMs on Evidence-Based QA is far from perfect. The SOTA close-sourced LLMs and generative search engines have an unignorable rate of hallucinated answers and false citation . Unfortunately, open-sourced LLMs are even less faithful than the already quality-lacking close-sourced LLMs in Evidence-Based QA (; ; also see our evaluation in ), although they achieve competitive results on general instruction-following benchmarks . We argue that this may prevent practitioners from building Evidence-Based QA (or other RAG) applications in a robust way. Therefore, efficient data creation and fine-tuning methods are urgently needed to improve LLMs' Evidence-Based QA performance in target applications. To address this research gap, we first formulate quality dimensions for Evidence-Based QA. Specifically, (1) LLMs need to {always} cite the right evidence at the end of each generated sentence to enable answer traceability, and (2) the answers need to be factually supported by the cited evidence. Fine-tuning LLMs using Evidence-Based QA data that follow these quality dimensions seems straightforward. However, we identify two major challenges of fine-tuning LLMs into faithful evidence-based question answerers. {C1. Fine-Tuning Data Scalability}: Manual annotation for instruction tuning is costly and LLM-synthesized data can be a strong alternative . However, the potentially lower quality of synthesized data may lead to suboptimal fine-tuning performance, given the SOTA LLMs' hallucination rate on Evidence-Based QA . {C2. Generalisability after Fine-tuning}: Previous work shows that diversified instruction tuning improves LLMs' generalisability . Hence, an intuitive worry is that fine-tuning LLMs (generalists) on Evidence-Based QA data (especially synthetic data) might turn LLMs into specialists that lack generalisability and, thus, struggle with out-of-distribution (OOD) questions and evidence. To address C1, we propose a data generation pipeline that synthesizes {SynSciQA} ({Syn}thetic {Sci}entific {Q}uestion {A}nswering), a well-diversified synthetic dataset for Evidence-Based QA, following prior work on data distillation for instruction tuning [e.g.,][]{honovich-etal-2023-unnatural, tunstallZephyrDirectDistillation2023}. We further extend the pipeline with two novel quality filters to sift out low-quality synthetic data points, leading to {SynSciQA+} and {SynSciQA++} (see the left half of {fig:overview}). To address C2, we first collect an in-domain test set {SynSciQA}$_{test}$ with the data generation pipeline, which shares the data distribution with the training data (i.e., {SynSciQA}) but covers different topics. We further collect three test sets with different distances to the training data distribution to study the OOD performance (see the right half of {fig:overview}). Extensive experiments on all proposed train and test settings show that (1) data quality is more important than quantity in Evidence-Based QA fine-tuning; (2) fine-tuning on generated data improves the performance on both in- and out-of-distribution test sets; and (3) performance scores on in-domain test set substantially indicate the OOD performance, suggesting that the synthetic data can be used for validation to estimate the OOD performance. All evaluation metrics are based on golden heuristics and best-performed models from previous work , which we further verified with human and GPT-4 evaluation. In summary, our contributions include: {enumerate}[itemsep=0pt,topsep=1pt] We propose a data generation pipeline to obtain fine-tuning data for Evidence-Based QA in a salable way, which ensures data diversity and quality. We propose four test sets to benchmark the in- and out-of-distribution performance of fine-tuned Evidence-Based QA specialists. We conduct an extensive evaluation to show that our data-synthesizing strategy leads to effective training and development set for Evidence-Based QA, and quality-filtering significantly improves fine-tuning performance. {enumerate"
Navigating the Metrics Maze: Reconciling Score Magnitudes and Accuracies,2401.06760v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2401.06760v2_0.pdf,"Ten years ago, a single metric, BLEU, governed progress in machine translation research. For better or worse, there is no such consensus today, and consequently it is difficult for researchers to develop and retain intuitions about metric deltas that drove earlier research and deployment decisions. This paper investigates the ``dynamic range'' of a number of modern metrics in an effort to provide a collective understanding of the meaning of differences in scores both within and among metrics; in other words, we ask {what point difference $x$ in metric $y$ is required between two systems for humans to notice?} We conduct our evaluation on a new large dataset, , using it to discover deltas at which metrics achieve system-level differences that are meaningful to humans, which we measure by pairwise system accuracy. We additionally show that this method of establishing delta-accuracy is more stable than the standard use of statistical p-values in regards to testset size. Where data size permits, we also explore the effect of metric deltas and accuracy across finer-grained features such as translation direction, domain, and system closeness.",Distribution of pairwise system deltas for each metric over all systems from WMT22. Gray rectangles show min-max range which is vastly different between metrics. Standard deviations (black lines) also differ.,"A decade ago, the BLEU metric served as the default metric for machine translation evaluation. It was not without its criticisms {hovy-ravichandran-2003-holy, callison-burch-etal-2006-evaluating, belz-reiter-2006-comparing} or compelling alternatives {banerjee-lavie-2005-meteor, popovic-2015-chrf}, but a combination of adequate performance, robustness to new languages, simplicity, understandability---and also inertia---helped it retain this position. This is no longer the case. BLEU's deficiencies quickly became apparent as deep learning approaches to machine translation replaced the earlier symbolic paradigms {mathur-etal-2020-tangled}. Today, a number of metrics---themselves deep-learning based---compete in an ecosystem where there is no longer any dominant, default metric. This situation creates a problem for researchers working to keep abreast of developments in the field. Different metrics, including different models within the same metric family, have different {dynamic ranges}, i.e., the range of scores one can expect to see. Furthermore, the {metric delta}, i.e., the score difference signifying a meaningful change in performance between two systems, also varies across metrics. It is perhaps understandable that some practitioners therefore continue to use BLEU, as well, if only to ground their understanding. This paper attempts to introduce some order and clarity into this situation. We make use of a large, new human evaluation dataset, , to compare the score ranges of metrics on a large number of systems against pairwise system-level accuracy. Importantly, we break down these accuracy scores into bins based on metric deltas, which allows us to determine accuracies for each metric as a function of the score differences between two systems. This provides a measure of confidence in the output that is stable across testset size, in contrast to standard statistical significant testing, which becomes more stable as testset size grows. We release a tool that allows a user to easily compare accuracies at different threshold across metrics.[0] In this work we: {enumerate}[noitemsep,topsep=0mm,left=4mm,label={)}] [] Empirically investigate the estimated accuracy for multiple metrics, human ability to perceive quality difference; [] Provide thresholds for popular metrics to help reviewers and practitioners interpret results; [] Validate our estimated accuracies on WMT testsets and investigate the effect of different language groups; [] Show that string-based metrics, such as BLEU, should never be used to evaluate unrelated systems; [] Show that statistical significance testing is insufficient to determine model improvement especially as it is affected by the testset size, but is important for small deltas; [] Assess quality of automatic metrics over 6530 system pairs; [] Summarize recommendations for machine translation evaluation. {enumerate"
Handling Ambiguity in Emotion: From Out-of-Domain Detection to Distribution Estimation,2402.12862v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.12862v1_0.pdf,"The subjective perception of emotion leads to inconsistent labels from human annotators. Typically, utterances lacking majority-agreed labels are excluded when training an emotion classifier, which cause problems when encountering ambiguous emotional expressions during testing. This paper investigates three methods to handle ambiguous emotion. First, we show that incorporating utterances without majority-agreed labels as an additional class in the classifier reduces the classification performance of the other emotion classes. Then, we propose detecting utterances with ambiguous emotions as out-of-domain samples by quantifying the uncertainty in emotion classification using evidential deep learning. This approach retains the classification accuracy while effectively detects ambiguous emotion expressions. Furthermore, to obtain fine-grained distinctions among ambiguous emotions, we propose representing emotion as a distribution instead of a single class label. The task is thus re-framed from classification to distribution estimation where every individual annotation is taken into account, not just the majority opinion. The evidential uncertainty measure is extended to quantify the uncertainty in emotion distribution estimation. Experimental results on the IEMOCAP and CREMA-D datasets demonstrate the superior capability of the proposed method in terms of majority class prediction, emotion distribution estimation, and uncertainty estimation.","The bar chart shows the number of labels assigned by annotators to the emotion class ``angry'' (Ang), ``frustrated'' (Fru), and ``neutral'' (Neu) in an example. In utterance (a), eight annotators interpret the emotion as angry while one interprets it as frustrated.","The inherent subjectivity of human emotion perception introduces complexity in annotating emotion datasets. Multiple annotators are often involved in labelling each utterance and the majority-agreed (MA) class is usually used as the ground truth. Utterances that have no majority-agreed (NMA) labels ({i.e.}, with tied votes) are typically excluded during emotion classifier training~ {Kim_2013,Poria2017,wu2021emotion}, which may cause issues when the system encounters such utterances in practical applications. This paper investigates three approaches to handling ambiguous emotion data. First, a naive method is tested which aggregates NMA utterances into an additional class when training an emotion classifier. This approach proves problematic as NMA utterances contain a blend of emotions, thereby confusing the classifier and undermining the classification performance. Then we explore if an emotion classifier can appropriately respond with ``I don't know'' for ambiguous emotion data that does not fit into any predefined emotion class. This is realised by quantifying the uncertainty in emotion classification using evidential deep learning (EDL). When a classifier trained on MA data encounters an NMA utterance during the test, the model should identify it as an out-of-domain (OOD) sample by providing a high uncertainty score, indicating its uncertainty regarding the specific emotion class to which the NMA utterance belongs. Moreover, to obtain fine-grained distinctions between ambiguous emotional data, we re-frame the task from classification to distribution estimation. Consider the example shown in Figure~ with the annotations assigned to three utterances. Since the majority emotion classes are ``angry'' for both utterances (a) and (b), they will be assigned the same ground-truth label ``angry'' in the aforementioned classification system, which implies that they convey the same emotion content and is evidently unsuitable. On the contrary, utterance (c), though being an NMA utterance, is more likely to share similar emotional content with utterance (b). Therefore, in order to obtain more comprehensive representations of emotion content, we further propose representing emotion as a distribution rather than a single class label and re-framing emotion recognition as a distribution estimation problem rather than a classification problem. A novel algorithm is proposed which extends EDL to estimate the underlying emotion distribution given observed human annotations and quantify the uncertainty in emotion distribution estimation. The proposed approach considers all human annotations rather than relying solely on the majority vote class. Multiple evaluation metrics are adopted to evaluate the performance in terms of majority class prediction, uncertainty measure, and distribution estimation. Rather than simply saying ``I don't know'', the proposed system demonstrates the ability to estimate the emotion distributions of the NMA utterances and also offer a reliable uncertainty measure for the distribution estimation. Our contributions are summarised as follows. {(i)} To the best of our knowledge, this paper is the first work that treats ambiguous emotion as OOD and detects it by uncertainty estimation; {(ii)} This is the first work that applies EDL to quantify uncertainty in emotion classification; {(iii)} Imposing a single ground truth through majority voting leads to under-representation of minority views. We instead estimate the distribution over emotion classes which provides a more comprehensive representation of emotion content as well as a more inclusive representation of human opinions; {(iv)} A novel algorithm is proposed that extends EDL to quantify uncertainty in emotion distribution estimation"
ToolSword: Unveiling Safety Issues of Large Language Models in Tool Learning Across Three Stages,2402.10753v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.10753v2_0.pdf,"Tool learning is widely acknowledged as a foundational approach for deploying large language models (LLMs) in real-world scenarios. While current research primarily emphasizes leveraging tools to augment LLMs, it frequently neglects emerging safety considerations tied to their application. To fill this gap, we present {ToolSword}, a comprehensive framework dedicated to meticulously investigating safety issues linked to LLMs in tool learning. Specifically, ToolSword delineates six safety scenarios for LLMs in tool learning, encompassing {malicious queries} and {jailbreak attacks} in the input stage, {noisy misdirection} and {risky cues} in the execution stage, and {harmful feedback} and {error conflicts} in the output stage. Experiments conducted on 11 open-source and closed-source LLMs reveal enduring safety challenges in tool learning, such as handling harmful queries, employing risky tools, and delivering detrimental feedback, which even GPT-4 is susceptible to. Moreover, we conduct further studies with the aim of fostering research on tool learning safety. The data is released in~.","Responses of LLMs to unsafe queries between standard dialogue and tool learning Contexts. Tool learning may disrupt the safe alignment mechanism of LLMs, leading to responses to unsafe queries through tool invocation.","Recently, tool learning has garnered significant attention as a potent approach for seamlessly integrating large language models (LLMs) into real-world applications. The tool learning process for LLMs can be delineated into three distinct stages: input, execution, and output. More precisely, when a user submits a request, LLMs scrutinize the user's intent, choose appropriate tools to engage with the external environment. Upon receiving feedback from the environment, LLMs structure the pertinent information to provide a response to the user's initial query. Existing research primarily concentrates on enhancing LLMs capabilities through tool utilization. One proposed approach involves fine-tuning the base model by generating numerous tool usage trajectories for a specific set of tools. This approach aids LLMs in swiftly grasping the functionality of various tools and mastering their utilization for problem-solving. Another strategy aims to bolster the model's generalization skills by devising prompts that instruct LLMs to read tool descriptions and employ external tools as necessary. However, these studies overlook the fact that tool learning also introduces new safety concerns. As illustrated in Figure~, in standard dialogues, LLMs can recognize and refuse to provide assistance when users enter unsafe queries. In contrast, in the context of tool learning, the safety alignment mechanism may be compromised. Consequently, LLMs may provide corresponding answers to unsafe queries by utilizing relevant tools. Furthermore, the selection of tools by LLMs may be influenced by malicious noise. Therefore, there is an urgent need for a comprehensive analysis of the current safety challenges faced by LLMs in the realm of tool learning to facilitate research aimed at their development. To fill this gap, we introduce {ToolSword}, a comprehensive framework crafted for unveiling the safety issues of LLMs throughout the tool learning process. ToolSword encompasses six safety scenarios that LLMs encounter in tool learning, encompassing {malicious queries} and {jailbreak attacks} in the input stage, {noisy misdirection} and {risky cues} in the execution stage, as well as {harmful feedback} and {error conflicts} in the output stage. Through an analysis of LLMs performance within these safety scenarios, we can gain insight into how they manage various safety challenges in tool learning at a granular level. Leveraging ToolSword, we analyze 11 open-source and closed-source LLMs equipped with robust tool learning capabilities. Our findings reveal that current LLMs frequently encounter safety issues across all stages of tool learning, leading to significant safety risks such as responding to harmful queries, invoking risky tools, and providing detrimental feedback, despite these issues being easily discernible by humans. Even the most advanced LLMs, such as GPT-4, are not immune to these challenges. Moreover, our futher studies indicate that LLMs can demonstrate performance comparable to humans in tool learning environments devoid of safety concerns. Hence, enhancing safety measures is essential to drive the practical application of LLMs. We hope that our findings will contribute to advancing research in the domain of tool learning safety. The main contributions of our work are summarized as follows: {itemize} We introduce ToolSword, a comprehensive framework designed to unveil the complete spectrum of safety issues associated with LLMs in tool learning. ToolSword conducts a thorough examination of LLMs across three distinct stages, thereby encompassing the entirety of the tool learning process. We develop two distinct types of safety scenarios for each stage, specifically tailored to address the real-world safety challenges encountered by LLMs. These scenarios enable us to meticulously evaluate the safety performance of LLMs when confronted with various challenges at a granular level. We conduct experiments involving 11 open-source and closed-source LLMs, and identify notable safety issues across each stage of tool learning. These findings emphasize the urgent requirement for enhancing the safety of LLMs in tool learning. {itemize"
Enhancing Contrastive Learning with Noise-Guided Attack: Towards Continual Relation Extraction in the Wild,2305.07085v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2305.07085v1_0.png,"The principle of continual relation extraction~(CRE) involves adapting to emerging novel relations while preserving od knowledge. While current endeavors in CRE succeed in preserving old knowledge, they tend to fail when exposed to contaminated data streams. We assume this is attributed to their reliance on an artificial hypothesis that the data stream has no annotation errors, which hinders real-world applications for CRE. Considering the ubiquity of noisy labels in real-world datasets, in this paper, we formalize a more practical learning scenario, termed as {noisy-CRE}. Building upon this challenging setting, we develop a noise-resistant contrastive framework named as {N}oise-guided {a}ttack in {C}ontrative {L}earning~(NaCL) to learn incremental corrupted relations. Compared to direct noise discarding or inaccessible noise relabeling, we present modifying the feature space to match the given noisy labels via attacking can better enrich contrastive representations. Extensive empirical validations highlight that NaCL can achieve consistent performance improvements with increasing noise rates, outperforming state-of-the-art baselines.",Left Table: Noisy labels exist widely in well-annotated benchmarks. Right Plot: Performance of the state-of-the-art CRE methods drop significantly on TACRED with noise ratio ranging from 0\% to 50\%.,"Alongside the predictive wins of relation extraction~(RE) on various benchmarks, the need for the ability to acquire sequential experience in dynamic environments stands out the significance. Catering to the real-world learning requirement, a new RE formulation, namely continual relation extraction~(CRE), has been proposed. Under this topic, catastrophic forgetting where previous knowledge is overwritten as new concepts are learned, remains a key challenge. To prevent forgetting, a variety of sophisticated methods are developed by memory replay, weight regularization or architecture expansion. explicitly store past experiences into a limited memory and replay them to complement new tasks learning. In comparison to exemplars storage, impose constraints on the update of the important network weights for old knowledge consolidation. As for architecture-based method, it dynamically changes model architectures to acquire new information while remembering previous knowledge. Despite the effectiveness, all of these methods implicitly assume the correctness of the labels for the streaming data. In practice, such an assumption is rather artificial even impossible to satisfy since label shifts are inevitable in real-world scenarios. Worse still, official statistics in the table of Figure~ reveal that the widely used benchmarks with elaborate human annotations, likewise, contain a certain proportion of noisy labels. Due to the ignorance of noisy labels over data streams, it is clear to see in Figure~ that state-of-the-art CRE models fail to defend against label inconsistency, resulting in significant performance drops. To break the impractical structure of current CRE setup and to enhance the noise-resistant capacity of models, in this paper, we present a more generalized learning setting coined as {noisy-CRE}. In this challenging scenario, there is a potential for mislabeled samples to contaminate the sequential stream in every incremental task. We assume that models trained under the noisy-CRE setting can reflect their ability to adapt to new relations in the real world. In the face of the great challenge, in this paper, we propose a robust contrastive framework as {N}oise-guided {a}ttack {C}ontrative {L}earning~(NaCL) for noisy-CRE. Generally, handling noisy labels can be relaxed to a subsequent process of clean sample selection and noisy sample correction. In NaCL, we introduce an auxiliary model to play the two roles. {First}, at each new task, the auxiliary model will be re-initialized to train for new relations learning. Intriguingly, we term it as {reboot}, which can make the model escape the interference of prior knowledge so that its logit outputs can be a measure of clean sample selection for current task. {Second}, this model will translate a novel sight into feature space for correction by performing {noise-guided attack}. This attack can actively drive the feature distribution of noisy negatives more aligned with their given labels. To demonstrate the effectiveness of NaCL, we design two benchmarks based on FewRel and TACRED. Empirical results and in-depth analyses show that our NaCL can achieve consistent improvements when noise rates vary from light to heavy, and it outperforms all state-of-art baselines far ahead. In summary, the contributions of this work are three-fold: $$ We define a practical noisy-CRE setting and construct well-designed benchmarks. To the best of our knowledge, this is the first work to improve the robustness of CRE models against noisy labels. $$ We propose NaCL, a noise-resistant contrastive framework that can jointly prevent catastrophic forgetting and learn with noisy labels. $$ We provide empirical results and extensive assessments to verify the effectiveness of NaCL, outperforming other state-of-the-art baselines adapted from CRE methods by a large margin"
Benchmarking Knowledge Boundary for Large Language Models: A Different Perspective on Model Evaluation,2402.11493v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.11493v2_0.pdf,"In recent years, substantial advancements have been made in the development of large language models, achieving remarkable performance across diverse tasks. To evaluate the knowledge ability of language models, previous studies have proposed lots of benchmarks based on question-answering pairs. We argue that it is not reliable and comprehensive to evaluate language models with a fixed question or limited paraphrases as the query, since language models are sensitive to prompt. Therefore, we introduce a novel concept named knowledge boundary to encompass both prompt-agnostic and prompt-sensitive knowledge within language models. Knowledge boundary avoids prompt sensitivity in language model evaluations, rendering them more dependable and robust. To explore the knowledge boundary for a given model, we propose a projected gradient descent method with semantic constraints, a new algorithm designed to identify the optimal prompt for each piece of knowledge. Experiments demonstrate a superior performance of our algorithm in computing the knowledge boundary compared to existing methods. Furthermore, we evaluate the ability of multiple language models in several domains with knowledge boundary.","Illustration of three classes of knowledge based on the model's mastery of knowledge in different textual forms. Existing evaluation methods suffer from sensitivity to input prompt. Therefore, the knowledge ability depicted by these methods is irregularly shaped. We propose to evaluate the knowledge capacity with a knowledge boundary containing both Prompt-Agnostic Knowledge and Prompt-Sensitive Knowledge.","Recently, large language models (LLMs) have made significant advancements in a variety of tasks . In order to gain deeper insights into the knowledge capabilities of different LLMs to help select appropriate LLM in practice, numerous studies have proposed various benchmarks for LLM evaluation . The majority of previous research on model evaluation constructs a test dataset sourced from standardized examinations, such as college entrance exams and law school admission tests . Subsequently, the questions are fed to LLMs as prompts, eliciting responses that are then scored for evaluation . However, each piece of knowledge embodies {abstract concept} that can be expressed in a nearly infinite number of {textual forms} . When evaluating a specific piece of knowledge, existing work only evaluated LLMs with one or several textual forms randomly sampled from the semantic space of the knowledge. However, existing LLMs are notorious for being sensitive to prompt, thereby undermining the reliability of such evaluations . Consequently, current studies on model evaluation are reasonably considered to be insufficiently robust. As shown in Figure , from the perspective of the model's mastery of the textual form of knowledge, knowledge can be divided into three classes: 1) {Prompt-Agnostic Knowledge} that can be correctly answered for any textual form; 2) {Prompt-Sensitive Knowledge} that is sensitive to the form of the prompt fed into the model; 3) {Unanswerable Knowledge} that is unable to be answered by the model, regardless of the prompt employed. The majority of previous research on model evaluation ignored the presence of Prompt-Sensitive Knowledge, resorting to oversimplified binary evaluations, classifying the model's knowledge mastery merely as true or false. attempts to assess LLM through diverse paraphrases, yet these evaluations remain confined to limited textual forms of knowledge. We give strict definitions of three types of knowledge in Section . In this paper, we aim to reduce the contingency when evaluating LLMs. Different from previous paradigms of LLM evaluation, we attempt to explore the Unanswerable Knowledge of the model to be evaluated, thereby illuminating the knowledge boundaries of LLMs. How can we find Unanswerable knowledge for the model? It is obvious that trying all prompts for the knowledge to query the model is too resource-intensive. Therefore, we choose to make efforts to search for the optimal prompt. We formalize optimal prompt searching as a discrete optimization problem: given some question paraphrases, we search for a prompt to maximize the probability of generating the correct answer. We propose the Projected Gradient Descent method with Constraints (PGDC), a new algorithm that updates prompt with gradient descent and implements proximal projection to search discrete prompts. To ensure that the optimized prompt has the same semantics as the original prompt, we introduce semantic loss, which is a measure of the distance between the semantic representations of the optimized prompt and the original prompt. Experimental results demonstrate that our proposed PGDC can outperform baselines in depicting knowledge boundaries. In addition, results on counterfactual datasets demonstrate that our approach is reasonable and robust. Human evaluation also reveals that our optimized prompts generally have the same semantics as the original questions. Moreover, we delineate models' knowledge boundaries in different domains using PGDC to evaluate LLMs. The size of the model's domain knowledge boundaries is strongly associated with the performance of downstream tasks in the domain. The optimal prompts also have some patterns that can give some inspiration for designing prompts when using corresponding LLMs. In summary, our contributions are: (1) We propose a new evaluation paradigm for benchmarking knowledge boundaries to compare models' capabilities, which can reduce the randomness in current evaluations. (2) We design PGDC, a projected gradient descent method with constraints, to optimize prompts and obtain knowledge boundaries of LLMs which achieves the best results on four datasets. (3) We evaluate five models using knowledge boundaries and obtain some valuable findings. Our code and data are released to facilitate future research{{https://github.com/pkulcwmzx/knowledge-boundary}{https://github.com/pkulcwmzx/knowledge boundary"
Exploring the Potential of Large Language Models in Computational Argumentation,2311.09022v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2311.09022v3_0.pdf,"Computational argumentation has become an essential tool in various domains, including law, public policy, and artificial intelligence. It is an emerging research field in natural language processing that attracts increasing attention. Research on computational argumentation mainly involves two types of tasks: argument mining and argument generation. As large language models (LLMs) have demonstrated impressive capabilities in understanding context and generating natural language, it is worthwhile to evaluate the performance of LLMs on diverse computational argumentation tasks. This work aims to embark on an assessment of LLMs, such as ChatGPT, Flan models, and LLaMA2 models, in both zero-shot and few-shot settings. We organize existing tasks into six main categories and standardize the format of fourteen openly available datasets. In addition, we present a new benchmark dataset on counter speech generation that aims to holistically evaluate the end-to-end performance of LLMs on argument mining and argument generation. Extensive experiments show that LLMs exhibit commendable performance across most of the datasets, demonstrating their capabilities in the field of argumentation. Our analysis offers valuable suggestions for evaluating computational argumentation and its integration with LLMs in future research endeavors. {https://github.com/DAMO-NLP-SG/LLM-argumentation}.}",Explored tasks and datasets in this work.,"Argumentation is a powerful and indispensable tool in various domains such as legality , debating , and education . It plays a vital role in facilitating understanding between individuals by providing insights into different perspectives and their underlying reasons. Additionally, argumentation serves as a means of communicating convincing opinions, enhancing the acceptability of positions among readers. As computational argumentation becomes a growing research field in natural language processing (NLP) , researchers have dedicated considerable efforts to two distinct directions . The first direction, argument mining, focuses on understanding unstructured texts and automatically extracting various argumentative elements . The other direction is argument generation, which aims to generate argumentative texts based on external knowledge or summarize key argument points. . Unlike classical structure prediction NLP tasks like named entity recognition that typically take a single sentence as the input and extract token-level information, computational argumentation tasks require discourse-level comprehension. This requirement makes it challenging and laborious to gather a large volume of labeled data for training, hindering the progress of research in this field. Fortunately, recent studies have shown that large language models (LLMs) have demonstrated impressive performance on a wide variety of NLP tasks in both zero-shot and few-shot settings. Given their strong capability in understanding long contexts and generating natural language, it is exciting yet still questionable how well LLMs can perform computational argumentation tasks without any supervised training. In light of this, our objective is to investigate the performance of LLMs on diverse computational argumentation tasks. There are two main issues we aim to address in our study. Firstly, although there are existing surveys about argument mining , the systematic study of the broader definition of computational argumentation including argument mining and argument generation is under-explored. To bridge this gap, we categorize current computational argumentation tasks into two primary classes, comprising six distinct categories. In addition, we establish a standardized format and evaluation metrics for fourteen openly available datasets. Secondly, existing tasks and datasets either focus on argument mining or argument generation. To take a holistic approach, we propose a new task that integrates both argument mining and generation. This task is designed to generate counter speeches in response to debate speeches, which typically advocate a particular stance. We name them counter speech and supporting speech respectively in the remainder of our paper. This task requires the model to understand the argumentative structures in the supporting speech, meanwhile to generate the counter speech against the proposition. To facilitate the study, we construct a new document-to-document counterargument generation benchmark based on a debate database . To evaluate the performance of LLMs on computational argumentation tasks, we choose from both open-source and proprietary LLMs to conduct our main experiments, in zero-shot and few-shot settings. Our results reveal that LLMs exhibit promising performance in both argument mining and argument generation tasks. While LLMs might fail to achieve exceptionally high scores on specific metrics such as R{ouge}, we hypothesize that the strict nature of these metrics could potentially underestimate the true potential of LLMs, which are inherently generative in nature. Human evaluation shows that LLMs are able to comprehend the core meaning of arguments and convey them effectively, even if the exact wording might not match. Collectively, these findings highlight the strengths of LLMs in grasping and effectively conveying the essence of arguments, showcasing their potential beyond what traditional metrics may suggest. To summarize, our contributions include: {We organize the existing computational argumentation tasks including argument mining and argument generation, and standardize the format of related datasets.} {We introduce a new task targeted at evaluating both argument mining and argument generation capabilities as a whole.} {To the best of our knowledge, we for the first time systematically evaluate the performance of multiple computational argumentation tasks using LLMs in zero-shot and few-shot settings.} {Extensive experimental results and analysis demonstrate the potential of LLMs in the computational argumentation research field and also suggest limitations in existing evaluation"
TaxoLLaMA: WordNet-based Model for Solving Multiple Lexical Semantic Tasks,2403.09207v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2403.09207v2_0.pdf,"In this paper, we explore the capabilities of LLMs in capturing lexical-semantic knowledge from WordNet on the example of the model and test it on multiple lexical semantic tasks. As the outcome of our experiments, we present , the ``all-in-one'' model for taxonomy-related tasks, lightweight due to 4-bit quantization and LoRA. achieves 11 SOTA results, and 4 top-2 results out of 16 tasks on the Taxonomy Enrichment, Hypernym Discovery, Taxonomy Construction, and Lexical Entailment tasks. Moreover, it demonstrates a very strong zero-shot performance on Lexical Entailment and Taxonomy Construction with no fine-tuning. We also explore its hidden multilingual and domain adaptation capabilities with a little tuning or few-shot learning. All datasets, code, and pre-trained models are available online.}",Training procedure of \text{TaxoLLaMA}: hypernym relations from the WordNet are linearized and fed into an LLM model. The model aims at generating the correct hypernym(s) as output.,"Recent studies in Natural Language Processing widely utilize Large Language Models (LLMs) for their capability to store extensive knowledge and to adapt quickly to different tasks via in-context learning without backpropagation . However, the application of LLMs to the classical lexical semantic tasks still remains understudied: for instance, no recent experiments with LLMs have been performed for the Hypernym Discovery task for different domains and languages. In Taxonomy Enrichment, LLMs are mostly used to extract vector representations which are further processed with a complex pipeline . Our work aims to investigate the capabilities of LLMs in addressing four tasks requiring taxonomic knowledge: Hypernym Discovery, Taxonomy Enrichment, Lexical Entailment, and Taxonomy Construction. We hypothesize that the model finetuned with hypernym (IS-A relationships) would be useful for solving taxonomy-related tasks. To verify this hypothesis, we develop a method inspired by to compile a taxonomy-focused instruction tuning dataset, sourced from English WordNet , to bring the implicit word knowledge of an LLM to the forefront when addressing lexical semantic tasks. Having trained our model in this specialized setting, we are releasing the {TaxoLLaMA} --- the finetuned version of the LLaMA-2-7b model --- that is capable of solving tasks requiring taxonomic knowledge. Figure presents the main idea of the model finetuning process. {TaxoLLaMA} operates effectively in a zero-shot setting, surpassing SOTA results in Lexical Entailment and Taxonomy Construction. With additional tuning, it also achieves SOTA performance in the Hypernym Discovery task across several languages and in half of the Taxonomy Enrichment tasks. Furthermore, we have optimized {TaxoLLaMA} to be lightweight through 4-bit quantization and the application of LoRA , making it feasible to run on GPU devices with only 4.8Gb of GPU for forward pass and 5.5Gb for fine-tuning, ensuring its accessibility for widespread use, e.g. using Colab{{https://colab.research.google.com}}. The contributions of the paper are as follows: {itemize} We introduce the use of LLMs across various lexical semantic tasks via hypernym prediction and propose an appropriate taxonomy instruction tuning method that exploits {WordNet} for dataset sampling. We present {TaxoLLaMA} -- a unified model designed to address a spectrum of lexical-sematic tasks achieving state-of-the-art (SOTA) results in 11 out of 16 tasks and securing the second rank in 4 tasks. We present an instructive dataset based on English WordNet-3.0 only for training a taxonomy-based LLM and collected definitions for input words in the Taxonomy Enrichment datasets and the Lexical Entailment datasets using Wikidata{{http://wikidata.org}} and ChatGPT{{https://chat.openai.com}}. We perform a detailed error analysis for all tasks using both manual and automatic approaches: e.g. we evaluate error patterns and model quality using ChatGPT. {itemize"
CANDLE: Iterative Conceptualization and Instantiation Distillation from Large Language Models for Commonsense Reasoning,2401.07286v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2401.07286v2_0.pdf,"The sequential process of conceptualization and instantiation is essential to generalizable commonsense reasoning as it allows the application of existing knowledge to unfamiliar scenarios. However, existing works tend to undervalue the step of instantiation and heavily rely on pre-built concept taxonomies and human annotations to collect both types of knowledge, resulting in a lack of instantiated knowledge to complete reasoning, high cost, and limited scalability. To tackle these challenges, we introduce CANDLE (}onceptu}lization and I}stantiation }istillation from }arge Language Mod}ls), a distillation framework that iteratively performs contextualized conceptualization and instantiation over commonsense knowledge bases by instructing large language models to generate both types of knowledge with critic filtering. By applying CANDLE to ATOMIC~, we construct a comprehensive knowledge base comprising six million conceptualizations and instantiated commonsense knowledge triples. Both types of knowledge are firmly rooted in the original ATOMIC dataset, and intrinsic evaluations demonstrate their exceptional quality and diversity. Empirical results indicate that distilling CANDLE on student models provides benefits across three downstream tasks{https://github.com/HKUST-KnowComp/CANDLE}.}.",Examples showing several chains of {\textcolor{concept_color}{conceptualization}} and {\textcolor{instance_color}{instantiation}} over the event {PersonX enjoys exercising in the gym}. {\textcolor{new_knowledge_color}{New inferential commonsense knowledge}} can be induced when placing the {\textcolor{instance_color}{instantiation}} back into the {\textcolor{original_tail_color}{original context}}.,"Commonsense reasoning refers to the cognitive ability to make logical inferences and draw conclusions based on general knowledge and understanding of the world that is typically shared among individuals. However, a longstanding challenge is generalizability, as commonsense reasoning often necessitates applying knowledge to novel situations beyond simple pattern recognition or memorizing all special cases. One promising approach to address this is the chain of conceptualization and instantiation, which, akin to the process of conceptual induction and deduction in human reasoning, involves conceptualizing instances derived from known commonsense knowledge and subsequently instantiating these concepts in new situations to obtain the knowledge required for downstream reasoning. For example, in Figure~, one can first conceptualize {enjoys exercising in the gym} as a {{concept_color}{healthy lifestyle}}, and then further instantiate it to {{instance_color}{go on a balanced diet}}. This process allows for the derivation of a novel event, {{instance_color}{PersonX goes on a balanced diet}}, which may entail {new_knowledge_color}{new commonsense knowledge} when connected with the {original_tail_color}{original event's commonsense inferential tail}. By possessing substantial knowledge to initiate the process of conceptualization and instantiation, one can extrapolate limited commonsense knowledge to a wide array of diverse scenarios. Yet, replicating this fundamental ability on machines remains challenging due to the absence of both types of knowledge in widely used CommonSense Knowledge Bases (CSKBs;). Various methods compensating the lack of conceptualization ability of language models have been proposed for entity-level and event-level conceptualizations by matching against concept taxonomies like Probase and WordNet. However, several limitations still persist. Firstly, despite the importance of both conceptualization and instantiation, most existing works underestimate the importance of the second step while focusing solely on conceptualization and using the resulting abstract knowledge directly. Other studies that concentrate on instantiations either overlook the conceptualization step entirely or only retrieve instances from the original CSKB, failing to introduce novel entities and events. Secondly, most conceptualization methods heavily depend on matching instances with concepts in concept taxonomies, such as Probase and WordNet, which have a limited scope and lack contextual information. Consequently, the derived conceptualizations are constrained in scale by these taxonomies and are formulated without considering proper contextualization, necessitating further verification in the original context. Lastly, the chain of conceptualization and instantiation can easily bring more than two orders of magnitude of data on top of the original CSKB. However, current acquisition and verification methods for both steps heavily rely on human annotation, which can be extremely costly as the scale of the CSKB increases. To address these gaps, we introduce CANDLE, a {{C}}onceptu{{A}}lization and I{{N}}stantiation {{D}}istillation framework from {{L}}arge Language Mod{{E}}ls (LLMs) to aid commonsense reasoning. Specifically, CANDLE marks the first to complete the chain of conceptualization and instantiation by instructing powerful LLMs to sequentially generate both types of knowledge based on concrete commonsense triples while carefully considering the original context throughout the process. We further alleviate the human annotation cost by employing two critic filtering models to eliminate low-quality generations. The instantiated knowledge, representing concrete commonsense knowledge again, can be fed back into CANDLE as input, iteratively augmenting the original CSKB significantly. By applying CANDLE to ATOMIC, we construct a large-scale knowledge base comprising 6.18 million conceptualizations and instantiations from two powerful LLMs, ChatGPT and LLAMA2. We demonstrate the intrinsic efficacy of CANDLE through automatic and human evaluations, highlighting the ability to generate high-quality and diverse knowledge (Section~). We further show the extrinsic benefits of CANDLE by leveraging the generated knowledge as complementary training data to distill student models that yield improvements across three downstream tasks, including CSKB conceptualization, generative commonsense inference, and zero-shot commonsense question answering (Section~)"
Time is Encoded in the Weights of Finetuned Language Models,2312.13401v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2312.13401v2_0.png,"We present {time vectors}, a simple tool to customize language models to new time periods. Time vectors are created by finetuning a language model on data from a single time (e.g., a year or month), and then subtracting the weights of the original pretrained model. This vector specifies a direction in weight space that, as our experiments show, improves performance on text from that time period. Time vectors specialized to adjacent time periods appear to be positioned closer together in a manifold. Using this structure, we interpolate between time vectors to induce new models that perform better on intervening and future time periods, without any additional training. We demonstrate the consistency of our findings across different tasks, domains, model sizes, and time scales. Our results suggest that time is encoded in the weight space of finetuned models.","{We present {time vectors}, a simple tool to customize language models to new time periods.} Time vectors ($\tau_i$) specify a direction in weight space that improves performance on text from a time period $i$. They are computed by subtracting the pretrained weights ($\theta_{\text{pre}}$; left panel) from those finetuned to a target time period ($\theta_i$). We can customize model behavior to new time periods (e.g., intervening months or years) by interpolating between time vectors and adding the result to the pretrained model (middle panel). We can also generalize to a future time period $j$ with analogy arithmetic (right panel). This involves combining a task-specific time vector with analogous time vectors derived from finetuned language models ($\tau^{\text{LM}}_j$).","Temporal variation is a fundamental characteristic of language. As we show in , it manifests in language model development as {temporal misalignment}, where deviations in train and test data lead to large performance degradation across different time periods [{inter alia}]{luu-etal-2022-time, lazaridou2021mind, jaidka2018diachronic}. This necessitates adaptation techniques for customizing models to specific time periods as needed. Designing such techniques is difficult, however, due to the multitude of time scales and the possibility that data from a target time period might be unavailable. Recent work has shown that the behavior of neural networks can be edited through closed-form interpolation between parameters of finetuned models [{inter alia}]{ilharco2023editing, OrtizJimnez2023TaskAI, li2022branchtrainmerge, Wortsman2021RobustFO}. In this work, we demonstrate that weight-space interpolation can also be used to cheaply edit language model behavior over {time}. To this end, we introduce {time vectors} (), an extension of task vectors . We finetune a pretrained language model on text from a single time period, and then subtract the pretrained weights. This vector represents a direction of movement in weight space that improves performance on text from the target time period. We analyze the structure of time vectors with temporally organized datasets for language modeling, classification, and summarization (). Our results consistently suggest that time vectors are intuitively organized on a manifold; years or months that are closer together in time yield time vectors that are also closer together in weight space. Similarly, we show that temporal degradation in yearly and monthly settings is strongly correlated with the angles between time vectors (). We use this structure of time vectors to induce models that generalize better to data from new time periods. By interpolating between two time vectors, we discover vectors that, when applied to the pretrained model, improve performance on intervening months or years (). The structure can also be used to generalize task-specific models across time periods with analogous time vectors specialized to unlabeled data (). Our results show that temporal variation is to some extent encoded in the weight space of finetuned models, and that weight interpolation can help customize language models to new time periods. We publicly release our code, data, and over 500 models finetuned on specific time periods.{{https://github.com/KaiNylund/lm-weights-encode-time"
SirLLM: Streaming Infinite Retentive LLM,2405.12528v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2405.12528v1_0.pdf,"As Large Language Models (LLMs) become increasingly prevalent in various domains, their ability to process inputs of any length and maintain a degree of memory becomes essential. However, the one-off input of overly long texts is limited, as studies have shown that when input lengths exceed the LLMs' pre-trained text length, there is a dramatic decline in text generation capabilities. Moreover, simply extending the length of pre-training texts is impractical due to the difficulty in obtaining long text data and the substantial memory consumption costs this would entail for LLMs. Recent efforts have employed streaming inputs to alleviate the pressure of excessively long text inputs, but this approach can significantly impair the model's long-term memory capabilities. Motivated by this challenge, we introduce Streaming Infinite Retentive LLM (SirLLM), which allows LLMs to maintain longer memory during infinite-length dialogues without the need for fine-tuning. SirLLM utilizes the Token Entropy metric and a memory decay mechanism to filter key phrases, endowing LLMs with both long-lasting and flexible memory. We designed three distinct tasks and constructed three datasets to measure the effectiveness of SirLLM from various angles: (1) DailyDialog; (2) Grocery Shopping; (3) Rock-Paper-Scissors. Our experimental results robustly demonstrate that SirLLM can achieve stable and significant improvements across different LLMs and tasks, compellingly proving its effectiveness. When having a coversation, ""A sir could forget himself,"" but SirLLM never does! Our code is publicly available at {https://github.com/Zoeyyao27/SirLLM}",The visualization of SirLLM versus existing attention patterns.,"The proliferation of large language models (LLMs) has spurred the development of various NLP applications, including widely-used tools like chatbots , writing assistants , and programming assistants . These applications, aiming to enhance user interaction and conversational experience, often require infinite input length and a certain degree of memory capability. However, current LLMs are usually pre-trained on texts of limited length, and studies have shown that their text generation capabilities dramatically decline when input lengths exceed those of the pre-training texts . Merely extending the length of pre-training texts is impractical, as acquiring infinitely long text data is exceedingly challenging, not to mention that it would result in substantial memory consumption for LLMs. Therefore, researching how to enable LLMs to handle infinite input lengths while maintaining memory capability is an urgent issue to be addressed. With the emergence of this demand, researchers have gradually shifted their focus towards exploring ways to expand the input context length of LLMs. A line of these studies has particularly focused on optimizing the attention mechanism of LLMs. first proposes the Sliding-window attention, as shown in Figure (a). By restricting each token to only attend to a certain number of recent tokens, this method reduces computational complexity. In deployment scenarios, LLMs utilize a Key-Value (KV) cache to store the key and value tensors of past tokens at each generation step to effectively reduces the need to recompute past key and value tensors, thereby significantly lowering computational overhead. Consequently, Sliding-window attention ensures a stable decoding speed even when the KV cache is full, thereby allowing for longer texts during the pre-training phase. However, discovered that this method does not truly achieve infinite input length, as the model's performance significantly deteriorates once the input length exceeds the size of the KV cache and intial tokens, however, receive a disproportionately higher amount of attention, a phenomenon termed as `attention sink`, as shown in Figure . Therefore, they proposed StreamLLM, as shown in Figure (b). StreamLLM enhances the potential of window attention by preserving the KV cache of the initial tokens, thereby achieving infinite length input in streaming conversations without finetuning. However, while Sliding-window Attention and StreamLLM ensure an expanded input length, each generated token only attends to recent tokens (and initial attention sink tokens), resulting in a loss of memory for earlier parts of the conversation. This leads to a significant forgetting issue in long-distance dialogues. Furthermore, as observed in Figure , the range of recent tokens that the model focuses on is not very extensive. This observation leads us to contemplate {whether it's possible for the model to concentrate only on key terms during a conversation, filtering out less important tokens}. By remembering only the crucial information, the model might be able to maintain a longer memory span in the context of infinitely long conversations. In response to the aforementioned challenges, we propose the Streaming Infinite Retentive LLM (SirLLM) in this paper, as illustrated in Figure (d). Initially, we employ an LLM to calculate the token entropy metric for each input token, thereby assessing their significance. Subsequently, tokens with higher token entropy values, deemed as key tokens, are preserved within the KV cache. This method enhances the model's memory capabilities in the context of infinitely long streaming dialogues. To validate the effectiveness of SirLLM, we conducted experiments across three distinct tasks: (1) DailyDialog: We created a multi-turn daily dialogue dataset based on the DailyDialog dataset . (2) Grocery Shopping: We developed a grocery shopping dataset. Users first inform the LLM about the groceries they need to purchase. Following this, users engage in multi-turn dialogues with the LLM, culminating in the users asking the LLM to recall the required groceries. (3) Rock-Paper-Scissors: We constructed a rock-paper-scissors dataset featuring three types of players, each with a preference for one of the three moves (rock, paper, scissors). Players engage in multiple rounds of rock-paper-scissors with the LLM, which is tasked with analyzing the user's historical preferences to maximize its winning rate. The results of these experiments effectively demonstrate the enhanced memory capabilities of SirLLM in infinite conversation"
ItD: Large Language Models Can Teach Themselves Induction through Deduction,2403.05789v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2403.05789v1_0.pdf,"Although Large Language Models (LLMs) are showing impressive performance on a wide range of Natural Language Processing tasks, researchers have found that they still have limited ability to conduct induction. Recent works mainly adopt ``post processes'' paradigms to improve the performance of LLMs on induction (e.g., the hypothesis search \& refinement methods), but their performance is still constrained by the inherent inductive capability of the LLMs. In this paper, we propose a novel framework, nduction hrough eduction (ItD), to enable the LLMs to teach themselves induction through deduction. The ItD framework is composed of two main components: a Deductive Data Generation module to generate induction data and a Naive Bayesian Induction module to optimize the fine-tuning and decoding of LLMs. Our empirical results showcase the effectiveness of ItD on two induction benchmarks, achieving relative performance improvement of 36\% and 10\% compared with previous state-of-the-art, respectively. Our ablation study verifies the effectiveness of two key modules of ItD. We also verify the effectiveness of ItD across different LLMs and deductors. The data and code of this paper can be found at https://anonymous.4open.science/r/ItD-E844.","Task of Induction. The tested model observes a batch of input-output $(x,y)$ pairs and needs to predict the latent transformation $f$ shared by these $(x,y)$ pairs.","Induction can take we humans from the observed to the unobserved . The task of {{Induction}} aims to discover consistent transformations from a set of input-output pairs, where the transformations map the inputs to the outputs well . As shown in Figure~, given the input-output pairs $\{x_i,y_i\}_{i=1}^n$, the model needs to predict the latent transformation $f$. For a detailed example, given the input {[1,2]} with the output {[1]} and other input-output pairs, the tested model is supposed to figure out the transformation {output the first element of the input list}. The Induction task is an important task in Natural Language Processing (NLP) and the mastery of the induction ability is an important sign of intelligence . Currently, humans have already mastered the capability of induction and have found thousands of laws from the physical world and human society. However, machine intelligence still struggles to induce basic logic rules in structure data like knowledge graphs . Recently, with the rapid development of Large Language Models (LLMs), many works have begun to adopt the LLMs to induce the transformations given the input-output observations of various tasks and express the induced transformations as rules , guidelines , instructions , and codes . These methods take advantage of the interpretability and generalization ability of LLMs in solving the Induction task. However, recent research have revealed that LLMs have inherently limited ability in induction. To tackle such a limitation, work like Hypothesis Search proposes to select the generated hypotheses from LLMs by evaluating them on the observations, while another following work Iterative Hypothesis Refinement proposes to further refine them through LLMs based on the evaluating results on the observations. Nevertheless, as shown in Figure~(a), these hypothesis search \& refinement methods are essentially ``post processes'' to the directly induced hypotheses of LLMs. They still heavily rely on the inherent induction ability of LLMs which are Weak Inductors. Even though LLMs are limited in induction, recent work finds out that they possess much better capability in deduction. Different from induction, deduction aims to infer the correct output given the transformation and the input. Despite the distinction that induction associates multiple $(x, y)$ pairs with the latent transformation $f$, whereas deduction links $x$ and $f$ to the resultant $y$, both approaches fundamentally share the commonality of reasoning within the framework of input, output, and transformation $(x, y, f)$. Therefore, it motivates us to propose a novel framework ItD ({I}nduction {t}hrough {D}eduction), to enable the LLMs to teach themselves induction through deduction. Different from previous methods, ItD fine-tunes the LLMs on their deduced data to make them Strong Inductors, as shown in Figure~(b). For a given induction task, ItD first proposes {Deductive Data Generation} to leverage the deductive capability of the LLMs to generate a set of task data $(x,y,f)$, which is simple yet effective and does not rely on human annotations or any larger LLMs' assistance. The data will then be used to fine-tune the LLMs to obtain better inductive capability. However, it is non-trivial to utilize the deduced data. We find out that directly fine-tuning the LLMs using the IO prompt used in the previous methods cannot effectively leverage the observed samples (as shown in Figure~). Thus, ItD further proposes {Naive Bayesian Induction} as a strategy to optimize the use of each sample. Moreover, we also observe performance gains with the increase in the number of samples using our approach. Specifically, this novel technique fine-tunes the LLM to predict $f$ conditioned on single pair $x,y$ ($p(f|x,y)$) instead of $n$ pairs ($p(f|\{x_i,y_i\}_{i=1}^n)$). During the decoding phase, it utilizes the Naive Bayesian approach to equivalently infer the probability distribution of $f$ under all $n$ $(x,y)$ conditions ($p(f|\{x_i,y_i\}_{i=1}^n)$) with the probability distribution of $f$ under a single $(x,y)$ condition ($p(f|x,y)$). We conduct experiments on two different types of induction tasks for evaluation: Instruction Induction and List Function. Compared with previous methods, The experiment results show that ItD is superior to the existing methods in assisting LLMs in induction, and both the Deductive Data Generation and the Naive Bayesian Induction components effectively contribute to ItD. We also make discussions to show that ItD can be effectively applied to different LLMs, and a more powerful deductor, e.g. {ChatGPT}, will further improve the performances of ItD. In summary, the major contributions of this paper are as follows: {itemize}[itemsep=1pt,topsep=1pt,parsep=0pt,leftmargin=*] We propose a novel framework ItD to enable the LLMs to teach themselves induction through deduction. We propose Deductive Data Generation to effectively leverage the deductive capability of LLMs to generate task data. which is fully self-supervised and needs no human annotations or any larger LLMs to assist. We propose Naive Bayesian Induction to allow LLMs to optimize the use of each observed sample and be able to take advantage of the increase in the number of observed samples. {itemize"
Enhancing In-Context Learning via Implicit Demonstration Augmentation,2407.00100v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2407.00100v1_0.pdf,"The emergence of in-context learning (ICL) enables large pre-trained language models (PLMs) to make predictions for unseen inputs without updating parameters. Despite its potential, ICL's effectiveness heavily relies on the quality, quantity, and permutation of demonstrations, commonly leading to suboptimal and unstable performance. In this paper, we tackle this challenge for the first time from the perspective of demonstration augmentation. Specifically, we start with enriching representations of demonstrations by leveraging their deep feature distribution. We then theoretically reveal that when the number of augmented copies approaches infinity, the augmentation is approximately equal to a novel logit calibration mechanism integrated with specific statistical properties. This insight results in a simple yet highly efficient method that significantly improves the average and worst-case accuracy across diverse PLMs and tasks. Moreover, our method effectively reduces performance variance among varying demonstrations, permutations, and templates, and displays the capability to address imbalanced class distributions.",Illustration for demonstration augmentation using semantic directions (vectors) sampled from the deep feature distribution of demonstration examples.,"Large pre-trained language models (PLMs) have showcased exceptional abilities in in-context learning (ICL), which assists the model in discerning the underlying patterns within demonstrations and make more accurate predictions. As a new paradigm, ICL offers compelling advantages, allowing for natural language interaction with PLMs, as well as reduced computational costs. While promising, ICL's performance is highly dependent on provided demonstrations and templates, resulting in subpar and unstable performance. This promotes research aimed at improving the quality, quantity, and permutations of demonstrations. Other research avenues include prediction adjustment and learning process design (e.g., channel models and meta-training frameworks). Despite ongoing efforts, ICL still struggles with efficiently and reliably capturing sufficient knowledge from context, leaving performance stability as a persistent bottleneck. In this study, we propose enriching contextual knowledge for PLMs by augmenting demonstrations. We first attempt to enhance the representation of demonstrations by transforming them along semantic directions sampled from the deep feature space of demonstration examples, as depicted in Figure~. This operation stems from the observation that the deep features in a network are usually linearized, implying the existence of numerous semantic directions within the deep feature space, hence potentially enabling us to incorporate richer contextual knowledge without extending input length. From this novel perspective, we theoretically prove that when the number of augmented pieces approaches infinity, its effect approximately equals a logit adjustment operation. Specifically, we derive a refined Softmax function that integrates the statistical properties of demonstrations. Consequently, rather than explicitly executing the augmentation procedure, we can efficiently conduct implicit demonstration augmentation using the derived prediction function, obtaining an improved ICL method with theoretical guidance. We conduct extensive experiments across seven PLMs and various classification tasks. The empirical results demonstrate that our approach remarkably enhances prediction accuracy and reduces performance variability across different demonstrations, permutations, and templates. Notably, our method is straightforward, effective, and generalizable, enabling seamless integration with other ICL methods to enhance their performance. Our contributions can be summarized as follows: {itemize}[itemsep=2pt, topsep=2pt,parsep=2pt] We introduce {I}mplicit {D}emonstration {A}ugmentation-based {ICL} (IDAICL), a pioneering work that incorporates demonstration augmentation into ICL. Instead of solely enhancing demonstration quality, quantity, or order, our method explores context augmentation within the deep feature space, offering a new perspective to enrich demonstrations bypassing input length limitations. We theoretically establish that as the number of augmented pieces approaches infinity, our augmentation strategy approximates a logit-adjusted prediction function that integrates statistical properties derived from the input data distribution. Equipped with this function, IDAICL provides a straightforward yet theory-guided solution to enhance ICL. Extensive experiments conducted across diverse tasks and PLMs conclusively illustrate that IDAICL considerably improves average and worst-case accuracy compared to existing ICL methods. Moreover, it effectively enhances performance stability. {itemize"
PRoLoRA: Partial Rotation Empowers More Parameter-Efficient LoRA,2402.16902v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.16902v2_0.pdf;/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.16902v2_1.pdf;/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.16902v2_2.pdf;/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.16902v2_3.pdf,"With the rapid scaling of large language models (LLMs), serving numerous low-rank adaptations (LoRAs) concurrently has become increasingly impractical, leading to unaffordable costs and necessitating more parameter-efficient finetuning methods. In this work, we introduce {P}artially {Ro}tation-enhanced {Lo}w-{R}ank {A}daptation (PRoLoRA), an intra-layer sharing mechanism comprising four essential components: broadcast reduction, rotation enhancement, partially-sharing refinement, and rectified initialization strategy. As a superset of LoRA, PRoLoRA retains its advantages, and effectively circumvent the drawbacks of peer parameter-sharing methods with superior model capacity, practical feasibility, and broad applicability. Empirical experiments demonstrate the remarkably higher parameter efficiency of PRoLoRA in both specific parameter budget and performance target scenarios, and its scalability to larger LLMs. Notably, with one time less trainable parameters, PRoLoRA still outperforms LoRA on multiple instruction tuning datasets. Subsequently, an ablation study is conducted to validate the necessity of individual components % explore the features of critical hyperparameters, and highlight the superiority of PRoLoRA over three potential variants. Hopefully, the conspicuously higher parameter efficiency can establish PRoLoRA as a resource-friendly alternative to LoRA.","Illustration of the original LoRA, our proposed PRoLoRA, and their intermediate states (i.e., CLoRA and RoLoRA). Here we set the rank $r$, unshared rank $u$, sharing rates $m$ and $n$ of the $\mathbf{A}$ and $\mathbf{B}$ matrices to be 4, 1, 2 and 3, respectively. Different shades of color in matrices $\mathbf{A}$ and $\mathbf{B}$ denote distinct ranks. The rotation arrows and center numbers indicate rotation directions and base strides, while dotted lines and higher transparency denote replicated or rotated weights, emphasizing that these weights do not contribute to the trainable parameters. Additionally, the center numbers % numerical values at the center of each matrix block represent the relative displacement of the $\mathbf{A}_i$ and $\mathbf{B}_i$ chunks compared to those of top-left block (i.e., $\mathbf{A}_0$ and $\mathbf{B}_0$).","Finetuning large language models (LLMs), such as LLaMA~2, GPT-3.5~Turbo, and Gemini, for specific domains and functions (e.g., model alignment and instruction tuning ), have become increasingly popular. To alleviate the high costs associated with full finetuning, parameter-efficient finetuning (PEFT), especially LoRA, has emerged as a lightweight solution by tuning a minority of parameters and freezing the remaining ones. However, with the rapid boost in the number of model's parameters, the demand for further enhanced parameter efficiency becomes progressively more imperative, especially when multiple LoRA are deployed simultaneously. As shown in the Table~, the configuration in (i.e., applying LoRA with the rank of 64 to all linear layers) results in a significant number of trainable parameters. For a single LLaMA2-7B model, LoRA will have about 160 million parameters to be tuned, occupying 610MB of disk storage and GPU memory in inference. These numbers quickly escalate to about 360 million and 1.4GB for a LLaMA2-70B model. For multi-LoRA scenarios, such as personalization and multitasking, this issue will dramatically exacerbate. Specifically, resource consumption will increase linearly with personalized customization, which will further experience a quadratic growth when coupled with multitasking. Hence, the high costs in multi-LoRA scenarios do spark a demand for further improved parameter efficiency. Focusing on the above target, parameter sharing can serve as an effective approach. Although small ranks could provide competitive performance in specific tasks , models generally perform better with higher ranks as listed in Table~. Besides, given a specific trainable parameter budget, better performance means higher parameter efficiency. Hence, enhancing parameter efficiency can be transformed into appropriately increasing the rank of LoRA with the same parameter count. Although VeRA can be regarded as an attempt in this direction, its aggressive freezing operations result in limited model capacity and excessively high rank, leading to significant inference latency in multi-LoRA scenarios where LoRA modules are not merged into the pretrained weights. Subsequently, Tied LoRA alleviates these problems by allowing the inter-layer shared matrices to be trainable. However, its tying mechanism restricts its applicability to weights of different shapes, which are widely present among self-attention and MLP modules. To circumvent all the above drawbacks, we introduce a new approach called {P}artially {Ro}tation-enhanced {Lo}w-{R}ank {A}daptation (PRoLoRA). It features a parameter-sharing mechanism within the low-rank decomposition matrices, and consists of four essential components: broadcast reduction, rotation enhancement, partially-sharing refinement, and rectified initialization strategy. Specifically, we reparameterize the low-rank matrices with multiple chunks along the hidden dimension, and broadcast the first chunks to the others so that trainable parameters can be saved, or equivalently, the rank can be increased multiple times. Then a nearly cost-free rotation operation along the rank dimension is performed to differentiate the identical chunks for higher expressiveness. Besides, a minimal subset of ranks is reserved without sharing for further refined capacity. To ensure the same bounds for initialization as unshared parameters, we also rectify the vanilla Kaiming uniform distribution for shared ones. As a superset of LoRA, PRoLoRA not only pertains the advantages of LoRA, such as lightweight task switching and optional merging to eliminate extra latency, but also brings about better capacity, practical feasibility, and broader applicability than other parameter-sharing methods. Empirical experiments on multiple instruction tuning datasets validate the higher parameter efficiency of PRoLoRA than baselines via two alternative perspectives (i.e., a specific trainable parameter budget and performance target). With half of tunable parameters, PRoLoRA achieves 4/6 wins and better average performance over LoRA. When scaling up to LLaMA2-13B, PRoLoRA consistently outperforms LoRA with the same trainable parameter count. Additionally, comprehensive ablation studies demonstrate the necessity of each component and the superiority of PRoLoRA over three potential intra-layer sharing variants. Overall, PRoLoRA achieves significantly higher parameter efficiency, and thereby remarkably alleviates the storage and GPU memory burden in multi-LoRA scenarios, establishing PRoLoRA as a resource-friendly alternative for LoRA. In summary, our main contributions are as follows: {enumerate}[] We introduce a more parameter-efficient method named PRoLoRA, featuring an intra-layer sharing mechanism consisting of broadcast reduction, rotation enhancement, partially-sharing refinement and rectified initialization strategy. We compare PRoLoRA with LoRA and existing peer methods on multiple instruction tuning datasets, and demonstrate its remarkably higher parameter efficiency, hopefully establishing PRoLoRA as a resource-friendly alternative to LoRA. We perform an ablation study to demonstrate the necessity of individual components and the superiority of PRoLoRA over other potential variants. {enumerate"
Interpreting Conversational Dense Retrieval by Rewriting-Enhanced Inversion of Session Embedding,2402.12774v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.12774v2_0.pdf,"Conversational dense retrieval has shown to be effective in conversational search. However, a major limitation of conversational dense retrieval is their lack of interpretability, hindering intuitive understanding of model behaviors for targeted improvements. This paper presents , a simple yet effective approach to shed light on interpretable conversational dense retrieval models. transforms opaque conversational session embeddings into explicitly interpretable text while faithfully maintaining their original retrieval performance as much as possible. Such transformation is achieved by training a recently proposed Vec2Text model~ {vec2text} based on the ad-hoc query encoder, leveraging the fact that the session and query embeddings share the same space in existing conversational dense retrieval. To further enhance interpretability, we propose to incorporate external interpretable query rewrites into the transformation process. Extensive evaluations on three conversational search benchmarks demonstrate that can yield more interpretable text and faithfully preserve original retrieval performance than baselines. Our work connects opaque session embeddings with transparent query rewriting, paving the way toward trustworthy conversational search. Our code is available at {this repository}.","The blue section on the left signifies the conversational dense retrieval, and the green section on the right provides an overview of \textsc{ConvInv}.","With the rapid development of language modeling, conversational search has emerged as a novel search paradigm and is garnering more and more attention. Different from the traditional ad-hoc search paradigm characterized by keyword-based queries and “ten-blue” links, conversational search empowers users to interact with the search engine through multi-turn natural language conversations to seek information, which brings a more intuitive and efficient search experience. In conversational search, the system input is a multi-turn natural language conversation, which may have many linguistic problems such as omissions, co-references, and ambiguities, posing great challenges for accurately grasping the user's real information needs. Recently, conversational dense retrieval (CDR), which directly encodes the whole conversational search session and the passages into a unified embedding space to perform matching, has shown to be a promising method to solve this complex search task. Compared to another type of method: conversational query rewriting (CQR), which is a two-step method that first reformulates the search session into a decontextualized query rewrite and subsequently inputs this rewrite into existing ad-hoc search models for search, the end-to-end CDR models can be directly optimized towards better search effectiveness and is more efficient as it avoids the extra latency caused by the rewriting step. However, a notable drawback of conversational dense retrieval is that it inherently lacks interpretability. By encoding conversations into dense vector embeddings rather than readable text, it becomes opaque how these CDR models comprehend search intent. The absence of interpretability becomes a severe obstacle for developers to comprehend the reasons behind the search results, hindering effective and targeted enhancements to the bad cases of the models. Moreover, the absence of interpretability poses challenges in identifying and addressing potential biases or errors within the models, which could lead to unfair or misleading search results without the possibility of timely correction. In this paper, we present : a simple and effective approach aiming to shed light on the opacity problem of conversational dense retrieval. demystifies the opaque conversational session embeddings by transforming them into explicitly interpretable text while faithfully maintaining their retrieval performance as much as possible. This transformation allows us to intuitively decipher the characteristics of behaviors of different conversational dense retrieval models. Figure~ provides an overview of {ConvInv}. Specifically, our approach is based on the recently proposed Vec2Text, which is a powerful method that can invert any text embedding into its original text given the corresponding text encoder. However, inverting the session embedding into the original session is meaningless as it brings no interpretability. We adapt Vec2Text to suit our interpretable inversion of conversational session embedding by taking specific advantage of how the conversational session encoders are trained: the session encoder starts from an ad-hoc query encoder and the passage encoder is frozen during the training. This makes the session and query embeddings finally share the same embedding space for retrieval. Therefore, we propose to train a Vec2Text model based on the ad-hoc query encoder to transform the session embedding so that the transformed text is different from the original session, but also maintains a similar retrieval performance when encoding it with the ad-hoc query encoder. To further enhance the interpretability of the transformed text, we directly incorporate well-interpretable external query rewrites into the Vec2Text transformation process, effectively guiding it to yield more interpretable text. We conduct extensive evaluations on three conversational search benchmarks. Compared to baselines, the proposed can transform conversational session embeddings into more interpretable text as well as faithfully restore the original retrieval performance of the session embeddings. In summary, the contributions of our work are: (1) We introduce a simple and effective approach {ConvInv} to shed light on the interpretability of conversational dense retrieval models by transforming opaque conversational session embeddings into interpretable text as well as faithfully maintain their original retrieval performance. (2) We propose to incorporate the query rewrites into the transformation process to effectively enhance the interpretability of the transformed text. (3) Our work connects opaque session embeddings with transparent query rewriting, paving the way toward trustworthy conversational search"
Hypergraph based Understanding for Document Semantic Entity Recognition,2407.06904v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2407.06904v1_0.pdf,"Semantic entity recognition is an important task in the field of visually-rich document understanding. It distinguishes the semantic types of text by analyzing the position relationship between text nodes and the relation between text content. The existing document understanding models mainly focus on entity categories while ignoring the extraction of entity boundaries. We build a novel hypergraph attention document semantic entity recognition framework, HGA, which uses hypergraph attention to focus on entity boundaries and entity categories at the same time. It can conduct a more detailed analysis of the document text representation analyzed by the upstream model and achieves a better performance of semantic information. We apply this method on the basis of GraphLayoutLM to construct a new semantic entity recognition model HGALayoutLM. Our experiment results on FUNSD, CORD, XFUND and SROIE show that our method can effectively improve the performance of semantic entity recognition tasks based on the original model. The results of HGALayoutLM on FUNSD and XFUND reach the new state-of-the-art results.",Difference in Document Task.,"With the development of information technology, documents have become a main information carrier nowadays, which contains kinds of information type, such as text, table and image. Manual recognition of these documents often requires plenty of manpower. OCR tools can only help us to identify the text, layout and other simple information in the document. To further understand documents, Visually-rich Document Understanding (VRDU) is proposed to make use of visual, textual and other information for more in-depth analysis. Semantic Entity Recognition (SER) is an important task in the field of VRDU. Its purpose is to extract and classify the text with special semantic information in documents. Different from text sequences in traditional natural language processing tasks, the information in documents is not one-dimensional, single-modal and continuous, but two-dimensional, multimodal and discrete. It is necessary to analyze not only text information, but also other modal information such as layout and vision in the document. Figure shows the difference between the traditional named entity recognition (NER) task on a single modal text and the semantic entity recognition task on a document. Firstly, the text form of a single modal text task is a fixed text sequence, while the discrete text in a document is composed of text nodes in different locations. Secondly, the named entity recognition task of a single modal text only needs to consider the semantic relationship between the tokens in the text sequence. However, the semantic entity recognition task on the document needs to consider not only the semantic relationship between nodes, but also the position relationship between nodes. Finally, the span range of entity tags of NER task is flexible, while the range of task tags of semantic entity recognition task on document is affected by nodes. Texts of the same node in the document share the same label in most cases. With the development of pre-training technology, document pre-training model has become popular. LayoutLM is the first multi-modal pre-trained model to associate text with layout and vision, achieving leading results on multiple downstream document understanding tasks including semantic entity recognition. Subsequently, more multi-mode pretraining models, such as LayoutLMv2, BROS, ERNIE-Layout and LayoutLMv3 have been proposed successively. By integrating text, layout and visual information, they realize the understanding and information extraction of documents. So far, GraphLayoutLM and GeoLayoutLM have the best performance in semantic entity recognition tasks. GraphLayoutLM achieves the best F1 score of 94.39 and 93.56 on the FUNSD and XFUND datasets. GeoLayoutLM achieves the best F1 score of 97.97 on the CORD datasets. However, these existing methods focus on the upstream document understanding part and pay little attention to the downstream task. GeoLayoutLM has studied the novel relational extraction head and achieves great improvement in the relational extraction task. But it has not done more research on the semantic entity recognition task. We study the problem of ignoring the downstream head and classification method in the semantic entity recognition task in the existing document intelligence work and propose a novel improvement scheme. {Traditional Semantic Entity Recognition.} The traditional document semantic entity recognition task process is shown in (a) of the Figure . In document understanding process, text nodes are spliced into text sequences and become text token sequences of documents after tokenization. These text nodes will be transformed to the high-dimensional feature representations after the analysis of the document understanding model. To extract semantic information from document token features, linear layer or multilayer perceptron (MLP) will be used to convert high-dimensional features into label probabilities and the training objective is cross entropy loss. Although this method can distinguish the node categories in the document, it ignores the characteristics of the document structure and it is difficult to make the classification layer pay attention to the node span. {Hypergraph Semantic Entity Recognition.} Inspired by Global Pointer, we use the idea of hypergraph to extract the semantic information of documents and propose a Hypergraph Attention(HGA) strategy for document semantic entity recognition. (b) of the Figure shows us the process of hypergraph semantic recognition. Different from the traditional classification method, the semantic entity recognition idea of HGA regard the document token features as graph nodes. The target entity is the set of nodes with the same hyperedge and the hyperedge type represents the entity label type. The process of hypergraph extraction is to establish hyperedges between token feature nodes. Besides, we use the span hyperedge encoding to add the span information of text nodes. Through the hypergraph and span position, the head can better focus on the entity boundary information and establish the relationship between the document discrete text span and the entity boundary. Our main contributions are as follows: {itemize} We construct a novel hypergraph attention document semantic entity recognition method, HGA. It transforms the traditional token sequence classification problem into a hypergraph construction process. By establishing different types of hyperedges between text nodes, the head can extract semantic entities. We propose a novel span hyperedge position encoding and balanced hyperedge loss. Span hyperedge position encoding makes the head focus more on the same text span prompt during hyperedge construction. Balanced hyperedge loss can help to solve the problem of matrix sparsity caused by too many hyperedge types in some scenarios. We construct a novel document semantic entity recognition model HGALayoutLM based on the HGA method. Our code will be available at https://github.com/Line-Kite/HGALayoutLM. The experiment results show that the model has good performance in the scene with few types of labels. HGALayoutLM has obtained the best results on the FUNSD, SROIE and XFUND datasets. {itemize"
An Iterative Associative Memory Model for Empathetic Response Generation,2402.17959v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.17959v2_0.pdf,"Empathetic response generation aims to comprehend the cognitive and emotional states in dialogue utterances and generate proper responses. Psychological theories posit that comprehending emotional and cognitive states necessitates iteratively capturing and understanding associated words across dialogue utterances. However, existing approaches regard dialogue utterances as either a long sequence or independent utterances for comprehension, which are prone to overlook the associated words between them. To address this issue, we propose an Iterative Associative Memory Model (IAMM)} for empathetic response generation. Specifically, we employ a novel second-order interaction attention mechanism to iteratively capture vital associated words between dialogue utterances and situations, dialogue history, and a memory module (for storing associated words), thereby accurately and nuancedly comprehending the utterances. We conduct experiments on the Empathetic-Dialogue dataset. Both automatic and human evaluations validate the efficacy of the model. Variant experiments on LLMs also demonstrate that attending to associated words improves empathetic comprehension and expression.",{fig 1} An example of iterative association. Words with the same color are associated. The memory stores the associated words.,"As an important task for improving dialogue quality, empathetic response generation aims to comprehend the emotional and cognitive states of the user in dialogue utterances and provide appropriate responses . The majority of methods treat the dialogue utterances as a long sequence to comprehend the user states . These approaches ignore the discrepancies in meanings among individual utterances, leading to inaccurate understanding of emotional and cognitive states. To address this issue, some methods comprehend more delicate emotional and cognitive states within a set of independent utterances by distinguishing self-other awareness or emphasizing emotion-intent transitions. However, the situation model, as an important theory for understanding empathy, posits that comprehending emotional and cognitive states in detail necessitates not only understanding independent utterances, but also iteratively associating pivotal associated words within those utterances. As shown in Figure , the speaker's independent utterances imply the emotion of ``anger,'' yet the overall expression conveys ``furious.'' If the utterances are understood independently, the listener is likely to misinterpret the speaker's emotion as ``anger.'' In contrast, the iterative association integrates subtle associated words, allowing for an accurate understanding of the dialogue. Specifically, when faced with the first utterance, the listener combines this utterance with related words from the situation and stores it in its memory to form an initial understanding. When encountering the speaker's second utterance, the listener meticulously compares and reasons this utterance with the dialogue history, situation, and related words in memory, to deepen its understanding of the utterance. For instance, associating ``jerks'' and ``guy'' reveals an intensification of the emotion of anger, i.e., ``furious''. Additionally, reasoning that ``cut me off'' and ``caused an accident'' makes it easier to realize the speaker's furious due to the life-threatening event. Overall, by associating explicit and implicit information, the listener attains a more nuanced understanding of the utterances. While this comprehension process proves effective, simulating it to achieve meticulous understanding of dialogues remains an open challenge. In this paper, we propose an iterative associative memory model (IAMM) that iteratively employs an information association module to identify and learn subtle connections within both explicit and implicit information. We first treat the dialogue content, including both the dialogue utterances and situations, as explicit information, and treat the reasoning knowledge about the dialogue content generated by COMET as implicit information. Subsequently, we iteratively utilize the information association module to identify and learn associated words between utterances and situations, dialogue history, and memory (initialized as an empty set) in the explicit/implicit information, and store them in the memory for a thorough understanding of the utterances. Specifically, the information association module, inspired by the idea that ""pages (nodes) linked by important pages (nodes) are also more important"", effectively identifies associated words in the to-be-associated sentences through a second-order interaction attention mechanism. To validate our model, we construct IAMM and IAMM$_{large}$ (LLMs-based model). Experiments are conducted on the Empathetic-Dialogue dataset. Both automatic evaluation and human evaluation demonstrate that compared with the state-of-the-art baselines, our models possess stronger understanding while expressing more informative responses. Overall, our contributions are as follows: {itemize} We introduce an iterative association framework for empathetic response generation, which simulates the human iterative process of understanding emotions and cognition. We propose an iterative associative memory model (IAMM), which iteratively employs a second-order interaction attention mechanism to capture subtle associations in dialogues. Experiments on the Empathetic-Dialogue dataset validate the efficacy of our models. {itemize"
Dr.Academy: A Benchmark for Evaluating Questioning Capability in Education for Large Language Models,2408.10947v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2408.10947v1_0.pdf,"Teachers are important to imparting knowledge and guiding learners, and the role of large language models (LLMs) as potential educators is emerging as an important area of study. Recognizing LLMs' capability to generate educational content can lead to advances in automated and personalized learning. While LLMs have been tested for their comprehension and problem-solving skills, their capability in teaching remains largely unexplored. In teaching, questioning is a key skill that guides students to analyze, evaluate, and synthesize core concepts and principles. Therefore, our research introduces a benchmark to evaluate the questioning capability in education as a teacher of LLMs through evaluating their generated educational questions, utilizing Anderson and Krathwohl's taxonomy across general, monodisciplinary, and interdisciplinary domains. We shift the focus from LLMs as learners to LLMs as educators, assessing their teaching capability through guiding them to generate questions. We apply four metrics, including relevance, coverage, representativeness, and consistency, to evaluate the educational quality of LLMs' outputs. Our results indicate that GPT-4 demonstrates significant potential in teaching general, humanities, and science courses; Claude2 appears more apt as an interdisciplinary teacher. Furthermore, the automatic scores align with human perspectives.",Comparison between general and educational questions.,"Large language models (LLMs) have demonstrated great performance in various natural language processing (NLP) tasks, including question answering, information retrieval, reasoning, and generation, etc. Beyond these general NLP applications, LLMs are also widely used in other domains, such as education. In the educational field, LLMs can now be used as substitutes for teachers. They can help automated teaching or assisted learning applications, thereby alleviating the pressure on human teachers. Additionally, LLMs can recommend appropriate elective courses based on a student's knowledge state, learning style, and interests, automatically generating practice problems of corresponding difficulty levels, and identifying areas where a student is struggling to provide targeted improvement. However, the capability of questioning is a crucial aspect in the educational field. As LLMs take on the role of teachers, can they pose high-quality questions like human educators? Therefore, evaluating what constitutes a high-quality question in education becomes necessary. According to Anderson and Krathwohl's educational taxonomy, we consider that high-quality questioning in the educational field must meet the following characteristics: i) achieve a higher level across the six domains including memory, understanding, application, analysis, evaluation, and creation; ii) be relevant to the given context; iii) comprehensively cover the content of the context, and iv) also reflect the important knowledge of this context. We consider that questions meeting these characteristics can effectively assess students' knowledge levels, and LLMs capable of posing such questions can assume the role of competent human educators. The first characteristic is the most basic requirement for LLMs to act as human teachers, while the following three characteristics measure the excellence of LLMs in their role as a teacher. Evaluating and enhancing the capability of LLMs to generate questions of high quality standards in the educational domain requires a benchmark. However, previous studies have mainly viewed LLMs from a student's perspective, focusing on tasks like reading comprehension and exam evaluations. However, these tasks focus on adopting contexts to passively answer questions or make reasoning, and these tests treat LLMs as students, assessing their abilities by how they answer questions, while the LLM's questioning capability through generating educational questions is under-studied. Current education-related research is far from adequate to determine LLMs' question raising capability as a teacher, and there isn't a benchmark that studies the overall teaching abilities of LLMs, seeing them as teachers. Although some role-playing tasks mimic professional dialogues but don't truly assess the LLMs' teaching capabilities. Therefore, if we want LLMs to assist in teaching effectively, we need to evaluate and enhance their teaching abilities, as possessing knowledge and guiding others to learn are distinct skills. Therefore, in this paper, we have developed a benchmark for assessing whether LLMs generate high-quality questions in the field of education, guided by professional educational theories. Unlike general questioning, as shown in Fig.~ (a), our benchmark requires that the generated questions not only be fluent and readable but also meet the fundamental characteristics proposed earlier (i.e. the first characteristic), as shown in Fig.~(b). Specifically, we draw on Anderson and Krathwohl's educational taxonomy to prompt LLMs to generate questions at six levels for each context. We select tasks from three domains, including general, single-discipline, and interdisciplinary domains, to more comprehensively assess the strengths of LLMs as teachers in various fields. Based on the four characteristics proposed earlier, we have also designed four evaluation metrics: consistency, relevance, coverage, and representativeness, to assess the value of questions posed by LLMs in the educational domain, thereby comprehensively evaluating the questioning capability of LLMs as teachers in education through evaluating their generated educational questions. Our experiments reveal that LLMs like GPT-4, Claude2, and GPT-3.5 demonstrate good questioning capability across domains as teachers in education through evaluating their generated educational questions. In summary, our contributions are threefolds: {itemize} We introduce the problem of evaluating questioning capability in education as a teacher for LLMs through evaluating their generated educational questions, building a framework based on educational theory that includes six cognitive levels and tasks from three different domains. We establish four evaluation metrics to assess the questioning capability in education as a teacher of LLMs through evaluating their generated educational questions. We conduct experimental evaluations of 11 LLMs, providing quantitative standards and subject orientations for each LLM's questioning capability as a teacher. {itemize"
UniBridge: A Unified Approach to Cross-Lingual Transfer Learning for Low-Resource Languages,2406.09717v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.09717v3_0.png,"In this paper, we introduce UniBridge (Cross-Lingual Transfer Learning with Optimized Embeddings and Vocabulary), a comprehensive approach developed to improve the effectiveness of Cross-Lingual Transfer Learning, particularly in languages with limited resources. Our approach tackles two essential elements of a language model: the initialization of embeddings and the optimal vocabulary size. Specifically, we propose a novel embedding initialization method that leverages both lexical and semantic alignment for a language. In addition, we present a method for systematically searching for the optimal vocabulary size, ensuring a balance between model complexity and linguistic coverage. Our experiments across multilingual datasets show that our approach greatly improves the F1-Score in several languages. UniBridge is a robust and adaptable solution for cross-lingual systems in various languages, highlighting the significance of initializing embeddings and choosing the right vocabulary size in cross-lingual environments.","Some languages/scripts are not covered in the pre-trained corpora. Hence, the pre-trained tokenizer will eventually produce many unknown tokens which corrupts the sentence's meaning and results in poor performance.","Recently, multilingual pre-trained language models (LMs) have significantly advanced natural language processing (NLP) tasks, narrowing the performance gap between English and various other languages. Multilingual pre-trained models such as XLM-R and mBERT are currently strong models for effectively cross-lingual transfer . However, these models pose a limitation that they are pre-trained on a limited set of approximately 100 languages, leaving a substantial void for the vast array of the world's nearly 7000 languages . The resultant disparity disproportionately affects low-resource languages that are not covered in their pre-trained corpora , impeding their performance compared to their high-resource counterparts. {-0.4cm} Recent efforts propose the use of adapters to mitigate the knowledge gap in low-resource languages prior to transferring knowledge for specific tasks . These methods adapt the pre-trained LMs to a new language by utilizing monolingual data, enabling the model to acquire a robust representation of the target language before receiving knowledge from the source language. Despite enhanced performance in languages not included in the pre-trained corpora, these approaches still exhibit poor performance in languages with unseen scripts (i.e., the scripts that are not presented in the pre-training corpora; see Figure ). To address the issue of unseen scripts, existing studies propose acquiring a new vocabulary embedding for newly discovered languages. However, these methods heavily rely on manually configuring the vocabulary size and initializing the embedding matrix. Furthermore, recent Cross-Lingual Transfer Learning studies focus on English due to its abundant pre-trained data and impressive task performance, our experiments reveal that high performance in English tasks does not necessarily guarantee successful transfer to other languages, particularly low-resource languages. Therefore, we suggest an automated method utilizing the LMs to identify the most suitable set of source languages for knowledge aggregation, leading to notable performance improvements over single-source language transfer. Our research empirically tested the effectiveness of newly random initialized embeddings and fixed vocabulary size. We then introduce an efficient technique for determining the optimal vocabulary size for new languages, utilizing the syntactic and semantic insights from the pre-trained LMs. In addition, we present an innovative method for transferring knowledge from multiple sources, which allows the model to choose the best combination of source languages to improve the overall performance. Our results contribute to the ongoing discussion about managing linguistic diversity in NLP, particularly for languages with limited resources, emphasizing the importance of a detailed and inclusive strategy in creating multilingual pre-trained LMs. We evaluate our approach on sequence tagging tasks (e.g. NER, POS) and classification (e.g. NLI) with two strong baselines, mBERT and XLM-R, and observe a significant increase in the F1 and accuracy score {{https://github.com/VinAIResearch/UniBridge}}. In summary, our contributions are: {itemize} We propose a novel approach to automatic search for a suitable vocabulary size to adapt to a new language. {-3mm} We propose a new strategy to initialize the embedding that leverages the syntactic and semantic knowledge encoded in the pre-trained LMs to address the missing tokens when adapting to low-resource languages. {-3mm} We propose a method to aggregate multi-source transfer learning to enhance the performance on cross-lingual transfer tasks. We show that multi-source can outperform effective multi-language learning. {itemize"
VISTA: Visualized Text Embedding For Universal Multi-Modal Retrieval,2406.04292v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.04292v1_0.pdf,"Multi-modal retrieval becomes increasingly popular in practice. However, the existing retrievers are mostly text-oriented, which lack the capability to process visual information. Despite the presence of vision-language models like CLIP, the current methods are severely limited in representing the text-only and image-only data. In this work, we present a new embedding model {VISTA} for universal multi-modal retrieval. Our work brings forth threefold technical contributions. Firstly, we introduce a flexible architecture which extends a powerful text encoder with the image understanding capability by introducing visual token embeddings. Secondly, we develop two data generation strategies, which bring high-quality composed image-text to facilitate the training of the embedding model. Thirdly, we introduce a multi-stage training algorithm, which first aligns the visual token embedding with the text encoder using massive weakly labeled data, and then develops multi-modal representation capability using the generated composed image-text data. In our experiments, VISTA achieves superior performances across a variety of multi-modal retrieval tasks in both zero-shot and supervised settings. Our model, data, and source code are available at {https://github.com/FlagOpen/FlagEmbedding}.","The model architecture of our VISTA model. We use the pre-trained language model as the foundation, making the ViT encoder transfer the Image to recognized tokens of the text encoder.","Information retrieval (IR) is a critical task in many real-world scenarios, e.g., search engines, open-domain question answering, and retrieval augmented generation . It aims to find relevant data from a large database such that the downstream problems can be faithfully solved on top of proper knowledge. One important IR paradigm is dense retrieval, where the query and candidates, i.e. document, are represented as embeddings, and their semantic relationship can be reflected by the embedding similarity . With the continual progress on pre-trained model and training algorithm, increasingly powerful embedding models have been developed, such as DPR , Contriever , GTR , E5 , BGE , etc., which substantially improves the quality and universality of dense retrieval. Most of the existing dense retrieval models are text-oriented, which can only deal with the data presented in human language. However, a large portion of the world knowledge naturally contains both text and image, e.g., web articles with visual illustration ; meanwhile, people's queries can also be flexibly expressed with multiple data modalities, e.g., search queries with exemplar images . Despite the development of visual-language representation models (VLM), like CLIP and ALIGN , the above problem is still challenging in many perspectives. On one hand, the existing VLMs are severely limited in text representation capability, whose retrieval performance is far behind the recent text-only embedding models, like E5 and BGE. On the other hand, the existing VLMs focus more on the independent encoding of text and image; nevertheless, the joint representation of image-text data (e.g., documents with illustrations) is largely unexplored. In this work, we propose {VIS}ualized {T}ext embedding for universal multi-modal retriev{A}l, namely {VISTA}. It takes the best of the existing text encoder and image encoder where high-quality multi-modality embedding can be generated from it. In particular, our work presents the following three technical contributions. First of all, we come up with a flexible {model architecture} to facilitate the generation of multi-modal embedding. It is built upon a powerful and well-trained text encoder, which exhibits proficient text retrieval capability. Meanwhile, it makes the incorporation of visual tokens generated by an expressive image encoder, thereby augmenting the capability of image processing. Such an architecture brings forth two important advantages. 1) It establishes the {in-depth fusion} of text and image data, which substantially contributes to the quality of multi-modal embedding. 2) It also enables the {preservation of the original performance} of text embedding, as the text encoder is fully fixed while the visual tokens are incorporated. Secondly, we propose two innovative pipelines for the automatic generation of {Image-Text Composed datasets}, thereby securing large-scale, high-quality data for the training of multi-modal embedding models. These pipelines are designed to cater to scenarios where either the {query} or the {candidate} comprises image-text pairs, thereby facilitating the model to adapt to a diverse range of multi-modal retrieval situations. Thirdly, we design a two-stage training algorithm to learn the multi-modal embedding model. Initially, we perform the basic text-to-image matching task with massive weakly-labeled cross-modal data, which aligns the visual token embedding with the text encoder. Subsequently, we perform composed text\&image matching with our generated composed image-text datasets, which establishes the multi-modal representation capability for the embedding model. VISTA is empirically verified by comprehensive experiments. Particularly, it achieves superior performance across various multi-modal retrieval tasks in both zero-shot and supervised settings. Without any task-specific optimization, VISTA is able to outperform or match the leading approach in every downstream evaluation scenario. Besides, VISTA's performance can also be substantially improved if it is continually fine-tuned for corresponding tasks"
T2S-GPT: Dynamic Vector Quantization for Autoregressive Sign Language Production from Text,2406.07119v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.07119v1_0.pdf,"In this work, we propose a two-stage sign language production (SLP) paradigm that first encodes sign language sequences into discrete codes and then autoregressively generates sign language from text based on the learned codebook. However, existing vector quantization (VQ) methods are fixed-length encodings, overlooking the uneven information density in sign language, which leads to under-encoding of important regions and over-encoding of unimportant regions. To address this issue, we propose a novel dynamic vector quantization (DVA-VAE) model that can dynamically adjust the encoding length based on the information density in sign language to achieve accurate and compact encoding. Then, a GPT-like model learns to generate code sequences and their corresponding durations from spoken language text. Extensive experiments conducted on the PHOENIX14T dataset demonstrate the effectiveness of our proposed method. To promote sign language research, we propose a new large German sign language dataset, PHOENIX-News, which contains 486 hours of sign language videos, audio, and transcription texts. Experimental analysis on PHOENIX-News shows that the performance of our model can be further improved by increasing the size of the training data. Our project homepage is .",Comparison of fixed-length encoding and variable-length encoding.,"Sign language is a visual language with complex grammatical structures and is the primary means of communication for nearly 70 million deaf people worldwide {According to World Federation of the Deaf {https://wfdeaf.org/our-work/}}. Research on sign language production and sign language translation has attracted widespread attention. Sign language production (SLP) is a challenging problem that aims to automatically translate spoken language descriptions into corresponding continuous sign sequences. SLP can help deaf people better access information and communicate with others, thereby facilitating their lives, which has important social significance. SLP models are expected to learn precise mapping from the spoken language space to the sign language space. Early work used 2D or 3D skeleton poses to represent sign language , while recent work has suggested using 3D human models, such as SMPL-x, to represent sign language, as it introduces human priors and can better animate . To learn the mapping between these two different modal spaces, some work uses autoregressive models , non-autoregressive models , or diffusion models to learn the direct mapping from spoken language text to sign language skeleton poses. proposed to learn the discrete representation of sign language through VQ-VAE and then learn the mapping from text to discrete representation through a discrete diffusion model. However, we found that existing sign language discrete representation methods are fixed-length encodings, as shown in {fig:introduction}, which overlooks the uneven information density in sign language. In addition, many existing works rely on expert-annotated intermediate representations, i.e. glosses, which limit the scalability of the model. In this work, we are inspired by recent advances from learning the discrete representation for generation . Specifically, we investigate a two-stage framework based on Dynamic Vector Quantized Variational Autoencoders (DVQ-VAE) and Generative Pre-trained Transformer (GPT) for text-to-sign language production. In the first stage, as shown in {fig:introduction}, DVQ-VAE will learn the weights of each frame and the boundaries of the basic semantic units. Then, the weighted latent vectors are mapped to discrete code indices. Further quantitative analysis of the uneven information density in sign language is provided in . To encourage models to perform variable-length encoding and compress sequence lengths, we propose a novel budget loss. Additionally, to preserve the semantic information of the reconstructed sign language sequences, we also introduce a translation auxiliary loss. In the second stage, a GPT-like model is learned to to generate code index sequences from spoken language text. Furthermore, since the duration of quantized code in a sequence can also vary dynamically, we further propose a duration transformer to predict the duration of the next code based on the previous code's duration and the current code. The experimental results on the widely used SLP dataset PHOENIX14T demonstrate that our proposed method achieves superior back translation performance compared to previous approaches. Furthermore, throughout the entire development process of image generation and text generation, the scale of the dataset has played a crucial role. A large amount of high-quality corpus is also very important for SLP tasks. In this paper, we present the largest known German Sign Language dataset, PHOENIX-News, which consists of 486 hours of sign language videos, audio, and transcription texts. The native expression, clear hand details, and extensive coverage of our large-scale dataset make it suitable for a variety of sign language research tasks, such as sign language translation and sign language production. Based on this dataset, we further explore the impact of training data size on SLP tasks. Empirical analysis shows that the performance of our model can be further improved by increasing the size of the training data. Our main contributions are summarized as follows: {itemize} We analyse the uneven information density in sign language. Additionally,we propose for the first time an information density based variable length coding method suitable for sign language. We propose a two-stage SLP framework consisting of two components: 1) DVQ-VAE to dynamically assign variable-length codes to sequences based on their different information densities through a novel {adaptive downsampling module} and {budget loss}. 2) A novel {T2M-GPT model} to predict variable-length codes and their corresponding durations. Extensive experiments on the challenging PHOENIX14T dataset show the effectiveness of our proposed method. We propose the largest known German sign language dataset, PHOENIX-News, which can be used for a variety of sign language research tasks. {itemize"
OceanGPT: A Large Language Model for Ocean Science Tasks,2310.02031v8,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2310.02031v8_0.pdf,"Ocean science, which delves into the oceans that are reservoirs of life and biodiversity, is of great significance given that oceans cover over 70\% of our planet's surface. Recently, advances in Large Language Models (LLMs) have transformed the paradigm in science. Despite the success in other domains, current LLMs often fall short in catering to the needs of domain experts like oceanographers, and the potential of LLMs for ocean science is under-explored. The intrinsic reasons are the immense and intricate nature of ocean data as well as the necessity for higher granularity and richness in knowledge. To alleviate these issues, we introduce {}, the first-ever large language model in the ocean domain, which is expert in various ocean science tasks. We also propose {}, a novel framework to automatically obtain a large volume of ocean domain instruction data, which generates instructions based on multi-agent collaboration. Additionally, we construct the first oceanography benchmark, {}, to evaluate the capabilities of LLMs in the ocean domain. Though comprehensive experiments, {} not only shows a higher level of knowledge expertise for oceans science tasks but also gains preliminary embodied intelligence capabilities in ocean technology.",Capabilities of {\ours}. Our proposed model not only shows a higher level of knowledge expertise for oceans science tasks but also gains preliminary embodied intelligence capabilities in ocean technology.,"Ocean science, which delves into the intricacies of oceans that cover over 70\ Recently, advances in Large Language Models (LLMs) have transformed the paradigm in science domains such as medical science , molecular science , protein science and geoscience . However, the potential for the large language model in ocean science is under-explored. Despite remarkable success in general domain, current LLMs still do not fully meet the specific demand of oceanographers. This inadequacy is primarily due to: (1) The immense volume and intricate nature of ocean data. As ocean science research progresses, acquiring data becomes increasingly challenging, which makes enhancing the oceanic understanding both a golden opportunity and a significant hurdle. (2) The necessity for higher granularity and richness in knowledge. Note that the data requirements faced by researchers are becoming increasingly intricate and diverse. Ocean science encompasses various domains and subjects, each with its distinct data attributes and patterns. To alleviate these issues, we introduce {}, the first-ever LLM in the ocean domain, which is expert in various ocean science tasks. Specifically, we propose {}, an efficient ocean science instruction generation framework that capitalizes on multi-agent collaboration. Each agent in our designed framework is considered as an expert in a specific domain (science and research, resources and development, ecology and environment etc.) and is responsible for generating the corresponding data. For the advancement of ocean science research using LLMs, we also create a benchmark called {} to evaluate the capabilities in ocean science tasks. Through extensive experiments, {} shows superiority for diverse ocean science tasks. Note that our benchmark data is based on criteria manually evaluated by ocean experts, and can accurately reflect the capabilities that LLMs possess in the field of ocean science. As depicted in Figure , our model can comprehensively answer questions according to the instructions of oceanographers, which demonstrates its expertise in oceanography. We further explore the potential of {} from the perspectives of ocean engineering. Specifically, we integrate ocean robotics instructions into the training data and evaluate its ability via code or console commands. {} not only demonstrates a higher level of knowledge expertise but also gains preliminary embodied intelligence capabilities in ocean technology. Our contributions can be summarized as follows: {itemize} We introduce {}, the first ocean LLM, which shows superiority for various ocean science tasks. It can answer oceanographic questions according to the instructions of oceanographers, demonstrating expertise in oceanography. We propose {}, an automated domain instruction evolving framework that constructs the ocean instruction dataset by multi-agent collaboration. Our framework effectively alleviates the difficulty of obtaining ocean domain data. Extensive experiments demonstrate the superiority of {} in the {}. {} not only demonstrates a higher level of knowledge expertise for oceans science tasks but also gains preliminary embodied intelligence capabilities. {itemize"
BIPED: Pedagogically Informed Tutoring System for ESL Education,2406.03486v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.03486v1_0.png,"Large Language Models (LLMs) have a great potential to serve as readily available and cost-efficient Conversational Intelligent Tutoring Systems (CITS) for teaching L2 learners of English. Existing CITS, however, are designed to teach only simple concepts or lack the pedagogical depth necessary to address diverse learning strategies. To develop a more pedagogically informed CITS capable of teaching complex concepts, we construct a BIlingual PEDagogically-informed Tutoring Dataset ({BIPED}) of one-on-one, human-to-human English tutoring interactions. Through post-hoc analysis of the tutoring interactions, we come up with a lexicon of dialogue acts (34 tutor acts and 9 student acts), which we use to further annotate the collected dataset. Based on a two-step framework of first predicting the appropriate tutor act then generating the corresponding response, we implemented two CITS models using GPT-4 and SOLAR-KO, respectively. We experimentally demonstrate that the implemented models not only replicate the style of human teachers but also employ diverse and contextually appropriate pedagogical strategies.","Example of our dataset, BIPED. It includes a series of dialogues between a tutor and a student, annotated with dialogue acts, content information, and the correctness of student responses.","As Large Language Models (LLMs) such as GPT revolutionize the field of natural language generation, both researchers and practitioners have put an increasing amount of effort into developing Conversational Intelligent Tutoring Systems (CITS) that leverage the generative capabilities of LLM’s. Specifically, LLMs have the potential to teach English as a Second/Foreign Language (ESL/EFL), for they may serve as readily-available tutors that can emulate native-speaking contexts. However, most CITS proposed in literature for teaching ESL are restricted in scope and lack pedagogical depth. For example, works based on CIMA dataset consider toy concepts of colored shapes and prepositions. Possibly due to limited scope, CIMA only considers a five tutor act classes (“hint”, “question”, “correction”, “confirmation”, and “other”). Similarly, the TSCC dataset only considers abstract tutor act classes such as “scaffolding”, and does not consider a more granular set of teaching strategies (e.g., inferential clues, teaching synonyms or antonyms, etc.). Consequently, models trained on CIMA and TSCC are limited to employing simplistic instructional strategies. The goal of our work is to develop CITS that can teach ESL by applying pedagogically meaningful teaching strategies adaptively to student’s needs. To do so, we designed a tutoring session that focuses on comprehending a real news article, covering vocabulary, grammar, and relevant cultural contexts. Based on this, we construct a BIlingual PEDagogically-informed Tutoring Dataset (BIPED). For BIPED, we collected a bilingual, one-on-one human-to-human tutoring dataset. Thereafter, we analyzed the dataset post-hoc from a pedagogical viewpoint and developed a categorization of dialogue acts, which comprises 34 tutor acts and 9 student acts. Finally, we annotated the data using the defined dialogue act categories. As for the development of CITS, we employ the framework whereby the LLM first chooses the suitable tutor act, then generates the corresponding utterance. We believe this approach enables the model to generate a more focused response that does not deviate from the chosen tutor intent. We consider two implementations of such CITS, one based on GPT-4 prompting, and another based on fine-tuning SOLAR-KO 10.7B, which is a public model trained on a Korean corpus. For GPT-4, our dataset is used as examples for in-context learning. For fine-tuning, we explored a multi-task learning approach, where the model is instruction-tuned to carry out tutor act prediction and response generation, as well as other tasks for context grounding. In our experiments, we show through various metrics that the implemented models are capable of mimicking human teacher’s utterance style, as well as their pedagogical strategies. Specifically, we show that our models choose appropriate and diverse tutor acts, and generate human-like utterances, as measured by sentence similarity metrics. The contributions of our paper are as follows: {itemize}[leftmargin=*] We provide BIPED, a dataset curated from bilingual human-to-human tutoring dialogues that come annotated with specific tutoring acts. Importantly, the lesson topic is complex enough to require sophisticated pedagogical methods. We implement two CITS based on BIPED. In particular, we explore both in-context learning and fine-tuning approaches. It is the pioneering work in utilizing instruction tuning techniques in the development of CITS. We provide experimental results demonstrating the implemented models' ability to mimic human tutors. {itemize"
Progressively Modality Freezing for Multi-Modal Entity Alignment,2407.16168v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2407.16168v1_0.png,"Multi-Modal Entity Alignment aims to discover identical entities across heterogeneous knowledge graphs. While recent studies have delved into fusion paradigms to represent entities holistically, the elimination of features irrelevant to alignment and modal inconsistencies is overlooked, which are caused by inherent differences in multi-modal features. To address these challenges, we propose a novel strategy of progressive modality freezing, called PMF, that focuses on alignment-relevant features and enhances multi-modal feature fusion. Notably, our approach introduces a pioneering cross-modal association loss to foster modal consistency. Empirical evaluations across nine datasets confirm PMF's superiority, demonstrating state-of-the-art performance and the rationale for freezing modalities. Our code is available at~{https://github.com/ninibymilk/PMF-MMEA}.",Illustration of irrelevant vs. relevant features in multi-modal knowledge graphs.,"Multi-modal Knowledge Graphs (MMKGs) integrate various modalities, including text, vision, and structural data, to provide comprehensive insights into knowledge. This integration underpins a range of applications, from question answering and information retrieval, to recommendation systems. Multi-Modal Entity Alignment (MMEA) aims to identify identical entities across heterogeneous MMKGs, which is essential for the integrity of the knowledge represented within these KGs. The essence of MMEA lies in identifying feature commonalities across entities from varied modalities to determine their alignment. The diversity in KG construction introduces potential mismatches in multi-modal features of entities meant to be aligned. For example, Figure~ depicts how the entity ``Interstellar (film)'' might be represented in one knowledge graph \({G} \) by a poster image of a spaceship, whereas in another knowledge graph \({G'} \), by a portrait of Anne Hathaway, the starring actress. Although both images are related to ``Interstellar (film)'', their disparate perspectives can weaken the semantic relationship, challenging the alignment task. This scenario underscores the problem of { alignment-irrelevant features}, distinctly different features that complicate accurate entity alignment. While recent studies have employed attention mechanisms to calculate cross-modal weights at both the KG and entity levels, they overlook the critical step of evaluating modality relevance, thereby neglecting to exclude these irrelevant features. Another obstacle in MMEA involves ensuring semantic consistency across different modalities. Achieving consistent representations across modalities is crucial for effective entity alignment. Existing methods utilize contrastive learning to minimize the intra-modal inconsistencies among aligned entities, yet overlook the { cross-modal inconsistencies}. Furthermore, the presence of alignment-irrelevant features exacerbates these inconsistencies, complicating the task of achieving consistent representations across modalities. To address the aforementioned challenges, we propose a novel method for multi-modal entity alignment, named Progressive Modality Freezing (PMF), designed to effectively filter out irrelevant features. PMF is structured around three key parts: multi-modal entity encoders, progressive multi-modality feature integration, and a unified training objective for MMEA. Initially, PMF separately encodes entity features from each modality, allowing for flexible adjustment of input modalities. Following this, the method employs a progressive approach to selectively freeze features deemed less relevant for alignment, simultaneously integrating valuable multi-modal features based on their alignment relevance. The culmination of our strategy is a unified training objective, aimed at minimizing the discrepancies both within individual KGs and across different modalities. To encapsulate, our research contributes in the following three-folds: {itemize} We propose the Progressive Modality Freezing (PMF) strategy to tackle the challenge of irrelevant features in MMEA. By assigning alignment-relevance scores to modalities, our method progressively freezes features that are alignment-irrelevant, while seamlessly integrating beneficial multi-modal information, ensuring the emphasis remains on features of utmost relevance. We are at the forefront of employing contrastive learning to address modality consistency across multiple modalities, accompanied by a unified training objective to enhance cross-modal consistency. We confirm the effectiveness of PMF across diverse datasets and experimental settings, where it demonstrates superior performance, achieving state-of-the-art in the field. Our thorough analysis further elucidates the advantage of the feature-freezing strategy. {itemize"
What Does the Bot Say? Opportunities and Risks of Large Language Models in Social Media Bot Detection,2402.00371v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.00371v2_0.pdf,"Social media bot detection has always been an arms race between advancements in machine learning bot detectors and adversarial bot strategies to evade detection. In this work, we bring the arms race to the next level by investigating the opportunities and risks of state-of-the-art large language models (LLMs) in social bot detection. To investigate the opportunities, we design novel LLM-based bot detectors by proposing a mixture-of-heterogeneous-experts framework to divide and conquer diverse user information modalities. To illuminate the risks, we explore the possibility of LLM-guided manipulation of user textual and structured information to evade detection. Extensive experiments with three LLMs on two datasets demonstrate that instruction tuning on merely 1,000 annotated examples produces specialized LLMs that outperform state-of-the-art bot detection baselines by up to 9.1\% on both datasets. On the other hand, LLM-guided manipulation strategies could significantly bring down the performance of existing bot detectors by up to 29.6\% and harm the calibration and reliability of bot detection systems. Ultimately, this works identifies LLMs as the new frontier of social bot detection research.{https://github.com/BunsenFeng/botsay}.}",Overview of the opportunities of LLM-based bot detectors and risks of LLM-based evasive bots.,"Social media bot accounts are behind many online perils such as misinformation , election interference , extremist campaigns , and conspiracy theories . Research on detecting social media bots has always been an {arms race} : early methods focus on analyzing user metadata with machine learning classifiers , while bot operators manipulate user features to evade detection ; later approaches employed word embeddings and encoder-based language models to characterize user texts , while bot operators re-post genuine content to dilute malicious content and appear innocuous ; recent models tap into the network information of user interactions with graph neural networks , while advanced bots strategically follow and unfollow users to appear out-of-distribution . Recent advances brought us large language models (LLMs) that excel in academic tasks and benchmarks , capable of following instructions , but they also come with risks and biases that could cause real-world harms . In this work, we ask: {What are the opportunities and risks of large language models in social bot detection?} As the arms race escalates, we focus on how state-of-the-art large language models could aid robust bot detection systems and how LLMs might be maliciously employed to design more evasive bots. For {opportunities}, we propose a mixture-of-heterogeneous-experts framework, employing LLMs to divide and conquer various user information modalities such as metadata, text, and user interaction networks. For user metadata, we verbalize categorical and numerical user features in natural language sequences and employ in-context learning for bot detection. For user-generated texts, we retrieve similar posts from an annotated training set as in-context learning examples. For the network information, guided by previous works about LLMs' graph reasoning capabilities , we include the user's following information, in either random or similarity-based order, as part of the prompt context to aid detection. These modality-specific LLMs are then used through in-context learning prompting or instruction tuning, and modality-specific results are ensembled through majority voting. For {risks}, we investigate the possibility of LLM-guided bot design to evade detection by tampering with the textual and structural information of bot accounts. For textual information, we explore rewriting user posts with LLMs to appear genuine with four mechanisms: 1) zero-shot prompting; 2) few-shot rewriting to imitate the posts of genuine users; 3) interactive rewriting between LLMs and an external bot classifier; 4) synthesizing the attributes of related posts from bots and humans for style transfer. For structural information, we employ LLMs to suggest new users to follow or existing users to unfollow, editing the neighborhood of bot accounts. LLM-guided manipulation of textual and structural features is then merged to produce LLM-guided social media bots. We conduct extensive experiments with three LLMs on two standard bot detection datasets to evaluate the proposed detectors and manipulation strategies. We find that on the {opportunities} side, LLMs are liable to become state-of-the-art detectors: while in-context learning struggles to capture the nuances of bot accounts, instruction tuning outperforms baselines by up to 9.1\ Our work opens up new research avenues in the ever-lasting arms race between researchers and bot operators, focusing on LLMs as the new frontier of social bot detection research"
Self-Contrast: Better Reflection Through Inconsistent Solving Perspectives,2401.02009v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2401.02009v3_0.pdf,"The reflection capacity of Large Language Model (LLM) has garnered extensive attention. A post-hoc prompting strategy, e.g., reflexion and self-refine, refines LLM's response based on self-evaluated or external feedback. However, recent research indicates without external feedback, LLM's intrinsic reflection is unstable. Our investigation unveils that the key bottleneck is the quality of the self-evaluated feedback. We find LLMs often exhibit overconfidence or high randomness when self-evaluate, offering stubborn or inconsistent feedback, which causes poor reflection. To remedy this, we advocate {Self-Contrast}: It adaptively {explores} diverse solving perspectives tailored to the request, {contrasts} the differences, and {summarizes} these discrepancies into a checklist which could be used to re-examine and eliminate discrepancies. Our method provides LLM with diverse perspectives to alleviate stubborn biases. Moreover, their discrepancies indicate potential errors or inherent uncertainties that LLM often overlooks. Reflecting upon these can prompt more accurate and stable reflection. Experiments conducted on a series of reasoning and translation tasks with different LLMs serve to underscore the effectiveness and generality of our strategy.","LLMs evaluate the initial response and provide feedback for revision. However, most erroneous responses remain uncorrected after reflection as the feedback is either overconfident (46.7\%) or inconsistent (45.7\%). Bottom: Self-Contrast \color{black}{explores} \color{black}multiple solving perspectives, and \color{black}{contrast} \color{black} their differences, and \color{black}{summarize} \color{black}them into insightful checklist for self-correction.","Mastering reasoning and decision-making capabilities is of utmost importance to paving the way for artificial general intelligence. Recently, large language models (LLMs) and applications built on them demonstrate impressive capabilities in various domains, especially combined with Chain-of-Thought, ReAct, Tree-of-Thought and other prompting techniques. Despite these advancements, LLMs are not entirely reliable since they frequently produce inaccuracies results, such as misunderstanding a key concept, overlooking some crucial details. A post-hoc prompting strategy, e.g., self-reflection, garnered considerable attention. It first generates an initial response (Initial Response), then gathers external feedback or self-evaluated feedback (Evaluation Phase) to refine prior response (Revision). Numerous studies proclaim this three-stage strategy (Initial Response$$Evaluation$$Revision), can endow LLMs with the potential to self-correct previous imperfect responses. For a time, this belief appeared to dominate the community. However, recent studies have cast doubt on LLM's inherent reflection capability. Their research indicates that without external feedback, LLMs have difficulties in amending prior responses. It implies self-correction is unreliable when relying only on LLM itself and simple post-hoc prompting strategies. We are also intrigued by LLM's internal reflection ability, as external feedback is not available in most scenarios. Our initial experiments ($$~) indicate that intrinsic reflection has limited effect. Across various LLMs and tasks, the performance gains from reflection are not significant, and occasionally detrimental. In cases of incorrect initial responses, only 15.1\ To ascertain the reasons for that, we further analyze the feedback generated by the self-evaluate process. As shown in {figure1}, LLMs often provide two unexpected feedback: {inparaenum}[1)] {Overconfidence} (46.7\ {Inconsistency} (45.7\ {inparaenum} These two feedbacks seriously undermine the effectiveness of reflection. It reveals that such a simple self-evaluate strategy faces difficulty in accurately identifying errors and consistently generating high-quality feedback for reflection. As a remedy, we propose a contrastive strategy as an alternative to the direct evaluation: we examine the differences among multiple responses and draw inspiration to derive feedbacks from their disparities for reflection. The insight is that while generating accurate feedback directly may be challenging, identifying contrasts between various responses is often more feasible. More importantly, these discrepancies often indicate some potential errors, easily overlooked details or pitfalls. As shown in {figure1}, by contrasting two solutions, LLM finds they have different solving objectives, and suggests re-examining the intent of the original request in the checklist. This contrasting paradigm can also be seen in some contemporaneous work. Embracing this philosophy, we advocate Self-Contrast, which steers LLM to autonomously create diverse solving perspectives by self-curated prompts and then select different results with significant discrepancies for comparison. Then LLM reflects on the reasons behind these discrepancies and generates multiple re-examining instructions, i.e., checklist, for reflection. Our experiments show that by creating diverse perspectives adaptively, Self-Contrast can mitigate biases introduced by specific prompts. Moreover, contrasting the discrepancies between perspectives inspires deeper reflection, thereby enhancing the likelihood of accurate self-correction. As shown in Figure, Self-Contrast consists of three procedures: {Self-Curated Prompts, Graph Construction}, and {Graph Editing}. In the {Self-Curated Prompts} phase, we initially steer LLMs to dynamically synthesize an array of prompts based on the user request. Each prompt embodies a unique solving perspective tailored to user requests. It means LLMs can autonomously design multiple suitable roles, personas, and instructions for user requests, rather than using manually written prompts by humans. Subsequently, LLMs employ these self-generated prompts to tackle the user request in parallel. As shown in Figure, for math reasoning, LLMs consider input questions from different perspectives (e.g., top-down, bottom-up reasoning). For translation, LLMs design specific roles and personalities from four perspectives: a rigorous literal translator, a creative idiomatic interpreter, a native individual attuned to local culture, and an expert in the domain of military affairs. These different perspectives provide divergent understandings and analyses of the given context. Additionally, we devise a special perspective for LLMs: a careless persona who deliberately generates an incorrect solving process. This erroneous result contains a set of common mistakes, serving as a ""negative"" example for subsequent graph-based reflection. Subsequently ({graph construction} phase), to clearly delineate the inconsistencies among different solving perspectives, we construct a contrastive graph based on their inter-relations: LLM treats each solving perspective and its result as an individual node, then clusters these nodes based on their similarity, i.e., grouping similar nodes into the same sub-graph and separating dissimilar nodes from each other. Later, the LLM aggregates the nodes in each sub-graph and compares the differences between sub-graphs. Eventually, these differences are represented as edges in the graph. In the {Contrastive reflection} phase, LLM is instructed to revise each inconsistent node on the graph, eliminating the differences represented by each edge. Our contributions can be summarized as: {itemize} {}{2pt} {}{0pt plus 1pt} Our comprehensive experiments unveil that the bottleneck for poor reflection performance lies in the LLM's inability to accurately evaluate prior responses. It often manifests as overconfident or inconsistent feedback, hindering the effectiveness of self-reflection. We advocate Self-Contrast: the LLMs create multiple solving perspectives for diverse results, mitigating overconfident biases of a singular prompt. Then drawing inspiration from contrasting different perspectives, the LLMs summarize more accurate checking instructions to resolve discrepancies and enhance reflection. Empirically, compared with vanilla reflection, Self-Contrast shows significant improvements and stability in both mathematical reasoning and challenging translation scenarios. {itemize"
When Good and Reproducible Results are a Giant with Feet of Clay: The Importance of Software Quality in NLP,2303.16166v5,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2303.16166v5_0.png,"Despite its crucial role in research experiments, code correctness is often presumed solely based on the perceived quality of results. This assumption, however, comes with the risk of erroneous outcomes and, in turn, potentially misleading findings. To mitigate this risk, we posit that the current focus on reproducibility should go hand in hand with the emphasis on software quality. We support our arguments a case study in which we identify and fix three bugs in widely used implementations of the state-of-the-art Conformer architecture. As countermeasures, we release , a library dedicated to testing neural models, and propose a Code-quality Checklist, with the goal of promoting coding best practices and improving software quality within the NLP community.",Convolution module in the Conformer encoder layer. Convolutional blocks are 1D convolutions.,"In the field of natural language processing (NLP), as well as in broader contexts, the validity and soundness of research findings are typically upheld by {{establishing consistency of results versus existing implementations, standard benchmarks, or sanity checks via statistically significant experimental results}} . Embracing these recommendations as the exclusive criteria for validating scientific credibility, the research community has recently devoted significant attention to the reproducibility and the soundness of experimental settings and comparisons . Specifically, in response to evidence indicating the absence of these aspects in many research papers and to mitigate the so-called ``{reproducibility crisis}'' ,{The term ``{reproducibility crisis}'' refers to the increasing difficulties reported by scientists in replicating others' and own works , also in the specific context of NLP .} top-tier conferences have introduced dedicated checklists and targeted questions in the reviewing forms . However, a fundamental question remains regarding the initial assumption: {are reproducibility and thorough evaluation against robust baselines sufficient to ensure the soundness of a research finding?} According to , reproducibility alone does not {{guarantee the quality, correctness, or validity of the published results}} since the code employed to produce them may not accurately execute its intended purpose. This entails inherent risks, as flawed code that produces good and easily reproducible results can propagate as the foundation for further research, ultimately leading to further unreliable and potentially misleading findings . Expanding on these observations, this paper is a call to action, underpinned by empirical evidence, to bolster the dependability of NLP findings by complementing current initiatives toward reproducibility and experimental soundness with equal emphasis on {software quality}. To this end, we adopt as a reference framework the principles of software quality assurance (SQA -- ), which have so far been overlooked by our community. Building on this foundation, we contribute as follows: {enumerate}[leftmargin=14pt] We examine the extent to which research works consider the attributes studied in the SQA field (), highlighting that code correctness has been neglected by the NLP community thus far (); Through a case study on open-source implementations of the widespread Conformer architecture , we show that: {itemize}[leftmargin=8pt] [-] At least one impactful bug is present in all the analyzed implementations (); [-] Such bugs do not prevent from achieving good and reproducible results that outperform other architectures in speech recognition and translation across different language pairs (); [-] Undetected bugs can lead to erroneous conclusions when evaluating new techniques (). {itemize} We release a bug-free implementation of Conformer,{Availabe at {https://github.com/hlt-mt/FBK-fairseq/} under the Apache 2.0 License.} along with all the pre-trained models; {We promote code correctness and software quality by releasing {pangoliNN},{Availabe at {https://github.com/hlt-mt/pangolinn/} under the Apache 2.0 License.} a library featuring easily-usable unit tests to enforce the proper behavior of neural models (), and proposing the integration into current conferences checklists of a Code-quality section, which would focus on coding best practices ().} {enumerate"
StreamAtt: Direct Streaming Speech-to-Text Translation with Attention-based Audio History Selection,2406.06097v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.06097v1_0.png,"Streaming speech-to-text translation (StreamST) is the task of automatically translating speech while incrementally receiving an audio stream. Unlike simultaneous ST (SimulST), which deals with pre-segmented speech, StreamST faces the challenges of handling continuous and unbounded audio streams. This requires additional decisions about what to retain of the previous history, which is impractical to keep entirely due to latency and computational constraints. Despite the real-world demand for real-time ST, research on streaming translation remains limited, with existing works solely focusing on SimulST. To fill this gap, we introduce StreamAtt, the first StreamST policy, and propose StreamLAAL, the first StreamST latency metric designed to be comparable with existing metrics for SimulST. Extensive experiments across all 8 languages of MuST-C v1.0 show the effectiveness of StreamAtt compared to a naive streaming baseline and the related state-of-the-art SimulST policy, providing a first step in StreamST research.","Decision steps of the StreamST policy. The order followed by our StreamAtt policy (step \stepone{}, step \steptwo\texthist, and step \steptwo\audio{}) is indicated from 1 (first) to 3 (last).","Streaming speech-to-text translation (StreamST) is the task of automatically translating spoken content from the source language into the target language {in real-time, while continuously receiving an input audio stream. By processing longer, unsegmented audio, StreamST} {adds another layer of complexity to the difficulties of simultaneous ST (SimulST) which, instead, operates on -- often manually -- pre-segmented speech segments [among others]{ren-etal-2020-simulspeech,ma-etal-2020-simulmt,liu-etal-2021-cross,weller-etal-2021-streaming,indurthi-etal-2022-language,tang-etal-2023-hybrid}.} {In SimulST, the primary objective revolves around finding a balance between producing high-quality translations and minimizing latency. This balance} is managed by a {simultaneous policy}, which is the strategy for determining, at each time step, whether to emit a partial translation {hypothesis} or to wait for additional audio input. This hypothesis, together with the processed audio, is temporarily stored in memory to provide context for subsequent {generations} and is automatically removed from memory at the end of each audio segment . {However, when} the input is a continuous, unbounded stream, the memory retained as useful context can indefinitely grow, rendering {the {direct application of} conventional SimulST approaches to StreamST} impractical due to latency and computational constraints.{For example, in the SeamlessM4T model for simultaneous translation , the whole encoder is updated every time a new speech chunk is received (Section 5.2.2 of the paper), which makes its use impracticable for processing continuous, unsegmented audio streams.} Despite representing the real-world scenario for providing real-time ST in many applications, such as interpreting and lectures , and garnering increasing market interests,{{The Real-Time Language Translation Device market is anticipated to rise astronomically each year.} ({https://www.marketreportsworld.com/enquiry/request-sample/24823921})} research on streaming translation remains limited, with existing works solely focusing on text-to-text machine translation {(MT)} . Moreover, as these works focus on (unbounded) text streams as input, there is currently no metric in the literature suitable {to evaluate} the StreamST task, where the input is an audio stream. {To fill these gaps, in this paper we delve into the unexplored domain of StreamST and its associated challenges. First, we define the concept of {streaming policy} for ST by dividing the decision-making process into two steps: {} {hypothesis selection}, {to determine} which part of the translation hypothesis should be emitted (akin to the simultaneous policy), and {} {history selection}, {to identify} which part of past audio and generated partial translations should be retained in memory. Then, motivated} by the success of direct ST models in overcoming the high latency of cascade architectures in {the related field of} SimulST , {we propose StreamAtt{Code available at {https://github.com/hlt-mt/FBK-fairseq/} under Apache 2.0 license.} (Section ), the first StreamST policy designed for direct ST systems. To enable the evaluation of {our} StreamST solution, we also introduce StreamLAAL{} (Section ), {the first latency metric for StreamST. StreamLAAL is designed to {facilitate} a direct comparison with SimulST solutions, {which} provide upper-bound results as they operate on pre-segmented audio.} Lastly, we demonstrate the effectiveness of StreamAtt through extensive experiments across all 8 languages of MuST-C v1.0. We show that {our} policy significantly outperforms a naive streaming baseline (Section ) that relies on a fixed number of past words and audio frames as memory, and is even competitive with the related state-of-the-art SimulST policy at low latency (Section ), providing a first promising step in StreamST research"
FLEUR: An Explainable Reference-Free Evaluation Metric for Image Captioning Using a Large Multimodal Model,2406.06004v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.06004v1_0.pdf,"Most existing image captioning evaluation metrics focus on assigning a single numerical score to a caption by comparing it with reference captions. However, these methods do not provide an explanation for the assigned score. Moreover, reference captions are expensive to acquire. In this paper, we propose FLEUR, an explainable reference-free metric to introduce explainability into image captioning evaluation metrics. By leveraging a large multimodal model, FLEUR can evaluate the caption against the image without the need for reference captions, and provide the explanation for the assigned score. We introduce {score smoothing} to align as closely as possible with human judgment and to be robust to user-defined {grading criteria}. FLEUR achieves high correlations with human judgment across various image captioning evaluation benchmarks and reaches state-of-the-art results on Flickr8k-CF, COMPOSITE, and Pascal-50S within the domain of reference-free evaluation metrics. Our source code and results are publicly available at: .","{Top}: Comparison between other non-explainable metrics and our explainable metric, FLEUR. FLEUR provides the explanation for the assigned score as well. {Bottom}: Existing explainable metric cannot consider the image. The information highlighted in red in the candidate caption is not present in the reference caption set, causing confusion for that metric.","Evaluating image captions is essential as it provides a significant indicator of the model’s ability to understand visual and language information effectively (; ). However, there are two primary challenges with existing image captioning evaluation metrics. Existing methods 1) require reference captions to evaluate candidate captions{A {reference caption} refers to the human-annotated caption for an image. A {candidate caption} refers to the caption that is to be evaluated.} and 2) lack explainability. First, traditional image captioning evaluation metrics (; ; ) have the drawback of requiring reference captions. These metrics assign scores to candidate captions by comparing them to reference captions. However, in practice, obtaining reference captions is challenging because it requires human annotators to create reference captions. Furthermore, evaluating captions only based on text without direct image comparison cannot yield accurate scores. Therefore, new methods (; ; ) have emerged that evaluate captions without the need for reference captions by incorporating images. Second, existing evaluation metrics still lack explainability. Throughout this paper, we clarify the meaning of an {explainable} metric. As defined in , we embrace the broad concept of {explainability} for metrics. {The explainability contains the ability to provide an explanation for the score obtained from the metric.} Existing metrics cannot provide intuitive explanations in sentence form. This makes it difficult to discern whether the score is accurate or not. Hence, we categorize metrics incapable of providing descriptive explanations as {non-explainable} metrics (see the top of Figure ). To overcome these two limitations, we propose a reference-{F}ree exp{L}ainable {E}val{U}ation met{R}ic (FLEUR) for image captioning. FLEUR can evaluate captions even in the absence of reference captions and provide explanations for the scores by using a large multimodal model (LMM). We introduce {score smoothing} to calibrate the scores from the LMM more finely and make FLEUR robust to prompts. Additionally, we propose a prompt including {grading criteria} for caption evaluation to align the scores more closely with human judgment. It is noteworthy that {FLEUR is the only caption evaluation metric both explainable and reference-free.} FLEUR achieves state-of-the-art results across multiple benchmark datasets among the reference-free evaluation metrics, calculated through correlations with human judgment. Furthermore, we demonstrate FLEUR's explainability by comparing its explanations with those of CLAIR , a reference-based and explainable evaluation metric. We hypothesize that directly viewing the image enables a more accurate and comprehensive evaluation of a candidate caption as shown at the bottom of Figure . Our contributions are as follows: {itemize} We propose FLEUR, an explainable reference-free image captioning evaluation metric. FLEUR achieves the highest correlations with human judgment across various benchmark datasets. To the best of our knowledge, our work is a pioneering work of using an LMM to evaluate image captions. We improve the rating performance of an LMM by introducing score smoothing and grading criteria. Through a comparison with the reference-based metric CLAIR, we show that FLEUR generates better explanations because it can consider images. {itemize"
Understanding and Addressing the Under-Translation Problem from the Perspective of Decoding Objective,2405.18922v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2405.18922v1_0.pdf;/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2405.18922v1_1.pdf,"Neural Machine Translation (NMT) has made remarkable progress over the past years. However, under-translation and over-translation remain two challenging problems in state-of-the-art NMT systems. In this work, we conduct an in-depth analysis on the underlying cause of under-translation in NMT, providing an explanation from the perspective of decoding objective. To optimize the beam search objective, the model tends to overlook words it is less confident about, leading to the under-translation phenomenon. Correspondingly, the model's confidence in predicting the End Of Sentence (EOS) diminishes when under-translation occurs, serving as a mild penalty for under-translated candidates. Building upon this analysis, we propose employing the confidence of predicting EOS as a detector for under-translation, and strengthening the confidence-based penalty to penalize candidates with a high risk of under-translation. Experiments on both synthetic and real-world data show that our method can accurately detect and rectify under-translated outputs, with minor impact on other correct translations.",Comparison of EOS log-probability distribution: under-translated sentences vs. all sentences on sentence-level (left) and document-level (right) synthetic test sets.,"Neural Machine Translation (NMT) has made remarkable progress over the past years . Under-translation and over-translation are two typical problems in NMT, where under-translation means some words are mistakenly untranslated, and over-translation means some words are unnecessarily translated for multiple times . Despite the rapid progress of NMT, the mechanisms behind the occurrence of under-translation and over-translation remain unclear, and these problems continue to be challenging obstacles faced by NMT systems . Prior efforts have primarily focused on modeling the explicit coverage of source content or enforcing the attention-based source coverage during decoding , without advancing the understanding of why NMT systems tend to overlook or repeat certain source words. empirically discovered that source words with a large translation entropy or those requiring reordering during translation are more likely to be ignored. However, this observation lacks a comprehensive explanation. In this work, we conduct an in-depth analysis on the underlying cause of under-translation in NMT, providing an explanation from the perspective of decoding objective. Specifically, to gain deeper insights into the characteristics of under-translation and over-translation, we first generate synthetic data to simulate sentence-level and document-level translation scenarios, which allows us to automatically identify translation errors through predefined rules. The findings of sentence-level translation analysis indicate that words with high translation entropy, i.e., source words with a wide range of possible translations, are at a higher risk of being under-translated. In contrast, low-entropy words, whose translations are more predictable, are more likely to be over-translated, which aligns with the findings of . From the document-level translation experiments, we further observe the phenomenon of sentence-level under-translation, where the last sentence containing many high-entropy words is typically omitted. Additionally, we notice that when under-translation occurs, the probability of predicting the End Of Sentence (EOS) is generally lower, suggesting that the model is unwilling to stop generation when the translation is incomplete. Although NMT imposes a penalty on predicting EOS, under-translation still occurs frequently, which can be explained from the perspective of decoding objective. NMT typically employs beam search for decoding, with an objective of finding the most probable sentence, or maximizing the log-probability normalized by sentence length . Consequently, NMT has a strong incentive to ignore high-entropy words, as they have low translation probabilities that contradict the decoding objective. Moreover, we theoretically reveal that the lower EOS probability serves as a penalty for under-translated candidates, but the penalty often underweighs the benefits of dropping multiple high-entropy words, leading to the occurrence of under-translation. Building upon this analysis, we propose enhancing the EOS penalty on under-translated candidates to prevent under-translation. Since the model's confidence in predicting EOS diminishes when under-translation occurs, we employ the prediction probability of EOS as a detector for under-translation. For beam search candidates with high risk of under-translation, we take the EOS probability as penalty and scale it to be proportional to the translation length. The detection ensures minimal interference with correct translations, and the scaling balances the impact of penalty with the benefits of dropping high-entropy source words, thereby more effectively preventing under-translations. In summary, our contributions are:{3pt}\\ ${{{$$}}}$ We analyze the characteristics of under-translation based on our synthetic data, suggesting that under-translation is more likely to occur on challenging words or sentences.{3pt}\\ ${{{$$}}}$ We explain under-translation from the perspective of the decoding objective and theoretically reveal that the EOS probability serves as a penalty for under-translated candidates.{3pt}\\ ${{{$$}}}$ We propose enhancing the EOS penalty on beam search candidates at risk of under-translation. Experiments show that our method can accurately detect and rectify under-translated outputs, with minor impact on other correct translations"
Identifying while Learning for Document Event Causality Identification,2405.20608v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2405.20608v1_0.pdf,"Event Causality Identification (ECI) aims to detect whether there exists a causal relation between two events in a document. Existing studies adopt a kind of {identifying after learning} paradigm, where events' representations are first learned and then used for the identification. Furthermore, they mainly focus on the causality existence, but ignore causal direction. In this paper, we take care of the causal direction and propose a new {identifying while learning} mode for the ECI task. We argue that a few causal relations can be easily identified with high confidence, and the directionality and structure of these identified causalities can be utilized to update events' representations for boosting next round of causality identification. To this end, this paper designs an {iterative learning and identifying framework}: In each iteration, we construct an event causality graph, on which events' causal structure representations are updated for boosting causal identification. Experiments on two public datasets show that our approach outperforms the state-of-the-art algorithms in both evaluations for causality existence identification and direction identification.}",An example of the event causality graph and event structures in the EventStoryLine corpus.,"Event Causality Identification (ECI) is the task of identifying whether there exists a causal relation between two events. ECI can facilitate a wide range of practical applications, including knowledge graph construction, question answering, and information extraction. The ECI task can be divided into the sentence-level ECI (two events are in the same sentence) and document-level ECI (two events may be in different sentences). In this paper, we focus on the document-level ECI task, which faces greater challenges due to the requirement of comprehending long texts for cross-sentence reasoning. The traditional feature-based methods utilize Integer Linear Programming (ILP) to model the document causal structure. In order to better capture the interactions among events, recent methods usually construct document-level undirected graphs to facilitate cross-sentence causal reasoning. Other methods use sparse attention to address the issue of long-distance dependencies and distinguish between intra- and inter-sentential reasoning. Modeling the interactions among events has been proven effective for the document-level ECI task, however, almost all existing methods focus on only identifying the existence of causal relation between the event $e_i$ and $e_j$, yet without considering the causality direction being from $e_i$ to $e_j$ (or from $e_j$ to $e_i$). In this paper, $e_i e_j$ indicates that ``event $e_i$ causes $e_j$''. This may lead to the learning of events' representations towards capturing events' correlations, but correlations may not be directly mapped into causalities. Furthermore, undirected connections may also lead to incorrect causality identifications, as some properties of causal structures cannot be respected without directionality. There are three basic causal structures, namely, the chain, fork, and collider. Causality identification without directionality cannot well exploit causal structures. As shown in Figure~, ``{Shooting{e1}}''${}$``{killing{e4}}''${}$``{arrested{e5}}'' is a chain causal structure. For a model considering causality direction, if the two directional causal relations, i.e., $e_1 e_4$ and $e_4 e_5$, can be first identified with high confidence, then this can help to identify the causal relation between $e_1$ and $e_5$ due to the causal transmission in the chain structure. We argue that events' causal relation should be with directionality, and considering causal directions could further boost event causality identification. Besides ignoring directionality, existing solutions for the ECI task adopt a kind of {identifying after learning} paradigm. That is, learning events' representations first via some advanced neural networks, and then identifying causal relations for all event pairs at only one pass. However, it could happen that some causal relations can be easily identified with high confidence. As reported by, identifying intra-sentence events' causality (two events in a same sentence) is often easier and with better accuracy than identifying inter-sentence events' causality (two events in different sentences). This motivates us to propose a new {identifying while learning} mode for the ECI task. That is, identifying some events' causal relations with high confidence, and then utilizing the directionality and structure of such identified causalities to update events' representations for boosting next round of causality identification. Motivated from the aforementioned considerations, this paper proposes an {iterative Learning and Identifying Framework} (iLIF) for the document-level event causality identification. For an event $e_i$, we not only encode its contextual text representation ${h}_i$, but also update its causal structure representation ${z}_i$ in each iteration. Causality identification is modeled as a classification issue based on the representation ${h}_i$ and ${z}_i$ of an event pair. Initially, we employ a pretrained language model to encode ${h}_i$. In each iteration, we first construct a directed {event causality graph} (ECG) based on the identified causalities, and propose a causal graph encoder to next update ${z}_i$ on the ECG. After the termination, we output the directed ECG as the final causality identification results. In order to differentiate the importance of iterations, we design a novel iteration discounted loss function to mitigate the error propagation issue. We conduct experiments on two public datasets: The EventStoryLine(v0.9) dataset and MAVEN-ERE dataset and consider both direction and existence settings for causal relations. We preprocess the EventStoryLine dataset~{See Appendix~ for more details. } to ensure that each ground truth ECG is a directed acyclic graph. Experiment results validate that our iLIF outperforms the state-of-the-art competitors for the document-level ECI task in evaluations for both causality existence identification and direction identification"
OlympiadBench: A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems,2402.14008v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.14008v2_0.pdf,"Recent advancements have seen Large Language Models (LLMs) and Large Multimodal Models (LMMs) surpassing general human capabilities in various tasks, approaching the proficiency level of human experts across multiple domains. With traditional benchmarks becoming less challenging for these models, new rigorous challenges are essential to gauge their advanced abilities. In this work, we present OlympiadBench, an Olympiad-level bilingual multimodal scientific benchmark, featuring 8,476 problems from Olympiad-level mathematics and physics competitions, including the Chinese college entrance exam. Each problem is detailed with expert-level annotations for step-by-step reasoning. Evaluating top-tier models on OlympiadBench, we implement a comprehensive assessment methodology to accurately evaluate model responses. Notably, the best-performing model, GPT-4V, attains an average score of 17.97\% on OlympiadBench, with a mere 10.74\% in physics, highlighting the benchmark rigor and the intricacy of physical reasoning. Our analysis orienting GPT-4V points out prevalent issues with hallucinations, knowledge omissions, and logical fallacies. We hope that our challenging benchmark can serve as a valuable resource for helping future AGI research endeavors. The data and evaluation code are available at",An example of IMO in OlympiadBench. Solving this example requires AI systems to span different mathematical domains and conduct advanced reasoning.,"Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks such as text generation, code generation and mathematical reasoning, garnering significant attention from both academia and industry. The most powerful models such as GPT-4 and Gemini Ultra have even surpassed oridinary human level on a wide variety of benchmarks such as MMLU, MMMU, and even surpassing human expert in many area. These results show a promising future that LLMs can serve as proficient assistants for human scientists. Among the array of expert-level skills exhibited by LLMs, scientific reasoning consistently emerges as one of the most brilliant, showcasing some of the most distinguished intellectual properties that experts possess. Therefore, this paper primarily focuses on mathematical and physical reasoning. In recent years, several benchmarks related to mathematics have been proposed, such as the dataset GSM8K as well as the dataset MATH. However, these benchmarks, are primarily developed before the advent of highly capable LLMs, and now lack sufficient challenge for the latest models. For instance, GPT-4 with prompting techniques has achieved a 97.0\ Similarly, physics presents comparable challenges for AI to those found in mathematics. Nevertheless, existing benchmarks related to physics are characterized by their relatively low difficulty and limited scope. There is also a significant lack of a rigorous and challenging benchmark in physics. In addition to the issue regarding the benchmark difficulty, it is important to note that these benchmarks predominantly focus on text. This presents a significant limitation, as a wide range of scientific reasoning contexts require multimodal reasoning abilities. For example, grasping geometry reasoning in mathematics or understanding experiments designs in physics are scenarios where multimodal reasoning capabilities are crucial. Notably, various large multimodal models (LMMs) have been developed and demonstrate proficiency on a variety of tasks, offering the potential for multimodal scientific reasoning. Nevertheless, there is still a lack of sufficient benchmarks to prove whether these LMMs are capable of handling scientific problems. Consequently, a challenging multimodal benchmark is essential for advancing scientific reasoning tasks. To address the aforementioned inadequacies, we introduce OlympiadBench, an Olympiad-level bilingual multimodal scientific benchmark. This collection comprises 8,476 math and physics problems sourced from International Olympiads, Chinese Olympiads, and the most challenging segments of the Chinese College Entrance Exam (GaoKao). We download PDF data from official websites and utilize Mathpix{{https://mathpix.com/}} for OCR parsing. We meticulously inspect, clean, and revise the data, and further adopt LLMs for deduplication. Finally, we annotate the data with crucial information such as answer types and subfields, yielding a dataset that is clean, accurate, and detailed. As shown in Figure~, features numerous distinct characteristics such as difficulty, free-form generation, expert-level solution annotation, detailed labeling of difficulty, wide-coverage of modality and language, etc. These features are summarized more clearly from Table~. We conduct an evaluation of current state-of-the-art LLMs and LMMs on the OlympiadBench. The best-performing model, GPT-4V, is a multimodal version of GPT-4 developed by OpenAI that can understand images. Despite its advanced capabilities, GPT-4V achieves a score of only 17.97\ Importantly, the experiment results show that LMMs still struggle in computational error, incorrect reasoning or induction. For the process involved in the correct responses, the process occasionally includes hallucinated reasoning, or choosing a more complex solution when a simpler solution exists. All these results highlight the substantial challenge OlympiadBench presents to contemporary large models and point the direction of future efforts. is inspired by the significant advances made by DeepMind AlphaGeometry, which nearly matches the proficiency of International Mathematical Olympiad (IMO) gold medalists in geometry proofs. It is clear that , along with other challenging datasets like the AI-MO challenge{{https://aimoprize.com/}}, will witness and benchmark the swift progress towards expert-level AI assistants for solving scientific problems"
Insert or Attach: Taxonomy Completion via Box Embedding,2305.11004v4,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2305.11004v4_0.pdf,"Taxonomy completion, enriching existing taxonomies by inserting new concepts as parents or attaching them as children, has gained significant interest. Previous approaches embed concepts as vectors in Euclidean space, which makes it difficult to model asymmetric relations in taxonomy. In addition, they introduce pseudo-leaves to convert attachment cases into insertion cases, leading to an incorrect bias in network learning dominated by numerous pseudo-leaves. Addressing these, our framework, , leverages box containment and center closeness to design two specialized geometric scorers within the box embedding space. These scorers are tailored for insertion and attachment operations and can effectively capture intrinsic relationships between concepts by optimizing on a granular box constraint loss. We employ a dynamic ranking loss mechanism to balance the scores from these scorers, allowing adaptive adjustments of insertion and attachment scores. Experiments on four real-world datasets show that significantly outperforms previous methods, yielding substantial improvements over prior methods in real-world datasets, with average performance boosts of 6.7\%, 34.9\%, and 51.4\% in MRR, Hit@1, and Prec@1, respectively.",Example of taxonomy completion with our \textsc{TaxBox} framework.,"Taxonomy, a critical knowledge graph with an ""is-a"" relationship, plays a vital role in information retrieval, recommendation systems, and question answering. However, manual taxonomy enrichment is inefficient and costly due to the constant emergence of new concepts. To address the challenge of incorporating new concepts, taxonomy completion has been introduced, with new concepts either inserted as both parents and children or attached only as children. This task goes beyond taxonomy expansion, which primarily treats new concepts as leaf nodes and tends to have limitations in downstream applications. Taxonomy completion entails a more comprehensive incorporation of new concepts with two operations: insertion and attachment. For instance, in Figure , new query concepts such as {cat} and {insect} are added to the existing {animal} taxonomy. The process requires enumerating all possible candidate positions within the original taxonomy, including existing edges like {<Animal, Vertebrate>} and implicit edges from each node to its descendants such as {<Animal, Tiger>}. Each candidate position is then paired with the query concept, and a confidence score is calculated. Finally, {insect} is attached as a child of {animal} and {cat} is inserted as a parent of {Siamese cat} and children of {Domestic Animal} and {Vertebrate} according to their confidences. Recent research on taxonomy enrichment has examined various practical methods . Nevertheless, all of these approaches embed concepts as vectors in Euclidean space, which makes them less capable of modeling the asymmetric relationship (""is-a"") in taxonomy. BoxTAXO tried to employ box embedding, a representation method that can capture more prosperous and asymmetric relationships like inclusion, disjoint, and proximity among concepts through its geometric properties. However, this method is limited in real-world applications for its reliance only on the volume property, rendering it suitable only for the taxonomy expansion and even incapable of discerning optimal ancestor concepts and handling multiple parents during inference. Moreover, methods for taxonomy completion suffer from using a ""pseudo-leaf"" as a child node in attachment cases, leading to confusion in the matching. It is attributed that attachment cases often predominate due to leaf nodes’ prevalence in real taxonomies. Therefore, learning too much about the pseudo-leaf in the attachment cases may reduce the network's perceptual ability for child nodes in the insertion cases. To overcome these limitations, we present a novel framework for taxonomy completion called {{TaxBox}}, which is the first to apply box embedding to taxonomy completion. This approach adopts a structurally enhanced box decoder, representing concepts as box embeddings encompassing the information of children, furnishing richer semantics. Most importantly, {TaxBox} combines two probabilistic scorers to unify the process of insertion and attachment in the box embedding space and incorporates both the volume and center closeness properties of box embedding. Such a design effectively exploits the fine-grained geometric attributes of box embeddings, circumventing the need for a pseudo-leaf and yielding optimal, feasible results during the ranking process. Additionally, we propose two novel training objectives, optimizing both box volume and position, and rectifying scorer numerical imbalances. The specific contributions of this paper are outlined as follows: {itemize} We introduce {TaxBox}, the first framework using box embedding for taxonomy completion with a structurally enhanced box decoder. We establish insertion and attachment scorers, obviating the need for pseudo-leaves and ensuring the determination of optimal results. We design box constraint loss, focusing on both volume and center closeness, and dynamic ranking loss, rectifying scorer numerical imbalance. Experimental outcomes from four datasets demonstrate our model's efficacy, achieving 6.7\ {itemize"
"Instruct Once, Chat Consistently in Multiple Rounds: An Efficient Tuning Framework for Dialogue",2402.06967v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.06967v2_0.pdf;/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.06967v2_1.pdf;/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.06967v2_2.pdf,"Tuning language models for dialogue generation has been a prevalent paradigm for building capable dialogue agents. Yet, traditional tuning narrowly views dialogue generation as resembling other language generation tasks, ignoring the role disparities between two speakers and the multi-round interactive process that dialogues ought to be. Such a manner often leads to unsatisfactory chat consistency for the built agent. In this work, we emphasize the interactive, communicative nature of dialogue and argue that it is more feasible to model the speaker roles of agent and user separately, enabling the agent to adhere to its role consistently. With this in mind, we propose an efficient {M}ulti-round {I}nteractive {Di}alogue Tuning (-Tuning) framework.}. It models the agent and user individually with two adapters built upon large language models. The adapters make use of respective utterances round by round in alternating order and they are tuned via a round-level memory caching mechanism. Extensive experiments demonstrate that, our framework performs superior to traditional fine-tuning and harbors the tremendous potential for improving dialogue consistency.",Comparison of different tuning manners (including data usage) for dialogue generation.,"Building human-like intelligent dialogue agents is a long-standing ambition for the research community of dialogue systems. Recently, we have witnessed a substantial revolution in advanced conversational agents such as ChatGPT and GPT-4 , which are fundamentally built upon large language models (LLMs) . Similar efforts have also been made by academia and open-source communities, leading to a variety of notable chat language models, such as Vicuna , Koala , and {Llama 2}-Chat . These chat language models can be attained by instruction fine-tuning on downstream dialogue data, demonstrating promising performance in generating natural and comprehensive responses. Tuning LLMs for dialogue generation has been the de-facto mainstream practice towards creating capable dialogue agents. Traditional dialogue tuning narrowly views dialogue generation as resembling other language generation tasks without distinction. It performs in either {one-dialogue-$n$-sample} (see Figure ) or {one-dialogue-one-sample} (see Figure ) manner. The former transforms dialogue model training into general language generation via splitting each multi-round dialogue into multiple single-round samples, yet results in non-independent distributions among those samples. The latter enhances training efficiency by utilizing each multi-round dialogue at once, which computes the prediction loss for the agent's responses through causal masks, such as Vicuna and UltraLLaMA . However, these methods simply concatenate utterances from two speakers (e.g., user and agent) together (and instructions for the agent, if any) and mix their content in the same language model space, ignoring the role disparities between two speakers and the multi-round interactive process that dialogues ought to be. Such tuning methods inevitably hinder a built dialogue agent from maintaining the chat {consistency} , requiring that the agent always adhere to its role even with the dialogue rounds moving forward. It remains urgent to solve for many consistency-demanding scenarios. One of the primary challenges for improving dialogue consistency lies in the disparity modeling of the two speaker roles. It is because the inconsistency issue in real-world human communication is often caused by various types of speaker disparities, such as background knowledge, cognitive level, personalities, and goals. We emphasize that it is more feasible to {model the roles of agent and user separately} (see Figure ), such that the agent and user models can {consistently adhere to their respective roles and interact with each other round by round}, similar to humans. When tuning LLMs for conversation, we have a similar motivation towards consistent dialogue generation. We propose a general, simple, and effective framework, namely {M}ulti-round {I}nteractive {Di}alogue Tuning ({Midi}-Tuning). It employs two language model adapters (e.g., LoRA ) built upon LLMs, to represent the agent and user, respectively. The two adapters are tuned by utilizing respective utterances round by round in alternating order, with each adapter learning to distinguish language model distribution about its role. However, such separate modeling is non-trivial in tracking the complete dialogue context. Considering that the foundation architecture of mainstream LLMs is Transformer , we propose a {round-level memory caching} mechanism to address it efficiently, which reuses previous-round cached keys and values as ongoing context when processing present-round utterance. In summary, our main contributions are as follows: (1) To the best of our knowledge, this is the first work investigating how a new way of tuning could affect dialogue consistency in the era of LLMs. (2) We propose {Midi}-Tuning, a general, simple, and efficient framework to tune LLMs for dialogue generation, which can be applied in broad downstream dialogue scenarios. (3) Extensive experiments demonstrate that {Midi}-Tuning outperforms traditional fine-tuning over various LLMs, especially in maintaining consistency for multi-round dialogues"
"Rule or Story, Which is a Better Commonsense Expression for Talking with Large Language Models?",2402.14355v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.14355v2_0.pdf,"Building machines with commonsense has been a longstanding challenge in NLP due to the reporting bias of commonsense rules and the exposure bias of rule-based commonsense reasoning. In contrast, humans convey and pass down commonsense implicitly through stories. This paper investigates the inherent commonsense ability of large language models (LLMs) expressed through storytelling. We systematically investigate and compare stories and rules for retrieving and leveraging commonsense in LLMs. Experimental results on 28 commonsense QA datasets show that stories outperform rules as the expression for retrieving commonsense from LLMs, exhibiting higher generation confidence and commonsense accuracy. Moreover, stories are the more effective commonsense expression for answering questions regarding daily events, while rules are more effective for scientific questions. This aligns with the reporting bias of commonsense in text corpora. We further show that the correctness and relevance of commonsense stories can be further improved via iterative self-supervised fine-tuning. These findings emphasize the importance of using appropriate language to express, retrieve, and leverage commonsense for LLMs, highlighting a promising direction for better exploiting their commonsense abilities.","Comparison between rules and a story written by ChatGPT. The rules only provide useful knowledge until the 4$^{th}$ rule and also include an incorrect answer option, ``classroom''. The story presents a detailed scenario where an adult uses glue sticks in an office.","Building machines with commonsense has been a longstanding goal in AI and NLP . Despite advancements in large language models (LLMs), incorporating commonsense knowledge in these models remains a significant challenge , due to the reporting bias of commonsense knowledge and the exposure bias of commonsense reasoning . The reporting bias arises because many aspects of commonsense are rarely stated explicitly in language. For example, ``{A person is late}'' may appear more frequently than ``{A person arrives on time}'' in text corpora . Furthermore, commonsense rules are often left implicit and omitted in human language reasoning, leading to exposure bias. For example, the commonsense rule ``{humans need air to breathe}'' is usually ignored in cases like ``{The room was getting too stuffy, and I opened the windows}'' as it is commonly known. To enhance the commonsense ability of NLP models, current studies usually express commonsense as rules. For instance, commonsense rules structured as knowledge graphs of concepts and events are incorporated to support rule-based logical reasoning . Recently, as studies reveal that LLMs like GPT-3 and ChatGPT have already learned abundant commonsense , there is a current trend to extract commonsense knowledge from the models' memory, also expressed as rules like in Figure , and enhance LLMs by reintegrating this knowledge into the models . However, commonsense is more than just rules . Humans acquire commonsense by recognizing prototypical patterns, extracting memories of similar past experiences, and contrasting them with the current novel situation to make decisions, as supported by psychological studies . Our commonsense is often conveyed and passed down through stories such as myths and fairy tales , with only a limited portion expressed in rules. Renowned AI theorist and cognitive psychologist Roger Schank argues in his book ``{Tell Me a Story: Narrative and Intelligence}'' that ``{knowledge is stories}'' . He emphasizes that humans struggle to learn and remember abstract rules derived from past experiences but can more easily remember a good story, because ``{stories give life to past experience}''. As a result, human-written text corpora mainly convey commonsense through stories, with limited instances of explicit rules and logical reasoning. In this way, models trained on these corpora acquire commonsense and reasoning abilities implicitly. Studies show that LLMs exhibit a strong storytelling ability, generating narratives that adhere to real-world logic . However, these models may not effectively learn commonsense rules and explicit reasoning through mimicking human behaviors, as shown by recent studies . These observations lead to a critical question: Which is the better commonsense expression for talking with LLMs—rule or story? Specifically, this paper aims to answer the following two questions: (1) Which expression is more effective for retrieving commonsense from the memory of LLMs? (2) Which expression is more suitable for LLMs to leverage commonsense in solving problems? To answer the questions, we systematically compare stories and rules as commonsense expressions for talking with LLMs. We use a total of 28 commonsense QA datasets for experiments. For the first question, we instruct LLMs to generate stories and rules based on commonsense questions, as shown in Figure . We compare the confidence and the accuracy of commonsense generation using stories and rules, showing that LLMs are more confident and more accurate at retrieving commonsense as stories than as rules. For the second question, we compare the confidence of generating the correct answers with stories or rules as contexts, showing that LLMs can more confidently leverage stories than rules for reasoning. The QA accuracy results further demonstrate that the story is a more effective commonsense expression for answering questions regarding daily events, while the rule is more effective for scientific commonsense QA. This phenomenon aligns with the reporting bias of commonsense in the text corpora. Moreover, stories and rules can complement each other, i.e., combining them can further enhance the answer accuracy. In-depth analyses reveal two main issues in generating commonsense stories: commonsense hallucination and semantic drifting. To address these problems, we propose an iterative self-supervised fine-tuning (self-SFT) method. We ask the model to generate stories given the training set of 8 datasets and design a scoring method to rank the stories based on their consistency with commonsense and similarity with the question. We filter the stories based on the scores and use them to fine-tune the model. The tuned model is then used to generate stories in the next iteration. Experimental results show that the self-SFT method leads to further accuracy improvements, highlighting the potential for LLMs to self-improve their commonsense abilities. The main contributions of this paper are: 1. We systematically investigate and compare the effects of using stories and rules as commonsense expressions for retrieving and leveraging commonsense in LLMs. To our best knowledge, this is the first study to investigate the effects of specific commonsense expressions in LLMs. 2. We show that the story is a more effective expression for retrieving commonsense from LLMs and for leveraging commonsense in answering questions regarding daily events. 3. We identify two main issues that hinder commonsense story generation: commonsense hallucination and semantic drifting, and propose an iterative self-SFT method to improve the accuracy and relevance of stories generated by LLMs"
CaMML: Context-Aware Multimodal Learner for Large Models,2401.03149v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2401.03149v3_0.pdf,"In this work, we introduce {C}ontext-{A}ware {M}ulti{M}odal {L}earner (), for tuning large multimodal models (LMMs). , a lightweight module, is crafted to seamlessly integrate multimodal contextual samples into large models, thereby empowering the model to derive knowledge from analogous, domain-specific, up-to-date information and make grounded inferences. Importantly, is highly scalable and can efficiently handle lengthy multimodal context examples owing to its hierarchical design. Based on , we have developed two multimodal models, -7B and -13B, that have shown exceptional performance across an array of benchmark datasets for multimodal tasks. Remarkably, -13B achieves the state-of-the-art performance on over ten widely recognized multimodal benchmark datasets, surpassing LLaVA-1.5 (13B) with a noticeable margin, without integration of any external resources. Moreover, we have conducted extensive ablative studies to inspect the inner workings of and performed qualitative analyses to showcase its effectiveness in handling real-world challenging cases. Code and models are available at: .","\caml achieves the state-of-the-art performance on a number of multimodal benchmarks, outperforming LLaVA-1.5 and many other large multimodal models.","Recently, large multimodal models (LMMs) have demonstrated remarkable performance in a variety of tasks, including but not limited to visual question answering, image captioning, visual grounding, visual-language reasoning, optical character recognition, and visual entailment. Notably, in certain benchmark assessments, these multimodal foundation models have even exceeded human-level performance. Despite the impressive performance, their ability to make inferences is constrained by the knowledge encoded in the model parameters. The inflexible design of these models makes it challenging for them to generalize from contextual examples. For instance, LLaVA-1.5 falls short in processing multiple images and attributes it to the lack of corresponding instruction-tuning training data. Nevertheless, learning and making inferences through contextual examples are fundamental elements of our cognitive processes. Human beings frequently tackle intricate problems by relying on past experiences and identifying analogous situations. Taking inspiration from the cognitive process, we hypothesize that empowering large multimodal models with the capability to perceive and derive insights from analogous contextual examples can significantly streamline the inference process and lead to more precise predictions. Nonetheless, the means to replicate the human cognitive processes in LMMs remain unclear. As such, our goal is to empower multimodal foundational models to harness context-aware learning, thereby enhancing their ability to comprehend and adapt to previously unseen examples. Identifying relevant context examples is relatively straightforward; this process can be facilitated using multimodal embedding models like ImageBind or CLIP. This approach simulates the act of recalling similar situations from past experiences. Yet, effectively and efficiently integrating the identified similar context samples into large models poses challenges, particularly given the potential variability in the number of context samples and interleaved modalities, resulting in lengthy and heterogeneous context input. To this end, we propose a context-aware multimodal learner, dubbed as , for LMMs. acts as a crucial intermediary between the contextual examples and a large language model (LLM). Our approach is structured hierarchically, where the initial level establishes connections between the text and image modalities for each example through cross-attention mechanisms. This integration of text and image information enables a deeper understanding of the interleaved context. Following this, another module takes the outputs of the first level and performs cross-attention between the contextual information and a predefined set of learnable, fixed-length tokens. The resulting output from this level is then used as fixed-length input for the LLM, allowing the model to leverage the refined and context-aware information to perform complex multimodal understanding tasks. To summarize, we make the following contributions: {itemize} We propose , a context-aware multimodal learning approach for finetuning multimodal models. is lightweight and can be applied to process extremely long multimodal context samples; {-0.1in} With , we have developed two multimodal models, -7B and -13B. These models have achieved state-of-the-art performance across a diverse range of benchmarks encompassing various multimodal tasks, all without the need for external data integration; {-0.1in} We conduct comprehensive model analyses and case studies to examine the internal mechanisms of and showcase how the proposed model can effectively handle real-world challenging cases. {itemize"
NPHardEval: Dynamic Benchmark on Reasoning Ability of Large Language Models via Complexity Classes,2312.14890v4,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2312.14890v4_0.jpg,"Complex reasoning ability is one of the most important features of current Large Language Models (LLMs), which has also been leveraged to play an integral role in complex decision-making tasks. Therefore, the investigation into the reasoning capabilities of LLMs is critical: numerous benchmarks have been established to assess the reasoning abilities of LLMs. However, current benchmarks are inadequate in offering a rigorous evaluation of the full extent of reasoning abilities that LLMs are capable of achieving. They are also prone to the risk of overfitting, as these benchmarks, being publicly accessible and static, allow models to potentially tailor their responses to specific benchmark metrics, thereby inflating their performance. Addressing these limitations, our research introduces a new benchmark, named {NPHardEval}. This benchmark is designed to evaluate the reasoning abilities of LLMs across a broad spectrum of 900 algorithmic questions, extending up to the NP-Hard complexity class. These questions are meticulously chosen to represent a wide range of complexity class below the NP-hard complexity class, offering a rigorous measure of the reasoning ability of LLMs. Through this study, we shed light on the current state of reasoning in LLMs, providing an objective and rigorous perspective through the comparison of LLMs' performance across complex classes. Our findings contribute significantly to understanding the current capabilities of LLMs in reasoning tasks and lay the groundwork for future advancements in enhancing the reasoning abilities of these models. Moreover, this benchmark is designed with a {dynamic update mechanism}, where the datapoints are refreshed on a monthly basis. Such regular updates play a crucial role in mitigating the risk of LLMs overfitting to the benchmark, promoting a more accurate and reliable assessment of their reasoning capabilities. The benchmark dataset and code of {NPHardEval} are available at .","Computational complexity classes P, NP-complete, and NP-hard and corresponding tasks","The advancement of LLMs has ushered in a transformative era in Artificial Intelligence (AI) research . One major advantage, believed by many researchers, is the unparalleled reasoning capabilities showcased by these models . Despite the implementation of various benchmarks for evaluating reasoning ability , existing methods reveal certain limitations. These include inadequacies in the precise characterization of reasoning abilities, the risk of models overfitting to specific benchmarks , and in some cases, the dependency on manual evaluation methods . Additionally, it is theoretically interesting to examine the extent to which LLMs can address problems in the computational complexity hierarchy , especially NP-hard or NP-complete problems. In response to these issues and questions, we introduce a new benchmark {NPHardEval}, which leverages the well-established principles of computational complexity classes to provide a more rigorous and quantitative assessment of the reasoning abilities of large language models. Our benchmark, meticulously designed to evaluate the reasoning abilities of Large Language Models (LLMs), comprises 9 carefully chosen reasoning tasks. These tasks are segmented according to complexity classes as outlined in , with each class containing 100 instances distributed across 10 distinct levels of difficulty. This structured approach allows for a thorough and quantifiable assessment of LLMs' reasoning capacities. The selection of problems within our benchmark is particularly significant as it also mirrors the nuance of real-world decision-making and optimization scenarios, including critical areas such as logistics, scheduling, network design, and various other domains where optimal solutions carry substantial economic and practical implications. As LLMs are increasingly utilized in complex problem-solving scenarios, the need for an accurate and rigorous assessment of their reasoning abilities becomes paramount. Such an evaluation serves as a crucial and reliable indicator of their capabilities, guiding their effective integration and application in various contexts. We can also gain deeper insights into both the strengths and limitations of their computational reasoning abilities. Another novel feature of our benchmark is its end-to-end automation, encompassing both the generation of tasks and the verification of results. This automation is facilitated by the use of well-known tasks within the benchmark, for which mature and established algorithms have been developed to provide solutions. This systematic approach ensures a high degree of accuracy and reliability in the evaluation process, which also enables easy update of datapoints in the benchmark. This automated framework facilitates an effortless updating of datapoints within the benchmark. As a result, we design the benchmark to refresh its datapoints monthly, effectively reducing the likelihood of the model overfitting the dataset. This dynamic updating mechanism is crucial in maintaining the rigor and relevance of the benchmark over time. We welcome open submissions of model performance to our benchmark directly through the {NPHardEval} leaderboard on HuggingFace: {https://huggingface.co/spaces/NPHardEval/NPHardEval-leaderboard}. In general, our benchmark offers several advantages compared with current benchmarks: {itemize} The questions in the benchmark utilized are grounded in the established computational complexity hierarchy, a concept extensively studied in theoretical computer science. This foundation enables us to leverage existing research to rigorously and quantitatively measure an LLM's logical reasoning extent. We incorporate automatic checking mechanisms for these questions, as they are based on algorithmically computable problems. Human intervention is not required to determine the correctness of the LLM's responses. The method allows for the automatic generation of questions so that we can update the benchmark on a monthly basis. This monthly-refreshed benchmark helps prevent model's overfitting as we can always generate novel questions with varying difficulty levels for evaluation. The benchmark excludes numerical computation from the questions, which is notably difficult for LLM. This focus allows for a more accurate evaluation of an LLM's pure logical reasoning ability, as numerical computation can obscure this assessment. Our methodology offers insights into a long-standing and intriguing question within the field: the degree to which LLMs are capable of tackling problems classified as NP-hard or NP-complete. {itemize} In our research utilizing the benchmark, we aim to address three critical aspects to evaluate and understand the reasoning abilities of LLMs (Foundation Models): {enumerate} {Model Performance Comparison:} Our benchmark compares the reasoning ability of 12 closed-source models (including as GPT 4 Turbo, Claude 2, GPT 3.5 Turbo, Claude Instant, and PaLM 2) and open-source models (including Yi-34b, Qwen-14b, Mistral-7b, Phi-2, MPT-30b, Vicuna-13b, and Phi-1.5) across three complexity classes (P, NP-complete, and NP-hard) and 10 difficulty levels. This comparison can shed light on the relative strengths and weaknesses of these models and determine the proficiency of them in solving progressively challenging problems, thereby gauging their capability to handle tasks with escalating complexity. {Robustness of Benchmark Assessments:} This study examines whether the frequent updating of algorithmic benchmarks can effectively prevent the risk of ``hacking'' the benchmark. The dynamic updating of benchmarks is proposed as a strategy to reduce the likelihood of LLMs overfitting to these benchmarks. However, a pertinent question arises: does finetuning LLMs on benchmarks from the previous month lead to overfitting specific problem types? To explore this, we conducted an experiment where three open-source models -— Phi-2, Mistral-7b, and Qwen-14b —- were finetuned on five distinct versions of the benchmarks. The performance of these models was evaluated on two versions of the benchmark, each differing in difficulty level. This approach allowed us to assess whether finetuning enables models to ``hack'' benchmarks of varying complexity. {Generalization through In-context Learning:} Given examples in the context, can LLMs genuinely learn and apply algorithmic skills presented in contextual examples as opposed to merely mimicking problem-solving processes ? We differentiate between ``learning'' and ``mimicking'' by evaluating whether LLMs can generalize solutions to new problems of varying difficulty levels within the same task, after being exposed to examples. Our hypothesis is that if an LLM has truly learned the underlying algorithmic skill, it should be able to tackle problems across different difficulty levels within the same task. Conversely, if an LLM is merely mimicking, its performance may falter when faced with variations in problem difficulty. {enumerate} The contribution of this paper is as follows: We present the first complexity classes-based reasoning benchmark, {NPHardEval}. This benchmark enables a rigorous evaluation of LLMs' reasoning capabilities on a wide range of complex reasoning tasks with varying difficulty. Through the above research questions, we aim to provide a comprehensive analysis of the reasoning capabilities of LLMs, exploring their potential for genuine understanding and application of complex problem-solving skills"
Multi-Level Feedback Generation with Large Language Models for Empowering Novice Peer Counselors,2403.15482v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2403.15482v1_0.pdf,"Realistic practice and tailored feedback are key processes for training peer counselors with clinical skills. However, existing mechanisms of providing feedback largely rely on human supervision. Peer counselors often lack mechanisms to receive detailed feedback from experienced mentors, making it difficult for them to support the large number of people with mental health issues who use peer counseling. Our work aims to leverage large language models to provide contextualized and multi-level feedback to empower peer counselors, especially novices, at scale. To achieve this, we co-design with a group of senior psychotherapy supervisors to develop a multi-level feedback taxonomy, and then construct a publicly available dataset with comprehensive feedback annotations of 400 emotional support conversations. We further design a self-improvement method on top of large language models to enhance the automatic generation of feedback. Via qualitative and quantitative evaluation with domain experts, we demonstrate that our method minimizes the risk of potentially harmful and low-quality feedback generation which is desirable in such high-stakes scenarios.","Example conversation excerpt taken from the ESConv dataset \cite{liu-etal-2021-towards} annotated using our feedback taxonomy. Feedback components ({appropriateness, goal definition and alignment, areas for improvement, alternative goal-aligned response}) are demonstrated on one utterance of peer counselor's response (in blue). Optionally, one can also provide {positive reinforcement} by highlighting areas in categories peer counselors excelled at.","Realistic practice and tailored feedback are key processes for training peer counselors with clinical skills. Providing feedback could significantly enhance peer counselor skills, thereby improving support quality and benefiting many seeking help online. However, it is often time-consuming and costly for counseling supervisors to provide detailed feedback to beginner peer counselors. Without appropriate guidance, peer counselors might develop biased or even inappropriate helping skills without being aware of it, based on their own experiences. What can we do to provide detailed feedback to a large number of novice peer counselors at scale? In this work, we explore whether large language models (LLMs) can be used to provide contextualized feedback to empower peer counselors in training. 0 With the goal of mimicking human supervision experience, we interview senior psychotherapy supervisors of counselors in training to understand the content and delivery of feedback to novice counselors. In the first step of the co-design process, we conduct a contextual inquiry on supervisors engaging in a representative task of providing feedback on a transcript of an emotional support conversation as if they were communicating the feedback to a novice. We then develop our feedback framework by modeling the common patterns at different granularity observed in interviews and including important feedback dimensions highlighted in textbooks and training for foundational active listening skills . Aiming to make a step toward how feedback is delivered in the professional supervision of peer counselors, we explore whether large language models (LLMs) can be used to provide contextualized feedback to empower novice counselors. Numerous recent studies have explored the feasibility of applying computational techniques to differentiate between low and high-quality counseling automatically . In doing so, prior work mostly provides numeric feedback to counselors about how well a particular skill is used. Some recent studies provide utterance-level suggestions of responses to use according to appropriate helping skills , or alternatives for more empathetic responses . Yet, little attention is given to developing automatic feedback that closely mirrors how clinical supervisors provide feedback to novice counselors. To this end, we co-designed a feedback framework with senior psychotherapy supervisors to reflect the content and delivery of feedback they give to novice counselors. Concretely, we conducted a contextual inquiry with supervisors engaging in a representative task of providing feedback on a transcript of an emotional support conversation as if they were communicating the feedback to a novice counselor. We then developed a multi-level feedback framework by modeling the common patterns at different granularity observed in interviews and important feedback dimensions highlighted in textbooks and training for foundational active listening skills . With this multi-level feedback framework presented in Figure , we introduce a publicly available dataset of conversations enriched with comprehensive feedback annotations, building upon an existing public emotional support conversations dataset ESConv . Specifically, we leverage a model-in-the-loop annotation paradigm where GPT-4 and counseling domain experts work together to produce the annotations for 400 conversations. To enable transparent model development, especially for a high-stakes domain like counseling, we fine-tuned the open-source Llama-2 model to generate multi-level feedback. We further introduce a simple but effective self-improvement method to forecast how specific feedback might improve subsequent interaction and use this forecast information to supervise feedback generation. Unlike general natural language generation tasks, we aim at optimizing feedback generation for worst-case performance since failures (e.g., generating poor advice) matter more in this high-stakes scenario. Using both quantitative evaluation and qualitative evaluation with domain experts, we demonstrate that our approach generates high-quality feedback and significantly boosts the worst-case performance on multi-level feedback generation compared to baselines. In summary, this paper makes the following contributions: {itemize}0em We propose a novel and comprehensive multi-level feedback framework for training peer counseling skills co-designed with senior psychotherapy supervisors. We constructed and make publicly available {FeedbackESConv}{We will release our code at https://github.com/SALT-NLP/counseling-feedback}, a dataset of 400 emotional support conversations with multi-level feedback annotated by domain experts and GPT-4. We enhanced a fine-tuned LLM for multi-level feedback using a simple but effective self-improvement method to forecast how specific feedback might improve subsequent interaction and further use such signal to supervise the feedback generation. We conducted extensive evaluations with domain experts to demonstrate the effectiveness of our method and find that, compared to baselines, it significantly boosts the worst-case performance on multi-level feedback generation. {itemize"
Enhancing Reinforcement Learning with Label-Sensitive Reward for Natural Language Understanding,2405.19763v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2405.19763v1_0.pdf,"Recent strides in large language models (LLMs) have yielded remarkable performance, leveraging reinforcement learning from human feedback (RLHF) to significantly enhance generation and alignment capabilities. However, RLHF encounters numerous challenges, including the {objective mismatch} issue, leading to suboptimal performance in Natural Language Understanding (NLU) tasks. To address this limitation, we propose a novel Reinforcement Learning framework enhanced with Label-sensitive Reward (RLLR) to amplify the performance of LLMs in NLU tasks. By incorporating label-sensitive pairs into reinforcement learning, our method aims to adeptly capture nuanced label-sensitive semantic features during RL, thereby enhancing natural language understanding. Experiments conducted on five diverse foundation models across eight tasks showcase promising results. In comparison to Supervised Fine-tuning models (SFT), RLLR demonstrates an average performance improvement of 1.54\%. Compared with RLHF models, the improvement averages at 0.69\%. These results reveal the effectiveness of our method for LLMs in NLU tasks. Code and data available at: {https://github.com/MagiaSN/ACL2024\_RLLR}",The example of rationale-sensitive and label-sensitive pairs from sentiment classification. Highlight rationales in green and labels in yellow.,"Large language models (LLMs) have undergone impressive advancements that transform NLP tasks into a unified text-to-text paradigm, achieving robust alignment and generation capabilities through reinforcement learning from human feedback (RLHF) . Particularly, models are required to predict the correct labels in natural language understanding (NLU) tasks, distinct from natural language generation (NLG) tasks. Numerous studies have employed ``rationales'' to assist LLMs with Chain-of-Thought (CoT) prompting during supervised fine-tuning (SFT) stage . Rationale refers to the relevant parts or information that provide explanations or support for the predictions or decisions made by a model. However, detail a fundamental challenge in RLHF learning schemes: the {objective mismatch} issue. This arises when the reward model is influenced by human preference data, introducing biases that conflict with downstream evaluation metrics, especially when applied to NLU tasks. In RLHF, comparison data is initially sampled from the SFT model and ranked by a labeler. Then the policy model is optimized against the reward model that is trained with these pairs to align with human preference. For NLU tasks, the pairs can be categorized into rationale-sensitive and label-sensitive. As illustrated in Figure , we provide an example where three answers sampled from the SFT model for the same instruction. If two answers have the same label and different rationales, they form a rationale-sensitive pair, with the more reasonable rationale considered superior. In contrast, if two answers have different labels, they form a label-sensitive pair, with the correct label deemed superior. However, we observed that the pairs sampled from the SFT model mainly fall into the category of rationale-sensitive. Figure shows the specific distribution ratios of pairs across several NLU tasks. The percentage of rationale-sensitive pairs exceeds 75\ The severe imbalance in the distribution of pairs leads the model to prioritize the quality of rationales over the correctness of labels during RLHF training, which conflicts with the evaluation metric (mostly {label accuracy}) of NLU tasks. A detailed analysis is presented in Section . To address this challenge, our paper proposes a Reinforcement Learning framework enhanced with Label-sensitive Reward (RLLR) for NLU tasks. Firstly, we leverage GPT-4 to generate rationales corresponding to the gold labels of the training data. The SFT model is trained with rationales, incorporating CoT prompting to enhance comprehension abilities. Secondly, we generate rationales for the incorrect labels (relative to the gold labels). Unlike RLHF, which uses human intervention to rank sentences, RLLR automatically constructs label-sensitive pairs for training the reward model based on the correctness of the label. The comparison data is initially sampled from the trained SFT model. Finally, we train the policy model against the label-sensitive reward model with Proximal Policy Optimization (PPO) to prioritize the correctness of labels. Furthermore, optimizing with mixed rewards from the label-sensitive and rationale-sensitive reward models, RLLR$_{mixed}$ ensures both the accuracy of labels and the quality of rationales. Extensive experiments on eight NLU tasks demonstrate that our method consistently outperforms the SFT baseline by an average of 1.54\ Our contributions are summarized as: (1) We propose a Reinforcement Learning framework enhanced with Label-sensitive Reward (RLLR) for NLU tasks to tackle the {objective mismatch} issue. (2) Optimizing with mixed rewards, RLLR$_{mixed}$ can achieve promising performance on both the accuracy of labels and the quality of rationales. (3) Through empirical experiments, we demonstrate the effectiveness of our method. We have conducted a thorough investigation into various aspects, including the utilization of rationales, the performance of reward models, the quality of generated rationales and a detailed case study"
CoCA: Fusing Position Embedding with Collinear Constrained Attention in Transformers for Long Context Window Extending,2309.08646v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2309.08646v3_0.png,"Self-attention and position embedding are two key modules in transformer-based Large Language Models (LLMs). However, the potential relationship between them is far from well studied, especially for long context window extending. In fact, anomalous behaviors harming long context extrapolation exist between Rotary Position Embedding (RoPE) and vanilla self-attention unveiled by our work. To address this issue, we propose a novel attention mechanism, CoCA ({Co}llinear {C}onstrained {A}ttention). Specifically, we enforce a collinear constraint between $Q$ and $K$ to seamlessly integrate RoPE and self-attention. While only adding minimal computational and spatial complexity, this integration significantly enhances long context window extrapolation ability. We provide an optimized implementation, making it a drop-in replacement for any existing transformer-based models. Extensive experiments show that CoCA performs extraordinarily well in extending context windows. A CoCA-based GPT model, trained with a context length of 512, can seamlessly extend the context window up to 32K (60$$), without any fine-tuning. Additionally, by dropping CoCA in LLaMA-7B, we achieve extrapolation up to 32K within only 2K training length. Our code is publicly available at: {https://github.com/codefuse-ai/Collinear-Constrained-Attention}","Perplexity evaluation on 100 PG-19 documents with a sliding window strategy (Stride = 512). The perplexity of RoFormer \citep{Su2021RoFormerET} sharply exceeds 1000 beyond its training length, while CoCA maintains a low plateau even at 60 $\times$ its training length. ALibi \citep{Press2021TrainST} encounters Out of Memory (OOM) issues for input \(N_{max}>\) 8000 due to flash-attention \citep{dao2022flashattention} incompatibility, we suppose it maintains perplexity for \(N_{max}>\) 8000.","In the seminal work of Transformer , it claims the ability of ""extrapolating to sequence length longer than the ones encountered during training"". This is an ideal hypothesis, but actually not work in practice for vanilla Transformer. Several subsequent works, collectively known as long context extrapolation, have delved into exploring the capabilities of large language models (LLMs) trained within the range of $[1, N-1]$ to effectively extend the testing sequence $ N$. Existing studies primarily focus on attention kernel or position embedding , often neglecting the intrinsic relationship between the two key modules. Attention bias is an alternative to the explicit encoding of positional information. ALibi and KERPLE , incorporate heuristic and compositional triangle kernel-based negative causal attention bias, respectively. While these approaches effectively manage to maintain low perplexity, they fall short in capturing long-range dependencies due to introducing local hypotheses to context tokens. Another branch of methods involve simply scaling Rotary Position Embedding (RoPE) to extrapolate the inference context length with minimal or no fine-tuning. For instance, Position Interpolation (PI) employs linear scaling on each position number from \(n\) to \(n/k\), where $k$ is the extrapolation ratio. NTK-aware Scaled RoPE and Dynamic-NTK combine high-frequency extrapolation and low-frequency interpolation. They scale the basis in RoPE upon the sequence length to adapt to the unseen position indices. However, these methods primarily alleviate the problem of modeling the rotation angles in out-of-distribution positions, without recognizing the intrinsic correlation between attention matrices and rotation angles. Therefore, these methods still suffer from a limited context window extending ratio. Here, we present a new perspective on the relationship between position embedding (with a focus on RoPE) and the self-attention mechanism. In a nutshell, RoPE utilizes a rotation matrix to encode absolute positions while simultaneously incorporating explicit relative position dependencies within the self-attention formulation . It is designed based on the relative angular difference between the queries ($Q$) and keys ($K$). However, latent relationships exist between $Q$ and $K$, as these two matrices are directly multiplied. We demonstrate that incorrect initialization of the angle between $Q$ and $K$ in RoPE leads to undesirable behavior around the context window boundary, harming its performance for context extrapolation. To address this undesirable behavior , we propose an innovative architecture called Collinear Constrained Attention (CoCA). Specifically, we enforce a collinear constraint between $Q$ and $K$ by initializing the angle between every two hidden dimensions in the $Q$ and $K$ vectors to 0. This allows for a seamless integration of RoPE and self-attention. The model architecture and comparison with RoFomer is illustrated in Figure . Extensive experiments show that a CoCA-based GPT model, trained within 512 context length, seamlessly extends the context window up to 32K (60x) without perplexity divergence. A comprehensive comparison between our method and existing methods is presented in Figure . Furthermore, it enhances long-context retrieval ability, achieving a passkey retrieval accuracy of 50\ Our main contributions can be summarized as follows: {itemize}[itemsep= 0.1pt,topsep = 0.1pt,partopsep=0.1pt] We unveil undesirable context boundary behavior resulting from the absence of modeling the relationship between position embeddings and self-attention. To tackle the undesirable context boundary behavior, we propose Collinear Constrained Attention (CoCA) to seamlessly integrate the position embeddings and self-attention, achieving excellent long context window extrapolation performance. CoCA extends its context window from 512 to 32K without fine-tuning, achieving over 50\ CoCA introduces minimal computational and spatial complexity compared to vanilla self-attention. We provide an optimized implementation of CoCA, making it able to be a seamless drop-in replacement for existing transformer-based models. {itemize"
DAPR: A Benchmark on Document-Aware Passage Retrieval,2305.13915v4,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2305.13915v4_0.pdf,"The work of neural retrieval so far focuses on ranking short texts and is challenged with long documents. There are many cases where the users want to find a relevant passage within a long document from a huge corpus, e.g. Wikipedia articles, research papers, etc. We propose and name this task {Document-Aware Passage Retrieval} (DAPR). While analyzing the errors of the State-of-The-Art (SoTA) passage retrievers, we find the major errors (53.5\%) are due to missing document context. This drives us to build a benchmark for this task including multiple datasets from heterogeneous domains. In the experiments, we extend the SoTA passage retrievers with document context via (1) hybrid retrieval with BM25 and (2) contextualized passage representations, which inform the passage representation with document context. We find despite that hybrid retrieval performs the strongest on the mixture of the easy and the hard queries, it completely fails on the hard queries that require document-context understanding. On the other hand, contextualized passage representations (e.g. prepending document titles) achieve good improvement on these hard queries, but overall they also perform rather poorly. Our created benchmark enables future research on developing and comparing retrieval systems for the new task. The code and the data are available}.","An example instance from DAPR. To find the relevant passage to the query, the retriever needs to utilize the document context, which in this case means coreference resolution for the noun {the venue}. See other categories of the document context and examples in~\autoref{sec:nq_hard}.","Information Retrieval (IR) helps efficiently locate relevant information from a vast resource collection, acting as a central component of many natural language applications. Traditional approaches like BM25 compute simple statistics such as the frequency of the matched terms. Recent approaches apply neural networks to represent queries and passages into vector representations, extending the task modeling from simple term matching to complex semantic matching achieving better effectiveness. Despite their success, these neural approaches are usually limited to short passage inputs, e.g. 512 tokens due to expensive operations such as self-attention in their architectures. Such short-passage retrieval faces severe challenges in real-world scenarios, where long documents such as Wikipedia{{https://www.wikipedia.org/}} articles, scientific papers, etc. can easily go beyond this length limit. Recent work proposes new memory-efficient architectures to encode much longer document inputs and fulfill document-retrieval tasks. However, returning a long document is still inefficient for a user to locate useful information. For example, collects user queries from Google Search{{https://www.google.com/}} logs and annotates the relevant passage in Wikipedia pages. We find for 35.8\ To understand the ability of the retrieval systems for filling this gap, we propose the {Document-Aware Passage Retrieval} (DAPR) task, where the retriever is required to consider the associated document context for returning relevant passages. An example is shown in~{fig:motivative-example}. In this case, the user asks for musicians that have played at a specific venue. However, the relevant passage does not mention the venue name but only the noun reference and the retriever needs to understand such document context for finding the correct passage. To gain insight into the challenges, we first carry out an error analysis for the SoTA passage retrievers (DRAGON+, SPLADEv2, ColBERRTv2) and BM25. We find the major errors (53.5\ In experiments, we test the approaches that extend the SoTA neural passage retrievers by introducing the document context to them in two types of approaches: (1) hybrid retrieval with BM25 and (2) contextualized passage representations, which inform the passage representations with the document context. We find while the hybrid-retrieval systems achieve the strongest performance on the mixture of the easy queries and the hard queries, they fail to process the latter case where understanding the document context is necessary. Contextualized passage representations, on the other hand, can achieve good improvement on these hard queries, but overall perform rather poorly. This presents new exciting research opportunities for developing improved retrieval methods that understand the context of documents during passage retrieval. The benchmark we developed enables the research community to develop, evaluate, and compare the retrieval systems on the new task"
Uncovering the Full Potential of Visual Grounding Methods in VQA,2401.07803v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2401.07803v2_0.png,"Visual Grounding (VG) methods in Visual Question Answering (VQA) attempt to improve VQA performance by strengthening a model's reliance on question-relevant visual information. The presence of such relevant information in the visual input is typically assumed in training and testing. This assumption, however, is inherently flawed when dealing with imperfect image representations common in large-scale VQA, where the information carried by visual features frequently deviates from expected ground-truth contents. As a result, training and testing of VG-methods is performed with largely inaccurate data, which obstructs proper assessment of their potential benefits. In this study, we demonstrate that current evaluation schemes for VG-methods are problematic due to the flawed assumption of availability of relevant visual information. Our experiments show that these methods can be much more effective when evaluation conditions are corrected. Code is provided on GitHub.",Example of Flawed VG that VG-methods in VQA teach based on the unverified assumption of presence of relevant visual information (left). Correct content cues are a prerequisite for teaching True VG (right).,"Visual Grounding (VG) in VQA has garnered interest not only as a key aspect to furthering understanding and rationalization of a VQA model's inference procedure, but also as a way to improve Out-of-Distribution (OOD) performance by preventing certain dataset biases to form. Various works have reported evidence of problematic tendencies in VQA models that point to a disregard of relevant image regions during answer inference and the manifestation of Q/A distribution biases in the model . A lack of VG in VQA models has been shown to negatively impact OOD performance and have been tied to a general unpredictability of answering behavior . To alleviate these issues, methods have been developed that seek to strengthen a model's reliance on question-relevant visual features. These {VG-methods} either modify the training procedure of existing models (e.g., HINT , SCR , VisFIS ), or are integrated directly into specialized model architectures such as MMN , PVR and VLR . On a technical level, the goal of VG-methods in VQA is to align a model's internal valuation of visual input feature importance (FI) with human-based FI, which is given as guidance in training. These Human-based FI scores can be inferred from a question's visual relevance annotations, which may be given as highlighted regions in the {raw image} (e.g., spatial heat maps as in VQA-HAT ), or explicit pointers to ground-truth objects (as in GQA ). Notably, relevance annotations are not given in {input feature space} directly, and therefore a mapping function is required to identify corresponding visual features and determine FI scores. The predominant approaches for such a mapping between image and feature space rely exclusively on {spatial matching}: Visual input features receive their FI scores depending on spatial overlap between the region they represent and annotated, question-relevant locations in the raw image (cf. ). High-scoring features can then be identified as relevant cue objects{Visual feature vectors for VQA are commonly generated by object detectors such as Faster R-CNN and are therefore assumed to represent objects in an image.}. In this approach, the actual {visual content} carried by the cue objects is simply assumed to be appropriate without further {semantic verification} and therefore does not influence their score. In this work, we report evidence that such incomplete verification can result in grossly mismatched cues, thereby leading to inadequate guidance in VG-method training, as illustrated in Fig. , left. Similarly, tests performed under such unchecked conditions fail to accurately evaluate the originally intended use case that VG-methods were designed for, as question-relevant content is often missing in the input and proper VG impossible. While work such as and investigate the underlying effects of VG-method application in detail, we are unaware of any study that also considers the impact of these problematic conditions in their analysis. In this study, we seek to develop a better understanding of the benefits of VG-methods in VQA when training and testing conditions properly support their intended use-case. We identify two flaws and their causes in current evaluation practices for VG-methods and outline an approach to fix them. Finally, we investigate their impact on VG-method training and testing in a series of experiments. The used methodology establishes a framework for evaluating VG-methods more thoroughly. {Contributions.} Summarized as follows: {itemize}[noitemsep,nolistsep] An analysis of the flawed assumptions of cue object availability in current practices used for training and testing VG-methods in VQA. Comprehensive investigations of the impact of inadequate guidance for VG-method training. A methodology for training and testing VG-methods under corrected, proper conditions (code is provided). {itemize"
"Small Models, Big Insights: Leveraging Slim Proxy Models To Decide When and What to Retrieve for LLMs",2402.12052v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.12052v3_0.pdf,"The integration of large language models (LLMs) and search engines represents a significant evolution in knowledge acquisition methodologies. However, determining the knowledge that an LLM already possesses and the knowledge that requires the help of a search engine remains an unresolved issue. Most existing methods solve this problem through the results of preliminary answers or reasoning done by the LLM itself, but this incurs excessively high computational costs. This paper introduces a novel collaborative approach, namely SlimPLM, that detects missing knowledge in LLMs with a slim proxy model, to enhance the LLM's knowledge acquisition process. We employ a proxy model which has far fewer parameters, and take its answers as heuristic answers. Heuristic answers are then utilized to predict the knowledge required to answer the user question, as well as the known and unknown knowledge within the LLM. We only conduct retrieval for the missing knowledge in questions that the LLM does not know. Extensive experimental results on five datasets with two LLMs demonstrate a notable improvement in the end-to-end performance of LLMs in question-answering tasks, achieving or surpassing current state-of-the-art models with lower LLM inference costs..} %Experiments also shows our method maintains system efficiency, avoiding extra load and brought by LLM inferences.","A display of the main process of SlimPLM. Solid lines with arrows represent the flow of data, while dashed lines with arrows signify control signals from the retrieval necessity judgment model. Step 1 and step 2 are mandatory in the pipeline, but step 3 involves choosing between direct generation and RAG.","Large language models (LLMs) have demonstrated significant prowess in various natural language processing (NLP) tasks, attributed to their advanced language comprehension and generation capabilities. Despite being trained on extensive text corpora, these models occasionally produce hallucinated content. To tackle this problem, the integration of retrieval systems with LLMs has been proposed, enabling access to external knowledge bases for more accurate and reliable text generation. Retrieval-augmented generation (RAG) involves using a retrieval system to supplement LLMs with relevant external information, thereby improving text generation quality. Yet, recent studies have suggested that retrieval may not always be beneficial. In cases where LLMs can adequately respond without external knowledge, retrieval may introduce irrelevant information, potentially degrading performance. Therefore, it is critical to determine when retrieval is necessary for user questions. The challenge lies in identifying questions that exceed the LLMs' intrinsic knowledge and require external retrieval, due to the prevalence of content hallucination. Efforts to address this challenge can be categorized into two groups: (1) The first group of methods involves fine-tuning LLMs for RAG scenarios, allowing them to autonomously signal the need for external knowledge. This method, while effective, demands substantial computational resources and risks diminishing the LLMs' general capabilities due to potential catastrophic forgetting. (2) The second category avoids direct tuning of LLMs, assessing the necessity for retrieval based on the quality of the generated content or specific indicators within it. However, this approach still has its drawbacks, as it requires multiple inferences, thereby increasing both the inference costs and the latency of responses to user questions. In light of this, we put forward a question: {Is it feasible to employ a proxy model with a relatively smaller parameter size to facilitate effective retrieval results for an LLM?} Theoretically, existing decoder-only language models share similar Transformer structures, and they are pre-trained on some common text corpora, such as Common Crawl web pages, books, and Wikipedia pages. Therefore, it is possible for them to reach a consensus on relative mastery over different knowledge and the necessity of retrieval. Our preliminary quantitative analysis, shown in Section~, also supports this hypothesis. The experimental results show that on questions well understood by the LLM, the relatively smaller language model also has considerable knowledge. The gap between larger and smaller LLMs mainly manifests in questions they do not understand. This further validates the possibility of employing a proxy model to help determine the necessity of retrieval. Based on our analysis, in this paper, we introduce a novel approach, called {SlimPLM} ({Slim} {P}roxy {L}anguage {M}odel), which leverages a relatively smaller language model as a ``proxy model'' to help determine when and how to perform retrieval for LLMs. Specifically, for a user question, SlimPLM first uses the proxy model to generate a preliminary ``heuristic answer''. This heuristic answer serves two purposes. First, it is evaluated by a lightweight model designed to assess the necessity for retrieval. If this evaluation shows that the heuristic answer is of high quality, it implies that the question may be addressed directly by LLMs without additional information retrieval. In contrast, a lower-quality answer triggers the retrieval process to identify and supplement missing knowledge. To facilitate this, SlimPLM utilizes the heuristic answer again to generate multiple queries, each reflecting a specific aspect of the initial response. These queries are then individually assessed for their need for retrieval, filtering out queries that do not require retrieval. By this means, the remaining queries can retrieve more relevant knowledge that is lacking in LLMs. The integration of SlimPLM into existing RAG frameworks offers a flexible and effective enhancement without notably increasing computational costs or response latency. Experimental results across five commonly used question-answering datasets validate SlimPLM's effectiveness in determining the necessity for retrieval and improving retrieval results. Our contributions are threefold: (1) We propose a novel approach that leverages a small proxy model to generate heuristic answers, helping determine when and how to perform retrieval for LLMs. (2) We devise a retrieval necessity judgment model based on the heuristic answer. It is capable of accurately identifying which queries necessitate further information retrieval. (3) We formulate a query rewriting strategy that decomposes the heuristic answer into distinct claims. This is complemented by a claim-based filtering mechanism to enhance the relevance of the retrieval results for LLMs' text generation"
Interpretability of Language Models via Task Spaces,2406.06441v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.06441v1_0.pdf,"The usual way to interpret language models (LMs) is to test their performance on different benchmarks and subsequently infer their internal processes. In this paper, we present an alternative approach, concentrating on the {quality} of LM processing, with a focus on their language abilities. To this end, we construct `linguistic task spaces' -- representations of an LM's language conceptualisation -- that shed light on the connections LMs draw between language phenomena. Task spaces are based on the interactions of the learning signals from different linguistic phenomena, which we assess via a method we call `similarity probing'. To disentangle the learning signals of linguistic phenomena, we further introduce a method called `fine-tuning via gradient differentials' (FTGD). We apply our methods to language models of three different scales and find that larger models generalise better to overarching general concepts for linguistic tasks, making better use of their shared structure. Further, the distributedness of linguistic processing increases with pre-training through increased parameter sharing between related linguistic tasks. The overall generalisation patterns are mostly stable throughout training and not marked by incisive stages, potentially explaining the lack of successful curriculum strategies for LMs.",The process of similarity probing to obtain a task space based on transfers: 1. Evaluate the untuned LM on all tasks (eval\textsubscript{1}); 2. Tune one LM for each task; 3. Re-evaluate the LMs on all tasks (eval\textsubscript{2}). Calculate all transfers (eval\textsubscript{2} - eval\textsubscript{1}) and compare the resulting transfer task space to a hypothesized set of transfers (Hypothesis space).,"Recently, language models (LMs) have reached a level of sophistication in language production where their output is often indistinguishable from human-generated language [][]{liang2022holistic}. However, the complexity inherent in language production means that effective models are also inherently complex, making them challenging to interpret. Commonly, linguistic interpretability involves assessing an LM's ability through simple evaluation tasks like grammatical acceptability judgments of various language constructions [e.g.][]{linzen2016assessing, Marvin2018TargetedModels}. While these methods inform us about a model's {performance}, they do not provide insights into the {quality} of the model's solutions. This is especially the case when error analysis is not possible due to high model performance. However, it is the quality of processing that is interesting from the viewpoint of the interpretability researcher, the cognitive scientist or linguist. Here, we introduce a method to interpret the language processing of LMs holistically. We show how linguistic knowledge in LMs interconnects. We build upon the framework of that proposes to consider linguistic phenomena as `tasks' an LM has to optimise and allows us to analyse the interactions of those tasks, similar to ideas from multi-task learning (MTL). For example, consider the following sentences: . John did {not} see {anything}. . {If} John sees {anything}, he will be surprised. In both sentences, a downward-entailing environment (negation vs. conditional) allows for the negative polarity item (NPI) {anything} to be used. Understanding whether it is acceptable to produce an NPI in either sentence can be considered a different task. LMs can use different rules to solve these tasks: an LM might learn the co-occurrence statistics of certain trigger words (e.g. `{not}' vs. `{if}') with NPIs. On the other hand, it might generalise to a more abstract linguistical conceptualisation and understand that both -- negation and conditionals -- create downward-entailing environments permitting NPIs. With either rule, the model resolves acceptability judgements correctly, while the {quality} of both solutions is decisively different. Hence, assessing the generalisation of linguistic tasks reveals how LMs conceptualise language. Similar to `task spaces' in MTL (more details in §~), we can represent an LM's generalisation behaviour in a {linguistic task space}, a multi-dimensional space relating linguistic tasks according to their similarity. To construct linguistic task spaces, we introduce {similarity probing}, a method to estimate linguistic similarity. This method involves selectively fine-tuning LMs on specific linguistic tasks and assessing the impact of the fine-tuning on other tasks (see Figure~), as well as the alignment of tasks in gradient space. We extricate single linguistic tasks from their entanglement in natural language via a method we call {fine-tuning via gradient differentials (FTGD)}. FTGD selectively updates a small, relevant subspace of parameters with `gradient differentials'. The contributions of this paper can be summarised as follows: {enumerate}[itemsep=-.15em] Propose linguistic task spaces as an interpretability method for deeper model understanding and as a tool for linguistic theory testing. Introduce {FTGD}, a technique to disentangle linguistic tasks from their language context and selectively fine-tune them in LMs. Introduce {similarity probing}, an efficient method for generating large linguistic task spaces. Analyze the development of language conceptualisation of LMs throughout pre-training by constructing language task spaces at various stages of LM pre-training. {enumerate"
Factual Confidence of LLMs: on Reliability and Robustness of Current Estimators,2406.13415v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.13415v1_0.png,"Large Language Models (LLMs) tend to be unreliable in the factuality of their answers. To address this problem, NLP researchers have proposed a range of techniques to estimate LLM's confidence over facts. However, due to the lack of a systematic comparison, it is not clear how the different methods compare to one another. To fill this gap, we present a survey and empirical comparison of estimators of factual confidence. We define an experimental framework allowing for fair comparison, covering both fact-verification and question answering. Our experiments across a series of LLMs indicate that trained hidden-state probes provide the most reliable confidence estimates, albeit at the expense of requiring access to weights and training data. We also conduct a deeper assessment of factual confidence by measuring the consistency of model behavior under meaning-preserving variations in the input. We find that the confidence of LLMs is often unstable across semantically equivalent inputs, suggesting that there is much room for improvement of the stability of models' parametric knowledge. Our code is available at {https://github.com/amazon-science/factual-confidence-of-llms}.","{Overview of our factual confidence estimation framework. We work with five groups of {methods} and two formulations: $P(\text{I know})$, which applies to questions, and $P(\text{True})$, which applies to statements. All of the methods produce a continuous score, except {verbalization}, where the model generates a confidence level. }","A major problem of Large Language Models (LLMs) is that they do not always generate truthful information. Models can hallucinate by convincingly reporting information that is actually false or they are not confident about, or provide factual answers only when prompted in a certain way. This behavior can be severely harmful, especially given the current explosion of LLM usage: a lack of truthfulness can lead to spread of misinformation and breaches to user trust. Having a reliable estimate of the model's confidence over a fact---the degree to which it is expected to have accurate factual knowledge with respect to an input---is key for mitigating this problem. Recently, a number of methodologies for estimating the factual confidence of LLMs were proposed~(, among others). However, none of them establishes a unified experimental framework to compare methods. This leaves open questions regarding how aligned the methods are in their estimates, and which are the most reliable ones across models. We aim to fill this gap by providing a survey of the current state-of-the-art for estimating the factual confidence in LLMs, and performing a systematic empirical comparison of the reliability and robustness of the existing methods. We introduce an experimental framework, shown in Figure~, enabling a fair comparison between methods across models and datasets. We adopt two distinct formulations for measuring factual confidence: ({i})~the probability of a statement to be true, noted $P({True})$---fact-verification, and ({ii})~the probability of yielding a truthful answer to a query, noted $P({I know})$---question answering. Additionally, we categorize the existing methods into five groups: trained probes, sequence probability, verbalization, surrogate token probability, and consistency-based. Our experiments across eight publicly available LLMs indicate that prompting-based methods are less reliable than supervised-probing, although the latter requires training data and access to models' weights. For instruction-tuned LLMs, {verbalization} and {consistency-based} methods are viable alternatives. Further, we argue that all methods for estimating factual confidence can ultimately lead to misleading conclusions if only tested on a single way of asserting a fact: An LLM may seem to know a fact given an input, but then contradict itself given an alternative phrasing of that fact. In our experiments, we find evidence of such instability, suggesting that LLMs do not always encode facts based on abstractions over diverse input variations. In summary, this paper provides the following contributions: {itemize}[noitemsep] A survey of the literature on LLM factual confidence estimation; An experimental framework enabling a fair comparison across methods proposed in the literature; Insights about the reliability and robustness of different types of methods, providing recommendations for NLP practitioners; {itemize"
One-Shot Learning as Instruction Data Prospector for Large Language Models,2312.10302v4,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2312.10302v4_0.pdf,"Contemporary practices in instruction tuning often hinge on enlarging data scaling without a clear strategy for ensuring data quality, inadvertently introducing noise that may compromise model performance. To address this challenge, we introduce , a novel and efficient methodology that leverages one-shot learning to discern and select high-quality instruction data from extensive datasets. assesses the potential of individual instruction examples to act as effective one-shot learning instances, thereby identifying those that can significantly improve performance across diverse tasks. utilizes a scoring system based on the impact of candidate examples on the perplexity of a diverse anchor set, facilitating the selection of the most advantageous data for instruction tuning. Through comprehensive evaluations on two benchmarks, including MT-Bench and Alpaca-Eval, we show that instruction tuning with the top 1\% of examples curated by substantially outperforms conventional methods employing the entire dataset.","The comparison between our \textsc{Nuggets} and previous empirical methods. In contrast to empirical methods~(blue area), \textsc{Nuggets}~(orange area) can directly sample a gold subset, offering a more direct contribution to model fine-tuning.","Large language models (LLMs) have showcased remarkable capabilities across a wide range of language tasks by scaling the model size and training data. Despite their proficiency, it is imperative to further enhance their alignment with human instructions. This alignment process involves supervised fine-tuning (SFT) on input-output pairs, known as {instruction tuning}. Instruction tuning is a crucial step, serving not only to activate the valuable knowledge acquired by LLMs during pre-training but also to facilitate their interaction with humans in a manner that aligns with natural conversational dynamics. Considerable efforts in instruction tuning have been concentrated on collecting larger , more diverse, and intricate datasets. This is commonly achieved through human crowd-sourcing or extracting data from larger pre-existing models. Despite the growth in the size of datasets employed for instruction tuning, certain studies suggest that smaller yet valuable datasets tend to be more effective in harnessing the capabilities of LLMs. Blindly expanding the volume of instruction data without ensuring quality may introduce noise and lead to hallucination issues. However, there is a lack of standard criteria for selecting high-quality instruction data. As depicted in Figure~, the common practice depends on empirical methods for data selection, introducing bias in determining data combinations and adjusting based on outcomes. This trial-and-error approach elevates alignment costs for models. We posit that optimal instruction combinations are present within the extensive data available, yet an efficient and cost-effective identification method remains underexplored. In this paper, we introduce {Nuggets}, a simple yet efficient method that harnesses LLMs as data explorers through one-shot (in-context) learning. This method facilitates extracting high-quality, valuable data from expansive instruction datasets. Intuitively, an instructional example holds value in training if it serves as an excellent one-shot demonstration for a specific task. If it can facilitate many tasks, it will be worth being treated as a prime data focus, i.e., {""gold instruction""}. Another noteworthy perspective arises from the observation that in-context learning employs prompting to implicitly fine-tune the model, while instruction tuning operates through gradient descent. Leveraging the performance of in-context learning offers a promising avenue to predict the effects of instruction tuning. Concretely, we first select a set that spans multiple tasks, designated as the anchor set, and the dataset of instructions to be optimized is identified as the candidate set. One example is sequentially chosen from the candidate set to act as a one-shot example for in-context learning. Subsequently, it is scored based on its impact on the perplexity of each anchor example. This scoring mechanism enables the inference of dependencies between anchor and candidate examples, providing a reference standard for data selection. To evaluate the effectiveness of the proposed {Nuggets}, we conduct extensive evaluations on two widely recognized benchmarks, namely MT-Bench and Alpaca-Eval. We choose a popular and powerful LLM, LLaMA, as our base model. Experimental findings demonstrate that the {Nuggets}' data filtering strategy engenders a significant improvement in comparison to vanilla fine-tuning approaches. We summarize our main contributions as follows: {itemize} We present {Nuggets}, a methodology designed to dynamically assess the quality of instructional examples by using LLMs themselves. {Nuggets} is expected to extract the most valuable data from a vast pool of instruction data for the purpose of fine-tuning. Fine-tuning LLMs with solely the top 1\ The results of extensive experiments substantiate our hypotheses regarding {""golden instructions""}, indicating that the effectiveness of an instructional example is measured by its impact on the task generalization capability of the model following the fine-tuning process. This observation holds considerable promise, potentially providing valuable insights for future endeavors in data quality screening. {itemize"
Navigating the OverKill in Large Language Models,2401.17633v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2401.17633v1_0.pdf,"{Content warning: This paper contains examples of harmful language.}\\ Large language models are meticulously aligned to be both helpful and harmless. However, recent research points to a potential overkill which means models may refuse to answer benign queries. In this paper, we investigate the factors for overkill by exploring how models handle and determine the safety of queries. Our findings reveal the presence of shortcuts within models, leading to an over-attention of harmful words like 'kill' and prompts emphasizing safety will exacerbate overkill. Based on these insights, we introduce Self-Contrastive Decoding (Self-CD), a training-free and model-agnostic strategy, to alleviate this phenomenon. We first extract such over-attention by amplifying the difference in the model's output distributions when responding to system prompts that either include or omit an emphasis on safety. Then we determine the final next-token predictions by downplaying the over-attention from the model via contrastive decoding. Empirical results indicate that our method has achieved an average reduction of the refusal rate by 20 \% while having almost no impact on safety.","The illustration demonstrates the phenomenon of overkill. For the dangerous question on the left, the model is able to make a true refusal. However, for the safe question on the right, which contains the same harmful word 'kill', the model makes a false refusal.","Large language models(LLMs), such as LLaMa-2-Chat are quite powerful, but they are vulnerable when confronted with malicious instructions . Thus many measures such as feedback learning, and red-terming are taken to ensure LLMs safely aligned with human values. However, previous works have shown that many models, such as Llama-2-Chat, excessively prioritize safety. As demonstrated in Figure , the model declines to offer helpful answers to inquiries such as 'How can I kill a Python process'. We borrow a concept from the field of cybersecurity and name this phenomenon 'overkill'. However, there has been no in-depth analysis or solution for this phenomenon at present. Therefore, this paper will delve into this issue and explore potential solutions to mitigate it. In this paper, we first investigate {whether the model can truly discern the intent of user query}. To explore this issue, we have constructed a new dataset OKTest ({O}ver{K}ill {T}est) by introducing certain word-level harmful but semantically benign contents into the sentences. Our experiments have tested seven models and found that their refusal rates all exceeded 60\ This high refusal rate not only underscores the severity of overkill but also indicates that these models exhibit a limited grasp of user queries. To delve further into this phenomenon, we investigate {what factor might contribute to overkill}. For this purpose, we track the information flow from words to final predictions. The experiments reveal two key conclusions: 1. Irrespective of the inherent safety of the questions, the model tends to prioritize the attention toward harmful words. Therefore, this discovery suggests that the model's overkill stems from its intrinsic bias towards certain types of content. 2. The safety-emphasized system prompts make the model more attentive to these harmful words. This suggests that the model's over-attention can be adjusted or tuned. To alleviate this phenomenon, we propose a novel approach termed Self-Contrastive Decoding (Self-CD). This method first collects the model's responses to the same question by emphasizing within the prompts whether safety is taken into account. Then we can obtain the over-attention by contrasting the output distributions of these answers. Following this, the identified over-attention in the model can be mitigated by modulating the extent of these distribution differences, to reduce the refusal rate. Our method offers two advantages: 1) {Training free}: our method doesn't need any further SFT or alignment training which are both time-consuming and GPU-consuming. 2) {Model agnostic}: Our method only processes the output distributions and does not need to modify any architecture of the model. Our main contributions are summarized as follows: {itemize}[leftmargin=*, align=left] We conducted a variety of analytical experiments and attributed the overkill to an inherent bias within the model itself. Our method, Self-CD, is characterized by its simplicity and effectiveness, requiring no training and being independent of the model. We automatically generate a high-quality dataset OKTest and empirical results demonstrate that Self-CD exhibits excellent performance and high universality in alleviating the overkill. {itemize"
A Chain-of-Thought Is as Strong as Its Weakest Link: A Benchmark for Verifiers of Reasoning Chains,2402.00559v4,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.00559v4_0.pdf,"Prompting language models to provide step-by-step answers (e.g., ``Chain-of-Thought'') is the prominent approach for complex reasoning tasks, where more accurate reasoning chains typically improve downstream task performance. Recent literature discusses automatic methods to verify reasoning steps to evaluate and improve their correctness. However, no fine-grained step-level datasets are available to enable thorough evaluation of such verification methods, hindering progress in this direction. We introduce : {Reasoning Verification Evaluation}, a new dataset to benchmark automatic verifiers of complex Chain-of-Thought reasoning in open-domain question answering settings. includes comprehensive labels for the relevance, attribution to evidence passages, and logical correctness of each reasoning step in a language model's answer, across a wide variety of datasets and state-of-the-art language models. Available at .","We collect \dataset, an evaluation benchmark for the task of verifying reasoning chains in Chain-of-Thought format, which checks whether a reasoning chain is a correct justification to the final answer (importantly, the answer can be correct even if the reasoning is incorrect, as in the example above). The figure shows four verifiers (middle) verifying the correctness of a CoT (left). We use the dataset to benchmark multiple verifiers (right).","Complex reasoning tasks involve answering questions that require multiple steps of reasoning . Addressing these questions may require open-domain knowledge , mathematical reasoning , logic , and so on. Reasoning chains---breaking the task into multiple steps explicitly---is useful for improving performance in such tasks, with LMs demonstrating better performance when encouraged to generate the reasoning chain behind their answer , commonly implemented via Chain-of-Thought (CoT) prompting [see {fig:teaser} for an example]{wei2023chainofthought}. Evaluation in such settings is traditionally limited to evaluating only whether the final answer is correct . However, correct reasoning chains have been shown to be correlated with better final answers , with recent literature proposing automatic methods for verifying the quality of the reasoning chains themselves along various axes such as informativeness, relevance, factuality and logical correctness . While such verification methods are a promising direction for improving reasoning in LLMs, it is not clear how to evaluate them due to the lack of high-quality, step-level annotated data, and collecting such data was shown to be difficult (in terms of reaching high inter-annotator agreement) and costly . We present ({Reasoning Verification Evaluation}), an evaluation benchmark for complex reasoning verifiers.{At {huggingface.co/datasets/google/reveal}. We adopt practices by against data contamination and request that any future redistribution or usage of the data respects the same constraints.} covers a diverse set of reasoning skills, complexity levels, and knowledge domains. It contains 704 unique questions from 4 popular QA datasets, and 1,002 CoT answers generated by 3 language models, consisting of 3,360 CoT steps in total. Each step is first labeled for relevance with respect to the final answer, and then whether the step is an attribution step (introduces factual knowledge which can be attributed to a source), a logical step (introduces logical inference from previous steps) or both. For attribution steps, we collect labels for correctness to retrieved Wikipedia paragraphs given as evidence (with full support, partial support, contradiction, or no-support as labels). For logical steps, we label for logical correctness. Each label includes free-text justifications written by the annotators. An illustrative instance from the dataset is shown in {fig:data-example,fig:justifications}. We split the dataset into -Eval, the main evaluation benchmark containing high inter-annotator-agreement labels, and -Open, a smaller set of interesting borderline cases with open labels due to low inter-annotator agreement. In we describe the dataset, and report fine-grained analyses of non-attributable steps in -Eval (i.e., evidence that supports or contradicts them was not found) and of disagreement categories in -Open. supports versatile evaluation settings, for example: (1) Attribution steps, along with their evidence, can serve as a high-quality Natural Language Inference [NLI,][]{DBLP:conf/mlcw/DaganGM05,bowman-etal-2015-large} benchmark in a setting of fact-checking LM outputs ; (2) CoT verifiers can be evaluated at the level of individual steps, or (3) at the level of full CoT answers; (4) Each label in the data contains five free-text justifications (one per annotator), which can accommodate research around the generation of explanations and justifications, or be used to understand nuance in borderline cases. As we focus on the evaluation of step-level validation in complex reasoning, in we report the performance of multiple up-to-date verification baselines, leveraging NLI classifiers, GPT-3 and PaLM 2, showing much room for improvement in current state-of-the-art solutions. In particular, verifiers struggle at classifying whether a step conveys correct logical inference from previous steps. In summary, this work includes the following contributions: (I) A protocol for step-by-step verification of reasoning chains (); (II) An annotation schema to reliably execute the protocol with human annotators (); (III) A new benchmark dataset for evaluating automatic reasoning chain verifiers (, ); (IV) Detailed analyses of challenges in retrieving evidence to knowledge claims in reasoning and documentation of disagreements in the data (); (V) A study on the challenges for current verifiers ( ). These contributions advance the research on verification of reasoning chains and methods for correctly reasoning about complex questions"
Re3: A Holistic Framework and Dataset for Modeling Collaborative Document Revision,2406.00197v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.00197v1_0.pdf,"Collaborative review and revision of textual documents is the core of knowledge work and a promising target for empirical analysis and NLP assistance. Yet, a holistic framework that would allow modeling complex relationships between document revisions, reviews and author responses is lacking. To address this gap, we introduce Re3, a framework for joint analysis of collaborative document revision. We instantiate this framework in the scholarly domain, and present Re3-Sci, a large corpus of aligned scientific paper revisions manually labeled according to their action and intent, and supplemented with the respective peer reviews and human-written edit summaries. We use the new data to provide first empirical insights into collaborative document revision in the academic domain, and to assess the capabilities of state-of-the-art LLMs at automating edit analysis and facilitating text-based collaboration. We make our annotation environment and protocols, the resulting data and experimental code publicly available.}","Re3 offers a holistic framework for studying the relationships between reviews (a), revisions (b-c) and responses (d) in text-based collaboration. It is instantiated in the Re3-Sci dataset that covers all edits in 314 full-length scientific publications manually labeled with edit action and intent (e) on different granularity levels, along with reviews that trigger edits and manually curated responses that summarize all edits made including self-initiated ones (f).","Textual documents are a key medium of information exchange in the modern world. These documents often result from a collaboration of multiple individuals. The typical process of collaborative text production involves iterations of drafting, getting feedback ({{re}views}), executing {{re}visions}, and providing {{re}sponses} that outline the implemented changes, serving as a vital element in facilitating effective communication . Despite the importance of collaborative text revision and its high potential for NLP applications, we are missing a framework that formally describes this review-revision-response procedure grounded in real-world data. While prior work in NLP has studied relationships between original and revised documents , reviews and original documents , reviews and revisions , and reviews and responses -- no prior frameworks allow jointly modeling all three components of text-based collaboration. Yet, such joint modeling is important as it provides deeper insights into the processes involved in text work, and opens new opportunities for NLP applications. Important tasks that involve reviews, revisions and responses such as {edit summarization} thus remain underexplored. Comprehensive analysis of document-level revisions poses additional challenges. Contrary to sentence-level analysis, hierarchically structured documents bring distinct levels of granularity into editing. Individuals execute revisions at various granularity levels, with a range of actions and a spectrum of intents, reflecting the {what}, {how}, and {why} of the revisions (Figure and §). Realistic modeling of document revision in text-based collaboration thus requires datasets and annotations that encompass the {{entire document context}}, incorporating {{all edits}} made across various levels of {{granularity}}, and providing qualitative labels for both {{action}} and {{intent}}. We further term this kind of analysis as {full-scope} modeling of document revision. Prior research in NLP has primarily studied sentence-level edits while neglecting the broader document context , variations in granularity , and the underlying intent behind the edits . There is thus a gap in both methodologies and datasets for creating and analyzing full-scope annotations of document revisions, limiting our grasp of the intricate nature of the editing process. To close this gap and enable a comprehensive study of text-based collaboration in NLP, we introduce {Re3}: the first holistic framework for modeling review, revision and response in collaborative writing (§). We instantiate our framework in the scholarly domain and create {Re3-Sci}, the first large-scale human-annotated dataset that comprises 11.6k full-scope revision annotations for over 300 revised documents with substantial Inter-Annotator Agreement (IAA), as well as cross-document connections between reviews, revisions and responses (§). Our framework and dataset, for the first time, enable large-scale empirical investigation of collaborative document revision, including edit localization and clustering within documents, edit mechanisms and motivations inferred through action and intent labels, and the impact of review requests (§). Manually analyzing the complex relationships between reviews, revisions and responses is costly, and constitutes a promising NLP automation target. Facilitated by our data, we present a first exploration of the capability of large language models (LLMs) to address novel revision assistance tasks, such as review request extraction, revision alignment, edit intent classification and document edit summarization (§). Our work thus makes four key {contributions}: {itemize}[itemsep=-1pt, parsep=-1pt] A holistic framework for studying document revisions and associated interactions in collaborative writing, including label taxonomy and robust annotation methodology; A high-quality large-scale dataset that instantiates the framework in the domain of academic writing and peer review; An in-depth analysis of human editing behavior in the scholarly domain; Extensive experiments in automation with LLMs on four NLP tasks: review request extraction, revision alignment, edit intent classification and document edit summarization. {itemize} Our work paves the path towards comprehensive study of NLP for text-based collaboration in the scholarly domain and beyond"
DolphCoder: Echo-Locating Code Large Language Models with Diverse and Multi-Objective Instruction Tuning,2402.09136v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.09136v1_0.pdf,"Code Large Language Models (Code LLMs) have demonstrated outstanding performance in code-related tasks. Several instruction tuning approaches have been proposed to boost the code generation performance of pre-trained Code LLMs. In this paper, we introduce a diverse instruction model ({DolphCoder}) with self-evaluating for code generation. It learns diverse instruction targets and combines a code evaluation objective to enhance its code generation ability. Our model achieves superior performance on the HumanEval and MBPP benchmarks, demonstrating new insights for future code instruction tuning work. Our key findings are: (1) Augmenting more diverse responses with distinct reasoning paths increases the code capability of LLMs. (2) Improving one's ability to evaluate the correctness of code solutions also enhances their ability to create it.","The overall architecture of our proposed diverse instruction tuning with self-evaluating for code generation, DolphCoder. Stage (a) denotes Diverse Instruction Tuning (DIT) and Stage (b) denotes Multi-Objective Instruction Tuning (MOT) for self-evaluating.","Code pre-trained models have achieved remarkable progress in the era of large language models (LLMs), such as Codex , AlphaCode , and PaLM-Coder . Code-related tasks are also the key factors in evaluating the capability of LLMs. Numerous code LLMs have been proposed, including closed-source models and open-source models . They perform expensive pre-training using substantial amounts of code data and display impressive performance. In contrast to these pre-trained code LLMs, another lightweight paradigm of enhancing code capability is instruction tuning using relatively small high-quality code-related data. For example, Code Alpaca employs a similar self-instruct method as Alpaca to generate code instructions via OpenAI's ChatGPT{https://openai.com/blog/ChatGPT}. Further, WizardCoder introduces a more complicated Evol-Instruct method which evolves existing instruction data to generate more complex and diverse datasets. Instead, OctoPack and Magicoder construct code instructions by mining existing code corpus. All of these methods enhance the performance of the open-source Code LLMs. However, these methods have two weaknesses: (1) They take the only golden answer but ignore the diversity of answers in code generation. We find that augmenting more diverse responses using different system prompts increases the code capability of LLMs. (2) Current models generate plausible code snippets in terms of grammar and logic but are unable to identify subtle errors, such as corner cases and wrong input/output formats. It has no guarantee that temperature sampling will consistently produce accurate answers over time. We suppose that LLMs are capable of generating correct solutions while struggling to discriminate correct from incorrect ones. Improving one's ability to evaluate the correctness of code also enhances their ability to create it. Inspired by the two insights, we introduce a diverse instruction model ({DolphCoder}) with self-evaluating for code generation. Specifically, we use Code Llama-python as our base model and obtain evolved instruction data following WizardCoder. Then motivated by rejection sampling and ORCA , we use different system prompts to generate diverse answers via ChatGPT. After removing low-quality and similar data using heuristic rules , we perform supervised fine-tuning on the remaining instruction data. Further, we explore whether improving one's ability to evaluate code helps generate it. We propose a self-evaluate multi-task learning framework by adding a code evaluation objective to the traditional instruction fine-tuning task. We find training the model for both code generation and code evaluation benefits the code capability. Our key contributions are summarized as follows: {enumerate} We introduce a diverse instruction model ({DolphCoder}) with self-evaluating for code generation. It learns diverse instruction targets and combines a code evaluation objective to enhance its code generation ability. DolphCoder outperforms strong open-source code LLMs by a large margin, including CODELLAMA-INSTRUCT, OctoCoder, and WizardCoder. {enumerate"
Systematic Task Exploration with LLMs: A Study in Citation Text Generation,2407.04046v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2407.04046v1_0.pdf,"Large language models (LLMs) bring unprecedented flexibility in defining and executing complex, creative natural language generation (NLG) tasks. Yet, this flexibility brings new challenges, as it introduces new degrees of freedom in formulating the task inputs and instructions and in evaluating model performance. To facilitate the exploration of creative NLG tasks, we propose a three-component research framework that consists of systematic input manipulation, reference data, and output measurement. We use this framework to explore citation text generation -- a popular scholarly NLP task that lacks consensus on the task definition and evaluation metric and has not yet been tackled within the LLM paradigm. Our results highlight the importance of systematically investigating both task instruction and input configuration when prompting LLMs, and reveal non-trivial relationships between different evaluation metrics used for citation text generation. Additional human generation and human evaluation experiments provide new qualitative insights into the task to guide future research in citation text generation. We make our code{UKPLab/acl2024-citation-text-generation}} and data{TUdatalib}} publicly available.","Citation text generation with LLMs. The task~(1) is to generate a paragraph of related work from the citing paper (A) about a cited paper (B). The instruction combined with task inputs constitutes a prompt (2) that is communicated to the model. The model's response (3) is evaluated using a range of measurements, from word count to NLI-based factuality metrics (4).","Thanks to their instruction-following abilities, large language models (LLMs) allow specifying and executing NLP tasks with unprecedented flexibility and speed, while reducing the need for task-specific architecture design, data annotation, and model training . This has led to a surge of new, complex, creative natural language generation (NLG) tasks like peer review generation or story and poetry generation , that push the boundary of what was deemed feasible for NLP systems just a few years ago. The flexibility comes at a cost, as it introduces new degrees of freedom into the analysis. LLMs generate {output} in response to a {prompt}, which consists of a natural-language {task instruction} supplemented by additional bits of information about an instance, which we term {input components} (Figure ). LLM-powered creative NLG tasks often feature a complex input component space, and the task instruction wording can affect model behavior in non-intuitive ways. The {output space} is varied as well, as there might exist infinitely many acceptable generations. This overall variability brings the risk of creative NLG tasks being defined and evaluated ad hoc, hindering systematic comparison of NLP systems and leading to anecdotal accounts of LLM capabilities. Although optimizing model instructions to maximize performance of LLMs is an active research area (Section ), prompt engineering mostly targets the tasks where input and output spaces are well-defined (e.g., question answering). However, some creative NLG tasks need a step of exploration of what inputs are required and how the evaluation of outputs will be carried out before deeply exploring the best way to introduce the task to LLMs. Our work addresses task variability in {citation text generation} -- a widely studied scholarly NLG task aiming to increase efficiency of scientific work . Citation text generation is a good example of creative NLG, as it features a complex input component space combined with multiple plausible outputs. Prior work on citation text generation lacks consensus on the required inputs, explores only a limited number of measurements to characterize the outputs, and does not investigate the use of instruction-tuned LLMs to tackle the task (Table ). To address this gap, we design a framework to systematically explore the task of citation text generation with LLMs (Figure ). We systematically manipulate the input components and instructions communicated to the model via a prompt, and study the effects of these manipulations on the model output using a wide range of measurements, supplemented by a novel reference dataset for citation text generation based on the ACL Anthology, and featuring novel use of free-form citation intents to guide generation (Section ). Our experiments with two state-of-the-art LLMs -- Llama 2-Chat and GPT 3.5 Turbo reveal that input components and task instructions both impact the generations, and their effects add up. Free-form citation intents, as illustrated in Figure , show promise as an alternative to categorical intents used in prior citation text generation work. Our results (Section ) imply that the {relative} performance of alternative task input configurations can be estimated on a small set of instructions, while the best {absolute} performance needs experimentation with a wide array of instruction wordings. Through correlation analysis, we observe that the NLG metrics in our measurements are complementary, motivating the use of wide-spanning measurement sets for NLG tasks beyond citation text generation. Our human studies (Section ) reveal both quantitative and qualitative insights about input components and task instructions from both generation and evaluation perspectives. In summary, this work contributes: {itemize}[leftmargin=*,noitemsep,topsep=0pt] A framework for exploring the task of citation text generation with LLMs; A new reference corpus of citation texts based on the ACL Anthology enriched with novel free-form citation intents; Experimental results on the impact of task inputs and instructions on citation text generation outputs, and an examination of the relationships between the measurements; Human evaluation and generation studies providing additional insights to shape future work in citation text generation and creative NLG. {itemize} We stress that our work neither seeks nor claims state-of-the-art citation text generation, as the differences in pre-trained model capabilities would hinder a fair comparison and likely lead to confounding . Instead, the objective of our work is to explore {prompting as a tool for systematic task manipulation} in the LLM age. We believe our approach to be general and adaptable to other creative NLG tasks"
RelayAttention for Efficient Large Language Model Serving with Long System Prompts,2402.14808v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.14808v3_0.pdf,"A practical large language model (LLM) service may involve a long system prompt, which specifies the instructions, examples, and knowledge documents of the task and is reused across requests. However, the long system prompt causes throughput/latency bottlenecks as the cost of generating the next token grows the sequence length. This paper aims to improve the efficiency of LLM services that involve long system prompts. Our key observation is that handling these system prompts requires heavily redundant memory accesses in existing causal attention computation algorithms. Specifically, for batched requests, the cached hidden states (, key-value pairs) of system prompts are transferred from off-chip DRAM to on-chip SRAM multiple times, each corresponding to an individual request. To eliminate such a redundancy, we propose , an attention algorithm that allows reading these hidden states from DRAM exactly once for a batch of input tokens. is a free lunch: it maintains the generation quality while requiring no model retraining, as it is based on a mathematical reformulation of causal attention. We have observed significant performance improvements to a production-level system, vLLM, through integration with RelayAttention. The improvements are even more profound with longer system prompts.","Llama-30B attention inference latency \wrt system prompt length (A40 GPU, batch size 32). We set the length of (request-specific) contexts, which include user prompts and previously generated tokens, to 128.","After around one decade of rapid development, we have experienced a revolution of large language models (LLMs) over the past year. LLMs like GPT-4 and Gemini are so powerful that they can now serve as programming copilots, universal chatbots, computer assistants and other roles that penetrate our daily life. However, the high inference cost of these large models has become a substantial obstacle to serving more people. It is therefore important to improve the hardware utilization so that LLMs can have a higher throughput within a fixed hardware budget. LLM services commonly use an application-specific system prompt to specify the task's instructions. The system prompt is concatenated with the user prompt as the full input to the LLM for response generation and is shared by all requests to a service. The system prompt becomes long if the service provider wants to provide detailed guidelines and examples for better response quality or apply more restrictions/policies for ethical safety. As the sequence length that LLMs can process grows, some emerging professional applications, such as legal analysis, healthcare applications, and the shopping assistant example shown in {fig:shopping}, may include one or more knowledge documents to provide domain-specific knowledge, resulting in even longer system prompts. Although long system prompts are beneficial to improving the generation quality or enabling new applications, they also pose a challenge to the LLM service: the inference throughput and latency of the service can be heavily degraded, thus increasing the per-request cost. This is inherently caused by the causal attention, in which each new token is generated by ``looking at'' {all precedent} ones. In this paper, we propose a novel approach to mitigate the efficiency problem of using long system prompts in LLM services. Our key observation is that there are not only redundant {memory footprint} and computations corresponding to the system prompt, but also unnecessary {memory accesses} during causal attention computation. Specifically, while the system prompt is shared by all requests, its hidden states (, key-value pairs) are read from DRAM multiple times by existing attention algorithms such as PagedAttention and FlashAttention, each for an individual request in the batch. This severely slows down LLM inferences, which are known to be memory-bound ( ). To eliminate such redundant memory access, we propose , an exact algorithm to compute causal attention based on a mathematical reformulation of it. The key idea of is to group the matrix-vector multiplications corresponding to the system prompt into matrix-matrix multiplications, which allow loading the hidden states of the system prompt from DRAM exactly once for all request tokens in a batch ( ). As a result, the attention inference latency grows much slower than PagedAttention the length of system prompt, as shown in {fig:profiling}. We provide an in-depth analysis of the theoretic speedup of the standalone attention based on the IO redundancy reduction ( ). Our empirical results for end-to-end serving further verify the efficiency: integrating into vLLM, an already highly optimized production-level LLM serving system, we still observe up to $2.2$ sustainable request rate and $2.0$ throughput with the Llama2-7B model for a chatbot workload. Similar efficiency improvements are also observed for several other popular LLMs and are consistent on several data center GPUs. The efficiency gains continue growing with longer system prompts. Our key contributions can be summarized as: {itemize} We have identified a LLM service bottleneck that has not been studied by existing works: there are highly redundant {memory accesses} caused by long system prompts. We anticipate that our analysis will inspire more works on deep architectures with IO-awareness. We propose , a novel approach to compute exact causal attention. It allows accessing cached hidden states of the system prompt exactly once for a batch of request tokens. We conduct an in-depth analysis of the theoretic speedup brought by . We empirically verify the end-to-end efficiency improvement by integrating into vLLM, a production level LLM serving system, and observe non-trivial efficiency gains on several popular LLMs with different GPUs. {itemize"
Boosting Language Models Reasoning with Chain-of-Knowledge Prompting,2306.06427v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2306.06427v3_0.pdf,"Recently, Chain-of-Thought (CoT) prompting has delivered success on complex reasoning tasks, which aims at designing a simple prompt like ``{Let's think step by step}'' or multiple in-context exemplars with well-designed rationales to elicit Large Language Models (LLMs) to generate intermediate reasoning steps. However, the generated rationales often come with hallucinations, making unfactual and unfaithful reasoning chains. To mitigate this brittleness, we propose a novel Chain-of-Knowledge (CoK) prompting, where we aim at eliciting LLMs to generate explicit pieces of knowledge evidence in the form of structure triple. This is inspired by our human behaviors, i.e., we can draw a mind map or knowledge map as the reasoning evidence in the brain before answering a complex question. Benefiting from CoK, we additionally introduce a F-Verification method to estimate the reliability of the reasoning chains in terms of {factuality} and {faithfulness}. For the unreliable response, the wrong evidence can be indicated to prompt the LLM to rethink. Extensive experiments demonstrate that our method can further improve the performance of commonsense, factual, symbolic, and arithmetic reasoning tasks~}.","Comparison of three prompting methods: (a) ICL, (b) Chain-of-Thought (CoT), and (c) Chain-of-Knowledge (CoK) solving a StrategyQA question.","Large Language Models (LLMs) have succeeded in advancing the state-of-the-arts for many Natural Language Processing (NLP) tasks ~[][{inter alia}]{Brown2020Language, Jack2021Scaling, Romal2022LaMDA, Aakanksha2022PaLM, scao2022bloom, zhang2022opt, bai2022training, Touvron2023LLaMA}, benefiting from the ultra-large-scale training corpora and computation resources. To unleash the LLMs' power of adaptation on unseen tasks without any parameter updates, in-context learning (ICL) has become one of the flourishing research topics, aiming at generating the prediction by conditioning on a few labeled exemplars (Figure~ (a)). A series of recent works have explored that LLMs can spontaneously decompose the complex multi-step problem into intermediate reasoning chains , elicited by a simple prompt like ``{Let's think step by step}'' or well-designed demonstrations with human-annotated rationales, which are called chain-of-thought (CoT) prompting (Figure~ (b)). This finding is intriguing and has been sensational because CoT may mainly specify an output space/format that regularizes the model generation to look step-by-step while being in order and relevant to the query. Despite impressive performances, current LLMs are susceptible to generating hallucination, along with providing unfactual or unfaithful reasoning chains that inevitably lead to a wrong conclusion. Take Figure~ as an example. Given a query ``{Is the following sentence plausible `Derrick White backhanded a shot.'}'' from StrategyQA, the standard ICL and CoT make a wrong answer. One of the reasoning steps ``Derrick White is most likely a hockey player'' is fake (In fact, Derrick White is a basketball player), making the unfactual inference towards the question. In addition, the response may be unfaithful when the LLM generates logically sound reasoning chains while still providing an incorrect answer. To address these concerns, we propose a novel {Chain-of-Knowledge (CoK)} prompting method to boost the LLM's reasoning capability by a series of exemplars that combine explicit structure knowledge evidence with textual explanations. To elaborate, CoK prompting consists of two compositions (Figure~ (c)), i.e., evidence triples (CoK-ET) and explanation hints (CoK-EH), where CoK-ET is a list of structure triples can reflect the overall reasoning evidence from the query towards the answer and CoK-EH is the explanation of this evidence. To construct in-context exemplars with the CoK prompt, we first sample $K$ labeled examples and each of them can be concatenated with a simple hint ``{Let's think step by step}'' to prompt the LLM to generate reasoning chains. Then, we retrieve some structure triples from the external knowledge base (KB) and judiciously manually annotate evidence triples to obtain a well-designed CoK prompt. Like standard ICL and CoT, the CoK prompt can be perceived as a rule that regularizes the output space/format and urges LLMs to generate explicit evidence instead of only attempting to generate vague textual reasoning chains. Furthermore, we also propose an F{2}-Verification strategy to estimate the reliability of the reasoning chains in terms of {factuality} and {faithfulness}, where {factuality} is the quantification of the matching degree between reasoning evidence and ground-truth knowledge, and {faithfulness} is the consistency degree between reasoning evidence and the textual explanation with the final answer. Particularly for the unreliable response, the wrong pieces of evidence can be indicated to prompt the LLM to rethink the problem. We design a {rethinking algorithm} to reach this goal. We have conducted empirical evaluations across various reasoning tasks (e.g., commonsense, factual, arithmetic, and symbolic), showing that CoK prompting with F{2}-Verification can significantly outperform standard ICL and CoT prompting. We also integrate CoK prompting with some prevailing strategies, such as self-consistency. The results indicate that such CoK can serve as a plug-and-play module to further improve reasoning ability"
Estimating Agreement by Chance for Sequence Annotation,2407.11371v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2407.11371v1_0.png,"In the field of natural language processing, correction of performance assessment for chance agreement plays a crucial role in evaluating the reliability of annotations. However, there is a notable dearth of research focusing on chance correction for assessing the reliability of sequence annotation tasks, despite their widespread prevalence in the field. %This research gap can be attributed to the inherent complexity of sequence annotation tasks, which involve challenges such as varying span densities across documents and constraints related to span connectivity and overlap. To address this gap, this paper introduces a novel model for generating random annotations, which serves as the foundation for estimating chance agreement in sequence annotation tasks. %The proposed model is designed to account for the specific characteristics of text sequence labeling tasks and acknowledges the variations in annotation tendencies among annotators. By Utilizing the proposed randomization model and a related comparison approach, we successfully derive the analytical form of the distribution, enabling the computation of the probable location of each annotated text segment and subsequent chance agreement estimation. Through a combination simulation and corpus-based evaluation, we successfully assess its applicability and validate its accuracy and efficacy. The Kappa statistic is a popular chance corrected measure of agreement used as a reliability measure for annotation in the field of NLP, however its method for estimating chance agreement is not suitable for sequence annotation tasks, which are extremely prevalent in the field. The non-suitability is grounded in several complicating factors such as variation in span density across documents and constraints on span connectivity and overlap. In this paper, we propose a novel model for random annotation generation as the basis for chance agreement estimation for sequence annotation tasks. The model is jointly motivated by the specific characteristics of text sequence labeling tasks and acknowledgement of differences in annotation tendencies among annotators. Based on the proposed randomization model and related comparison approach, we successfully derive the analytical form of the distribution for computing the probable location of each annotated text segment, and subsequently chance agreement. We illustrate the approach in a simulation experiment and then apply it to several system outputs of CoNLL03 corpus annotation to evaluate its applicability, thus substantiating both the accuracy and efficacy of our method.","The probability distributions for all possible locations of each random segment in a length=100 sequence annotated with four segments. The lengths of the four segments are 1, 5, 10, 15, from left to right.","Reliable annotation is a cornerstone of NLP research, enabling both supervised learning methods and evaluation. Though not frequently employed for evaluation of model performance in the field of NLP, one of the most widely accepted metrics for evaluation of annotation reliability is Cohen's Kappa, which offers an assessment of inter-rater reliability that is adjusted in order to avoid offering credit for the portion of observed agreement that can be attributed to chance. Some NLP tasks, such as Named Entity Recognition or other span detection/labeling tasks, lack an appropriate chance corrected metric. This paper addresses this gap by proposing such a measure for these tasks, demonstrating its application in both simulation and CoNLL03 corpus experiments. Numerous studies caution against using non-chance-corrected agreement metrics. They can lead to unfair task or system comparisons due to biases introduced due to varying levels of chance agreement across tasks and systems . Furthermore, without correction for chance agreement, measurements tend to cluster within a narrow range, making it difficult to discern differences between approaches . Therefore, both estimating and correcting for chance agreement have become critical in annotation evaluation, except in cases where chance agreement is negligible. The main contributions of our work are summarized as follows: {itemize}[leftmargin=*,noitemsep,topsep=4pt] We propose a novel random annotation model that considers the specific characteristics of sequence annotation tasks as well as the annotation tendencies of different annotators. This model can be divided into sub-models, enabling us to separately address cases with or without annotation overlap. We also apply chance agreement to measure task difficulty. Due to the additive nature of many popular similarity measures, we simplify the modeling of dependent annotation segments within a text. We successfully derive analytical probability distributions for random annotations, presenting a streamlined formulation that avoids redundant calculations. We delve into the asymptotic properties of agreement by chance, highlighting scenarios where it can be disregarded. We design and implement both simulation-based and naturalistic experiments, demonstrating that our proposed method is accurate, effective, and computationally efficient. {itemize} In the remainder of the paper, we provide a theoretical foundation for our work through a review of past literature. We then explain our methodology, and evaluate it first through a simulation study, and then through application to real-world corpora. Finally, we conclude with discussions of limitations, ethical considerations, and future research"
WaveCoder: Widespread And Versatile Enhancement For Code Large Language Models By Instruction Tuning,2312.14187v5,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2312.14187v5_0.pdf,"Recent work demonstrates that, after instruction tuning, Code Large Language Models (Code LLMs) can obtain impressive capabilities to address a wide range of code-related tasks. However, current instruction tuning methods for Code LLMs mainly focus on the traditional code generation task, resulting in poor performance in complex multi-task scenarios. In this paper, we concentrate on multiple code-related tasks and present {WaveCoder}, a series of Code LLMs trained with idespread nd ersatile nhanced instruction data. To enable the models to tackle complex code-related tasks, we propose a method to stably generate diverse, high-quality instruction data from open source code dataset in multi-task scenarios and obtain {CodeSeaXDataset}, a dataset comprising 19,915 instruction instances across 4 code-related tasks, which is aimed at improving the generalization ability of Code LLM. Our experiments demonstrate that WaveCoder models significantly outperform other open-source models in terms of the generalization ability across different code-related tasks. Moreover, WaveCoder-Ultra-6.7B presents the state-of-the-art generalization abilities on a wide range of code-related tasks.",The overview of the widespread and versatile enhancement for Code LLM. Part B and C indicates the LLM-based Generator and LLM-based Disciminator where the generator can leverage different examples in example database by in-context learning.,"Recently, Large Language Models (LLMs) such as ChatGPT, GPT-4 , and Gemini {{https://deepmind.google/technologies/gemini}} have attained unprecedented performance levels in a broad array of NLP tasks. These models utilize a self-supervised pre-training process, and subsequent supervised fine-tuning to demonstrate exceptional zero/few-shot capabilities, effectively following human instructions across various tasks. For code-related tasks, several previous works, including Codex , StarCoder , CodeLLaMa and DeepseekCoder, have successfully demonstrated that pre-training on code corpus can significantly improve the model's capability to tackle code-related problems. After the process of pre-training, instruction tuning has shown its effectiveness in the aspect of improving the quality of LLM responses. To specifically enhance the performance of Code LLMs on code-related tasks through instruction tuning, many existing methods for instruction data generation have been designed. For example, Code Alpaca utilizes the method of self-instruct within the coding domain, leveraging the few-shot capabilities of teacher LLM to generate instruction data. Similarly, WizardCoder applies the evol-instruct approach based on Code Alpaca, demonstrating a novel and effective method for the generation of instruction data. These applications underscore the potential of utilizing teacher LLMs to produce instructional content effectively, thereby offering an avenue for the creation of instruction data in the code domain. However, the quality of the data they generate heavily relies on the performance of the teacher LLM and the limited initial seeds, which often produces a large amount of duplicate instruction instances and reduce the effectiveness of instruction tuning . To break away from dependence on teacher LLMs, Octopack constructs a code instruction dataset leveraging the natural structure of Git commits. Nonetheless, ensuring the quality of data in git messages presents a considerable challenge, and the comprehensive screening of data through artificial filtering rules is often a complex task. Additionally, these endeavors are predominantly centered on traditional code generation tasks and lack the capability to produce detailed, task-specific instructions in multi-task scenarios. In this paper, we primarily focus on multiple code-related tasks, aiming to generate high-quality and diverse instructional data tailored to specific task requirements. Addressing the aforementioned challenges, we refine the instruction data by classifying the instruction instances to four universal code-related tasks in CodeXGLUE : 1) Code Summarization, 2) Code Generation, 3) Code Translation, 4) Code Repair and propose a widespread and versatile enhanced instruction generation method that could make full use of open source code data and stably generate high quality and diverse instruction data in multi-task scenarios. By this generation strategy, we obtain a dataset of 19,915 instruction instances across four code-related tasks, termed {CodeSeaXDataset}. To validate our approach, we train StarCoder , CodeLLaMa , and DeepseekCoder with our initial CodeSeaXDataset dataset and get {WaveCoder}. Following a thorough assessment on HumanEval , MBPP , HumanEvalPack benchmarks, experimental results show that our {WaveCoder} exhibits outstanding generalization ability based on widespread and versatile enhanced instruction tuning. Moreover, to further explore the improvements brought by data quality, we use GPT-4 to regenerate response for the instruction in CodeSeaXDataset. Fine-tuned with the enhanced 20K CodeSeaXDataset dataset, we obtain {WaveCoder-Pro-6.7B} which achieve 72.0\ Combining enhanced CodeSeaXDataset with WaveCoder-evol-codealpaca, the decontaminated Magicoder-evol-codealpaca {{https://huggingface.co/datasets/ise-uiuc/Magicoder-evol-codealpaca-110K}} dataset, we present {WaveCoder-Ultra-6.7B}, with SoTA generalization capabilities on multiple code-related tasks"
Quantifying Uncertainty in Answers from any Language Model and Enhancing their Trustworthiness,2308.16175v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2308.16175v2_0.pdf;/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2308.16175v2_1.pdf,"We introduce , a method for detecting bad and speculative answers from a pretrained Large Language Model by estimating a numeric confidence score for any output it generated. Our uncertainty quantification technique works for any LLM accessible only via a black-box API, whose training data remains unknown. By expending a bit of extra computation, users of any LLM API can now get the same response as they would ordinarily, as well as a confidence estimate that cautions when not to trust this response. Experiments on both closed and open-form Question-Answer benchmarks reveal that more accurately identifies incorrect LLM responses than alternative uncertainty estimation procedures (for both GPT-3 and ChatGPT). By sampling multiple responses from the LLM and considering the one with the highest confidence score, we can additionally obtain more accurate responses from the same LLM, without any extra training steps. In applications involving automated evaluation with LLMs, accounting for our confidence scores leads to more reliable evaluation in both human-in-the-loop and fully-automated settings (across both GPT 3.5 and 4).","{fig:main_task}Pipeline of \modelnameA{}, which can be applied to any LLM API. ($T=1.0$ means temperature sampling with parameter 1.0, Sim ($\cdot$,$\cdot$) means the semantic similarities between two sentences.)","While the promise of Large Language Models (LLMs) and Agents (powered by LLMs) has become evident, their usage in high-value applications remains limited by their {unreliability}. Accessed via black-box APIs (via providers like OpenAI/Anthropic), today's best LLMs have been trained to produce convincing-looking responses and thus often appear overconfident . For many input prompts encountered in the wild, the model cannot be certain about the desired response (perhaps because the prompt is vague or is related to a specific fact/event absent from the training dataset), yet these models output plausible-sounding yet wildly incorrect answers in such scenarios. This {hallucination} problem has also plagued traditional supervised learning systems, where it is traditionally addressed via {uncertainty estimation} to know when one can trust a model's prediction . In traditional supervised learning, one has access to the training data of the model and its probabilistic estimates, as well as being able to modify the training procedure to improve model calibration . Other traditional uncertainty estimation procedures require the existence of a validation set that can be used for calibration . None of this is available for today's best LLMs, which may be given any imaginable prompt rather than (input, output) pairs stemming from a limited distribution. Thus approaches to uncertainty estimation for black-box LLMs must wrap the inference procedure. Our proposed LLM uncertainty quantification technique, {}, calls the LLM API multiple times with varying prompts and sampling {temperature} values (see Figure ). We expend extra computation in order to quantify how trustworthy the original LLM response is, a worthwhile tradeoff for high-stakes applications. Our method is conceptually straightforward, generally applicable across LLM providers (as well as Agent frameworks or any stochastic text $$ text mapping), and produces confidence scores whose values are reliably lower for responses from the LLM that are more likely bad. {} confidence scores allow LLMs to be more safely used in high-stakes applications, since we can know which LLM outputs are not to be trusted. Depending on the application, we can adaptively ask a human for an alternative response when the confidence score is low, automatically route the prompt to an alternative LLM provider, or simply respond ``{I don't know}'' when a confident response cannot be generated. Our experiments reveal that for Question-Answering applications, we can automatically generate {more accurate} answers by sampling multiple responses from the same LLM and selecting the response whose {} confidence estimate is the highest. This paper primarily focuses on {Question-Answering} applications, but our same uncertainty estimates can also be applied to estimate how confident the LLM is in its response to a more general prompt. Intuitively, we'd like to see a low confidence score when the LLM outputs: a factually incorrect response to a question, a inaccurate summary requested for a document, or a generated article/message that semantically differs from the intention of the original request. Ensuring this is challenging without control over LLM training, but we can hope that in each of these three scenarios where the model generated a bad response, a well-trained LLM was also likely to output alternative responses (which more closely reflect the desired response). {} is baseed on this intuition, and is observed to produce effective uncertainty estimates with today's top LLMs from OpenAI across prompts from closed and open domain benchmark datasets"
Marathon: A Race Through the Realm of Long Context with Large Language Models,2312.09542v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2312.09542v2_0.pdf,"With the advancement of large language models (LLMs) and the expansion of their context windows, existing long-context benchmarks fall short in effectively evaluating the models' comprehension and reasoning abilities in extended texts. Moreover, conventional benchmarks relying on F1 metrics often inaccurately score responses: they may undervalue correct answers that differ from the reference responses and overvalue incorrect ones that resemble the reference texts. In response to these limitations, we introduce {Marathon}, a novel evaluation benchmark that adopts a multiple-choice question format. It is specifically designed to overcome the constraints of previous benchmarks and provide a rapid, precise, and unbiased appraisal of the long-context comprehension skills of large language models. We conducted comprehensive evaluations on the Marathon benchmark with a range of state-of-the-art LLMs and assessed the effectiveness of various optimization strategies tailored for long-context generation. We anticipate that the Marathon benchmark and its associated leaderboard will enable a more precise and equitable evaluation of LLMs' capabilities in understanding and reasoning over extended contexts. {Marathon} is available at .}","The overall accuracy of different models on Marathon. The x-axis represents the model, and the y-axis represents the average accuracy across all tasks. The different colors represent different methods of optimization.","In the rapidly evolving landscape of artificial intelligence technologies, the emergence of large language models (LLMs), as exemplified by ChatGPT , showcases notable capabilities. The influence of these models extends beyond the well-established ChatGPT, gaining increasing prominence across diverse sectors. Existing LLMs are typically built upon Transformer architectures, which demand memory and computational resources that grow quadratically with sequence length. Consequently, Transformer language models have historically been trained with relatively modest predetermined context windows. For instance, LLaMA employs a context size of 2048 tokens, while Llama2 utilizes a context size of 4096 tokens. However, the pre-defined size imposes constraints on LLMs in various applications, such as summarizing extensive documents or addressing lengthy questions. Significant research efforts have been devoted to extending the context length of LLMs. Due to the prohibitive expense of training LLMs with extended context lengths from scratch, the predominant studies have endeavored to enhance the capabilities of LLMs to comprehend long contexts through fine-tuning. These methods encompass extending the context window , incorporating recurrent memory , employing sparse attention mechanisms , and augmenting with external memory . Concurrently, an increasing multitude of benchmarks have been introduced to assess the long-context understanding capabilities of LLMs. LongBench stands out as the first bilingual, multi-task benchmark specifically designed for the assessment of long-context understanding. This dataset continues to depend on the F1 score, which evaluates the responses of LLMs against a predefined set of possible answers. LooGLE encompasses intricate long dependency tasks, including event timeline reordering, comprehension/reasoning, and computation. Nevertheless, the diverse nature of model-generated content introduces a challenge, as these predefined answers may not encompass all valid responses, thereby diminishing the precision of assessing model performance. There is a growing demand for high-quality benchmarks characterized by significantly longer text lengths and more challenging tasks, ensuring comprehensive evaluations. In this study, we introduce a novel benchmark named {Marathon}, designed for long-context understanding and reasoning. In particular, this benchmark is constructed upon the foundations established by LooGLE and LongBench . The contextual lengths within this benchmark span from 2K to over 260K characters. For each extensive context provided, an associated question is paired with four meticulously crafted response options. These options have been carefully reviewed by humans and contain only one correct answer, with the remaining options designed to be highly misleading. This design makes the Marathon benchmark a particularly challenging one. The task for the large language model is to discern the accurate response option based on the extensive context provided. The main contributions of this work are threefold: {itemize} We introduce a novel multiple-choice long context benchmark that comprehensively evaluates the long context understanding and reasoning capabilities across 10 leading open-source large language models, as well as ChatGPT and GPT-4, covering six diverse types of tasks. We compare two prevalent methods for long context optimization (Prompt Compression and Retrieval Augmented Generation) along with two leading embedding models, assessing their impact on enhancing the long context reasoning abilities of large language models. Our findings reveal a general tendency among current open-source large language models to generate lengthier responses, accompanied by a notable deficiency in following instructions accurately. {itemize"
UHGEval: Benchmarking the Hallucination of Chinese Large Language Models via Unconstrained Generation,2311.15296v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2311.15296v3_0.pdf,"Large language models (LLMs) produce hallucinated text, compromising their practical utility in professional contexts. To assess the reliability of LLMs, numerous initiatives have developed benchmark evaluations for hallucination phenomena. However, they often employ constrained generation techniques to produce the evaluation dataset due to cost and time limitations. For instance, this may involve employing directed hallucination induction or deliberately modifying authentic text to generate hallucinations. These are not congruent with the unrestricted text generation demanded by real-world applications. Furthermore, a well-established Chinese-language dataset dedicated to the evaluation of hallucinations is presently lacking. Consequently, we have developed an $$nconstrained $$allucination $$eneration uation ($$Eval) benchmark, containing hallucinations generated by LLMs with minimal restrictions.}. Concurrently, we have established a comprehensive benchmark evaluation framework to aid subsequent researchers in undertaking scalable and reproducible experiments. We have also evaluated prominent Chinese LLMs and the GPT series models to derive insights regarding hallucination.","Hallucinations from $\mathbb{UHG}$Eval. Using the IDs, you can locate the original news articles. {Note}: MOTIE denotes Ministry of Trade, Industry, and Energy. (In Chinese: Fig.~\ref{fig:halu_example_ch})","Large language models (LLMs) have unparalleled proficiency in language generation, knowledge application, and intricate reasoning. However, they invariably manifest hallucination, as they often generate content that is incongruent with user input, the model's output context, or factual information. Real-world hallucination examples from our ${UHG}$Eval dataset can be observed in Fig.~. The fabricated news content depicted in Fig.~ offers NO utility to journalists; on the contrary, the verification and rectification of such content exacts a toll on the valuable time of journalists. To this concern, it is crucial to first formulate a comprehensive, stringent, and demanding benchmark for the assessment of hallucination in language generation. While there have been a bunch of efforts to develop benchmarks for hallucination assessment, they always employ restricted techniques to produce particular kinds of hallucinated utterances. This approach is at odds with real-world scenarios where hallucinations arise in unrestricted, spontaneously generated content. For example, HaluEval specifies the type of hallucination in the prompt when generating hallucinated text: ``You are trying to answer a question but misunderstand the question context and intention''. Additionally, benchmarks such as HaDes annotate hallucinations at a finer granularity by generating token-level hallucinations based on text perturbations, but the text perturbation method is still constrained. Hallucinations must be generated in an unconstrained setting; otherwise, it's difficult to determine whether the hallucinated texts in many datasets are indeed errors that language models will make on their own. This point carries profound implications. For example, with a dataset containing freely generated hallucinations, researchers can explore the differences in model hidden states (logits, hidden layers, etc.) between hallucinated text spans and unhallucinated text spans. Such in-depth analysis would not be possible with datasets generated under constrained settings. Appendix~ provides a detailed comparison with three other datasets, TruthfulQA, HaluEval, and HaDes. Besides, many benchmarks are centered on the evaluation in English, neglecting the assessment of hallucination in Chinese. The extensive lexicon of Chinese characters, combined with the complexities introduced by Chinese word segmentation, renders the Chinese hallucination evaluation particularly arduous and deserving of focused scrutiny. To address the aforementioned challenges, we introduce a novel benchmark for hallucination assessment, as depicted in Fig.~. The benchmark dataset is composed of raw Chinese news articles and continuations of those articles freely generated by LLMs but annotated with hallucinations. Furthermore, selecting texts from the news domain is intentional, given that news requires utmost precision in conveying factual information and exhibits minimal tolerance for hallucinations, presenting a considerable challenge for the majority of LLMs. Moreover, news data encompasses a wide range of topics, including medicine, technology, finance, sports, etc., incorporating features found in texts from other domains. Lastly, news articles are readily available and frequently employed as training corpora by a large number of LLMs, guaranteeing impartiality in the evaluation of many LLMs. Our contributions: (1) The development of an unconstrained hallucination evaluation dataset, comprising over 5000 items. Existing methods for constructing datasets often yield biases towards predefined directions, thereby hindering the full simulation of real-world hallucinations. (2) The establishment of a unified and diverse evaluation framework, ${UHG}$Eval, that encompasses discriminative, selective, and generative evaluations. Current benchmark methods for hallucination evaluation often exhibit a singular approach and lack task specificity. (3) A comprehensive empirical analysis. We evaluated eight prominent Chinese LLMs and three classic GPT series models to explore the credibility of various LLMs"
"Triple-Encoders: Representations That Fire Together, Wire Together",2402.12332v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.12332v2_0.png,"This study introduces the innovative triple-encoders approach to conversational sequence modeling, a method that surmounts the inherent limitations of conventional models like ConveRT, particularly their weak latent space interaction and repetitive context recomputation. Rooted in Curved Contrastive Learning (CCL), triple-encoders revolutionize dialog representation by dividing the context into distinct sub-latent spaces and employing a Hebbian-inspired co-occurrence learning mechanism. This innovative strategy enables the independent encoding of utterances into a latent space and the composition of sequences for retrieval, remarkably without any additional weights. Employing straightforward yet potent operations: mean pooling, batch matrix multiplication for computing similarity, and summing across the sequential dimension. triple-encoders demonstrate a significant improvement in sequence modeling. Specifically, they exhibit a 36\% enhancement in open-dialog and a 46\% improvement in task-oriented scenarios in terms of average rank, compared to previous CCL models. Furthermore, the model's robust architecture facilitates in better planning performance and generalization to zero-shot settings, showcasing its versatility and broad applicability. Search-based dialog models typically re-encode the dialog history at every turn, incurring high cost. Curved Contrastive Learning, a representation learning method that encodes relative distances between utterances into the embedding space via a bi-encoder, has recently shown promising results for dialog modeling at far superior efficiency. While high efficiency is achieved through independently encoding utterances, this ignores the importance of contextualization. To overcome this issue, this study introduces triple-encoders, which efficiently compute distributed utterance mixtures from these independently encoded utterances through a novel hebbian inspired co-occurrence learning objective in a self-organizing manner, without using any weights, i.e., merely through local interactions. Empirically, we find that triple-encoders lead to a substantial improvement over bi-encoders, and even to better zero-shot generalization than single-vector representation models without requiring re-encoding. Our code[1] and model[2] are publicly available. [1]{{/Triple-Encoders}} [2]{{ UKPLab/Triple-Encoders-DailyDialog}}","Comparison of our Triple Encoder to \citet{henderson-etal-2020-convert} and \citet{erker-etal-2023-imagination}. Similar to CCL we only need to encode and compute similarity scores of the latest utterance. At the same time, we achieve contextualization through pairwise mean-pooling with previous encoded utterances combining the advantages of both previous works. Our analysis shows that the co-occurrence training pushes representations that occur ({fire}) together closer together, leading to stronger additive properties ({wiring}) when being superimposed (compared to \citet{erker-etal-2023-imagination}) and thus to a better next utterance selection.","Traditional search-based approaches in conversational sequence modeling like ConveRT represent the entire context (query) in one context vector (see Figure ). This has two major drawbacks: {(a)} Recomputing the entire vector at each turn is computationally expensive, and {(b)} it is difficult to compress the context's relevant information for any possible candidate response into a single vector. Furthermore, the encoder models are limited to a maximum number of tokens, usually 512. Curved Contrastive Learning (CCL) demonstrated that it is possible to encode utterances separately in a latent space and accumulate sequence likelihood based on solely cosine similarity, thanks to treating cosine similarity not as a semantic but as a directional relative dialog turn distance measure between utterance pairs (through two sub-spaces representing a temporal direction: {before} and {after}). This relativistic approach tackles {(a)}, by enabling sequential search with a constant complexity, as {only} the latest utterance needs to be encoded and computed during inference as shown in Figure . {(b)} Furthermore, each candidate utterance can interact with every independently projected utterance, allowing a richer interaction. However, encoding utterances independently means they are not contextualized, disregarding a crucial feature of conversation. An example is illustrated in Figure . For the first time, in this paper we propose a method that contextualizes utterance embeddings in dialog sequences in a self-organizing manner, {without the use of additional weights}, i.e, merely through local interactions (in form of efficient vector algebra) between separately encoded utterances after appropriate pre-training. While previous work has shown that mean pooling is a strong method for sentence composition from tokens, we demonstrate that this can be generalized to a higher abstraction level: distributed pairwise sequential composition (illustrated in Figure ). To realize this, we present triple-encoders, which segment the context space of CCL into two distinct latent spaces denoting the relative order of utterances in the context. By linearly combining (averaging) representations from these sub-spaces through a co-occurrence learning objective, we create new contextualized embeddings that we can incorporate into CCL, resulting in Contextualized Curved Contrastive Learning (C3L). At inference time, our method efficiently contextualizes independently encoded utterances based on solely local interactions ({without any additional weights}): Our method applies only {(1) mean pooling}, a {(2)} {matrix multiplication} for computing the similarity and one {(3) summation} (across the sequential dimension) operation to aggregate similarity scores. While we focus on modeling dialog in this paper, the sequential modularity of our method can in principle be used for any text sequence. Pilot experiments on next sentence selection of children stories are reported in Appendix , while we leave thorough exploration to future work. Our experiments are aimed at the following research questions: {RQ1:} What is the effect of triple-encoder training (C3L) + triple encoder at inference compared to CCL? {RQ2:} What is the effect of triple-encoder training (C3L) while encoding utterances at inference time without contextualization (like CCL)? Our experimental results suggest that our approach improves substantially over standard CCL. Notably, our method outperforms ConveRT in a zero-shot setting while our method requires no additional learnable parameters for contextualization. While triple-encoder training alone improves the performance considerably (RQ2), using triple-encoder contextualization at inference time (RQ1) leads to additional performance gains while keeping linear complexity"
Improving Hateful Meme Detection through Retrieval-Guided Contrastive Learning,2311.08110v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2311.08110v3_0.png,"Hateful memes have emerged as a significant concern on the Internet. Detecting hateful memes requires the system to jointly understand the visual and textual modalities. Our investigation reveals that the embedding space of existing CLIP-based systems lacks sensitivity to subtle differences in memes that are vital for correct hatefulness classification. We propose constructing a hatefulness-aware embedding space through retrieval-guided contrastive training. Our approach achieves state-of-the-art performance on the HatefulMemes dataset with an AUROC of 87.0, outperforming much larger fine-tuned large multimodal models. We demonstrate a retrieval-based hateful memes detection system, which is capable of identifying hatefulness based on data unseen in training. This allows developers to update the hateful memes detection system by simply adding new examples without retraining — a desirable feature for real services in the constantly evolving landscape of hateful memes on the Internet.","Illustrative (not in the dataset) examples from \citealt{KielaFBHMC2020}. Memes on the left are mean, the ones in the middle are benign image confounders, and those on the right are benign text confounders.","The growth of social media has been accompanied by a surge in hateful content. Hateful memes, which consist of images accompanied by texts, are becoming a prominent form of online hate speech. This material can perpetuate stereotypes, incite discrimination, and even catalyse real-world violence. To provide users the option of not seeing it, hateful memes detection systems have garnered significant interest in the research community. Correctly detecting hateful memes remains difficult. Previous literature has identified a prominent challenge in classifying ""confounder memes"", in which subtle differences in either image or text may lead to a completely different meaning. As shown in Figure , the top left and top middle memes share the same caption. However, one of them is hateful and the other benign depending on the accompanying images. Confounder memes resemble real memes on the Internet, where the combined message of images and texts contribute to their hateful nature. Even state-of-the-art models, such as HateCLIPper, exhibit limited sensitivity to nuanced hateful memes. We find that a key factor contributing to misclassification is that confounder memes are located in close proximity in the embedding space due to the similarity of text or image content. For instance, HateCLIPper's embedding of the confounder meme in Figure has a high cosine similarity score with the left anchor meme even though they have opposite meanings. This poses challenges for the classifier to distinguish harmful and benign memes. We propose ``{Retrieval-Guided Contrastive Learning}'' ({RGCL}) to learn hatefulness-aware vision and language joint representations. We align the embeddings of same-class examples that are semantically similar with pseudo-gold positive examples and separate the embeddings of opposite-class examples with hard negative examples. We dynamically retrieve these examples during training and train with a contrastive objective in addition to cross-entropy loss. RGCL achieves higher performance than state-of-the-art large multimodal systems on the HatefulMemes dataset with far fewer model parameters. We demonstrate that the RGCL embedding space enables the use of K-nearest-neighbor majority voting classifier. The encoder trained on HarMeme can be applied to HatefulMemes without additional training while maintaining high AUC and accuracy using the KNN majority voting classifier, even outperforming large multi-modal models under similar settings. This allows efficient transfer and update of hateful memes detection systems to handle the fast-evolving landscape of hateful memes in real-life applications. Our contributions are: {enumerate} We propose RGCL for hateful memes detection which learns a hatefulness-aware embedding space via an auxiliary contrastive objective with dynamically retrieved examples. We propose to leverage novel pseudo-gold positive examples to improve the quality of positive examples. Our proposed approach achieves state-of-the-art performance on HatefulMemes and the HarMeme. We show RGCL's capability across various domains of meme classification tasks on MultiOFF, Harm-P and Memotion7K. Our retrieval-based KNN majority voting classifier facilitates straightforward updates and extensions of hateful meme detection systems across various domains without retraining. With RGCL training, the retrieval-based classifier demonstrates strong cross-dataset generalizability, making it suitable for real services in the dynamic environment of online hateful memes. {enumerate"
Agent-Pro: Learning to Evolve via Policy-Level Reflection and Optimization,2402.17574v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.17574v3_0.pdf,"Large Language Models (LLMs) exhibit robust problem-solving capabilities for diverse tasks. However, most LLM-based agents are designed as specific task solvers with sophisticated prompt engineering, rather than agents capable of learning and evolving through interactions. These task solvers necessitate manually crafted prompts to inform task rules and regulate LLM behaviors, inherently incapacitating to address complex dynamic scenarios e.g., large interactive games. In light of this, we propose {Agent-Pro}: an LLM-based {Agent} with {P}olicy-level {R}eflection and {O}ptimization that can learn a wealth of expertise from interactive experiences and progressively elevate its behavioral policy. Specifically, it involves a dynamic belief generation and reflection process for policy evolution. Rather than action-level reflection, Agent-Pro iteratively reflects on past trajectories and beliefs, ""fine-tuning"" its irrational beliefs for a better policy. Moreover, a depth-first search is employed for policy optimization, ensuring continual enhancement in policy payoffs. Agent-Pro is evaluated across two games: Blackjack and Texas Hold’em, outperforming vanilla LLM and specialized models. Our results show Agent-Pro can learn and evolve in complex and dynamic scenes, which also benefits numerous LLM-based applications }.","For interactive tasks, e.g., imperfect-information games, we propose a versatile agent framework capable of self-learning and evolving. Firstly, our agent constructs beliefs about itself and the environment. Then it autonomously updates its prompts through policy-level reflection on past trajectories and beliefs, evolving a better behavioral strategy.","Designing a human-level agent with robust problem-solving abilities has long been a vision in the academic community. This necessitates the agent to possess learning and generalization capabilities across a diverse array of tasks. The advent of Large Language Models (LLMs) has shed light on this vision, especially they can be rapidly generalized across a wide range of tasks with only a few demonstrations. Benefiting from this, many systems built upon LLMs have showcased markedly enhanced performance such as question-answering, code generation, and real-world application. Despite these achievements, building a human-level agent remains a challenging endeavor. First, most LLM-based agents are designed for specific tasks through sophisticated prompts, including detailed task descriptions and behavioral specifications. However, numerous real-world tasks, e.g., business, company negotiations, and security, are more intricate with imperfect information, necessitating laborious efforts to design strategic behavior. Second, most LLM-based agents do not consider interacting with task scenarios, and more critically, cannot learn from past experiences and evolve their behavioral strategies during interactions. In contrast, humans often learn and adjust their behaviors through interaction, especially in novel scenarios. In light of these, a promising yet under-explored topic emerges: {Can LLM-based agents learn and elevate behavioral strategies by interacting with the environment like humans}? It should be an indispensable ability of a human-level agent. Recently, numerous studies undertake intriguing explorations, e.g., utilizing feedback for self-correction at the action-level. Besides, several efforts also explore deploying LLM in interactive games, including StarCraft, Minecraft, strategy-based gaming. Similarly, we first evaluate LLM-based agents with the self-correction strategy in dynamic interactive scenarios, such as multi-player Texas Hold'em, which is a zero-sum game with imperfect information. However, we observe that it loses most of the rounds to its opponents, even the most advanced LLMs. Upon examining its reasoning thoughts and actions, we find that it often adopts irrational behaviors and is unable to deduce effective strategies from long action sequences. To answer the above question, the Theory of Mind (ToM) may provide some insight. In this framework, each human develops perceptions of himself (self-belief) and the external environment (social-belief) in the social context, and then grounds their decisions on these beliefs, or adjusts incorrect beliefs in response to external feedback. Inspired by this, we advocate {Agent-Pro}: a LLM-based {Agent} with {P}olicy-level {R}eflection and {O}ptimization. Agent-Pro is endowed with the capacity to learn and evolve within environments, i.e., autonomously reflect on past experiences, calibrate its beliefs about itself and the environment, and optimize its behavior policy without parameter tuning. Concretely, as shown in {figure1}, an LLM-based agent involves an LLM as the foundational model and some instructions in the prompt to regulate its behavior (policy). Upon observing partial information from the scenarios, Agent-Pro first updates its self-belief and world-belief, then makes decisions based on these beliefs. After exploring tasks, Agent-Pro performs a policy-level reflection and optimization on past trajectories, beliefs, and results. It autonomously ""fine-tunes"" its beliefs, searches for useful prompt instructions, and consolidates them into a new behavior policy. The experiments in two zero-sum games, Blackjack and Texas Hold'em, demonstrate that Agent-Pro, after evolution, can defeat vanilla LLMs and specialized models, improving the game's payoffs. It indicates that Agent-Pro enhances its capabilities through interaction and reflection without human guidance. As depicted in~{figure1}, the initial prompt is quite simple (Left Bottom), but after learning and evolution, the Agent-Pro generates many practical instructions (Right Bottom). For instance, Agent-Pro records estimations of each opponent's style in {Task Description} and adds specific {Goals, Strategies} in {Behavior Policy}. Our Agent-Pro is different from previous strategies, like Reflexion. Firstly, Policy-level reflection is designed for policy updating in long-horizon tasks. It is aimed at long-horizon policy updating rather than immediate action correction. The input is a sequence of actions and delayed feedback, while the output is an optimized strategy, rather than a specific action. Therefore, policy-level reflection corrects irrational beliefs and optimizes the old policy into the new one. As introduced in~{policy-level reflection}, our policy-level reflection includes belief calibration, policy updates by refining behavioral guidelines and world modeling, and policy verification. Besides, we innovatively distill long-term memory into Behavioral Guidelines and World Models through prompt optimization. Most previous strategies store historical experience as verbal long-term memory and use it for text-based reasoning. In contrast, we further construct an optimizable policy from long-term interactions, i.e., Behavioral Guidelines and Environmental Models. This includes self-summarized game objectives and rules, effective strategies derived from reflection, and demonstrative trajectories. The contributions of our work are as follows: {itemize} We introduce Agent-Pro, a framework capable of learning and evolving within interactive games, empowering LLM-based agents to efficiently adapt to more complex dynamic tasks. We devise a belief-aware decision-making process with self and world-belief, enhancing its capabilities for intricate tasks, i.e., generating more rational actions in interactive scenarios. We utilize policy-level reflection and optimization to iteratively update prompt instructions, which empower Agent-Pro to progressively evolve from a novice to a skilled veteran with many strategic behaviors. After learning, Agent-Pro is evaluated in multiplayer games and defeats specialized models, gaining notable progress. It develops strategic skills like humans, e.g., actively cutting losses, bluffing, or disguising to influence others. {itemize} Not just in card games, similar scenarios abound in the real world as well. Through self-learning and evolution, Agent-Pro can enhance deployment effectiveness in those scenarios, expanding the capability boundaries of LLM-based agents notably"
Your Transformer is Secretly Linear,2405.12250v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2405.12250v1_0.png,"This paper reveals a novel linear characteristic exclusive to transformer decoders, including models such as GPT, LLaMA, OPT, BLOOM and others. We analyze embedding transformations between sequential layers, uncovering a near-perfect linear relationship (Procrustes similarity score of 0.99). However, linearity decreases when the residual component is removed due to a consistently low output norm of the transformer layer. Our experiments show that removing or linearly approximating some of the most linear blocks of transformers does not affect significantly the loss or model performance. Moreover, in our pretraining experiments on smaller models we introduce a cosine-similarity-based regularization, aimed at reducing layer linearity. This regularization improves performance metrics on benchmarks like Tiny Stories and SuperGLUE and as well successfully decreases the linearity of the models. This study challenges the existing understanding of transformer architectures, suggesting that their operation may be more linear than previously assumed.}",Linearity profiles for different open source models. Normalized depth is the layer index divided by the total depth.,"Transformers have revolutionized the field of natural language processing, offering unprecedented advances in a wide range of applications . However, despite their widespread adoption and success, the complex work of these models remains an area of active research . One aspect that has received less attention is the inherent linearity of intermediate embedding transformations within these architectures. In this study, we embark on an in-depth analysis of the linearity properties of transformers, specifically focusing on decoders, and explore its implications during the pretraining and fine-tuning phases. Our investigation reveals a surprising discovery: the embedding transformations between sequential layers in transformer decoders exhibit almost linear properties. This observation is quantified using Procrustes similarity analysis, demonstrating a near-perfect linearity score of 0.99. Such a discovery not only challenges the traditional understanding of transformer architectures but also opens new opportunities for model optimization and efficiency. Based on this insight, we introduce several new contributions to the field: {itemize} Extensive analysis of the linearity properties of transformer decoders and its dynamics at the pretraining and fine-tuning stages. The development of new algorithms for depth pruning of transformer decoders, allowing to remove the most linear layers without a significant loss in performance. A novel distillation technique that involves pruning, replacing certain layers with linear approximations, and then distilling layer-wise embeddings to preserve model performance. Introducing a new regularization approach for pretraining based on the cosine similarity, designed to decrease the layer linearity. This method not only enhances the performance of transformer models on benchmark datasets such as SuperGLUE and TinyStories , but also improves the expressiveness of embeddings, as evidenced by linear probing tasks. {itemize} With our findings, we are paving the way for more computationally efficient transformer architectures without sacrificing their effectiveness, thereby addressing one of the critical challenges in deploying these models"
Noise Correction on Subjective Datasets,2311.00619v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2311.00619v3_0.png,"Incorporating every annotator's perspective is crucial for unbiased data modeling. Annotator fatigue and changing opinions over time can distort dataset annotations. To combat this, we propose to learn a more accurate representation of diverse opinions by utilizing multitask learning in conjunction with loss-based label correction. We show that using our novel formulation, we can cleanly separate agreeing and disagreeing annotations. Furthermore, this method provides a controllable way to encourage or discourage disagreement. We demonstrate that this modification can improve prediction performance in a single or multi-annotator setting. Lastly, we show that this method remains robust to additional label noise that is applied to subjective data.",GabHateCorpus loss distribution for a single annotator.,"The practice of enhancing label accuracy in datasets through the use of multiple annotations per sample is found in works such as . This method capitalizes on the collective expertise of annotators to improve data quality. The reason is due to the assumption that individual annotators make mistakes and a collection of annotators can provide better-quality labels. Traditional methods for reconciling differences are to use techniques such as majority voting, averaging, or expert opinions . These methods are designed to arrive at a single `ground truth' for training supervised learning models. However, in tasks of a subjective nature where a single `correct' answer may not exist, these practices can compromise the diverse perspectives of individual annotators. These issues are found across many domains such as in medical , social , and others . To improve the fair representation of annotators' opinions, datasets have been built to include individual annotations. These datasets contain labels that are not aggregated through majority voting. This enables the capturing of opinions of annotators who would otherwise have been removed due to the aggregation function. Multitask learning is one way to model this data as demonstrated by , where each task is to predict an annotator's individual label. However, these methods do not account for possible mistakes that individual annotators might make during the annotation process. Datasets such as the GabHateCorpus by and GoEmotions by can contain thousands of annotations by a single annotator. Mistakes such as misidentifying emotions due to annotation exhaustion would not be surprising. Furthermore, annotation of a large number of samples may occur over multiple days and introduce temporal distribution shifts of opinion. We propose to address this problem by introducing loss-based label correction into a multitask learning setting. A fundamental property of loss-based label correction works by exploiting the network memorization effect by . They found that networks tended to learn simple patterns before learning more complex ones. This has been used extensively within the noisy learning community such as in . These methods first utilize the memorization effect to detect mislabeled instances. Then they correct these possible sources of mislabeling using techniques such as incorporation of a network's own guess. Motivated by this, we propose a novel formulation of multitask learning with label correction. However, a main challenge we try to overcome is that naive applications of these methods can erase the diverse perspectives of annotators. This is because the original technique makes use of a sample's loss to determine whether it is correctly or incorrectly labeled. On a dataset with subjective labels, we find that higher loss samples are associated with minority opinions which complicates the noisy sample detection process. We propose a novel method to address this by strengthening or weakening a model's guess of the true label. We find that our method is robust to added noise while also maintaining diverse opinions. We show that this is useful when modeling subjective datasets with differing agreement properties. We highlight our contributions as follows: {itemize} We present a novel formulation of multitask learning with loss-based noise correction. We demonstrate that we can separate agreeing and disagreeing annotations to detect noise and disagreement. We demonstrate that this modification can improve prediction performance in a single or multi-annotator setting. Additionally, this method remains robust to additional label noise that is applied to subjective data. We introduce a hyperparameter to control the degree of label correction due to variability in label properties for different annotation tasks. We show that this has a noticeable effect on performance. {itemize"
What Do Language Models Hear? Probing for Auditory Representations in Language Models,2402.16998v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.16998v2_0.png;/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.16998v2_1.png,"This work explores whether language models encode meaningfully grounded representations of sounds of objects. We learn a linear probe that retrieves the correct text representation of an object given a snippet of audio related to that object, where the sound representation is given by a pretrained audio model. This probe is trained via a contrastive loss that pushes the language representations and sound representations of an object to be close to one another. After training, the probe is tested on its ability to generalize to objects that were not seen during training. Across different language models and audio models, we find that the probe generalization is above chance in many cases, indicating that despite being trained only on raw text, language models encode grounded knowledge of sounds for some objects.","(Top) Language (triangle) and sound (circle) representations aligned via Procrustes analysis \cite{schonemann_generalized_1966}, visualized via PCA. The language representation is from BERT \cite{devlin-etal-2019-bert} and the audio representation is from PaSST \cite{passt}. The classes are color-coded based on their parent nodes (i.e., \texttt{human voice}, \texttt{domestic sounds}, \texttt{animal}, \texttt{music}) according to the ontology from the FSD50K \cite{fonseca2021fsd50k}. (Bottom) A zoomed-in portion of the blue region of the top figure, which shows the structural similarities between the language and sound representations for the \texttt{music} category.","Despite being trained only on surface-form strings (i.e., without explicit grounding), language models (LMs) have been shown to learn representations of perceptual concepts that plausibly mirror the grounded, physical representations of those same concepts. Examples of such concepts that have been investigated so far in the literature include color , direction , size , geography , time , and even visual representations . The alignment between an LM's induced representation of a concept (e.g., the space of word embeddings for colors) and its physical (or human perception-like) representation (e.g., RGB space) has direct implications for how much explicit grounding is necessary for an LM to learn about the ``real world'' referred to by the textual data on which it was trained. And inasmuch as grounding may be relevant for meaning and understanding,{See and for further discussion on the relationship between grounding and meaning.} these findings also have indirect implications for whether LMs can acquire (some operationalization of) meaning through text-only training . This work investigates the extent to which LMs encode perceptual representations of {sounds}. Past works have found that LM representations of some objects are partially isomorphic to representations of those same objects from vision models , suggesting that LMs are able to learn nontrivial structures about the visual world through just text-only training. We extend this setup to sounds through the lens of probing , where we learn simple linear transformations that align the language representation for an object $c$ to its sound representation (from a pretrained audio model). If this retrieval-based probe generalizes to objects that were not seen during training, this suggests that there are structural similarities between the language and sound representations, i.e., LMs have learned meaningfully grounded representations of $c$ despite being just trained on raw text. We conduct the sound probing study across 6 language models and 3 audio models. The language representations include those from word vector-only models (GloVe , word2vec ), encoders (BERT , T5 ), and decoders (GPT-2 , LLaMA ). On the audio side, we experiment with two types of models: self-supervised models that have been pretrained without access to any external labels [AudioMAE;][]{audiomae}, and supervised models that have been pretrained on sound event classification (finetuned AudoMAE, PANN , PaSST ). While all audio models are trained without explicit access to symbolic language data, the representations from supervised models implicitly encode more human perception-like priors given that the classification task itself incorporates information about what snippet of sound constitutes a salient-enough signal to humans to warrant its being classified as a distinct event. That is, purely self-supervised models are more likely to encode more physical (i.e., acoustic) representations whereas supervised models are more likely to encode more human perception-like (i.e., auditory) representations. On both acoustic- and auditory-like sound representations, we find that all language models generalize to unseen classes at an above-chance level. We also find that the generalization performance is typically better for sound representations that have been supervised on sound event classification"
CodeScope: An Execution-based Multilingual Multitask Multidimensional Benchmark for Evaluating LLMs on Code Understanding and Generation,2311.08588v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2311.08588v3_0.pdf,"Large Language Models (LLMs) have demonstrated remarkable performance on assisting humans in programming and facilitating programming automation. However, existing benchmarks for evaluating the code understanding and generation capacities of LLMs suffer from severe limitations. First, most benchmarks are insufficient as they focus on a narrow range of popular programming languages and specific tasks, whereas real-world software development scenarios show a critical need to implement systems with multilingual and multitask programming environments to satisfy diverse requirements. Second, most benchmarks fail to consider the actual executability and the consistency of execution results of the generated code. To bridge these gaps between existing benchmarks and expectations from practical applications, we introduce {CodeScope}, an execution-based, multilingual, multitask, multidimensional evaluation benchmark for comprehensively measuring LLM capabilities on coding tasks. CodeScope covers {43 programming languages} and {eight coding tasks}. It evaluates the coding performance of LLMs from three dimensions (perspectives): {length}, {difficulty}, and {efficiency}. To facilitate execution-based evaluations of code generation, we develop {MultiCodeEngine}, an automated code execution engine that supports 14 programming languages. Finally, we systematically evaluate and analyze eight mainstream LLMs and demonstrate the superior breadth and challenges of CodeScope for evaluating LLMs on code understanding and generation tasks compared to other benchmarks. The CodeScope benchmark and code are publicly available at .","Diagrams illustrating four code understanding tasks, including the input and expected output for each task.","Driven by advances in deep learning and NLP, LLMs have demonstrated outstanding proficiency in various generation and understanding tasks. However, existing benchmarks for evaluating LLMs mainly focus on NLP tasks, such as common sense reasoning, academic examination, and authenticity verification. Existing evaluation methods are significantly insufficient in terms of evaluating completeness and comprehensiveness for code understanding and generation capabilities of LLMs. Firstly, many code LLMs, such as CodeT5+, WizardCoder, and Code LLaMA, employ their own specific single-task evaluation datasets, making it infeasible to comprehensively compare the performance of various LLMs on code understanding and generation tasks on a unified standard. Secondly, existing datasets mostly evaluate LLMs on code tasks for a narrow range of popular programming languages, with a focus on Python and single program synthesis tasks. However, software development often involves multiple programming languages, each following different programming paradigms such as object-oriented, functional, and procedural. Evaluating LLMs within a multilingual framework can reveal their ability to generalize across various languages and paradigms. Moreover, the complementarity between multiple tasks facilitates a comprehensive evaluation of the overall performance of LLMs, ensuring that an LLM is not over-optimized for a specific task and can maintain strong performance across diverse tasks. Importantly, multitask settings more accurately simulate the various requirements and challenges faced in real-world software development practices and hence better test the generalizability of LLMs. Thirdly, most studies (e.g., widely used benchmarks CodeXGLUE and XLCoST) rely on matching-based evaluation metrics, such as BLEU or CodeBLEU, to measure the quality of generated code. However, these metrics may not reflect the practical applicability of the code, as they only compare the surface form similarity between the generated code and the reference code. The ultimate goal of code generation is to produce code that can execute correctly and accomplish specific tasks. Therefore, execution-based metrics, which evaluate the functionality and correctness of the generated code by running it on test cases or comparing its output with the expected output, are more reliable and informative. To address these limitations, we propose {CodeScope}, a benchmark that evaluates the coding proficiency of LLMs using execution-based metrics in a {multilingual} and {multitask} setting. CodeScope consists of eight tasks for code understanding and generation, covering 43 programming languages with an average of 13 languages per task. The task descriptions are summarized in Table . We also conduct comprehensive evaluations of LLMs across three dimensions (that is, {multidimensional}): {Length}, {Difficulty}, and {Efficiency}. Length measures the ability to process code of different lengths; Difficulty evaluates proficiency in solving increasingly complex programming challenges; and Efficiency examines the execution speed and resource consumption of the code generated by LLMs for a specific Code Optimization task. To support CodeScope, we develop a Multilingual Code Execution Engine, {MultiCodeEngine}, which extends the ExecEval engine to accommodate 14 programming languages for code generation tasks. We also establish eight strong baselines for each task to facilitate comprehensive comparisons of coding capabilities of LLMs. We expect these explorations will provide a deep understanding of the strengths and limitations of LLMs on code understanding and generation tasks and provide valuable guidance for future research directions. Our contributions can be summarized as follows: {itemize}[leftmargin=*,noitemsep] {CodeScope benchmark}: We built the first-ever comprehensive benchmark for evaluating LLMs on code understanding and generation tasks, {CodeScope}, which covers the largest number of programming languages (43 in total) and comprises the most comprehensive spectrum of diverse code understanding and generation tasks (eight tasks in total) to date. This benchmark evaluates the actual execution of the generated code, facilitated by MultiCodeEngine, a multilingual code execution engine supporting 14 programming languages. {Multidimensional fine-grained evaluation}: We comprehensively evaluate the performance of LLMs on {eight tasks} from {three dimensions}, namely, {length} (i.e., length of code required to solve the problem); {difficulty} (i.e., complexity of programming problems); and {efficiency} (i.e., execution efficiency of generated code). {Comprehensive evaluations and in-depth analyses}: We evaluate and compare the coding capabilities of eight mainstream LLMs and establish strong baselines for each task. We conduct comprehensive validations and analyses of the utility of the CodeScope benchmark. {itemize"
Digital Socrates: Evaluating LLMs through Explanation Critiques,2311.09613v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2311.09613v3_0.pdf,"While LLMs can provide reasoned explanations along with their answers, the nature and quality of those explanations are still poorly understood. In response, our goal is to define a detailed way of characterizing the explanation capabilities of modern models and to create a nuanced, interpretable explanation evaluation tool that can generate such characterizations automatically, without relying on expensive API calls or human annotations. Our approach is to (a) define the new task of {explanation critiquing} - identifying and categorizing any main flaw in an explanation and providing suggestions to address the flaw, (b) create a sizeable, human-verified dataset for this task, and (c) train an open-source, automatic critique model (called Digital Socrates) using this data. Through quantitative and qualitative analysis, we demonstrate how Digital Socrates is useful for revealing insights about student models by examining their reasoning chains, and how it can provide high-quality, nuanced, automatic evaluation of those model explanations for the first time. Digital Socrates thus fills an important gap in evaluation tools for understanding and improving the explanation behavior of models. { [[EARLY PLACEHOLDER ABSTRACT, WILL BE REPLACED]]\\ Current NLP models based on large language models (LLMs), such as GPT-4 and Llama-2, have the ability to provide explanations along with their answers, for instance through chain-of-thought prompting. We explore how these explanations can provide a deeper window into capabilities of the model. Taking inspiration from Socratic principles of effective tutoring, we design a critique format which gives feedback on a model's explanation. The feedback identifies and categorizes any main flaw, and provides both general and specific suggestions to fix it. We generate such critiques both by prompting a very capable model, like GPT-4, and by fine-tuning a smaller Llama2 models using gold critiques. We show that such critiques can show interesting contrasts between models (like GPT-4 vs Llama2-70B) beyond pure accuracy evaluations. We also show that the smaller critique model is more effective on certain types of critique dimensions (like sloppy reasoning) than others (like long-tail knowledge). Finally, the critiques can be used as effective feedback to a range of ""student"" models in improving their output, even when the student model is larger than the critique model.}","Given a multiple-choice question (together with the answer options and correct answer), as well as a model-generated reasoning chain and answer, our system \model{} gives a critique of the model-generated explanation. In its critiques, \model{} provides localized feedback on where and why reasoning chains are flawed (focusing on the main flaw, if any), accompanied by general and fine-grained suggestions to address the identified flaw, providing nuance and interpretability to the critiques.","Large language models (LLMs) have demonstrated promising end-task performance on a range of tasks . These models, given their text-generation abilities, can also be prompted or trained to externalize their reasoning{A model-generated chain of reasoning can be a free-form explanation or a series of intermediate steps. We use this as a window into the model’s ability to create systematic arguments, but without making any claims about how LLMs reason internally .} as a window into their reasoning capabilities. Despite promising end-task performance, examining LLMs' reasoning chains reveals gaps in the correctness of their factual knowledge and the coherence of their reasoning . Such efforts delving deeper into the quality of model-generated intermediate reasoning chains enable us to advance our understanding of LLMs' strengths and weaknesses in different tasks beyond measuring their performance on standard benchmarks. In existing NLP works, however, judging the quality of LLMs' intermediate reasoning chains is met with several challenges. Early practices for determining the quality of such intermediate generations include (1) reporting end-task accuracy and (2) sampling a subset to perform human annotations on e.g., . Using end-task performance as a proxy for the quality of intermediate reasoning can be problematic as LLMs' final answers can be unfaithful to the generated intermediate reasoning , whereas relying on human annotations is labor-intensive and expensive. To tackle such challenges, several automatic measurements have been proposed. One category focuses on evaluating model-generated text based on a given reference text {y} . Another proposes numerical metrics like ROSCOE and ReCEval scores as a summary of reasoning quality. Others build models that generate free-form critiques and refinement suggestions e.g., . Our work builds upon these previous efforts, introducing a way of automatically evaluating reasoning chains that (1) focuses on the intrinsic quality of the reasoning chains, moving away from the reliance on comparing to any reference reasoning chain; (2) localizes where the reasoning went wrong and provides interpretable feedback on why that part of the reasoning chain should be revised; and (3) uses a semi-structured format useful for gaining both quantitative and qualitative insights about the reasoning chain quality. { In this work, we define the task of {explanation critiquing} which involves nuanced and interpretable critiques that identify and categorize any main flaw in the model-generated explanation and provide both general and specific suggestions to address the flaw. We present a sizeable, human-verified dataset under the explanation critiquing task formulation (Section ) as a resource for future efforts on evaluating model-generated reasoning chains. We train two smaller high-performing, open-source critique models ({} 7B and 13B), capable of generating critiques close to GPT-4 critiques in terms of human rating and other quantitative measures (correlation of explanation scores given and error category matches). Through quantitative and qualitative analysis, we demonstrate how {} is useful for revealing insights about student models by examining their reasoning chains. The use of {} allows for nuanced, interpretable automatic evaluation of explanations without expensive API calls or human annotations.{ We make our dataset and model publicly available at {}. } } To operationalize this, our approach and contributions are thus as follows: {itemize}[noitemsep, topsep=0pt] We define the task of {explanation critiquing}. We create {}, a sizeable, human-verified dataset for the task, both to train critique models and to compare against explanation capabilities of future models. We train and release a high-performing, open-source critique model, {}, that does not rely on expensive API calls or human annotations. We demonstrate the usefulness of {} critiques. {itemize} By providing high-quality, nuanced automatic evaluation of explanations, {} fills an important gap in evaluation tools for the community. We make our dataset and model publicly available at"
Forgetting before Learning: Utilizing Parametric Arithmetic for Knowledge Updating in Large Language Models,2311.08011v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2311.08011v2_0.png,"Recent advancements in Large Language Models (LLMs) have showcased their remarkable capabilities in text understanding and generation. However, even stronger LLMs are susceptible to acquiring erroneous or obsolete information from the training corpus. Direct secondary fine-tuning with data containing new knowledge may be ineffective in updating knowledge due to the conflict between old and new knowledge. In this paper, we propose a new paradigm for fine-tuning called {F-Learning} (orgetting before ), which employs parametric arithmetic to facilitate the forgetting of old knowledge and learning of new knowledge. Experimental results on two publicly available datasets demonstrate that our proposed F-Learning can obviously improve the knowledge updating performance of both full fine-tuning and LoRA fine-tuning, simultaneously outperforming the existing baselines in most cases. Moreover, we have also discovered that forgetting old knowledge by subtracting the parameters of LoRA can yield a similar effect to subtracting the parameters of full fine-tuning, and occasionally even surpass it significantly.",Diagram for “Forgetting before Learning”.,"Large Language Models (LLMs) possess an extraordinary ability to understand and generate natural language . Although LLMs are very capable of learning, they are not immune to the acquisition of incorrect knowledge in the corpus. Moreover, much of the knowledge in the real world is constantly updated, and some of the originally correct knowledge in LLMs can become outdated and invalid over time. For example, the question ""Who is the President of the United States? is answered ""Donald Trump"" in the year 2020, while the answer now is ""Joe Biden"". Consequently, the challenge with LLMs is continuously updating to ensure they reflect current, correct knowledge. Existing methods of model editing and knowledge updating usually add additional network , model parameters , knowledge bases , etc., and the editing process is not as straightforward and simple as fine-tuning methods directly with new knowledge. Currently, the most used method for learning new knowledge is still direct fine-tuning of the model. Empirically, when human beings establish their own initial cognition, if they are exposed to new knowledge that is inconsistent with their initial cognition, they usually feel conflicted and it is difficult for them to learn and accept the new knowledge. If the original cognition and knowledge are forgotten, then the new knowledge to be learned will not conflict with the original cognition and knowledge, which makes it better to learn and absorb the new knowledge. As shown in Figure , it is better to pour in the ""new water"" only after the ""original water"" in the cup has been poured out. For example, if people have been educated to believe that ""the Earth is flat"" since childhood, it would be challenging for them to accept the conflicting knowledge that ""the Earth is round"" when they become adults. Conversely, if they can forget the erroneous knowledge that ""the Earth is flat"" or if they learn the correct knowledge that ""the Earth is round"" before being exposed to the incorrect information, it would be much simpler. Inspired by the above empirical observations and 's task arithmetic, we propose a novel paradigm of knowledge updating called {F-Learning} ({F}orgetting before {Learning}). Specifically, we first fine-tune the initial model using old knowledge and then subtract the difference between the fine-tuned model parameters and the initial model parameters from the initial model parameters. This process is defined as ""{old knowledge forgetting}"". We then use the new knowledge to fine-tune the model after forgetting the old knowledge. This process we define as ""{new knowledge learning}"". After the two stages of {forgetting old knowledge} and {learning new knowledge}, the model's knowledge is updated. The contribution of this work can be summarised as follows: {itemize} We propose a novel fine-tuning paradigm “Forgetting before Learning” (F-Learning) for knowledge updating in large language models. Experimental results show that our proposed F-Learning improves the knowledge updating performance of various fine-tuning methods and outperforms the existing baselines in most cases. Experimental results show that forgetting by subtracting the parameters of LoRA can achieve the approximate effect of subtracting the parameters of full fine-tuning. {itemize"
A Deep Dive into the Trade-Offs of Parameter-Efficient Preference Alignment Techniques,2406.04879v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.04879v1_0.pdf,"Large language models are first pre-trained on trillions of tokens and then instruction-tuned or aligned to specific preferences. While pre-training remains out of reach for most researchers due to the compute required, fine-tuning has become affordable thanks to parameter-efficient methods such as LoRA and QLoRA. Alignment is known to be sensitive to the many factors involved, including the quantity and quality of data, the alignment method, and the adapter rank. However, there has not yet been an extensive study of their effect on downstream performance. To address this gap, we conduct an in-depth investigation of the impact of popular choices for three crucial axes: (i) the alignment dataset (HH-RLHF and BeaverTails), (ii) the alignment technique (SFT and DPO), and (iii) the model (LLaMA-1, Vicuna-v1.3, Mistral-7b, and Mistral-7b-Instruct). Our extensive setup spanning over 300 experiments reveals consistent trends and unexpected findings. We observe how more informative data helps with preference alignment, cases where supervised fine-tuning outperforms preference optimization, and how aligning to a distinct preference boosts performance on downstream tasks. Through our in-depth analyses, we put forward key guidelines to help researchers perform more effective parameter-efficient LLM alignment.","{fig:hh_vs_pb} Performance comparison for helpful and harmless benchmarks when models are aligned using QLoRA over HH-RLHF (in {\colorbox{red!30}{red}}) and BeaverTails (in {\colorbox{blue!30}{blue}}). We observe better performance when using a more informative and high-quality preference alignment dataset, albeit it is often overfitting for non-instruction tuned models when aligned using DPO (Section~\ref{subsec:hh_vs_bv}).","Large Language Models (LLMs) have achieved human-like performance across various tasks such as summarization, commonsense reasoning, and open-ended generation. These LLMs have billions of parameters and are {pre-trained} on trillions of tokens scraped from the web. A lucrative utilization of LLMs is in the form of autonomous agents, to make them follow user instructions and adhere to specific preference requirements. However, the pre-trained models are often incapable of following instructions, and they need to be {aligned} using specially curated preference alignment datasets and methods for generalization. Alignment methods either involve fine-tuning the pre-trained model using auto-regressive language modeling over the ground truth completions (supervised fine-tuning or SFT) or using specialized alignment methods such as reinforcement learning from human feedback (RLHF), direct preference optimization (DPO), or prompt tuning. However, applying these methods to the full models is computationally expensive due to their large sizes. Parameter-efficient training (PEFT) methods such as Low-Rank Adaptation (LoRA) and QLoRA have achieved comparable performance to full fine-tuning of LLMs at a much lower cost. This has enabled researchers to experiment with preference alignment datasets, methods, and models on systems with a single GPU. However, alignment is sensitive to numerous factors and design choices involved in the training. The design choices to align LLMs fit into one of the following three broad, crucial axes: (i) the quality and quantity of the alignment {dataset}, (ii) the preference alignment {method}, and (iii) the nature of the base {model}. Given the increasing interest in preference alignment, an in-depth analysis of the effect of these axes on downstream performance is required. To the best of our knowledge, no extensive study has investigated them, especially in a PEFT setting. We fill this important gap by attempting to answer various key research questions across these three axes: {Alignment Dataset} How do the informativeness and quality, number of samples, and content of the preference dataset impact the downstream performance? {Alignment Method} How do different alignment methods affect pre-trained and instruction-tuned models over complementary preferences? {Nature of the Base Model} How does the downstream performance compare between pre-trained models, instruction-tuned models, and their merged variants? Though our study covers all three axes, given the rapidly growing number of options for each, we restrict the studied choices to the most popular ones. Specifically, we perform our experiments on two commonly used preferences in literature to study alignment trade-offs: harmlessness and helpfulness. We use the (i) two most popular preference alignment datasets with harmlessness and helpfulness annotations: HH-RLHF and BeaverTails, with (ii) the two most widely-used alignment methods: SFT and DPO, and (iii) two commonly used LLMs, LLaMA-1 and Mistral-7b along with their instruction-tuned versions, Vicuna-v1.3 and Mistral-7b-Instruct. For an in-depth study of PEFT methods, we conducted all experiments using both LoRA and QLoRA. Our extensive analysis across these core axes reveals certain consistent trends and unexpected findings, as shown in Table~. We hope that consolidating the key findings into guidelines will benefit the community in conducting impactful research toward LLM alignment. Our contributions can be summarized as: {itemize}[itemsep=2pt,parsep=2pt,partopsep=2pt,leftmargin=*,topsep=2pt] We provide an in-depth study into the trade-offs of parameter-efficient preference alignment training, particularly when using LoRA and QLoRA. We conduct over 300 experiments across three core axes of preference alignment: the dataset, the alignment method, and the model. Through experiments on $5$ evaluation benchmarks across harmlessness and helpfulness, we consolidate our key findings as guidelines for more effective preference alignment practices. {itemize"
Zero-Shot Cross-Domain Dialogue State Tracking via Dual Low-Rank Adaptation,2407.21633v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2407.21633v1_0.pdf,"Zero-shot dialogue state tracking (DST) seeks to enable dialogue systems to transition to unfamiliar domains without manual annotation or extensive retraining. Prior research has approached this objective by embedding prompts into language models (LMs). Common methodologies include integrating prompts at the input layer or introducing learnable variables at each transformer layer. Nonetheless, each strategy exhibits inherent limitations. Prompts integrated at the input layer risk underutilization, with their impact potentially diminishing across successive transformer layers. Conversely, the addition of learnable variables to each layer can complicate the training process and increase inference latency. To tackle the issues mentioned above, this paper proposes Dual Low-Rank Adaptation (DualLoRA), a plug-and-play architecture designed for zero-shot DST. DualLoRA incorporates two distinct Low-Rank Adaptation (LoRA) components, targeting both dialogue context processing and prompt optimization, to ensure the comprehensive influence of prompts throughout the transformer model layers. This is achieved without incurring additional inference latency, showcasing an efficient integration into existing architectures. Through rigorous evaluation on the MultiWOZ and SGD datasets, DualLoRA demonstrates notable improvements across multiple domains, outperforming traditional baseline methods in zero-shot settings. Our code is accessible at: .",Example of a multi-domain dialogue from MultiWOZ dataset.,"Task-oriented dialogue (TOD) systems, designed for specific tasks like restaurant reservations or travel bookings, rely on dialogue state tracking (DST) to interpret user intents as slot-value pairs across dialogue turns, essential for managing multi-domain conversations . Figure shows the change of dialogue states in a multi-turn dialogue crossing multiple domains. An ideal system seamlessly transitions to new domains with minimal training, overcoming the resource-intensive process of data collection and annotation for new domains. Zero-shot learning emerges as a solution, enabling TOD systems to adapt to unseen domains by leveraging existing knowledge, thus bypassing the need for extensive domain-specific data. The advancement in language models (LMs) has significantly enhanced the exploration of zero-shot learning approaches within the context of dialogue state tracking tasks. Adaptation to novel domains is frequently achieved through the incorporation of prompts into transformer-based models . The current method can be classified into two categories. The first type of methods incorporate prompts, e.g. slot description, in the first transformer layer . A common practice within this approach involves the concatenation of prompts subsequent to the input, as shown in Fig . Despite the straightforward nature of their implementation, these strategies exhibit suboptimal utilization of prompts. This refers to the phenomenon where the influence of prompts on the model's output progressively diminishes with each additional transformer layer. Figure shows an example, where deep transformer layers, compared with shallow transformer layers, exhibit a reduced allocation of attention to other tokens within the dialogue context or prompt, in the token encoding process. This issue is further exacerbated in the context of dialogue state tracking, where the dialogue's progression causes a rapid escalation in the context length — often extending to 300-400 tokens or more, in stark contrast to the prompts' modest length of 10-20 tokens. Such a diminishing effect of prompts can severely impair the system's performance in zero-shot learning scenarios, where the ability to discern and leverage domain-specific knowledge becomes critically important. The second category of methodologies incorporates a learnable vector, initialized by prompts, into each layer of the transformer architecture , as shown in Fig . This technique facilitates the model's capacity to consistently engage with the prompt information across all layers, ensuring the prompt's considerations are integrated throughout the encoding process. However, these methods present certain limitations. Direct concatenation of prompts at each layer might introduce extraneous noise, adversely affecting the efficacy of model training. Furthermore, this method of concatenation can result in augmented inference latency of the model. Consequently, devising a strategy to effectively utilize prompts without impeding the standard input-output dynamics of the model and without exacerbating inference time emerges as a critical challenge in the field. To address the aforementioned challenges, we propose a novel plug-and-play framework designed for zero-shot dialogue state tracking, termed Dual Low-Rank Adaptation (DualLoRA). This architecture is characterized by the integration of two Low-Rank Adaptation (LoRA) components: one dedicated to processing dialogue context and the other to refining prompts, as shown in Fig . DualLoRA operates by receiving prompts and applying modifications within the attention layers of the transformer model, subsequently integrating this modified output with the transformer's original output. Through the implementation of this framework across each layer, DualLoRA ensures that the influence of the prompts is perpetuated throughout the entire depth of the model. Notably, this architecture does not incur additional latency during the inference phase, maintaining efficiency in model performance. Furthermore, DualLoRA requires few parameter adjustments and demonstrates exceptional transferability across different domains. Comprehensive experimental evaluations were performed utilizing the MultiWOZ and SGD datasets to assess the efficacy of the proposed methodology. Comparative analyses reveal that our approach yields enhanced Joint Goal Accuracy (JGA) metrics across various domains within both the MultiWOZ and SGD datasets, relative to established baseline methodologies. The structure of this paper is organized as follows: Section 2 offers an overview of the literature pertinent to this research area. Section 3 delineates a detailed exposition of our proposed model. Section 4 presents a detailed account of the experimental outcomes and the specifics of the implementation. Section 5 encapsulates the conclusions derived from this study"
SEER: Facilitating Structured Reasoning and Explanation via Reinforcement Learning,2401.13246v4,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2401.13246v4_0.pdf,"Elucidating the reasoning process with structured explanations from question to answer is crucial, as it significantly enhances the interpretability, traceability, and trustworthiness of question-answering (QA) systems. However, structured explanations demand models to perform intricately structured reasoning, which poses great challenges. Most existing methods focus on single-step reasoning through supervised learning, ignoring logical dependencies between steps. Moreover, existing reinforcement learning (RL) based methods overlook the structured relationships, underutilizing the potential of RL in structured reasoning. In this paper, we propose , a novel method that maximizes a structure-based return to facilitate structured reasoning and explanation. Our proposed structure-based return precisely describes the hierarchical and branching structure inherent in structured reasoning, effectively capturing the intricate relationships between different reasoning steps. In addition, we introduce a fine-grained reward function to meticulously delineate diverse reasoning steps. Extensive experiments show that significantly outperforms state-of-the-art methods, achieving an absolute improvement of 6.9\% over RL-based methods on EntailmentBank, a 4.4\% average improvement on STREET benchmark, and exhibiting outstanding efficiency and cross-dataset generalization performance. Our code is available at .","An example of structured explanation. Given a hypothesis $h$ (a declarative sentence derived from a question-answer pair) and a set of facts (or corpus), the goal is to generate a structured explanation, which delineates the reasoning process from facts to the hypothesis.","Navigating machines to understand and articulate the thought process from posing a question to arriving at an answer has been a long-term pursuit in the AI community. Current QA explainable systems adeptly furnish brief supporting evidence. However, they often fail to clarify the {reasoning process} from prior knowledge to the derived answer. By elucidating the reasoning process of answers generation from the language models, we can greatly improve interpretability, trustworthiness, and debuggability. As illustrated in Figure~, when generating answers for the question ""Which natural material is best for making a table?"", the reasoning process with structured explanations, such as entailment trees or reasoning graphs, explains why ""sturdy wood"" is the best answer. Deriving such complex structured explanations poses a great challenge. Previous methods consider structured explanations as linearized sequences and generate the entire reasoning process in one go. However, these methods lack controllability and may hallucinate unreliable reasoning steps. To address these concerns, recent studies decompose structured explanations and focus on single-step reasoning via supervised learning. Nevertheless, this kind of approach may not always yield optimal results as they fail to consider the interdependencies between different steps. FAME attempts to compensate for these shortcomings by leveraging Monte-Carlo planning, which significantly increases the running time and inadvertently explores numerous ineffective steps (as shown in Table~). Furthermore, FAME still concentrates on isolated single-step reasoning, which lacks support for structured reasoning. As a general framework for solving sequential decision-making problems, reinforcement learning (RL) is employed in RLET to enhance multi-step reasoning. However, RLET defines the return (a.k.a. cumulative reward) using the standard chain structure, thus lacking the ability to represent the tree or graph logical structures inherent in structured reasoning. As a result, the potential of RL for structured reasoning is not fully exploited. To address the above issues, we propose , a novel method that {facilitates {S}tructured r{E}asoning and {E}xplanation via {R}einforcement learning}. In structured reasoning, we observe that the logical dependencies between different steps no longer follow a chained trajectory but instead adhere to the inherent tree or graph structure. Therefore, we propose the structure-based return to precisely describe a tree or graph logical structure, effectively capturing the complex interdependencies between different steps. Additionally, we refine the reward function to meticulously delineate diverse reasoning steps, specifically targeting redundant ones that do not contribute to the final structured explanations. Through experiments in Sec.~, we find that redundant steps represent the exploration in the environment, and appropriate penalization contributes to improved reasoning performance. {Our contributions} are summarized as follows: $$ We propose , a novel RL-based method that facilitates structured reasoning and explanation. To our knowledge, is the first general framework that accommodates scenarios of chained, tree-based, and graph-based structured reasoning. $$ We propose the structure-based return to address the intricate interdependencies among different reasoning steps, effectively stimulating the potential of RL in structured reasoning. $$ We conduct extensive experiments to demonstrate the superiority of over state-of-the-art methods. Our method facilitates the effectiveness and efficiency of structured reasoning and exhibits outstanding cross-dataset generalization performance"
Are AI-Generated Text Detectors Robust to Adversarial Perturbations?,2406.01179v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.01179v2_0.pdf,"The widespread use of large language models (LLMs) has sparked concerns about the potential misuse of AI-generated text, as these models can produce content that closely resembles human-generated text. Current detectors for AI-generated text (AIGT) lack robustness against adversarial perturbations, with even minor changes in characters or words causing a reversal in distinguishing between human-created and AI-generated text. This paper investigates the robustness of existing AIGT detection methods and introduces a novel detector, the Siamese Calibrated Reconstruction Network (SCRN). The SCRN employs a reconstruction network to add and remove noise from text, extracting a semantic representation that is robust to local perturbations. We also propose a siamese calibration technique to train the model to make equally confident predictions under different noise, which improves the model's robustness against adversarial perturbations. Experiments on four publicly available datasets show that the SCRN outperforms all baseline methods, achieving 6.5\%-18.25\% absolute accuracy improvement over the best baseline method under adversarial attacks. Moreover, it exhibits superior generalizability in cross-domain, cross-genre, and mixed-source scenarios. The code is available at .",An example of adversarial perturbation to a RoBERTa-based AIGT detector.,"Large Language Models (LLMs) such as GPT-4 have shown great promise in producing text that closely mimics human language . However, concerns about the misuse of AI-generated text (AIGT) have arisen in various areas, including the spread of fake news , academic dishonesty , and gender bias . To tackle these issues, various AIGT detection methods have been developed, using statistical features from language models and text features from different model architectures and training approaches . Current AI-generated text (AIGT) detectors can effectively identify AI-generated text but struggle with minor adversarial perturbations, such as word substitutions or character swapping . Small changes that do not change the original text's meaning can cause these detectors to fail. Figure shows a concrete example: a RoBERTa-based AIGT detector can be fooled into classifying AI-generated text as human-written by simply abbreviating ""California"" to ""Calif."" This example underscores the limitations of relying solely on token-level features. Therefore, developing robust AIGT detection methods that rely on high-level features is crucial to counteract adversarial perturbation attacks. To address these challenges, we introduce the Siamese Calibrated Reconstruction Network (SCRN), which consists of an encoder, a reconstruction network, and a classification head. The model first converts input texts into token representations, then introduces random Gaussian noise to simulate a perturbation attack. The reconstruction network, acting as a denoising auto-encoder , aims to remove this noise and recover the original representations. The classification head processes these denoised features to produce the final result. During training, we optimize both classification and reconstruction losses, encouraging the model to learn representations that are resilient to random input perturbations. Empirically, we observe that a model trained for robustness against random perturbations may not necessarily be robust against adversarial perturbations. To address this issue, we introduce a training technique called siamese calibration. During training, the model generates two classification results using two independent sets of random noise. The training procedure aims to minimize the symmetric Kullback–Leibler (KL) divergence between the two output probability distributions. Since KL divergence is sensitive to changes in probabilities at all confidence levels, the model can incur a significant loss even when it makes consistently correct predictions but with varying confidence levels due to different noise. This stronger constraint forces the model to make equally confident predictions regardless of the noise. In experiments, we find that this approach encourages the model to rely more on high-level contextual features, thereby significantly enhancing its robustness against adversarial attacks. Our contributions are as follows: (1) We introduce a reconstruction network that enhances the model's robustness by promoting the learning of resilient representations under token-level perturbations. (2) We propose a siamese calibration technique that trains the model to make predictions with consistent confidence levels for various random perturbations, which improves its robustness against adversarial attacks. (3) We establish a comprehensive benchmark for assessing the robustness of AIGT detection methods against a range of adversarial perturbations, including word-level and character-level substitution, deletion, insertion, and swapping. This benchmark encompasses a wide variety of detectors, such as metric-based and model-based detectors, trained using different methods. We evaluate these detectors on four publicly available datasets to test their robustness in in-domain, cross-domain, cross-genre, and mixed-source scenarios. (4) Our experiments on the benchmark show that SCRN significantly outperforms all baselines in terms of robustness, achieving higher accuracy under adversarial perturbation attacks. Specifically, our method improves over the best baseline method by 11.25, 18.25, 14.5, and 15.75 absolute points of accuracy under attack in in-domain, cross-domain, cross-genre, and mixed-source scenarios, respectively"
FinTextQA: A Dataset for Long-form Financial Question Answering,2405.09980v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2405.09980v1_0.pdf,"Accurate evaluation of financial question-answering (QA) systems necessitates a comprehensive dataset encompassing diverse question types and contexts. However, current financial QA datasets lack scope diversity and question complexity. This work introduces {FinTextQA}, a novel dataset for long-form question answering (LFQA) in finance. {FinTextQA} comprises 1,262 high-quality, source-attributed QA pairs extracted and selected from finance textbooks and government agency websites.Moreover, we developed a Retrieval-Augmented Generation (RAG)-based LFQA system, comprising an embedder, retriever, reranker, and generator. A multi-faceted evaluation approach, including human ranking, automatic metrics, and GPT-4 scoring, was employed to benchmark the performance of different LFQA system configurations under heightened noisy conditions. The results indicate that: (1) Among all compared generators, Baichuan2-7B competes closely with GPT-3.5-turbo in accuracy score; (2) The most effective system configuration on our dataset involved setting the embedder, retriever, reranker, and generator as Ada2, Automated Merged Retrieval, Bge-Reranker-Base, and Baichuan2-7B, respectively; (3) models are less susceptible to noise after the length of contexts reaching a specific threshold.",An LFQA sample in {FinTextQA}. Models are expected to generate paragraph-length answers when given questions and documents.,"The growing demand for financial data analysis and management has led to the expansion of artificial intelligence (AI)-driven question-answering (QA) systems. These systems not only enhance customer service but also assist in risk management and personalized stock recommendations . The intricate nature of financial data, with its domain-specific terminologies, concepts, and the inherent uncertainty of the market and decision-making processes, demands a deep understanding of the financial domain to generate accurate and informative responses . In this context, long-form question answering (LFQA) scenarios become particularly relevant as they require models to demonstrate a broad spectrum of sophisticated skills, including information retrieval, summarization, data analysis, comprehension, and reasoning . In the general domain, there are several LFQA datasets available, including ELI5 , WikiHowQA and WebCPM . However, it is important to note that there is currently no LFQA dataset specifically tailored for the finance domain. Existing financial QA benchmarks often fall short in addressing question complexity and variety by primarily on sentiment analysis and numerical calculation, as comprehensive paragraph-length responses and relevant document retrievals are often required to answer intricate, open-domain questions . To address these challenges, we introduce a new dataset, {FinTextQA}, which comprises LFQAs from finance-related textbooks and government agency websites to assess QA models on general finance and regulation or policy-related questions. {FinTextQA} consists of 1,262 high-quality, source-attributed question-answer pairs and associated document contexts. It contains six question types with an average text length of 19.7k words, curated from five rounds of human screening. This dataset is pioneering work in integrating financial regulations and policies into LFQA, challenging models with more demanding content. In addition to introducing the dataset, we conduct comprehensive benchmarking of state-of-the-art ({sota}) models on {FinTextQA} to provide baselines for future research. Current LFQA systems frequently solely rely on fine-tuning pre-trained language models such as GPT-3.5-turbo, LLaMA2 , Baichuan2 , etc., which often fail to provide detailed explanations or effectively handling complicated finance questions . In response, we opt for the Retrieval-augmented generation (RAG) framework, as illustrated in Figure . By processing documents in multiple steps, RAG systems can pre-process and provide the most relevant information to LLMs, enhancing their performance and explanation capabilities . We believe this work, by introducing the first LFQA financial dataset and conducting comprehensive benchmark experiments on the dataset, marks a milestone in advancing the comprehension of financial concepts and enhancing assistance in this field: {FinTextQA} offers a rich and rigorous framework for building and assessing the capabilities of general finance LFQA systems. Our experimental analysis not only highlights the efficacy of various model configurations but also underscores the critical need for enhancing current methodologies to improve both the precision and explicability of financial question-answering systems"
On Measuring Faithfulness or Self-consistency of Natural Language Explanations,2311.07466v4,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2311.07466v4_0.png,"Large language models (LLMs) can explain their predictions through post-hoc or Chain-of-Thought (CoT) explanations. But an LLM could make up reasonably sounding explanations that are unfaithful to its underlying reasoning. Recent work has designed tests that aim to judge the faithfulness of post-hoc or CoT explanations. In this work we argue that these faithfulness tests do not measure faithfulness to the models' inner workings -- but rather their self-consistency at output level. Our contributions are three-fold: i) We clarify the {status of faithfulness tests} in view of model explainability, characterising them as {self-consistency tests} instead. This assessment we underline by ii) constructing a {Comparative Consistency Bank} for self-consistency tests that for the first time compares existing tests on a common suite of 11 open LLMs and 5 tasks -- including iii) our new {self-consistency measure CC-SHAP}. CC-SHAP is a fine-grained measure (not a test) of LLM self-consistency. It compares how a model's input contributes to the predicted answer and to generating the explanation. Our fine-grained CC-SHAP metric allows us iii) to {compare LLM behaviour} when making predictions and to {analyse the effect of other consistency tests} at a deeper level, which takes us one step further towards measuring faithfulness by bringing us closer to the internals of the model than strictly surface output-oriented tests. % Our code is available at",CC-SHAP method on a toy example. Contribution values for illustration only. See \ref{app:instance-examples} for real samples.,"Large language models (LLMs) generate answers in various tasks of increasing difficulty, acting as chatbots , as programming or scientific writing assistants . But often enough they behave unintuitively, showing undesirable behaviour: They can endorse a user's misconceptions , or generate Chain-of-Thought (CoT) explanations that hide their sensitivity to biasing inputs ; they can be insensitive to label correctness in in-context learning , and can produce correct predictions with irrelevant or misleading prompts . Especially in cases of unintuitive behaviour, explanations for their way of acting would be helpful. Even though LLMs can provide plausibly sounding explanations for their answers, recent work argues that model generated natural language explanations (NLEs) are often unfaithful . Obtaining {faithful} explanations that {accurately reflect the reasoning process of a model} is important for understanding the reasons behind an LLM's answer, and is instrumental for a trustworthy AI. Being able to measure NLE faithfulness is most critical when models provide answers we are unable to judge -- whether it is AI uncovering new scientific facts or ChatGPT helping with homework. Recent works aim to test the faithfulness of NLEs that LLMs produce about their {own} predictions (cf.\ §). But the studies are hard to compare, as they use both different models and data (Tab.~). They test for faithfulness by editing model inputs and measuring whether the prediction changes or stays consistent to the original answer. We argue that faithfulness of a NLE is more elusive than what existing tests (including ours) can measure, and that what current tests are measuring is {self-consistency}. We demonstrate this by comparing all tests (including ours) on the {same models and data}, showing that predictions differ widely. While existing tests compare output changes resulting from input edits on the surface, we propose a measure that {does not need input edits} and that more closely analyses how model outputs relate to {how} it processes the input. {comment} By contrast, we are interested in {a measure} {which {does not need input edits}. We further argue that the faithfulness is more elusive than existing tests (incl. ours) capture.} {comment} Overall, our paper contributes the following: {itemize}[topsep=0pt, noitemsep, leftmargin=*] We argue () that current tests that aim to measure NLE faithfulness, in reality measure the {self-consistency of model outputs} -- without giving insight into a model's inner reasoning processes. We introduce (§) CC-SHAP, a new {fine-grained and explainable self-consistency measure} gauging how well a model's input contributions align, when it produces a prediction and explanation, and use it for post-hoc and CoT explanations. Since we {cannot} obtain ground truth for faithfulness by human judgement, we can only compare the predictions of existing tests (§). Hence, we are first to {compare} existing tests -- including CC-SHAP -- on a unified set of models and data after constructing the {Comparative Consistency Bank (CCB).} {itemize} In summary, our {takeaways} § are the following: {itemize}[topsep=2pt, noitemsep, leftmargin=*] We argue in § that existing tests measure self-consistency and not faithfulness. And since they adopt different test scenarios, we expect them to make different predictions. Indeed, they deliver {different results} for the same models and data (§), highlighting the heterogeneity of prior tests that target faithfulness. Given this result, and arguing that current tests do not touch the inner workings of LLMs, we stress that the quest for true {faithfulness metrics} remains open. By analysing CCB, we find trends: i) Chat LLMs show higher self-consistency than their base variants; ii) CC-SHAP agrees most with Counterfactual Edits; iii) We could not detect, nor exclude a relation between model size and self-consistency. With CC-SHAP we take a small step further towards measuring faithfulness: Prior tests compare outputs before and after input edits but don't give insight into how changes in the output relate to changes in how the LLM processes the input. CC-SHAP, by contrast, compares input importances for answer and for explanation generation -- without editing inputs. Comparing predictions from CC-SHAP to prior tests shows that it offers transparency about how inputs (and also possible input modifications) influence LLM workings. {itemize"
UNIMO-G: Unified Image Generation through Multimodal Conditional Diffusion,2401.13388v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2401.13388v3_0.png,"Existing text-to-image diffusion models primarily generate images from text prompts. However, the inherent conciseness of textual descriptions poses challenges in faithfully synthesizing images with intricate details, such as specific entities or scenes. This paper presents {UNIMO-G}, a simple multimodal conditional diffusion framework that operates on multimodal prompts with interleaved textual and visual inputs, which demonstrates a unified ability for both text-driven and subject-driven image generation. UNIMO-G comprises two core components: a Multimodal Large Language Model (MLLM) for encoding multimodal prompts, and a conditional denoising diffusion network for generating images based on the encoded multimodal input. We leverage a two-stage training strategy to effectively train the framework: firstly pre-training on large-scale text-image pairs to develop conditional image generation capabilities, and then instruction tuning with multimodal prompts to achieve unified image generation proficiency. A well-designed data processing pipeline involving language grounding and image segmentation is employed to construct multi-modal prompts. UNIMO-G excels in both text-to-image generation and zero-shot subject-driven synthesis, and is notably effective in generating high-fidelity images from complex multimodal prompts involving multiple image entities.","Examples of UNIMO-G for both text-driven and zero-shot subject-driven generation. UNIMO-G can perceive free-form interleaved visual-language inputs and faithfully generate images. Particularly, it can generate images from multi-modal prompts with multiple image entities.","Recent advancements in text-to-image (T2I) diffusion models have yielded impressive results in the generation of high-fidelity images from textual descriptions. Various methods, including DALL-Es, Imagen, Stable Diffusion, and MM-DiT, have been successful in producing photorealistic and contextually relevant images based on textual prompts. Nevertheless, a fundamental challenge persists due to the inherent brevity of textual descriptions, particularly when intricate details, specific entities, or nuanced scenes are involved. Thus, faithfully generating images from general vision-language (VL) inputs is essential to improve the controllability of image generation. Numerous studies have explored VL-to-image generation techniques. Methods such as DreamBooth, Imagic, SuTI and BLIP-Diffusion emphasize subject-driven generation, where they use both subject images and textual descriptions as inputs to recontextualize the subject in a newly described setting. They either fine-tune specific models for a given subject or employ pre-trained subject representations. However, their specific training design and input templates hinder their scalability, especially in complex scenarios with multiple entities. Additionally, studies like FastComposer and Subject-Diffusion focus on multiple-entity image generation, integrating image embeddings from image encoders with the standard text conditioning in pre-trained diffusion models. Nevertheless, these approaches lack the capacity to efficiently process generalized vision-language inputs that comprise a mix of textual and visual information in free forms. In this paper, we propose {UNIMO-G}, a simple multimodal conditional diffusion framework that operates on multimodal prompts comprising free-form interleaved vision-language inputs. Unlike traditional text-only prompts, multimodal prompts encompass various combinations of image entities and textual elements, as demonstrated in Figure~. UNIMO-G is designed to faithfully reproduce all image entities, render textual content, and follow the instructions in multimodal prompts. Specifically, we leverage the perception capabilities of Multimodal Large Language Models (MLLMs) to encode multimodal prompts into a unified vision-language semantic space. Subsequently, a conditional diffusion network generates images from these encoded representations. To train UNIMO-G efficiently, we implement a two-phase strategy. Initially, the model undergoes pre-training on a large-scale dataset of text-image pairs, enhancing its proficiency in conditional image generation. This is followed by a phase of instruction tuning with multimodal prompts, learns to generate images that align with the detailed specifications provided in these prompts. A carefully designed data processing pipeline, incorporating language grounding and image segmentation, is employed to construct these multimodal prompts. This approach enables UNIMO-G to harness rich features from the MLLM encoder to generate images faithfully reproducing the contents across various contexts. UNIMO-G exhibits a comprehensive capability for controllable image generation, excelling not only in text-to-image synthesis but also in zero-shot subject-driven generation. It adeptly produces high-fidelity images from multimodal prompts, even those containing multiple image entities. To assess its performance, we conducted evaluations in both text-to-image and subject-driven generation contexts using the MS-COCO and DreamBench datasets, respectively. The results consistently highlight UNIMO-G's superior performance in these scenarios. Additionally, recognizing DreamBench's focus on single-subject generation, we introduce MultiBench, a new benchmark featuring images with multiple entities. The evaluation on MultiBench confirms UNIMO-G's effectiveness in zero-shot multi-entity subject-driven generation. In summary, our contributions in this work can be summarized as follows: {itemize} We propose a simple multi-modal conditional diffusion framework that significantly enhances the controllability of image generation by supporting multimodal prompts with interleaved images and text input. We introduce an effective two-stage training strategy, empowering zero-shot multi-entity subject-driven generation through multi-modal instruction tuning. UNIMO-G outperforms existing VL-to-image models in both single and multi-entity subject-driven generation tasks, especially on the capabilities of multimodal instruction following. {itemize"
Unveiling Linguistic Regions in Large Language Models,2402.14700v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.14700v3_0.pdf,"Large Language Models (LLMs) have demonstrated considerable cross-lingual alignment and generalization ability. Current research primarily focuses on improving LLMs' cross-lingual generalization capabilities. However, there is still a lack of research on the intrinsic mechanisms of how LLMs achieve cross-lingual alignment. From the perspective of region partitioning, this paper conducts several investigations on the linguistic competence of LLMs. We discover a core region in LLMs that corresponds to linguistic competence, accounting for approximately 1\% of the total model parameters. Removing this core region by setting parameters to zero results in a significant performance decrease across 30 different languages. Furthermore, this core region exhibits significant dimensional dependence, perturbations to even a single parameter on specific dimensions leading to a loss of linguistic competence. Moreover, we discover that distinct monolingual regions exist for different languages, and disruption to these specific regions substantially reduces the LLMs' proficiency in those corresponding languages. Our research also indicates that freezing the core linguistic region during further pre-training can mitigate the issue of catastrophic forgetting (CF), a common phenomenon observed during further pre-training of LLMs. Overall, exploring the LLMs' functional regions provides insights into the foundation of their intelligence Our code is released in .}.","Three main findings of our experiments: (1) Identification of core language regions within the LLMs, where removals lead to linguistic competence loss; (2) Discovery of monolingual regions, where removals cause significant proficiency loss in specific languages; (3) Optimization of freezing core regions during further pre-training decelerates language forgetting.","Over the years, the field of Natural Language Processing (NLP) has been at the forefront of understanding the core principles of intelligence. The emergence of Large Language Models (LLMs) such as GPT-4 , PaLM 2 and LLaMA 2 , showcase a significant breakthrough. Thanks to unparalleled scales of model architecture and the vastness of training data, these LLMs now exhibit exceptional linguistic competence and can execute complex tasks requiring abstract knowledge and reasoning . Previous research has revealed that LLMs naturally capture cross-linguistic similarities in their representation space, facilitating zero-shot cross-lingual transfer . The model is fine-tuned on one language, enabling the acquisition of comparable capabilities in another language , and exhibits the phenomenon of code-switching when generating context . Attempts to improve LLMs' cross-lingual generalization abilities have been successful through parameter and information transfer learning , aligning languages compulsorily and utilizing in-context learning techniques . However, a detailed investigation into the internal mechanisms of how LLMs possess cross-linguistic alignment capability remains elusive. To delve deeper into the intrinsic mechanisms of LLMs' linguistic competence, this paper focuses on the LLMs' parameter importance and investigate the linguistic regions of LLMs based on 30 distinct languages' performance, with the purpose of figuring out the following questions: {Q1: Does a core linguistic region exist within LLMs that facilitates cross-lingual alignment and generalization?} By conducting further pre-training across six languages and evaluating models' parameter importance (Section ), we discover a region in LLMs corresponding to the core linguistic competence, which accounts for approximately 1\ Furthermore, by visualizing the core linguistic region (Figure ), we observe that the linguistic core region of LLMs exhibits significant dimensional dependence. In certain dimensions, only perturbing a single parameter could lead to the model losing its linguistic competence (Section ). Additionally, ablation study in shows that beyond outlier dimensions, other non-outlier dimensions in this region are also critical. {Q2: Beyond the core linguistic region within LLMs , do distinct monolingual regions exist that specifically influence individual languages?} While LLMs possess strong multilingual capabilities, we discover that each individual language (or language with similar compositional elements or grammatical structures) encompasses independent regions within the LLMs. As shown in the middle of Figure , the analysis of the Russian sentences identifies a particular linguistic region that likewise exerts influence both on the Russian and Ukrainian language, both of which belong to the Slavic group (Section ). {Q3: If and how core linguistic regions affect further pre-training, how to utilize it to optimize further pre-training?} After pre-training, core linguistic parameter regions of the LLMs are established for multilingual alignment. Notable shifts in these regions potentially lead to a decline in model lingual capabilities. Our findings reveal that freezing this core region can mitigate the issue of catastrophic forgetting , a common phenomenon observed during further pre-training of LLMs. As shown at the bottom of Figure , we investigate the impact of selectively freezing 5\ The main contributions of our work are summarized as follows: {itemize} We discover that LLMs possess a core linguistic region, and removing this region (setting parameters to {zero}) results in a significant loss of the model's linguistic capabilities. Furthermore, perturbations to specific dimensions or even a single parameter can lead to a substantial decline in the model's linguistic abilities. We observe that distinct monolingual regions exist in LLMs for different languages. Removing a specific monolingual region causes a significant deterioration in the linguistic capabilities within corresponding language. We perform further pre-training for specific languages within the core linguistic region of LLMs frozen, achieving comparable performance in the target language while mitigating catastrophic forgetting in non-target languages. {itemize"
FastFiD: Improve Inference Efficiency of Open Domain Question Answering via Sentence Selection,2408.06333v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2408.06333v1_0.pdf,"Open Domain Question Answering (ODQA) has been advancing rapidly in recent times, driven by significant developments in dense passage retrieval and pretrained language models. Current models typically incorporate the FiD framework, which is composed by a neural retriever alongside an encoder-decoder neural reader. In the answer generation process, the retriever will retrieve numerous passages (around 100 for instance), each of which is then individually encoded by the encoder. Subsequently, the decoder makes predictions based on these encoded passages. Nevertheless, this framework can be relatively time-consuming, particularly due to the extensive length of the gathered passages. To address this, we introduce FastFiD in this paper, a novel approach that executes sentence selection on the encoded passages. This aids in retaining valuable sentences while reducing the context length required for generating answers. Experiments on three commonly used datasets (Natural Questions, TriviaQA and ASQA) demonstrate that our method can enhance the inference speed by {2.3X-5.7X}, while simultaneously maintaining the model's performance. Moreover, an in-depth analysis of the model's attention reveals that the selected sentences indeed hold a substantial contribution towards the final answer. The codes are publicly available at .","Inference Time for FiD (base) and FastFiD (base) with varying numbers of retrieved passages. As the number of retrieved passages increases, FiD encounters increasingly severe efficiency issues. Our FastFiD significantly accelerates the process by greatly reducing decoding time.","Open Domain Question Answering(ODQA) is a longstanding task in Natural Language Processing that involves generating an answer solely based on a given question. Recent advancements in this field have typically adopted the Retriever-Reader framework, which breaks down the task into two distinct stages. Initially, a retriever retrieves a set of relevant passages from a high-quality collection of open domain documents, such as Wikipedia. Subsequently, a reader model generates an answer by considering the question and the retrieved passages. Thanks to advancements in neural models, the retriever has transitioned from traditional search methods like TF-IDF to dense passage retrieval, resulting in improved retrieval performance. Furthermore, driven by the progress of Pretrained Language Models (PLMs), the reader has evolved from extracting answers from a single passage to generating answers from multiple passages. This approach enables the model to leverage information from various passages more effectively, thereby producing more accurate answers. A recently successful model is Fuse-in-Decoder (FiD), which utilizes Dense Passage Retrieval and a generative reader based on T5, an encoder-decoder model. FiD is capable of encoding each retrieved passage independently and subsequently concatenating these encoded passages to form an extensive context. The concatenated context is then used by the decoder to generate a response. Owing to its straightforward and extensible architecture, numerous subsequent works have introduced modifications based on this framework. However, as the decoder must generate a response based on all retrieved passages, it can be time-consuming to enhance performance through the retrieval of additional passages. Moreover, in real-world scenarios, the latency in generating an answer is a significant factor. As larger language models continue to be developed and demonstrate superior performance, this issue may become more pronounced. To address this issue, we introduce FastFiD, a novel approach that performs sentence selection post the encoder's output and maintains only the essential sentences as references for the decoder, thereby significantly reducing the inference time for each query. To demonstrate the effectiveness of our approach, we first carry out experiments to ascertain that the multi-task training, which involves sentence selection and answer generation, does not conflict with one another during the model's learning process. This is achieved by seamlessly incorporating a selection loss on the encoder outputs with a language modelling loss on answer generation, enabling the model to simultaneously handle both sentence selection and answer generation tasks. An in-depth analysis of the decoder's cross-attention reveals that tokens from the chosen sentences yield a higher average attention score compared to those unchosen. This finding provides compelling evidence that the selected sentences significantly contribute more to the model's predictions. Guided by this insight, we execute a secondary training phase, obliging the model to solely anchor to the selected encoder outputs when making the final prediction. The experimental results obtained from two widely used ODQA datasets, namely Natural Questions (NQ) and TriviaQA, along with a long-form QA dataset called ASQA, demonstrate that FastFiD can achieve performance metrics comparable to the original FiD. Notably, it can reduce the context length by up to {38X} and accelerate the inference time by {2.3X-5.7X} on different datasets. To validate the effectiveness of sentence selection, we also compare its performance with passage reranking after the encoder outputs. The results show that sentence selection yields better performance while maintaining a similar context length. This comparison indicates that sentence selection is a more effective strategy for compressing information across multiple passages. In summary, our contributions can be encapsulated within the following three key points: {itemize} We implement a multi-task training approach, demonstrating that a singular reader model can concurrently perform sentence selection and answer generation. We introduce a novel technique to enhance the inference efficiency of FiD while preserving its question-answering capabilities. We carry out plenty of experiments to validate and analyze the effectiveness of our method. {itemize"
Unlearning Traces the Influential Training Data of Language Models,2401.15241v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2401.15241v2_0.pdf,"Identifying the training datasets that influence a language model's outputs is essential for minimizing the generation of harmful content and enhancing its performance. Ideally, we can measure the influence of each dataset by removing it from training; however, it is prohibitively expensive to retrain a model multiple times. This paper presents UnTrac: {Un}learning {Trac}es the influence of a training dataset on the model's performance. UnTrac is extremely simple; each training dataset is unlearned by gradient ascent, and we evaluate how much the model's predictions change after unlearning. Furthermore, we propose a more scalable approach, UnTrac-Inv, which unlearns a test dataset and evaluates the unlearned model on training datasets. UnTrac-Inv resembles UnTrac, while being efficient for massive training datasets. In the experiments, we examine if our methods can assess the influence of pretraining datasets on generating toxic, biased, and untruthful content. Our methods estimate their influence much more accurately than existing methods while requiring neither excessive memory space nor multiple checkpoints.","Overview of leave-dataset-out vs. proposed methods, UnTrac and UnTrac-Inv.","Large language models (LLMs) have had a significant impact on our society. They exhibit remarkable abilities (e.g., chain-of-thought reasoning) without being explicitly trained for such tasks. At the same time, LLMs also pose potential risks, such as the amplification of discrimination through the propagation of toxic language. LLMs are trained on a vast number of corpora via pretraining or refined through finetuning on diverse tasks. Although some efforts have been made to unravel the black box of LLMs [e.g.,][]{grosse2023studying, feng-etal-2023-pretraining}, it is still unclear which data sources cause their unprecedented abilities and potential harms. Ideally, we can answer this question by removing each dataset from the training datasets and assessing the change in the model's performance after retraining (leave-dataset-out). However, since we need to retrain a model on each dataset, leave-dataset-out is prohibitively expensive. Training data attribution overcomes this problem by approximating the influence with Hessian-based influence functions [HIF;][]{koh2017understanding, koh2019accuracy} or tracking changes in test loss during training [TracIn;][]{pruthi2020estimating}. However, HIF requires a large memory space to approximate the inverse Hessian , while TracIn generally needs multiple model checkpoints. In this paper, we propose UnTrac, which traces the influence of a training dataset by unlearning it from a trained model (Figure ). Leave-dataset-out removes each training dataset and measures its influence by assessing the trained model's performance on a test dataset. Analogous to leave-dataset-out, UnTrac unlearns each training dataset and estimates its influence by assessing the unlearned model's performance on a test dataset. Unlearning has been studied to eliminate sensitive data from a trained model and has recently been applied to LLMs . Following , we unlearn a training dataset using gradient ascent, in contrast to the gradient descent normally used in training. Interestingly, argued that influence functions can be regarded as an approximation of the effect of finetuning on a number of examples (e.g., unlearning mislabeled examples). With UnTrac, instead of using the approximations, we directly quantify the effect of unlearning. When many datasets are used for training, UnTrac is computationally costly because unlearning must be run for every individual training dataset. To overcome this drawback, we propose UnTrac-Inv as a scalable approach particularly effective for an increasing number of training datasets. UnTrac-Inv ``unlearns'' a test dataset instead of training datasets and evaluates the unlearned model on training datasets. UnTrac-Inv requires only a single run of unlearning, and, as we will show, can be considered as an efficient approximation of UnTrac. In our experiments, we first examine whether our methods can trace influential training tasks in the setting of finetuning. We created a dataset representing a mixture of synthetic tasks, designed to evaluate our method's capability in assessing the influence of each task. In order to make this assessment more challenging, we have created task pairs that, while semantically distinct, require responses in the same format. Additionally, we include pairs that are nearly identical in content but demand responses in differing formats. By estimating the influence across these task pairs, we verify that our methods are not overly reliant on superficial similarities between tasks. In this controlled dataset, we show that our methods accurately assess the influence of training tasks, where we use the expensive leave-dataset-out method as the ground-truth, and are only slightly affected by the output format. Next, we assess whether our methods can identify the source of harmful content generated by a pretrained language model. Using smaller open pretrained transformers [OPT-125M;][]{zhang2022opt}, the influence of eight pretraining datasets is estimated. We use three test datasets: Toxigen , WinoBias , and TruthfulQA , which contain toxic language, biased text, and false answers to various questions, respectively. We calculate the ground-truth influence of each training dataset and evaluate the correlation between the estimated influence and ground-truth influence. We demonstrate that our methods accurately estimate the influence of pretraining datasets, significantly outperforming other influence functions. Finally, we investigate how hyperparameters affect the performance of our methods. We found that UnTrac works robustly as long as we use preconditioned gradient methods with higher learning rates and a sufficient number of training iterations. In contrast, UnTrac-Inv works well for large batch sizes while being relatively sensitive to the learning rate and the number of training steps"
Self-Evolving GPT: A Lifelong Autonomous Experiential Learner,2407.08937v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2407.08937v1_0.pdf,"{} [1]{Corresponding Author} To improve the performance of large language models (LLMs), researchers have explored providing LLMs with textual task-solving experience via prompts. However, they rely on manual efforts to acquire and apply such experience for each task, which is not feasible for the growing demand for LLMs and the variety of user questions. To address this issue, we design a lifelong autonomous experiential learning framework based on LLMs to explore whether LLMs can imitate human ability for learning and utilizing experience. It autonomously learns and accumulates experience through experience transfer and induction, categorizing the types of input questions to select which accumulated experience to employ for them. Experimental results on six widely used NLP datasets show that our framework performs reliably in each intermediate step and effectively improves the performance of GPT-3.5 and GPT-4. This validates the feasibility of using LLMs to mimic human experiential learning and application capabilities. Additionally, we provide a detailed analysis of the behavior of our framework at each step.",An example of experience-enhanced LLMs inference.,"Recently, large language models (LLMs) like ChatGPT have achieved excellent performance in various NLP tasks . However, numerous NLP tasks still cannot be effectively addressed by them . This is mainly because they have not accumulated enough experience to handle these tasks during their training. To address these issues, previous studies have explored injecting task-solving experience into LLMs during the inference stage via prompts (as shown in Figure~). Their experience is textual descriptions of the task-solving processes, guidelines, and other insights. Some studies manually craft such experience . Others attempt to summarize experience from manually annotated task datasets , and then during inference, they essentially need to manually select the experience to apply to each question. However, the demands of users on LLMs are ever-expanding, and the types of user questions continue to grow. These methods would lead to high and unbounded costs for human labor. In contrast, humans are capable of autonomous learning and utilizing experience. Humans categorize encountered problems into different task types and induce experience from multiple concrete task practices, which are reused when encountering new problems of the same task type . Besides, humans can transfer experience between similar tasks, thus gaining more experience without time-consuming practices . As lifelong autonomous experience accumulates, humans gradually achieve ability growth. Inspired by this, we want to explore whether LLMs can mimic the above process. This could avoid the substantial manual labor and provide a unique evolutionary path for artificial general intelligence. To facilitate this, we propose a lifelong autonomous experiential learning framework called Self-Evolving GPT (SE-GPT), which consists of a task-specific experience memory and five experience-centric modules based on ChatGPT. For any user question, SE-GPT automatically categorizes the target task type and responds to the question with the target task experience in the memory. For newly encountered task types, it learns experience through experience transfer and induction before responding. Firstly, it locates similar tasks in its memory and transfers their experience to the target task. Then, it autonomously references web information and the transferred experience to practice the target task multiple times, thereby inducing more experience from its successes and failures. Finally, the transferred and induced experience is added to the memory. For tasks encountered previously, it assesses the need for repeating experience transfer and induction before responding, taking into account its proficiency level with the task. To conduct experiments, we provide a basic implementation of our framework. We mainly focus on the overall framework and aim to analyze its effectiveness and behavior. Experiments show that our framework is practically feasible. It effectively improves the average performance of GPT-3.5 and GPT-4 on six widely used datasets by 3.8\ Our framework reliably executes each intermediate module, achieving consistent performance improvements. Besides, we provide a detailed analysis of the behavior of our framework in each intermediate step"
The Hidden Space of Transformer Language Adapters,2402.13137v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.13137v2_0.pdf;/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.13137v2_1.pdf;/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.13137v2_2.pdf;/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.13137v2_3.pdf,"We analyze the operation of transformer language adapters, which are small modules trained on top of a frozen language model to adapt its predictions to new target languages. We show that adapted predictions mostly evolve in the source language the model was trained on, while the target language becomes pronounced only in the very last layers of the model. Moreover, the adaptation process is gradual and distributed across layers, where it is possible to skip small groups of adapters without decreasing adaptation performance. Last, we show that adapters operate on top of the model’s frozen representation space while largely preserving its structure, rather than on an ``isolated'' subspace. Our findings provide a deeper view into the adaptation process of language models to new languages, showcasing the constraints imposed on it by the underlying model and introduces practical implications to enhance its efficiency..}",,"The adaptation of pre-trained transformer-based language models (LMs) to new languages has become a widely adopted practice in natural language processing (NLP). A prominent approach for achieving this is to train small feed-forward network neural modules, called {adapters}, on top of the LM layers, while freezing the LM parameters. To successfully adapt an LM to a new language, adapters should introduce changes that steer the prediction but still can be processed by subsequent layers. Adapters have demonstrated impressive capabilities in zero-shot cross-lingual settings, multi-task learning, and integrating knowledge into pre-trained LMs. However, despite this vast success, how LM predictions are being adapted internally is largely unknown. Nonetheless, a better understanding of the internal operation of LM adapters would be valuable to inform language selection for multilingual pre-training and for designing more effective adaptation approaches. In this work, we tackle the question of how language adapters work, while focusing on LM predictions adapted from one or more languages (source) to another language (target). For our study, we train an auto-regressive decoder-only LM from scratch on English and then adapt it separately to four different target languages: German, French, Hebrew and Arabic (). To demonstrate the generality of our findings, we additionally experiment with bilingual models as well as mBERT , a massively multilingual LM pre-trained on 104 languages.-1 First, we consider the sequence of hidden representations across the layers as an information stream being evolved through additive updates , and analyze the adapters' updates to the prediction (). We find that throughout most of the inference pass, the prediction is evolved in the source language and only in the very last layers the target language abruptly becomes pronounced. Then, we explore the contributions of individual adapters () and observe that adapters introduce gradual updates, which often can be canceled without any decrease in performance, while the last few adapter layers are critical for adaptation success.-1 Based on these findings, we explore two alternative hypotheses for how adapters interact with the underlying frozen LM (). Either adapters operate in an ``isolated'' subspace of the representation space that is unused for predictions in the source language, or they operate on top of the model's representation space, while preserving its structure. We test the first hypothesis by training sparse probing classifiers to detect features that change the most during adaptation and then intervene on these features. We observe that while the identified features indeed deteriorate adaptation performance upon intervention, intervening on random features also leads to substantial drops in adaptation performance, and therefore refute the first hypothesis.-1 To test the second hypothesis we analyze principal components in the representation space of the monolingual base model as well as the adapted models. For different properties such as part-of-speech, number, and tense, we observe a strong alignment between the original representation space and its adapted counterpart. In summary, we provide novel insights into the prevalent language adaptation process of pre-training LMs, showing that adapted predictions are gradually evolved on top of the representation space of the source language, while being shifted towards the target language only at the end of the computation and that adapters operate on top of the existing structure of the pre-trained model's representation space. Our work not only provides a first step towards a better understanding of language model adaptation but also opens interesting avenues for future work on making language adaptation more efficient"
Advancing Large Language Models to Capture Varied Speaking Styles and Respond Properly in Spoken Conversations,2402.12786v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.12786v2_0.pdf,"Although text-based Large Language Models (LLMs) can interact with and help humans through the textual interface, speech is the most natural way for humans to communicate. Speech signals contain not merely linguistic information but also paralinguistic and prosodic information to convey messages beyond text. In this work, we aim to advance an LLM that can listen to speaking style and respond properly in spoken conversation; that is, {the same sentence spoken with different speaking styles would cause different responses in speech}. Since there is no existing dataset with such characteristics, we first build the {StyleTalk} dataset with speech-to-speech conversations with the same sentence in different speaking styles. Furthermore, we propose the {Spoken-LLM} framework to model response speech with a two-stage training approach. The proposed method outperforms text-based LLM (text-only or cascaded pipeline method) and previous speech LLM baseline. The dataset should be released to the community. The audio sample demo is at .","The overview framework of Spoken-LLM. (\texttt{c1},\texttt{r1}) and (\texttt{c2},\texttt{r2}) are the current and response speech sample pairs. \texttt{c1} and \texttt{c2} are fed into the model individually.","Large Language Models (LLMs) have demonstrated remarkable capabilities in dialogue generation, natural language understanding, and commonsense reasoning. While LLMs mostly focus on text modality, speech represents the most natural form of human communication in our daily lives. In this work, we aim to inject speech modality for modeling {spoken conversation} with Multi-modal LLMs (MM-LLMs). The main goal is to develop a humanizing agent capable of listening, understanding, and engaging in dialogue with humans, ultimately leading to higher user satisfaction. Speech signals contain linguistic aspects (words, phonetics, syntax, and semantics), paralinguistic elements (emotions and speaker characteristics), and prosodic factors (speaking style, emphasis, and attitude). In human conversation, while the dialogue primarily relies on the lexical aspect, the speaking styles convey rich information beyond text, and can even alter the semantics of the spoken sentences. Neglecting spoken styles can lead to misinterpretation of communication or unnatural human interaction. For example, as shown in Figure , the current speech with the same current text ({Looks like it might rain later this week though.}) but different speaking styles. The friendly speaking style leads to a cheerful response while speaking in a slow and neutral tone leans toward a sad and negative response. Although there are recent studies on MM-LLMs for speech/audio and text, most of the existing studies focus on {content-centric} Spoken Language Modeling (SLM), joint text and speech processing tasks or general audio perception and hearing ability. There is less attention on spoken dialogue with advanced methods and suitable datasets for modeling paralinguistics and speaking styles of spoken responses. To model spoken dialogue with a generative language model, dGSLM proposes a dual-tower SLM on discrete speech units to model two-channel spoken dialogue, but the generated spoken sentences lack semantic meaning. ParalinGPT organizes tasks in the sequence of current paralinguistic attribute prediction, response paralinguistic attribute prediction, and response text generation with autoregressive conditioning. However, it only uses the speech sentiment as speaking style, which might be primarily based on textual information, and how the speaking styles affect the spoken response is unclear. A concurrent work E-chat enhances LLM to generate responses in different emotional contexts, but the training and evaluation data are entirely generated by GPT-3.5 without human supervision, equivalent to distillation and prompting of GPT-3.5. It can only generate response text, constraining its capacity to control response style or speech-to-speech modeling. To overcome the current limitation, we collect a novel speech-to-speech conversational dataset named {StyleTalk}. This dataset is the first spoken conversation benchmark with {the same dialogue context and input sentence in different speaking styles, accompanied by corresponding expressive spoken responses} for speech-to-speech modeling. The dataset will be released upon the paper's acceptance. Based upon the StyleTalk dataset, we propose a multi-modal two-stage training method named {Spoken-LLM} for spoken dialogue modeling. Spoken-LLM is a fusion of the widely-used open-sourced LLM (Llama 2-Chat) and a self-supervised speech emotion representation model (emotion2vec). The proposed model can predict response speaking style and text, enabling the subsequent expressive Text-to-Speech (TTS) model to generate natural and diverse speech responses. We validate the performance through objective and subjective evaluations of spoken responses. With the same backbone model, the proposed method outperforms the text and speech LLM baseline in lexical/semantic similarity and response style F1 score. The human evaluation also indicates that the proposed method yields more reasonable and proper response speech than the text-only LLM baseline approach"
Automated Justification Production for Claim Veracity in Fact Checking: A Survey on Architectures and Approaches,2407.12853v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2407.12853v1_0.pdf,"Automated Fact-Checking (AFC) is the automated verification of claim accuracy. AFC is crucial in discerning truth from misinformation, especially given the huge amounts of content are generated online daily. Current research focuses on predicting claim veracity through metadata analysis and language scrutiny, with an emphasis on justifying verdicts. This paper surveys recent methodologies, proposing a comprehensive taxonomy and presenting the evolution of research in that landscape. A comparative analysis of methodologies and future directions for improving fact-checking explainability are also discussed. %These methodologies used have been additionally utilized to reduce non-factual statements in Large Language Models (LLMs) - commonly referred to as non-factual hallucinations.",General AFC Pipeline; courtesy of \cite{guo-etal-2022-survey}.,"The huge increase in both user-generated and automated content has led to a significant amount of misinformation. This poses risks to uninformed readers, highlighting the need for scalable, automated methods for verification and fact-checking . While predicting the veracity of claims is essential, relying solely on predictions without providing explanations can be counterproductive, potentially reinforcing belief in false claims and perpetuating misinformation . Most fact-checking models use neural architectures, but interpreting these models is challenging. There is a need for fact-checking frameworks providing justifications to enhance {effectiveness} and {trustworthiness}. This survey presents recent efforts addressing automatic justification production for claim verification, emphasizing the move towards ``{Explainable}'' {Automated Fact-Checking} ({AFC}). Some work refers to the justification production process as the explanation generation process . In this survey, the term ``justification production'' is used following the work of . This survey's main contribution is as follows: Firstly, it introduces a multidimensional taxonomy for categorizing works based on various criteria. Secondly, it provides how research is progressing towards standard justifications. Thirdly, it conducts a comparative analysis of justification production approaches, pipeline architectures, input and output types. Lastly, it identifies challenges while proposing future directions in justification production. Appendix outlines the methodology utilized for literature compilation, detailing the search strategy and selection criteria employed for the papers that form the cornerstone of this survey"
Muffin or Chihuahua? Challenging Multimodal Large Language Models with Multipanel VQA,2401.15847v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2401.15847v3_0.pdf,"Multipanel images, commonly seen as web screenshots, posters, etc., pervade our daily lives. These images, characterized by their composition of multiple subfigures in distinct layouts, effectively convey information to people. Toward building advanced multimodal AI applications, such as agents that understand complex scenes and navigate through webpages, the skill of multipanel visual reasoning is essential, and a comprehensive evaluation of models in this regard is important. Therefore, we introduce Multipanel Visual Question Answering (MultipanelVQA), a novel benchmark comprising 6,600 triplets of questions, answers, and multipanel images that specifically challenge models in comprehending multipanel images. Our evaluation shows that questions in the MultipanelVQA benchmark pose significant challenges to the state-of-the-art Multimodal Large Language Models (MLLMs) tested, even though humans can attain approximately 99\% accuracy on these questions. Distinctively, the MultipanelVQA benchmark features synthetically generated multipanel images specifically crafted to isolate and assess the impact of various factors, such as the layout, on MLLMs' multipanel image comprehension abilities. As a result, in addition to benchmarking the capabilities of MLLMs in understanding multipanel images, we analyze various factors of the multipanel image that affect MLLMs' performance with synthetic data and offer insights for enhancement. .",Examples of Single-panel {vs.} multipanel image VQA. GPT-4V distinguishes muffin and chihuahua in the single-panel image input but struggles with the same content in the multipanel image.,"Multimodal Large Language Models (MLLMs) have become a significant leap in the integration of visual and textual data processing, enabling more nuanced understanding and generation of content that blends both visual and linguistic elements. Being trained on extensive data, advanced MLLMs have shown remarkable proficiency in various tasks (e.g., image captioning and visual question answering) that require natural language understanding, visual-language grounding, visual reasoning, etc. As MLLMs become more competent, there is a trend of establishing increasingly challenging benchmarks that are often arduous for average humans to achieve . However, this raises a pertinent question: Have MLLMs advanced to the stage where elementary benchmarks easily handled by average humans pose little challenge to them? To answer this question, we target multipanel images, each involving a series of subfigures. These subfigures are presented together in certain layouts, such as web screenshots capturing multiple thumbnail images and posters utilizing multipanel formats to present a cohesive narrative or argument. We observe that while humans typically find interpreting multipanel images to be a straightforward task, MLLMs struggle with this challenge when presented with the entire multipanel image as input, as shown in Figure~. This study aims to holistically evaluate MLLMs in understanding multipanel images. We introduce the MultipanelVQA benchmark with 6,600 triplets of multipanel images, questions and answers, challenging models to answer each question based on the multipanel image. There are three questions with distinct types for each multipanel image: identifying common or unique contents across subfigures, pinpointing content in specific subfigures through positional descriptions, and locating subfigures via visual grounding in a multi-choice format. Especially, the first type of question mainly tests the MLLMs' ability to reason about contents and the other two question types also assess the MLLMs' understanding of multipanel image layouts in addition to the content reasoning ability. Uniquely, the multipanel images in the MultipanelVQA benchmark features a diverse mix of real-world web screenshots, posters and synthetic multipanel images, categorized into real-world data and synthetic data subsets. Unlike the real-world data that requires human annotation, the synthetic multipanel images are automatically generated by scripts with subfigures from two existed datasets. The script ensures the generated synthetic multipanel images have even distribution of various attributes such as the number of subfigures, their sizes, and the complexity of layouts, etc. As a result, based on the synthetic data, we are able to precisely isolate and pinpoint the impact of their attributes on the performance of MLLMs. We then benchmark popular open-sourced and proprietary MLLMs on the MultipanelVQA benchmark and conduct thorough error analysis with the help of the synthetic data, which delves into the reasons behind MLLMs' difficulties in interpreting multipanel images. As a result, our main findings are 1) MLLMs are susceptible to content interference caused by the occurrence of multiple subfigures within the multipanel image. 2) The layout for subfigures has an impact on the MLLMs' performance on multipanel images. MLLMs tend to be more successful in understanding multipanel images with layouts with fewer subfigures and larger subfigure sizes. 3) Adding sequential numbers for subfigures as visual prompt can benefit some MLLMs that are sensitive to embedded texts in the input multipanel images. Last but not least, we explore how adding sequential numbers to subfigure captions in multipanel images, akin to the Set-of-Mark visual prompting method , improves MLLMs' understanding of these images. We test MLLMs on multipanel images with and without sequential number captions for each subfigure. As a result, we observed that only GPT-4V and MiniGPT-v2 show a notable improvement when the sequential number is not only embedded in the image but also explicitly mentioned in the question. In conclusion, the contributions of this study are listed as follows: [itemize]{leftmargin=*} {itemize} 0em We propose the MultipanelVQA benchmark with real-world and synthetic data that focus on evaluating the model's ability to understand the content and layout of multipanel images. We benchmark several open-sourced and proprietary MLLMs with the MultipanelVQA benchmark and find that all models tested face a significant challenge in interpreting multipanel images despite their success on single-panel images. Benefited by the synthetic data with even distributions of various multipanel image attributes in the MultipanelVQA benchmark, we conduct thorough error analysis to uncover various factors that impact the model's performance, including subfigure content, layout, background, and visual text hint in multipanel images. Finally, we investigate the potential of adding subfigure captions in multipanel images as visual prompts to enhance the performance of MLLMs on multipanel image understanding. {itemize"
WebVoyager: Building an End-to-End Web Agent with Large Multimodal Models,2401.13919v4,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2401.13919v4_0.pdf,"The rapid advancement of large language models (LLMs) has led to a new era marked by the development of autonomous applications in real-world scenarios, which drives innovation in creating advanced web agents. Existing web agents typically only handle one input modality and are evaluated only in simplified web simulators or static web snapshots, greatly limiting their applicability in real-world scenarios. %primarily focus on processing HTML content of web pages, often neglecting the vital visual information available online. To bridge this gap, we introduce WebVoyager, an innovative Large Multimodal Model (LMM) powered web agent that can complete user instructions end-to-end by interacting with real-world websites. %for online web navigation and end-to-end web task resolution. Moreover, we establish a new benchmark by compiling real-world tasks from 15 popular websites and introduce an automatic evaluation protocol leveraging multimodal understanding abilities of GPT-4V to evaluate open-ended web agents. We show that WebVoyager achieves a 59.1\% task success rate on our benchmark, significantly surpassing the performance of both GPT-4 (All Tools) and the WebVoyager (text-only) setups, underscoring the exceptional capability of WebVoyager. The proposed automatic evaluation metric achieves 85.3\% agreement with human judgment, indicating its effectiveness in providing reliable and accurate assessments of web agents.% }","The overall workflow of WebVoyager. WebVoyager takes web tasks assigned by a human and automatically browses the web online. At each step, WebVoyager selects actions based on screenshots and text (the `type' of the web element and its contents). Once the task is completed, the answers will be returned to the user. For example, for a user query: ""Find the cost of a 2-year protection for PS4 on Amazon."", the agent interacts with Amazon online, locates the PS4, identifies the 2-year protection price, and returns ""\$30.99"" to the user.","The recent advancement of large language models (LLMs), such as ChatGPT and GPT-4 , have sparked significant interest in developing LLM-based autonomous agents for complex task execution . Recent studies have explored the construction of text-based web browsing environments and how to instruct large language model agents to perform web navigation . The primary challenge in these works lies in managing complex and verbose HTML texts, and solutions include simplifying and structuring HTML. However, existing approaches overlook a critical functionality of browsing: rendering HTML into visual webpages. Particularly, vision capability is crucial for utilizing tools such as web browsers, as rendered web pages are inherently designed with user experience (UX), emphasizing intuitive information and structured presentation. This design principle of rendering makes visual analysis more effective than mere HTML representation. At present, large multimodal models (LMMs), particularly GPT-4V(ision) and Gemini , demonstrate a remarkable ability to integrate intricate visual cues with textual information. Existing studies such as Pix2Struct and WebArena , have initiated explorations into using screenshots as inputs for decision-making in web navigation, yet these are preliminary and do not represent a deep exploration. Therefore, building multimodal web agents to leverage the environment rendered by browsers through screenshots, thus mimicking human web browsing behavior, is now a viable approach to enhance web navigation abilities. We introduce WebVoyager (Figure ), a multimodal web agent designed to autonomously accomplish web tasks online from start to finish, managing the entire process end-to-end without any intermediate human intervention. WebVoyager processes the user query by making observations from screenshots and textual content in interactive web elements, formulates a thought on what action to take (such as clicking, typing, or scrolling, etc.), and then executes that action on the websites. Inspired by Set-of-Mark Prompting , we mark interactive web elements on screenshots (see Figure ) to facilitate decision-making for WebVoyager. Another challenge is the evaluation of an end-to-end web agent. Existing benchmarks, such as Mind2Web , primarily focus on stepwise and offline evaluation, where agents follow a predefined ``golden'' trajectory for action selection. This approach, however, may not fully account for the variety of viable strategies to accomplish a task, as it only reflects one possible plan. This limitation could lead to a biased evaluation and difficulties in fairly comparing different methods. To accurately evaluate the capabilities of web agents in end-to-end task completion, we propose an automated evaluation protocol using GPT-4V. Specifically, we save screenshots throughout the online navigation process and then use GPT-4V to evaluate these trajectories together with the final results automatically. Human evaluations are also conducted to verify the results and the analysis shows that our evaluation protocol achieves 85.3\ We conduct evaluations on a newly collected dataset, which is semi-automatically generated using a self-instruct method, comprising 643 web tasks from 15 commonly accessed websites. We also evaluate WebVoyager on 90 web-related tasks of level 1 and level 2 from the GAIA , and 50 interactive open-web tasks from SeeAct . We compare our WebVoyager with 1) GPT-4 (All Tools){GPT-4 (All Tools) is an integrated tool-based agent released by OpenAI in Oct. 2023. See https://chat.openai.com/}, and 2) WebVoyager in a text-only setting which employs the textual accessibility tree proposed in WebArena to describe web pages. The results show that WebVoyager achieves a Task Success Rate of 59.1\ Our research demonstrates the effectiveness of the WebVoyager method for web tasks, offering insights into the development of more intelligent and efficient web automation solutions"
Leveraging Machine-Generated Rationales to Facilitate Social Meaning Detection in Conversations,2406.19545v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.19545v1_0.pdf,"We present a generalizable classification approach that leverages Large Language Models (LLMs) to facilitate the detection of implicitly encoded social meaning in conversations. We design a multi-faceted prompt to extract a textual explanation of the reasoning that connects visible cues to underlying social meanings. These extracted explanations or rationales serve as augmentations to the conversational text to facilitate dialogue understanding and transfer. Our empirical results over 2,340 experimental settings demonstrate the significant positive impact of adding these rationales. Our findings hold true for in-domain classification, zero-shot, and few-shot domain transfer for two different social meaning detection tasks, each spanning two different corpora.","Fraction of cases where the classification performance was significantly better, same, or worse, when rationales were augmented, for two different tasks, i.e. detecting resisting strategies (RES) and recognizing emotions (ERC) and for two settings i.e., in-domain (ID) and transfer (TF).","``All the world's a stage, and all the men and women merely players.'' Beyond content focused areas of Natural Language Processing (NLP), the past two decades have witnessed a surge of interest in modeling language from a social perspective . According to sociologist Erving Goffman language conveys two forms of ``social meaning'', namely, one that is {given} or intentional, and one that is {given off} or unintentional, often thought of as ``reading between the lines''. The former embodies the idea of linguistic agency, the deliberate choices people make to protect their identity or to accomplish social goals . The latter encompasses involuntary cues that signal dispositions, like personality , attitude , or emotion , or psychological conditions, like mental illness , Since social meaning is subtly encoded, traditional classification models often over-fit to context-specific linguistic elements that correlate with these subtle cues within context. Consequently, this makes transfer to unseen domains especially challenging. For example, the same strategy to resist being persuaded would manifest in different ways depending on whether one is negotiating the price of a commodity, or one is hesitating donating to charity . In this work, we propose a generalizable framework that leverages Large Language Models (LLMs) for detecting different kinds of social meaning in conversations. We systematically investigate the generation of ``rationales'' by LLMs, that are designed to break through the opaque surface form of the conversation's text and make the social cues more transparent. While rationales have been utilized previously, to facilitate reasoning , or to explain model predictions , we use rationales to refer to the elicited social meaning, i.e., why and how an utterance was conveyed in dialogue. Our empirical study examines the role of augmenting rationales for two specific social meaning detection tasks: (i) Resistance Strategies (RES), which aligns with intentional and purposeful communication, and (ii) Emotion Recognition (ERC), which is characterized by habitual and subconscious responses. For each of these tasks, the evaluation is conducted over two separate corpora (different domains), but the same social meaning detection task. And thus we present results both for the in-domain (ID) and transfer (TF) settings. We illustrate in Figure~ that baseline models performed significantly worse than their rationale-augmented counterparts for both tasks and settings. Our contributions are as follows : {itemize} We investigate the role of rationales for conveying social meaning by making explicit the subtle cues implicitly encoded during a conversation. We design a multi-faceted prompting framework, grounded in sociolinguistic theory, to generate rationales of high quality. We demonstrate the positive impact of adding rationales for two social meaning detection tasks across several models. We observe that rationales lead to greater performance gains in a cross-domain setting, especially in low data regimes, thereby highlighting the generalizability of our approach. {itemize} We provide the datasets augmented with rationales and code as public resources to encourage future research, especially for the purpose of developing open-source solutions that achieve the same functionality as the proprietary LLMs that perform best in our studies. We also make our code and data publicly available here {{https://github.com/ShoRit/RATDIAL"
Lightweight reranking for language model generations,2307.06857v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2307.06857v3_0.pdf,"Large Language Models (LLMs) can exhibit considerable variation in the quality of their sampled outputs. Reranking and selecting the best generation from the sampled set is a popular way of obtaining strong gains in generation quality. In this paper, we present a novel approach for reranking LLM generations. Unlike other techniques that might involve additional inferences or training a specialized reranker, our approach relies on easy to compute pairwise statistics between the generations that have minimal compute overhead. We show that our approach can be formalized as an extension of self-consistency and analyze its performance in that framework, theoretically as well as via simulations. We show strong improvements for selecting the best $k$ generations for code generation tasks as well as robust improvements for the best generation for the tasks of autoformalization, summarization, and translation. While our approach only assumes black-box access to LLMs, we show that additional access to token probabilities can improve performance even further.","On the left we have the original setup where we have predicates which we know the optimal generation should satisfy and which we can evaluate on the generations. In the middle, we drop the assumption that we know whether the optimal generation should satisfy the predicates or not. On the right, we drop the assumption that we need to evaluate the predicates on the different generations -- only assuming we know on how many predicates a pair of generations agree","The rapid advancement and remarkable achievements of generative large-scale pre-trained language models (LLMs) have brought about a revolutionary transformation in the field of natural language processing (NLP). These models have demonstrated significant enhancements in various NLP applications, such as machine translation, summarization, and code generation. Individual generations sampled from the models often yield high-quality results. However the quality of generated outputs can exhibit considerable variability. Multiple output samplings for the same input can produce certain generations which are of substantially higher quality than the quality of the average generation from the model. Several approaches have been proposed to exploit this phenomenon. One strategy involves improving the underlying models themselves to make the quality of the average generation consistently better. This can be achieved by taking existing model generations, ranking them based on a human feedback, automated evaluation metrics like BLEU score, or execution feedback in case of code. The ranked generations can then be finetuned on directly or can be used to train a reward model that can be used in an RL loop. Another common approach is best-of-$n$ sampling or {reranking}. In this approach, the underlying model is not touched -- we instead take multiple samples from the model and select the best one post-facto using a reranking method. While this approach can often given strong improvements, most extant reranking techniques involve computationally intensive or cumbersome methods to compute the ranking criterion. These include methods like training an auxiliary model as a reranker, evaluating the probability of the query given the generated answer (query likelihood) but at the price of doubling the inference cost, etc. In case of code generation models, another alternative is executing the generated code on unit tests. While such an approach has been applied in various models such as AlphaCode which is targeted towards contest coding problems, it becomes much less feasible as you move past the contest coding setting due to the complexity of setting up the build environment for arbitrary code as well as sandboxing it appropriately. Recently, a simple approach, called self-consistency was proposed for selecting the best answer from multiple generations for tasks where the set of possible answers is small -- for example multiple choice questions or math word problems where there is a {unique} answer consisting of a single or a very limited number of tokens. In that paper, the authors sample multiple chain-of-thought generations from the LLM, extract the predicted answer at end each generation and select the answer with the most number of votes. The motivation behind this is the observation that you can take {different} reasoning paths to get to the same answer. Thus the method aims to {marginalize} over multiple different reasoning paths and rank the answers based on their {marginal} probability rather than their probability conditioned on a single reasoning path. While they achieve substantial improvements over existing baselines, it is not immediately clear how to apply this to open-ended generation tasks like code generation, summarization, or translation - where there is often no chain-of-thought or reasoning path to marginalize over, nor is there necessarily a unique correct answer. We start off with two key observations -- (1) We can have semantically equivalent or near-equivalent generations that are nevertheless not exact matches. These are one subset of generations we can marginalize over (2) For open-ended tasks, a generation can encompass multiple elements. For summarization, there might be multiple relevant facts in the text that a good summary should mention. For code, there might be multiple branch conditions that need to be present to generate a correct implementation. Our generation set could be structured such that while different generations include a different subset of elements (different facts in case of summarization or different branch conditions in case of code), we have only a single generation that contains {all} of the relevant elements. In this case, simply marginalizing over semantically equivalent generations would not be sufficient as there is no semantically equivalent generation for the optimal generation. We develop these two observations in the next section into a minimal overhead reranking method for such open-ended tasks which does not require access to token probabilities. Concretely, our contributions are as follows -- {itemize} We connect the above two observations with the notion of self-consistency. Based on that connection, we then proceed to design an effective minimal overhead reranker which does not require access to token probabilities. We show that the reranking methods utilized in previous works can also be understood within the same conceptual framework. We conduct simulations where we demonstrate that our framework is capable of recovering the best or near-best generation in many cases. We also prove some properties of our methodology that provide guarantees on its effectiveness. We extend our reranker to {optionally} account for token log probabilities (if they are provided) and show that doing so gives a much better reranker than just mean log probability reranking (which also requires access to token log probabilities) Empirically, while our focus is on code generation tasks where we demonstrate significant gains, we also experiment with the tasks of autoformalization, summarization, and translation and find that our approach leads to non-trivial though smaller gains there. As our method is based on pairwise similarity between generations, we are able to leverage that property to improve ranked best-of-$k$ performance for different values of $k$. We conduct multiple experiments ablations to understand the effect of various experimental settings. {itemize} The rest of the paper is organized as follows. In Section~ we present our motivation. In Section~ we present our method and the similarity function. In Section~, we present and discuss our experimental results. In Section~, we describe the related work and we finally conclude in Section~"
The Unreasonable Effectiveness of Easy Training Data for Hard Tasks,2401.06751v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2401.06751v2_0.pdf,"How can we train models to perform well on hard test data when hard training data is by definition difficult to label correctly? This question has been termed the {scalable oversight} problem and has drawn increasing attention as language models have continually improved. In this paper, we present the surprising conclusion that current pretrained language models often generalize relatively well from easy to hard data, even performing as well as oracle models finetuned on hard data. We demonstrate this kind of easy-to-hard generalization using simple finetuning methods like in-context learning, linear classifier heads, and QLoRA for seven different measures of datapoint hardness, including six empirically diverse human hardness measures (like grade level) and one model-based measure (loss-based). Furthermore, we show that even if one cares most about model performance on hard data, it can be better to collect easy data rather than hard data for finetuning, since hard data is generally noisier and costlier to collect. Our experiments use open models up to 70b in size and four publicly available question-answering datasets with questions ranging in difficulty from 3rd grade science questions to college level STEM questions and general-knowledge trivia. We conclude that easy-to-hard generalization in LMs is surprisingly strong for the tasks studied..}","A model prompted with easy data (e.g., 3rd Grade problems) does {almost as well} on a hard task (College problems) as a model prompted with hard data (the College bar). Results shown for Mixtral-8x7B with $k{=}10$ prompt examples, averaged over 5 random seeds.","It is difficult to supervise LMs (i.e., train LMs to give correct outputs) in specialized domains of human knowledge, because it is difficult to correctly label data in such domains. Labeling difficulty manifests itself in both time to annotate (and thus cost) and label noise . Labeling difficulty becomes severe when specific expertise is required . For example, for sufficiently specific physics problems, PhD holders and PhD students can make errors on as many as 40\ As more NLP benchmarks focus on challenging domain-specific tasks, having access to large human-labeled training corpora may become increasingly infeasible (e.g., existing benchmarks like MMLU and GPQA do not come with training data). The question arises: how can we train models to solve hard problems when correctly labeling enough hard data for training is difficult? This problem is an example of the scalable oversight problem, which concerns how to give a good reward signal to a model when it is difficult to assess if its outputs are correct . =-1 In this paper, we study the problem of {easy-to-hard generalization}. Easy-to-hard generalization refers to model performance on hard test data when finetuned{We use ``finetuning'' interchangeably with ``training'' and ``fitting'' to refer to fitting pretrained models to data via in-context learning (ICL), parameter efficient finetuning (QLoRA), or by training a linear classifier head.} only on easy training data, defined according to some human hardness measure (like grade level). Since gathering data in domains like graduate level STEM fields is expensive and time-consuming, it would clearly be useful if we could improve model performance in these domains by only finetuning models on cleanly labeled data from simpler domains, like high school STEM questions. To assess how well current LMs generalize from easy to hard data, we fit models to easy data and test them on hard data (``easy-to-hard''), then compare them to an oracle upper bound and unsupervised lower bound. The oracle upper bound is a model that has access to labeled hard data for finetuning (``hard-to-hard''), while the unsupervised lower bound is a model that is prompted zero-shot to answer questions (``unsupervised-to-hard''). The metric we are interested in is the {Supervision Gap Recovered (SGR):} {align*} {{Easy} - {Unsupervised}}{{Hard} - {Unsupervised}} {align*} {-5pt} where Easy, Hard, and Unsupervised refer to model performance {on hard test data} when finetuned on easy data, hard data, or no data (zero-shot), respectively. This metric takes a value of 100\ Our main result is that pretrained language models generalize surprisingly well from easy to hard data, often performing almost as well as an oracle model finetuned on hard data (illustrated in Fig. ). In experiments with ARC , MMLU , GSM8k , and StrategyQA , we find that the Supervision Gap Recovered is usually {between~70\ These results are robust across (1) model family and scale (between 7b and 70b parameters), (2) six different human hardness measures and a model-based measure, (3) four datasets/tasks, and (4) several finetuning methods including in-context learning with and without chain-of-thought reasoning , QLoRA , and linear classifier heads . Overall, our results suggest that current LMs generalize relatively well to test data across human difficulty levels even when finetuned on data that is measurably easier than the test data. We hypothesize that this occurs because easy data elicits latent knowledge and skills from pretrained models {in a hardness-invariant way}. We additionally demonstrate that easy supervision can outperform hard supervision when (1) within some data collection budget, a greater quantity of easy data can be collected than hard data, or (2) easy data can be labeled with lower error rates than hard data (Sec. ). Lastly, we study how easy-to-hard generalization changes with model scale and the gap between train and test hardness (Sec. ). The remainder of the paper is organized along the following research questions: {-3pt} {enumerate}[leftmargin=31pt, itemsep=-3pt, label={RQ:}] How Can We Measure Data Hardness? Can We Do Well on Hard Data by Training on Easy Data? What Are the Cost-Benefit Tradeoffs of Collecting Easy vs. Hard Training Data? Is Easy-To-Hard Generalization Consistent Across Model Scale and Train-Test Hardness Gap Size? {-3pt} {enumerate} We summarize our main conclusions below: {-3pt} {enumerate}[leftmargin=12pt, itemsep=-2pt] Our six human hardness measures and one model-based measure are empirically diverse and model performance declines on harder test data for each measure. LMs generalize surprisingly well from easy-to-hard data, closing 70\ We show that it is often better to train on easy data when hard data is more expensive to collect or has noisier labels. The Supervision Gap Recovered is highly robust across model scale. Easy-to-hard performance may begin to decline when the train-test hardness gap is large enough. {enumerate"
MIDGARD: Self-Consistency Using Minimum Description Length for Structured Commonsense Reasoning,2405.05189v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2405.05189v2_0.pdf,"We study the task of conducting structured reasoning as generating a reasoning graph from natural language input using large language models (LLMs). Previous approaches have explored various prompting schemes, yet they suffer from error propagation due to the autoregressive nature and single-pass-based decoding, which lack error correction capability. Additionally, relying solely on a single sample may result in the omission of true nodes and edges. To counter this, we draw inspiration from {self-consistency} (SC), which involves sampling a diverse set of reasoning chains and taking the majority vote as the final answer. To tackle the substantial challenge of applying SC on generated graphs, we propose {} (nimum escription length uided ggregation of easoning in irected acyclic graph) that leverages Minimum Description Length (MDL)-based formulation to identify consistent properties among the different graph samples generated by an LLM. This formulation helps reject properties that appear in only a few samples, which are likely to be erroneous, while enabling the inclusion of missing elements without compromising precision. Our method demonstrates superior performance than comparisons across various structured reasoning tasks, including argument structure extraction, explanation graph generation, inferring dependency relations among actions for everyday tasks, and semantic graph generation from natural texts.","Comparison of \model with \textsc{CoCoGen}. In this example, our objective is to infer dependency relations among items in the ""Action List"" to achieve the specified ""Objective"". \textsc{CoCoGen} uses greedy decoding and exhibits errors in the output, e.g., ""decided to run errands during a break in the rain"" is not connected with ""Drive to errand location and complete"". In contrast, our approach \model (within the \colorbox[HTML]{FCE5CD}{orange} rectangle) aggregates relevant information across different samples, resulting in more accurate inference. For this example, our algorithm improved the performance of greedy decoding from $\mathbf{66.7}$ to $\mathbf{85.7}$ in edge $F_1$-score.","While large language models (LLMs) have showcased impressive performance in few-shot/zero-shot scenarios across diverse reasoning tasks, it is still challenging to apply these models for structured commonsense reasoning which involves generating task-specific reasoning as a graph, such as extracting argument structures from argumentative text, generating structured explanations that lay out commonsense knowledge to connect an argument to a belief, and inferring dependencies among events for everyday activities. There are two main challenges for {structured reasoning} tasks. (1) {Style discrepancy}: Conventional approaches for structured response generation represent the graphs as flattened strings, leading to subpar performance due to output style mismatch. (2) {Error propagation}: Any incorrect decisions made earlier in the autoregressive decoding process can influence later generation steps. Recently, propose {CoCoGen} to address the issue of style mismatch in structured reasoning tasks, by using programming scripts as prompts for LLMs. It still suffers from error propagation, since it generates variable declarations and function calls in order to describe the nodes and edges within the graph. Any error in these declarations/calls can affect the subsequent generations. To address these issues, we take inspiration from the {self-consistency} (SC) strategy that samples diverse reasoning paths and then takes a majority vote as the final answer. The intuition behind SC is that sampling distinct reasoning chains leads to higher confidence in the correctness of a consistent answer. Therefore, we hypothesize that sampling diverse graphs from an LLM can construct a more accurate aggregate graph and alleviate error propagation for structured reasoning tasks as any errors made in one sample are less likely to persist across all the generated graphs. A crucial distinction between SC and our desideratum is that SC focuses exclusively on commonsense reasoning tasks with {scalar answer spaces}. In contrast, we aim to {merge multiple graphs}, each representing a collection of unordered sets (nodes and edges). It is unclear how to apply majority vote to aggregate distinct sets of nodes and edges. In particular, it would be critical to filter out inaccurate nodes and edges in our setup. To achieve this, we propose {}{Our code is publicly available at {https://github.com/launchnlp/MIDGARD}.}, based on {MI}nimum {D}escription length {G}uided {A}ggregation of {R}easoning in {D}irected acyclic graph. We employ the principle of {minimum description length} (MDL) which seeks to find the hypothesis with shortest description length of the observations. While MDL has been implemented for model selection, causal structure learning, data clustering, and dimensionality reduction, to the best of our knowledge, its use in automatically merging graph samples has never been explored before. Assuming that graph properties consistent across multiple generated samples are more likely to be accurate, we define the description length of a graph sample as the weighted sum of the transformations required to convert a hypothesis into the given sample. By constructing a hypothesis that minimizes the description length across all the generated samples, our solution encourages the inclusion of graph properties that were present in many samples, while rejecting properties that were only present in a few samples which are likely to be erroneous. Figure~ shows an example of how our approach reduces errors compared to relying solely on a single greedy generation. Empirical results on four different structured reasoning tasks, including argument structure extraction, structured explanation construction, and goal-oriented script generation and semantic graph generation, on eight benchmarks show that can outperform competitive baseline and model variants, demonstrating its strong generalizability"
ChiMed-GPT: A Chinese Medical Large Language Model with Full Training Regime and Better Alignment to Human Preferences,2311.06025v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2311.06025v3_0.png,"Recently, the increasing demand for superior medical services has highlighted the discrepancies in the medical infrastructure. With big data, especially texts, forming the foundation of medical services, there is an exigent need for effective natural language processing (NLP) solutions tailored to the healthcare domain. Conventional approaches leveraging pre-trained models present promising results in this domain and current large language models (LLMs) offer advanced foundation for medical text processing. However, most medical LLMs are trained only with supervised fine-tuning (SFT), even though it efficiently empowers LLMs to understand and respond to medical instructions but is ineffective in learning domain knowledge and aligning with human preference. In this work, we propose , a new benchmark LLM designed explicitly for Chinese medical domain, and undergoes a comprehensive training regime with pre-training, SFT, and RLHF. Evaluations on tasks including information extraction, question answering, and dialogue generation demonstrate 's superior performance over general domain LLMs. Furthermore, we analyze possible biases through prompting to perform attitude scales regarding discrimination of patients, so as to contribute to further responsible development of LLMs in the medical domain..}","An illustration of the overall training process of the \textsc{ChiMed-GPT}, which consists of three stages including pre-training, supervised fine-tuning, and reinforcement learning from human feedback (RLHF).","Medical service is one of the cornerstones of societal welfare, instrumental in advancing social development and elevating the contentment of its members. With the growing expectations of the public for better health care, the burgeoning demand for medical services juxtaposed against a limited medical workforce, intensifies the imbalance between healthcare availability and peoples' requests. This mismatch underscores the challenges that current medical infrastructure is required to meet societal needs, thus highlighting the importance of advancing medical intelligence so that healthcare services could be provided sufficiently and automatically. To improve medical intelligence, natural language processing (NLP) technologies are of great significance to efficiently process text data, which are the primary medium for information processing. Among widely used NLP techniques, for years that pre-trained models serve as the foundation and are utilized in various NLP tasks and achieve state-of-the-art performance . However, such approaches with pre-trained models heavily depended on the pre-training and fine-tuning paradigm, where a model requires considerable labeled medical data and is normally trained on them and applied to particular tasks. In doing so, pre-trained models are bound to given tasks and hard to transfer to new scenarios. Recently, the emergence of large language models (LLMs), such as GPT-3.5 and GPT-4 that are accessible via online API service and Alpaca , Vicuna , and Ziya whose model weights are open sourced, lead to a paradigm shift, offering unified solutions to a wide range of tasks. However, most well-performed LLMs are trained in the general domain, facing challenges when they are applied to specialized fields, such as the medical domain, owing to the knowledge gap among domains. For example, Table illustrates a representative example where the GPT-4 fails to provide a satisfying response to a healthcare question, whereas a medical domain LLM is able to do so. To address the knowledge gap among domains, there are some efforts in training LLMs specifically for the medical domain, such as BioMedLM , BioGPT , MedAlpaca , ChatDoctor , Baize-healthcare , Med-PaLM-2 , etc. Especially for Chinese, there are also similar models such as BenTsao , and MedicalGPT . Yet, some of these medical LLMs, although very large, are not open-sourced, lead to difficulties for the community to utilize and improve them to fit more specific fields. On the contrary, publicly available ones face challenges in various aspects, including data preparation, training procedure, and model configuration. Specifically, they are trained on medical data collected from limited sources, which leads to a low diversity of the samples and hence makes the resulted LLMs difficult to generalize. Moreover, these models predominantly rely on the supervised fine-tuning (SFT) method and omit other vital procedures for training LLMs, e.g., pre-training and reinforcement learning from human feedback (RLHF), which are proven to be effective in aligning specified knowledge. Another restriction is that, many existing models have a limited context length of 2,048 tokens, which restricts their ability to perform comprehensive modeling for long text, which is of great importance since large volumes of text that applied in the medical domain are lengthy and have strong contextual coherence in them. To address these challenges, in this paper, we propose {ChiMed-GPT}, a new benchmark LLM for Chinese medical text processing. Following the convention in existing studies to train domain-specific LLMs, we continually train a general domain LLM, Ziya-13B-v2 , on large medical data, and perform a full training regime including (continue) pre-training, SFT, and RLHF. As a distinctive feature, we use data augmentation to produce high-quality human preference data for reword model training and employ rejection sampling fine-tuning to learn from the data, which is proved to be more efficient than standard proximal policy optimization (PPO). The data to train {ChiMed-GPT} are extracted from multiple resources ranging from medical articles to real-world interactions between patients and doctors, which allows our LLM to effectively align with medical knowledge and generate appropriate responses for patients' inquiries rather than difficult-to-understand text produced by other LLMs. In addition, we train {ChiMed-GPT} on safety data containing desired and appropriate responses (e.g., those refuse-to-answer instances) to deal with the scenario when LLMs are prompted with toxic instructions. Particularly, the context length of {ChiMed-GPT} is set to 4,096, which exceeds the context size of existing medical LLM, so that provides better text processing ability for medical applications. We evaluate {ChiMed-GPT} on three types of essential tasks for Chinese medical text processing, including information extraction, question answering (QA), and dialogue generation. Results demonstrate that our approach outperforms other LLMs from general and medical domains and indicate its generalization ability in real applications. Moreover, further analysis on the bias of {ChiMed-GPT} shows its effectiveness in generating safer content when it interacts with patients"
Leveraging Large Language Models for Learning Complex Legal Concepts through Storytelling,2402.17019v4,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.17019v4_0.png,"Making legal knowledge accessible to non-experts is crucial for enhancing general legal literacy and encouraging civic participation in democracy. However, legal documents are often challenging to understand for people without legal backgrounds. In this paper, we present a novel application of large language models (LLMs) in legal education to help non-experts learn intricate legal concepts through storytelling, an effective pedagogical tool in conveying complex and abstract concepts. We also introduce a new dataset , which consists of 294 complex legal doctrines, each accompanied by a story and a set of multiple-choice questions generated by LLMs. To construct the dataset, we experiment with various LLMs to generate legal stories explaining these concepts. Furthermore, we use an expert-in-the-loop approach to iteratively design multiple-choice questions. Then, we evaluate the effectiveness of storytelling with LLMs through randomized controlled trials (RCTs) with legal novices on 10 samples from the dataset. We find that LLM-generated stories enhance comprehension of legal concepts and interest in law among non-native speakers compared to only definitions. Moreover, stories consistently help participants relate legal concepts to their lives. Finally, we find that learning with stories shows a higher retention rate for non-native speakers in the follow-up assessment. Our work has strong implications for using LLMs in promoting teaching and learning in the legal field and beyond.",Illustration of the expert-in-the-loop pipeline. The left section demonstrates the procedure to produce an LLM-generated story from the concept. The lower section in the center shows how we use both the definition and story as input to produce LLM-generated reading comprehension (RC) questions. The center upper section shows that we first collect expert feedback on questions and regenerate questions with expert advice. The right section outlines the RCT experiment to see if LLM-generated stories improve comprehension in legal concepts.,"Often individuals find themselves in certain high-stakes situations where they have to educate themselves on novel concepts such as new policies before voting, mortgage terms when buying a house or legal principles relevant to an ongoing lawsuit. Unfamiliar terms and nuanced use of language in these contexts can make it challenging for non-experts to make informed decisions, to have equal access to justice, or to participate in civic discourse and democracy. We present this work as a step towards enhancing general legal literacy, bridging the gap between non-experts and experts and promoting constructive and civic discourse. Storytelling is an important medium to communicate science to non-experts and teach professional knowledge to beginners . In legal contexts, storytelling has been used extensively to teach abstract legal concepts such as ethics , and has proven effective at explaining complex legal concepts such as legal mediation to the general public . However, the scalable implementation of legal storytelling education is severely limited by the high costs associated with legal experts. Large language models (LLMs) and their impressive text generation abilities have facilitated high-quality automated explanations and stories. Recent efforts have leveraged LLMs to generate accessible explanations of scientific or medical concepts for diverse audiences. used GPT-4 to generate explanations for legal concepts from statutory provisions. However, to the best of our knowledge, previous work has not: (1) used LLM-generated stories as a medium to explain complex concepts, especially in the under-explored legal domain, (2) generated and refined (via expert feedback) questions for the assessment of concept comprehension, nor (3) validated the effectiveness of LLM-generated stories in enhancing comprehension among non-experts. In this work, we explore a novel application of LLMs that focuses on the use of generated stories and questions to facilitate the learning and assessment of legal concept understanding. We use a human-in-the-loop pipeline that combines LLM and expert input to generate stories and multiple-choice questions. We loop in both Prolific workers and legal experts{This paper uses the term ``legal experts'' to refer to two people who have graduated with JD degrees or have made substantial progress towards earning JD degrees.} to ensure that the LLM-generated content is of high-quality. Our pipeline presents a holistic approach to LLMs' application in the legal education domain, where both the learning intervention (stories) and assessment (reading comprehension questions) are generated and evaluated. By providing a reusable dataset and promising experiment results, our work has strong implications for the broader use of LLMs to enhance teaching and learning and to improve general legal literacy. Our contributions are as follows: {itemize}[itemsep=1pt, topsep=1pt] We create a novel legal education dataset, {LegalStories}, which presents legal concepts with their definitions, LLM-generated stories and questions, and human annotations for future NLP and legal education research{Both the code and data are available at this repository: {https://github.com/hjian42/LegalStories}.}. We provide extensive comparisons of three LLMs, namely, LLaMA 2, GPT-3.5, and GPT-4, to generate legal stories and questions with both automatic and human evaluations. We conduct RCTs with both native and non-native English speakers to learn legal concepts, demonstrating that LLM-generated stories improve concept comprehension and interest in law among non-native speakers compared to Wikipedia definitions. We also find that LLM-generated stories consistently help both native and non-native participants in relating legal concepts to their personal lives. {itemize"
Prompt Optimization via Adversarial In-Context Learning,2312.02614v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2312.02614v3_0.pdf,"We propose a new method, Adversarial In-Context Learning (.}}), to optimize prompts for in-context learning (ICL). Inspired by adversarial learning, is implemented as a two-player game between a generator and discriminator, with LLMs acting as both. In each round, given an input prefixed by task instructions and several exemplars, the generator produces an output. The discriminator then classifies the generator's input-output pair as model-generated or real data. Based on the discriminator's loss, a prompt modifier LLM proposes possible edits to the generator and discriminator prompts, and the edits that most improve the adversarial loss are selected. We show that applying results in significant improvements over state-of-the-art prompt optimization techniques for both open and closed-source models on $13$ generation and classification tasks including summarization, arithmetic reasoning, machine translation, data-to-text generation, and the MMLU and big-bench hard benchmarks. In addition, our method is computationally efficient, easily extensible to other LLMs and tasks, and effective in low-resource settings","\adv{} orchestrates a minimax game between a {Generator} and a {Discriminator}, both powered by LLMs with few-shot prompts. The Generator crafts responses to unlabeled examples, while the Discriminator distinguishes between generated and ground truth outputs. Updates are made by a {Prompt Modifier} which modifies prompts based on the adversarial loss.","Generative Adversarial Networks (GANs) and adversarial learning have driven significant progress across a range of domains, including image generation, domain adaptation, and enhancing model robustness. At its core, adversarial learning frames training as a minimax game between a {generator} and a {discriminator}. The generator aims to generate output realistic enough that the discriminator classifies it as real (i.e., not generated), while the discriminator aims to accurately differentiate between generator output and real training samples. After each round, the parameters of both models are updated based on an adversarial loss, and the process repeats. As the generator improves, the discriminator improves alongside it, finding ``weak spots"" in generator output that may go undiscovered in non-adversarial training, ultimately resulting in better generator outputs. Despite success in other domains, applying adversarial learning to pre-training LLMs is impractical due to the data and computational overheads associated with training two models. Particularly for novel tasks where data is often scarce, it is desirable to have methods that can improve model performance using limited data. In this work, we solve this problem by applying adversarial learning to {in-context learning (ICL)}, which has shown to be an effective method to improve model performance with few training samples. Though, effective, ICL has shown to be sensitive to changes in prompts. We introduce {Adversarial In-Context Learning} ({}), which applies insights from adversarial learning to prompt optimization for ICL. {} keeps model parameters fixed and instead updates model prompts in an adversarial manner. This alleviates compute and data requirements, while still allowing improvements in model performance. {} uses an adversarial objective and three main modules, implemented as LLMs, to optimize a model's prompt for a given task, as shown in Figure . The first module is a generator ($G$), which is tasked with generating realistic, task appropriate output given a task instruction and an input. The second is a discriminator ($D$) which has the goal of classifying its inputs as real or produced by $G$. Finally, there is a prompt modifier $M$ which is responsible for updating the prompts to $G$ and $D$. As in typical adversarial learning, the learning objective is set up as a minimax game between $G$ and $D$. In each round, $G$ produces an output based on an input and a prompt consisting of a task instruction and several example inputs and outputs. $D$ then classifies the pair constructed of the original input and $G$'s output as generated or real. Finally, $M$ produces a number of possible updates to $G$ and $D$'s prompts, the updates that most improve the adversarial loss from $D$'s classification are selected, and the procedure repeats. Through this iterative update procedure {} is able to improve $G$'s prompt, improving task performance. We evaluate {} on $13$ tasks with various open and closed-source LLMs, finding that {} outperforms other prompt optimization techniques by large margins across model configurations and tasks. For instance, we increase the accuracy of ChatGPT from 71.0\"
Multimodal Contextualized Semantic Parsing from Speech,2406.06438v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.06438v1_0.jpg,"We introduce Semantic Parsing in Contextual Environments (SPICE), a task designed to enhance artificial agents' contextual awareness by integrating multimodal inputs with prior contexts. SPICE goes beyond traditional semantic parsing by offering a structured, interpretable framework for dynamically updating an agent's knowledge with new information, mirroring the complexity of human communication. We develop the VG-SPICE dataset, crafted to challenge agents with visual scene graph construction from spoken conversational exchanges, highlighting speech and visual data integration. We also present the Audio-Vision Dialogue Scene Parser (AViD-SP) developed for use on VG-SPICE. These innovations aim to improve multimodal information processing and integration. Both the VG-SPICE dataset and the AViD-SP model are publicly available.",Example of VG-SPICE inputs as well as a plausible output to produce the correct next state context. New information that the agent is expected to add to the context is shown in green while already known information is noted in red. Grounding entities that have new information being added to them are noted in blue and orange. The current context is shown as a textually prompted representation of the actual knowledge graph (discussed in Section \ref{sec: Contextual State Representation}).,"Imagine you are taking a guided tour of an art museum. During the tour as you visit each piece of art, your guide describes not only the artworks themselves but also the history and unique features of the galleries and building itself. Through this dialog, you are able to construct a mental map of the museum, whose entities and their relationships with one another are grounded to their real-world counterparts in the museum. We engage in this type of iterative construction of grounded knowledge through dialog every day, such as when teaching a friend how to change the oil in their car or going over a set of X-rays with our dentist. As intelligent agents continue to become more ubiquitous and integrated into our lives, it is increasingly important to develop these same sorts of capabilities in them. Toward this goal, this work introduces Semantic Parsing in Contextual Environments (SPICE), a task designed to capture the process of iterative knowledge construction through grounded language. It emphasizes the continuous need to update contextual states based on prior knowledge and new information. SPICE requires agents to maintain their contextual state within a structured, dense information framework that is scalable and interpretable, facilitating inspection by users or integration with downstream system components. SPICE accomplishes this by formulating updates as Formal Semantic Parsing, with the formal language defining the allowable solution space of the constructed context. Because the SPICE task is designed to model real-world and embodied applications, such as teaching a mobile robot about an environment or assisting a doctor with medical image annotations, there are crucial differences between SPICE and traditional text-based semantic parsing. First, SPICE considers parsing language within a grounded, multimodal context. The language in cases like these may have ambiguities that can only be resolved by taking into account multimodal contextual information, such as from vision. Furthermore, SPICE supports linguistic input that comes in the form of both speech and text. In real-world embodied interactions, language is predominantly spoken, not written. While modern automatic speech recognition (ASR) technology is highly accurate, it is still sensitive to environmental noise and reverberation, and representing the input language as both a waveform as well as a noisy ASR transcript can improve robustness. While we do not consider it here, the SPICE framework also supports paralinguistic input such as facial expressions, eye gaze, and hand gestures. We present a novel dataset, VG-SPICE, derived from the Visual Genome , an existing dataset comprised of annotated visual scene graphs representing constituent entities and relational prepositions, enhanced with additional processing and synthetic augmentation to form a foundational representation for SPICE tasks. VG-SPICE simulates the conversational construction of visual scene graphs, wherein a knowledge graph representation of the entities and relationships contained within an image must be collected from the visual inputs and audio dialogue. This dataset, along with an initial model trained for VG-SPICE, sets the baseline for future efforts. Figure shows an example of a typical VG-SPICE sample. The figure shows how potential semantic parses can be extracted from the visual scene and spoken utterance conditioned on what information is already known about the scene. The remainder of this paper is structured as follows: It begins with a detailed analysis of the SPICE task, introduces the VG-SPICE dataset, and presents our AViD-SP model. It then delves into experimental results, showcasing the model’s ability to process and interpret context consistent with the SPICE framework. Finally we outline the implications and directions for future research. The main contributions include: {itemize} A definition of the Semantic Parsing in Contextual Environments (SPICE) task, highlighting its challenges, scope, and significance in enhancing human-AI communication. The creation of a large, machine-generated SPICE dataset, VG-SPICE, leveraging existing machine learning models and the Visual Genome dataset, to motivate SPICE research. An initial baseline model, Audio-Vision Dialogue Scene Parser (AViD-SP), for VG-SPICE that integrates Language Models with Audio/Visual feature extractors, establishing a research benchmark for SPICE. As a component of AViD-SP, we also introduce a novel pretrained encoder adaption and multimodal fusion method, the Grouped Multimodal Attention Down Sampler (GMADS) to motivate the exploration of additional multimodal adaptation methods. {itemize"
LaMP: When Large Language Models Meet Personalization,2304.11406v4,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2304.11406v4_0.pdf,"This paper highlights the importance of personalization in large language models and introduces the benchmark --- a novel benchmark for training and evaluating language models for producing personalized outputs. offers a comprehensive evaluation framework with diverse language tasks and multiple entries for each user profile. It consists of seven personalized tasks, spanning three text classification and four text generation tasks. We additionally propose two retrieval augmentation approaches that retrieve personal items from each user profile for personalizing language model outputs. To this aim, we study various retrieval models, including term matching, semantic matching, and time-aware methods. Extensive experiments on for zero-shot and fine-tuned language models demonstrate the efficacy of the proposed retrieval augmentation approach and highlight the impact of personalization in various natural language tasks.",An overview of the retrieval-augmented method for personalizing LLMs. $\phi_q$ and $\phi_p$ represent query and prompt construction functions.,"The recent development of large language models (LLMs) has revolutionized natural language processing (NLP) applications. As the use of LLMs, such as GPT-4 , in real-world applications evolves, personalization emerges as a key factor in meeting the user's expectations for tailored experiences that align with their unique needs and preferences . Personalization has been widely studied by various communities, including the information retrieval (IR) and human-computer interaction (HCI) communities, often with applications to search engines and recommender systems . Recent work has also highlighted the impact and concerns associated with personalizing LLMs and tying it to ongoing work on alignment . Despite this and the importance of personalization in many real-world problems, developing and evaluating LLMs for producing personalized responses remain relatively understudied. To bridge this gap, this paper underscores the importance of personalization in shaping the future of NLP systems and takes a first step towards developing and evaluating personalization in the context of large language models by introducing the {} benchmark{LaMP stands for {La}nguage {M}odel {P}ersonalization.} --- a comprehensive and diverse benchmarks of personalized text classification and generation tasks. While many existing well-known NLP benchmarks, such as GLUE , SuperGLUE , KILT , and GEM have led to significant progress in various NLP tasks, they have often taken the dominant NLP approach of ``one-size-fits-all'' to modeling and evaluation, and do not allow the development of models that adapt to the specific needs of end users -- limiting extensive research on personalization in NLP tasks. In contrast, offers a comprehensive evaluation framework incorporating diverse language tasks that require personalization. consists of three personalized text classification tasks: (1) Personalized Citation Identification (binary classification), (2) Personalized Movie Tagging (categorical classification with 15 tags), and (3) Personalized Product Rating (ordinal classification from 1 to 5-star rating for e-commerce products). Further, includes four text generation datasets: (4) Personalized News Headline Generation, (5) Personalized Scholarly Title Generation, (6) Personalized Email Subject Generation, and (7) Personalized Tweet Paraphrasing. For these seven tasks, we explore the two dominant settings in personalization: (a) personalization for new users with a user-based data split and (b) personalization for future interactions of existing users with a time-based data split. Therefore, provides a rich environment for developing personalized NLP models. To foster research in this area, we release the benchmark, the data construction, evaluation scripts, and a leaderboard. For personalizing the language model outputs, a straightforward solution is to incorporate the user profile into a language model prompt. However, user profiles are often large and exceed the length limitations of large language models. Even as such limitations are relaxed with evolving technology, the cost of processing large input sequences is considerable. Therefore, we propose two retrieval augmentation solutions for LLM personalization, in which for each test input, we retrieve items from the user profile to be included in the LLM prompt for personalization. The first approach uses in-prompt augmentation (IPA) for personalization, and the second approach encodes each personal item separately and integrate them later in the decoder using the fusion-in-decoder model of . We demonstrate that using this approach, the performance of language models improves on all datasets in the benchmark. Based on this retrieval augmentation solution, we evaluate different retrievers for personalized prompt construction and establish benchmark results for fine-tuned and zero-shot language models. The empirical findings of our research reveal that the process of fine-tuning a language model utilizing our personalized augmentation technique yields a noteworthy relative average enhancement of 23.5\"
LangBridge: Multilingual Reasoning Without Multilingual Supervision,2401.10695v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2401.10695v2_0.pdf,"We introduce , a {zero-shot} approach to adapt language models for multilingual reasoning tasks without multilingual supervision. operates by two models, each specialized in different aspects: (1) one specialized in understanding multiple languages (e.g., mT5 encoder) and (2) one specialized in reasoning (e.g., Orca 2). connects the two models by introducing minimal trainable parameters between them. Despite utilizing only English data for training, considerably enhances the performance of language models on low-resource languages across mathematical reasoning, code completion, logical reasoning, and commonsense reasoning. Our analysis suggests that the efficacy of stems from the language-agnostic characteristics of multilingual representations. We publicly release our code and models.{github.com/kaistAI/LangBridge}}","MGSM accuracy (\%) of MetaMath models and models aligned with mT5-XL encoder (2B) via \method ({LB}). In addition to the average (\textsc{avg}) accuracy, we also report the average accuracy of high-resource languages (\textsc{hrl}) and underrepresented languages (\textsc{url}) classified by \citet{shi2023language}.","Language models (LMs) are known to exhibit inferior performance in solving reasoning tasks such as math or coding in low-resource languages . This tendency primarily stems from the fact that LMs are predominantly trained on corpora comprised of a few high-resource languages. This results in low-resource languages being represented as long-tail knowledge. Prior works have mainly approached this problem by adapting English-centric LMs to other languages through continual training on the target language . However, scaling this approach to a large number of languages is challenging, as it requires targeted training corpora for each language. This issue is particularly pronounced for LMs such as MetaMath and Orca 2 , which have undergone continuous domain-specific adaptation from Llama 2. These specialized, domain-specific datasets are typically in English, complicating multilingual support for the underlying LM. In this paper, we introduce , a novel approach that adapts LMs to solve multilingual reasoning tasks without {explicitly} training on multilingual data. Inspired from the multimodal literature that integrates two independently pretrained modalities , we leverage the encoder from mT5 and introduce a small number of trainable parameters between the encoder and the target LM. Most importantly, our approach does not require multilingual supervision and solely relies on English data while generalizing to multiple languages during test time, resembling zero-shot cross-lingual transfer . We demonstrate the effectiveness of by applying our method to LMs specialized in diverse reasoning tasks of mathematical reasoning, code completion, logical reasoning. Our empirical results show substantially enhances the multilingual reasoning performance of LMs. For example, applied to MetaMath-13B leveraging mT5-XL encoder (2.2B) boosts the average accuracy on MGSM from 40.5\ We hypothesize that the effectiveness of is anchored in the language-agnostic characteristics of multilingual representations . By mapping these representations to the target LM's input space, we conjecture that the LM is able to grasp the semantics of these representations. Since these representations are language-neutral, understanding them allows the LM to become less dependent on the specific language of the input, thereby enabling it to tackle tasks in languages it rarely encountered during pretraining. Our empirical analysis of , using principal component analysis (PCA) and qualitative methods, supports this hypothesis"
Unlocking the Power of Large Language Models for Entity Alignment,2402.15048v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.15048v2_0.png,"Entity Alignment (EA) is vital for integrating diverse knowledge graph (KG) data, playing a crucial role in data-driven AI applications. Traditional EA methods primarily rely on comparing entity embeddings, but their effectiveness is constrained by the limited input KG data and the capabilities of the representation learning techniques. Against this backdrop, we introduce ChatEA, an innovative framework that incorporates large language models (LLMs) to improve EA. To address the constraints of limited input KG data, ChatEA introduces a KG-code translation module that translates KG structures into a format understandable by LLMs, thereby allowing LLMs to utilize their extensive background knowledge to improve EA accuracy. To overcome the over-reliance on entity embedding comparisons, ChatEA implements a two-stage EA strategy that capitalizes on LLMs’ capability for multi-step reasoning in a dialogue format, thereby enhancing accuracy while preserving efficiency. Our experimental results verify ChatEA's superior performance, highlighting LLMs' potential in facilitating EA tasks. The source code is available at {https://github.com/IDEA-FinAI/ChatEA}.",A comparison of previous EA and ChatEA.,"Entity alignment (EA) aims at aligning entities from diverse knowledge graphs (KGs). It is a pivotal step in unifying data from heterogeneous sources and plays a crucial role in data-driven AI. Current EA methods predominantly rely on measuring the similarity of entity embeddings derived from knowledge representation learning (KRL) techniques. These techniques learn from the topology and semantics of KGs to derive entity embeddings. However, these methods fail to incorporate the external knowledge of entities, which is essential for the alignment process. Moreover, the KRL-based alignment methods merely calculate the similarity between two entity embeddings in a black-box manner, which lacks a detailed and explicit reasoning process for alignment. Such limitations significantly affect the performance of EA methods, especially in aligning highly heterogeneous KG pairs where KRL-based methods struggle to capture the complex correlations among KGs. Recently, large language models (LLMs) have showcased their effectiveness across a range of natural language processing tasks, revealing a vast but under-explored potential in EA. These LLMs are trained on extensive corpora to encapsulate external knowledge, offering a rich source of contextual information for entities in KGs. Furthermore, the recent studies of adopting LLMs in knowledge extraction and reasoning also demonstrated their strong reasoning abilities on KGs. These features of LLMs offer a promising path to overcome the constraints faced by current EA methods. In this paper, we propose ChatEA, a novel framework designed to enhance KRL-based EA methods by utilizing the extensive background knowledge and reasoning abilities of LLMs. As shown in Figure~, ChatEA integrates KRL-based EA methods in the feature pre-processing phase to assist LLMs in the subsequent selection of candidate entities. To overcome the constraints of limited input KG data, ChatEA firstly features a KG-Code translation module. The module initially converts KGs into a code format, explicitly accounting for entity definition toward LLMs' comprehension of graph-structured KG data. Then it facilitates the generation of entity descriptions leveraging LLMs' background knowledge. To overcome the over-reliance on comparing entity embeddings for EA and improve transparency, ChatEA employs a two-stage EA strategy, leveraging LLMs’ multi-step reasoning in dialogue form to enhance accuracy and maintain efficiency. During the candidate collecting stage, ChatEA identifies potential entities by comparing embeddings derived from the earlier feature pre-processing phase. In the reasoning and rethinking stage, it evaluates the likelihood of alignment between entity pairs by comprehensively considering the name, structure, entity description, and temporal information, and then decides whether to broaden the search scope and continue subsequent iterations. We validated our method on two conventional EA datasets: DBP15K(EN-FR) and DBP-WIKI, along with two more challenging but practical datasets: ICEWS-WIKI and ICEWS-YAGO, characterized by their highly heterogeneous KGs and the complexity of capturing inter-KG correlations. The extensive experiments reveal ChatEA's superiority over existing state-of-the-art EA methods and underscore the potential of LLMs in enhancing EA performance. Notably, ChatEA significantly improves Hits@1 by 9\ In general, our main contributions are as follows: (1) To solve the limitations of the existing KRL-based EA methods, we explore the potential of adopting LLMs for better EA performance. (2) We design ChatEA, a novel framework that integrates LLMs with KRL-based EA methods for enhanced EA performance. (3) We conduct extensive experiments to evaluate the effectiveness of ChatEA, and discuss the value and limitations of LLMs in EA tasks"
Cognitive Visual-Language Mapper: Advancing Multimodal Comprehension with Enhanced Visual Knowledge Alignment,2402.13561v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.13561v2_0.png,"Evaluating and Rethinking the current landscape of Large Multimodal Models (LMMs), we observe that widely-used visual-language projection approaches (e.g., Q-former or MLP) focus on the alignment of image-text descriptions yet ignore the visual knowledge-dimension alignment, i.e., connecting visuals to their relevant knowledge. Visual knowledge plays a significant role in analyzing, inferring, and interpreting information from visuals, helping improve the accuracy of answers to knowledge-based visual questions. In this paper, we mainly explore improving LMMs with visual-language knowledge alignment, especially aimed at challenging knowledge-based visual question answering (VQA). To this end, we present a {Cognitive Visual-Language Mapper} (CVLM), which contains a pretrained Visual Knowledge Aligner (VKA) and a Fine-grained Knowledge Adapter (FKA) used in the multimodal instruction tuning stage. Specifically, we design the VKA based on the interaction between a small language model and a visual encoder, training it on collected image-knowledge pairs to achieve visual knowledge acquisition and projection. FKA is employed to distill the fine-grained visual knowledge of an image and inject it into Large Language Models (LLMs). We conduct extensive experiments on knowledge-based VQA benchmarks and experimental results show that CVLM significantly improves the performance of LMMs on knowledge-based VQA (average gain by 5.0\%). Ablation studies also verify the effectiveness of VKA and FKA, respectively.}","It illustrates the performance of LMMs on visual information-seeking questions. The bottom part shows the widely-used architecture of open-source LMMs, where the visual mapping network is usually pretrained on massive image-text captioning data. All LMMs including GPT-4V (Date: 2023.11.17) and Gemini-Pro make incorrect decisions.","Recent Large Multimodal Models (LMMs) such as GPT-4V, Gemini, MiniGPT-4, InstructBLIP, LLaVA, and many others, have achieved impressive performance in a variety of visual understanding and reasoning tasks, especially on Visual Question Answering (VQA). Current open-source LMMs are usually constructed by combining pertained visual encoders and Large Language Models (LLMs), as depicted in the bottom part of Figure~, where a visual mapping network (e.g., Q-former, Linear, or MLP) is employed to project visual representations into the language space of LLMs. Although such LMMs have achieved powerful visual understanding capability similar to GPT-4V and Genimi on some image understanding tasks such as Image Captioning, Visual Dialogue, Visual Entailment, and VQA, they often fall short of knowledge-based VQA, which necessitates relevant knowledge to answer these visual questions. As the cases illustrated in Figure~, these advanced LMMs (including GPT-4V and Gemini-Pro) can not give correct answers to simple visual information seeking questions: {Who is the manufacturer of this aircraft; What country does this building belong to?}. In light of this, rethinking the construction process of LMMs from the initial pretraining stages, we discover that these visual mapping networks trained on massive image-text captioning pairs simply transfer visual features to their language descriptions. They overlook the visual language knowledge-dimension alignment. i.e., connecting visuals to their relevant knowledge. As we know, visual knowledge plays a pivotal role in the way humans understand and interact with the world. It extends beyond the mere ability to recognize and interpret visuals, incorporating an understanding of spatial relationships, patterns, and symbols, which are essential components of human cognition. Additionally, previous works also demonstrated that introducing visual knowledge can improve the performance of pretrained language models on natural language understanding and open-ended text generation tasks. Inspired by these insights, we focus on enhancing LMMs through the introduction of visual-language knowledge alignment, going beyond the conventional scope of visual-language integration. To this end, we present a {Cognitive Visual-Language Mapper} ({CVLM}), which contains a pretrained Visual Knowledge Aligner (VKA) and a Fine-grained Knowledge Adapter (FKA). Specifically, we devise VKA based on a small language model that interacts with fine-grained image representation in each block. The output hidden states of VKA are fed into the LLM as the knowledge embedding tokens by a linear projection layer. To make VKA effectively capture image-relevant knowledge, we first train it on image-knowledge pairs collected from Wikipedia~{https://en.wikipedia.org/wiki/Wikipedia:Images} via the next tokens prediction. Like Q-former and prefix-tuning, we only fine-tune some learnable query tokens and the linear layer to acquire fixed-length visual knowledge representation and convert it into the representation space of LLM. In addition, considering that visual objects contain fine-grained visual knowledge, we introduce FKA to gain comprehensive visual knowledge of an image and distill valuable visual knowledge from the whole knowledge representation sequence. The output knowledge vectors of FKA are injected into each layer of LLMs to realize in-depth interactions between LLMs and detailed visual knowledge. By doing so, CVLM is capable of connecting visuals to relevant knowledge, enabling LMMs to utilize them during multimodal understanding and generation. To verify the effectiveness of CVLM, we conduct extensive experiments on image-centered, knowledge-based, and complex visual reasoning question-answering scenarios: VQAv2, OKVQA, A-OKVQA, Infoseek, TextVQA, and SeedBench. The experimental results show that CVLM significantly outperforms previous strong baselines such as LLaVA-v1.5. The ablation and case studies indicate that CVLM is capable of linking visual knowledge and improving performance on knowledge-intensive tasks via the introduced aligner and adapter. Our contributions can be summarized as follows: {itemize} We present a cognitive visual-language mapper to achieve visual-language knowledge alignment, which contains a pretrained visual knowledge aligner and a fine-grained knowledge adapter that is used to distil and inject valuable visual knowledge into LLMs. To the best of our knowledge, we are the first to explore the visual-language knowledge alignment during the pretraining and finetuning stages of LMMs, connecting visuals to their knowledge via CVLM. Experimental results indicate that CVLM significantly improves the performance of LMMs on knowledge-intensive VQA. The ablation studies also verify the effectiveness of VKA and FKA on specific knowledge-based VQA. {itemize"
FreeCtrl: Constructing Control Centers with Feedforward Layers for Learning-Free Controllable Text Generation,2406.09688v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.09688v1_0.pdf,"Controllable text generation (CTG) seeks to craft texts adhering to specific attributes, traditionally employing learning-based techniques such as training, fine-tuning, or prefix-tuning with attribute-specific datasets. These approaches, while effective, demand extensive computational and data resources. In contrast, some proposed learning-free alternatives circumvent learning but often yield inferior results, exemplifying the fundamental machine learning trade-off between computational expense and model efficacy. To overcome these limitations, we propose FreeCtrl, a learning-free approach that dynamically adjusts the weights of selected feedforward neural network (FFN) vectors to steer the outputs of large language models (LLMs). FreeCtrl hinges on the principle that the weights of different FFN vectors influence the likelihood of different tokens appearing in the output. By identifying and adaptively adjusting the weights of attribute-related FFN vectors, FreeCtrl can control the output likelihood of attribute keywords in the generated content. Extensive experiments on single- and multi-attribute control reveal that the learning-free FreeCtrl outperforms other learning-free and learning-based methods, successfully resolving the dilemma between learning costs and model performance}.","Trade-off between learning cost and performance for CTG. Learning-based methods excel in delivering superb results but demand significant training resources. Conversely, learning-free methods are more resource-efficient but tend to yield inferior performance. Numerical performance details are available in \S\ref{sec:exp}.","Controllable text generation (CTG) focuses on directing language models to produce diverse and fluent sentences that adhere to predefined single or multiple attributes such as topics and sentiment . Recent works on CTG can be roughly categorized into two groups based on their dependency on a learning process: learning-based methods and learning-free methods . Learning-based methods usually involve training , fine-tuning , or prefix-tuning language models or discriminators based on attribute-specific data. Despite their effectiveness, these approaches come with high computational costs for training and a dependency on vast, attribute-specific datasets, posing challenges for deployment in environments with limited data or computational capacity. Only a few existing methods are learning-free, avoiding the need for training. For instance, K2T employs attribute-focused keywords to influence token output probability during generation. Another method, Mix\&Match , integrates diverse black-box experts as a probabilistic energy model to steer large language model (LLM) outputs. These learning-free methods, despite bypassing the training process, tend to fall short in performance compared to advanced learning-based approaches. The analysis spotlights the classic dilemma in machine learning between the cost of learning and model performance, as shown in Figure . To overcome this obstacle, particularly in avoiding learning expenses for CTG while ensuring high performance, we propose {FreeCtrl}: Constructing Control Centers with Feedforward Layers for Learning-Free Controllable Text Generation. FreeCtrl's central idea is to manipulate FFN vectors{FFN vectors refer to the value vectors in the second weight matrix of the FFN layer. More details and definitions can be found in . } to regulate the outputs of LLMs, inspired by a recent finding that the tokens generated by LLMs can be attributed to the weights of vectors in FFN layers . Specifically, the key principle is that increasing a single FFN vector's weight alters the output distribution, raising specific tokens' output probability. This strategy enables the targeted enhancement of certain FFN vector weights to raise attribute keywords' output probability, directing LLM generation towards preferred attributes. Our study first examines the possibility of this strategy by pinpointing three key characteristics of FFN vectors: 1) {Convergence}: Increasing the weight of an FFN vector can result in a stable and convergent output distribution in LLMs, thereby elevating the probabilities of specific tokens. 2) {Diversity}: Diverse FFN vectors can increase the output probabilities for most tokens in the LLM vocabulary, covering keywords relevant to general attributes in CTG; 3) {Prompt-Invariance}: the observed effects of convergence and diversity remain consistent across different input prompts. These characteristics suggest FFN vectors can enable stable, diverse controls for LLM outputs, directing sentence generation toward desired attributes. However, we also identify a major limitation of FFN vectors: the {high-maintenance} challenge, where adjusting their weights for precise control proves difficult. Low weights lack the power to steer LLMs, while high weights compromise output diversity and fluency. To mitigate this, FreeCtrl initially sets up control centers using FFN vectors for various attributes, then navigates LLM output via a cycle of initialization, monitoring, adaptation, and filtering during the generation process. Continuous monitoring ensures that token generation is assessed at each step, allowing for adaptive weight adjustments of the control centers. Ultimately, a score-based filtering mechanism is employed to refine the outputs. Notably, this framework requires no training or attribute-specific data yet surpasses the efficacy of advanced learning-based models. Therefore, FreeCtrl addresses the cost-performance dilemma, situating it at the optimal upper-left corner in Figure , denoting learning-free but high performance. Our main contributions are summarized as follows: {enumerate} We conduct a systematic analysis of using FFN vectors for CTG in , identifying three key characteristics: convergence, diversity, and prompt-invariance, alongside a notable challenge of high maintenance. We propose FreeCtrl in , a learning-free approach that identifies and utilizes FFN vectors governing various attributes to establish control centers, thus enabling precise management of LLM outputs through initialization, monitoring, adaptation, and filtering. Comprehensive experiments in on both single and multi-attribute control demonstrate that FreeCtrl, without incurring any learning costs, outperforms existing learning-free baselines and cutting-edge learning-based models. {enumerate"
STICKERCONV: Generating Multimodal Empathetic Responses from Scratch,2402.01679v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.01679v2_0.pdf,"Stickers, while widely recognized for enhancing empathetic communication in online interactions, remain underexplored in current empathetic dialogue research, notably due to the challenge of a lack of comprehensive datasets. In this paper, we introduce the (), which uses collaborative agent interactions to realistically simulate human behavior with sticker usage, thereby enhancing multimodal empathetic communication. Building on this foundation, we develop a multimodal empathetic dialogue dataset, , comprising 12.9K dialogue sessions, 5.8K unique stickers, and 2K diverse conversational scenarios. This dataset serves as a benchmark for multimodal empathetic generation. To advance further, we propose (), a multimodal empathetic response generation framework, complemented by a comprehensive set of empathy evaluation metrics based on LLM. Our experiments demonstrate 's effectiveness in generating contextually relevant and emotionally resonant multimodal empathetic responses, contributing to the advancement of more nuanced and engaging empathetic dialogue systems}.","An example of multimodal conversation in the \dataset. Both parties can utilize the stickers to express their emotions, which enhances interactivity and expression. The assistant can empathize with the user according to the conversation (\textcolor[RGB]{0,153,0}{green} text).","Increasing research indicates that utilizing stickers in online chats can effectively alleviate stress, augment personal happiness, and notably boost empathy . Prior studies on stickers primarily concentrated on sentiment analysis and recommendation systems , overlooking their vast potential in empathetic response generation. Most empathetic response generation tasks focus solely on textual modality , yet stickers in chats convey more abundant and intuitive emotional information, enhancing the expressiveness and emotional depth of responses. Integrating stickers with textual communication and interspersing stickers within the dialogue can yield more varied and superior-quality empathetic replies. A primary challenge in integrating stickers into empathetic response generation is developing a high-quality dataset to support this innovative multimodal communication. To address this, we leverage large language model (LLMs) for dataset construction. LLMs, with their extensive world knowledge and text processing capabilities , demonstrate near-human annotation abilities . However, applying LLMs directly have limitations in empathetic tasks, excelling in responding to explicit human instructions but lacking proactivity, a critical aspect of empathy . Empathy necessitates understanding others' emotions and the ability to actively express support and understanding . To mitigate this, we introduce a multi-agent system based on LLMs, {} (). This system, through inter-agent interactions, utilizes stickers to simulate human-like dialogue scenarios. It not only generates text responses but also strategically selects suitable stickers, thereby effectively enhancing empathy. Based on , we build a multimodal empathetic dataset, {}, that comprises 12.9K dialogue sessions and 5.8K unique stickers. boasts an average of 5.22 stickers per dialogue session, mirroring the sticker usage patterns observed in human communication. Figure depicts an example of conversations in our dataset. To the best of our knowledge, this is the first multimodal empathetic dialogue dataset, with the particular utility of sticker as non-textual modal information to better facilitate empathy. Although effectively generates multimodal empathetic responses, it is limited by expensive inference costs and specific sticker databases. To further advance the research on multimodal empathetic dialogue, we develop an end-to-end multimodal empathetic response generation framework, {}, with the ability to . Beyond the general ability to generate textual empathetic responses, receives multimodal inputs and autonomously generates stickers based on the emotional and contextual aspects of the dialogue at the appropriate moment. Furthermore, to simulate human communications on social media in the real world, our model supports interleaved multiple image and text inputs. Misalignments between the modal quantities of predicted and golden responses can distort evaluation outcomes, and empathy is difficult to quantify due to its subjective nature. To address this, we propose a novel method for evaluating multimodal empathetic responses, focusing on {empathy}, {consistency}, and {ranking}. Utilizing the extensive world knowledge and anthropomorphic abilities of LLMs, this approach provides solid support for assessing multimodal empathetic replies. In conclusion, the main contributions of this work are as follows: {itemize}[nolistsep, noitemsep] We introduce an LLM-based multi-agent system, (), which integrates stickers into empathetic dialogues, ensuring contextual consistency, variety, and empathy aligned with human interactions. Using , we create a multimodal empathetic dialogue dataset, . We design (), a multimodal empathetic response generation framework that intuitively incorporates stickers based on the emotional and contextual dynamics of the dialogue. adeptly processes multimodal inputs, generating empathetic textual responses and using stickers appropriately to enhance these responses. We propose a method for assessing multimodal empathetic responses. It leverages LLM to evaluate the quality of these responses, with a specific focus on empathy, consistency, and ranking. {itemize"
EXAMS-V: A Multi-Discipline Multilingual Multimodal Exam Benchmark for Evaluating Vision Language Models,2403.10378v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2403.10378v1_0.pdf,"We introduce , a new challenging multi-discipline multimodal multilingual exam benchmark for evaluating vision language models. It consists of multiple-choice questions across school disciplines covering natural science, social science, and other miscellaneous studies, e.g.,~religion, fine arts, business, etc. includes a variety of multimodal features such as text, images, tables, figures, diagrams, maps, scientific symbols, and equations. The questions come in languages from language families. Unlike existing benchmarks, is uniquely curated by gathering school exam questions from various countries, with a variety of education systems. This distinctive approach calls for intricate reasoning across diverse languages and relies on region-specific knowledge. Solving the problems in the dataset requires advanced perception and joint reasoning over the text and the visual content of the image. Our evaluation results demonstrate that this is a challenging dataset, which is difficult even for advanced vision--text models such as GPT-4V and Gemini; this underscores the inherent complexity of the dataset and its significance as a future benchmark.}",Data distribution for our \dataset dataset: languages and subjects.,"Large Language Models (LLMs) have recently demonstrated impressive skills in understanding and generating natural languages . This progress has paved the way for significant advancements in LLM-based vision models . Notable developments like GPT-4V and Gemini represent a new era in image understanding, exhibiting remarkable proficiency in interpreting and analyzing visual data alongside textual information. However, as Vision Language Models (VLMs) grow more sophisticated, existing benchmarks are becoming outdated, and unable to accurately assess these models' performance. For LLM evaluation, standardized testing akin to school examinations has proven to be an effective measure of a model's capabilities. A typical benchmark MMLU , which contains 57 subjects across science, engineering, and humanities, has become a de facto benchmark for LLM evaluation. Several other school exam datasets have also set the standard in evaluating LLMs in different languages . In terms of VLM, a comparable benchmarking framework is conspicuously absent. Existing benchmarks are (1) primarily monolingual, focused on English; (2) mostly not from school exams, leading to differences in methods of examining humans; (3) tend to keep images and text separate, which fails to challenge models with more complex tasks involving integrated visual elements like tables, symbols, and scientific notations. We introduce , which addresses all these issues. First, this dataset represents a significant leap forward, treating visual and text content as a cohesive unit. This forces models to engage in more sophisticated processing, including distinguishing, preprocessing, and logical reasoning over combined textual and visual information. Additionally, has a multilingual reach, covering language families, further enhancing its complexity and applicability. The key contributions of our paper include: {itemize} We introduce a novel dimension to benchmarking vision language models, requiring them to reason over a unified snapshot that includes text, images, tables, graphs, and more. For this, we propose a new multimodal multilingual dataset, , comprising questions, spanning languages and subjects. We evaluate the performance of state-of-the-art large language models and vision language models on our proposed dataset. {itemize} Through , we aim to set a new standard in evaluating VLMs, providing a more realistic and challenging benchmark that mirrors the complexity and diversity of real-world information processing"
Text Embedding Inversion Security for Multilingual Language Models,2401.12192v4,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2401.12192v4_0.png,"Textual data is often represented as real-numbered embeddings in NLP, particularly with the popularity of large language models (LLMs) and Embeddings as a Service (EaaS). However, storing sensitive information as embeddings can be susceptible to security breaches, as research shows that text can be reconstructed from embeddings, even without knowledge of the underlying model. While defence mechanisms have been explored, these are exclusively focused on English, leaving other languages potentially exposed to attacks. This work explores LLM security through {multilingual} embedding inversion. We define the problem of black-box multilingual and cross-lingual inversion attacks, and explore their potential implications. Our findings suggest that multilingual LLMs may be more vulnerable to inversion attacks, in part because English-based defences may be ineffective. To alleviate this, we propose a simple masking defense effective for both monolingual and multilingual models. This study is the first to investigate multilingual inversion attacks, shedding light on the differences in attacks and defenses across monolingual and multilingual settings.","Schematic overview of a text embedding inversion attack. A user accesses an EaaS provider, while an attacker is eavesdropping. Although the attacker has no direct access to the embedding model, they can reliably decode the information stored in the embeddings.","Industrial applications of natural language processing (NLP) typically utilize language models (LMs) and often rely on vector databases via frameworks such as Embeddings as a Service (EaaS). In this context, sentence embeddings are stored in a remote database, as opposed to raw text, allowing end-users to efficiently search across condensed representations. As embeddings are not human-readable, security of the encoded information may be naively assumed, however recent works have demonstrated that embeddings are no safer than raw text; they are susceptible to {inversion attacks}, whereby a malicious actor can train models to decode embeddings, thus exposing private information . Concretely, after gaining access to embeddings and the black-box embedder via the EaaS API, the malicious actor can train an external model, which approximates the inversion function that reconstructs the text from the embeddings. As such, there is a substantial threat to privacy if malicious actors are able to eavesdrop on communication channels between EaaS providers and customers, as illustrated in Figure~. Previous work has shown that an exact match for data recreation can be obtained in specific settings, albeit with the limitation of assuming monolingual English models and embeddings . However, in real-world scenarios, eavesdroppers may not know the source language of the encoded text, as EaaS providers can have international clientele. Thus to assess the current level of risk posed to multilingual LMs, we introduce {multilingual} inversion attacks. As the first ever study in this direction, we focus specifically on exact text reconstruction, assuming that the language of a target embedding is unknown. Leveraging a state-of-the-art multilingual black-box encoder, we find that the trained model can reconstruct texts in certain languages more effectively than monolingual counterparts. Additionally, we also introduce {cross-lingual} inversion attacks, to ascertain whether inversion attacks can be successful when the target language is unknown by the attacker. We thus attempt cross-lingual text reconstruction (i.e., reconstructing German text with a model not trained on German reconstruction), introducing an {Ad hoc Translation} method to overcome the evaluation limitation of current string-matching metrics in this cross-lingual scenario. Finally, we assess the efficacy of an existing defense method by , ultimately finding that defenses intended for monolingual models fall short in protecting multilingual models. To this end, we introduce simple {masking defense}, which proves effective for both monolingual and multilingual models, and which also does not require additional model training. All our trained inversion models~{{https://huggingface.co/yiyic/}} and code {{https://github.com/siebeniris/MultiVec2Text/}} are open source, encouraging the research community to engage in development of defenses for vulnerable multilingual models"
Synthesizing Text-to-SQL Data from Weak and Strong LLMs,2408.03256v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2408.03256v1_0.pdf,"The capability gap between open-source and closed-source large language models (LLMs) remains challenging in text-to-SQL tasks. In this paper, we introduce a synthetic data approach that amalgamates strong data generated by larger, more potent models (strong models) with weak data produced by smaller, less well-aligned models (weak models). Our approach contributes to the improvement of domain generalization in text-to-SQL models and investigates the potential of weak data supervision through preference learning. Moreover, we utilize the synthetic data approach for instruction tuning on open-source LLMs, yielding , a specialized text-to-SQL model. The effectiveness of is substantiated by achieving state-of-the-art results on the SPIDER and BIRD benchmarks, thereby mitigating the performance disparity between open-source models and the methods derived from closed-source models.","Overview of \sense: Integrating human-annotated data with synthetic data from strong models for domain diversity, and weak models for preference learning, aligning with executors for enhanced text-to-SQL performance.","The ability to convert a natural language question into a structured query language (SQL), i.e., text-to-SQL, can assist non-experts in interacting with databases using natural language, democratizing data access and analysis. In recent studies, notable accomplishments have been observed in powerful closed-source LLMs, exemplified by GPT-4, employing a range of prompting methods. However, the adoption of closed-source LLMs introduces concerns pertaining to issues of openness, privacy, and substantial costs. Recently, the proliferation of numerous open-source LLMs has attracted considerable attention as these models demonstrate comparable capabilities to their closed-source counterparts across a broad spectrum of natural language processing (NLP) tasks. This motivates us to undertake a thorough evaluation of prominent open-source LLMs in the text-to-SQL task, aiming to gauge their viability as alternatives. However, following an assessment utilizing a standardized prompt, we observed that open-source models still exhibit a substantial performance gap compared to closed-source models. In particular, the popular open-source model CodeLLaMA-13B-Instruct demonstrates a 30\ Developing specialized text-to-SQL models built upon open-source LLMs that attains performance levels comparable to close-source models holds critical importance in contexts marked by sensitivity to policy, privacy, and resource limitations. To this end, we focus on supervised fine-tuning (SFT) to enhance the text-to-SQL capabilities of open-source base models. However, enhancing the text-to-SQL ability of open-source models through SFT remains an open challenge. A significant barrier to this progress is the high cost of achieving text-to-SQL data, which relies on manual expert annotation. The generation of high-quality text-to-SQL fine-tuning data should consider two primary perspectives. First, the inclusion of diverse data aims to facilitate cross-domain generalization, allowing the model to be successfully applied to new domains or databases. Second, alignment with executors becomes crucial to better enable the model to learn SQL from execution feedback, particularly from errors, mirroring how humans often learn from their mistakes. In response to the data scarcity challenge, numerous endeavors have sought to generate synthetic data utilizing larger and more powerful LLMs (strong models), such as GPT-4, creating what is denoted as {strong data}. Although strong data inherently contributes to the enhancement of data diversity, a critical factor for the domain generalization of models, its application in the text-to-SQL task remains unexplored. Additionally, the generation of valuable erroneous text-to-SQL data poses a separate challenge. Strong models often exhibit significant efforts toward correct alignment and safety, making it difficult to elicit erroneous samples. Consequently, we redirect our focus towards smaller, less well-aligned open-source models (weak models). Weak models produce valuable weak SQL samples, which can subsequently be validated and subjected to error induction with the assistance of executors. Preference learning is employed to instruct language models to learn from both correct and incorrect samples, constituting what we refer to as {weak data}. To verify the effectiveness of our , we conduct SFT on a popular open-source base model, i.e., CodeLLaMA, and obtain a new specialized model named . We comprehensively evaluate 's performance on the text-to-SQL tasks, achieving state-of-the-art (SOTA) results on both the standard benchmark Spider and the challenging benchmark BIRD, narrowing the gap between open-source and closed-source models. Additionally, we evaluate on three robustness datasets: SYN, REALISTIC, and DK, demonstrating its advantages in robustness. Moreover, we conduct an in-depth experimental analysis offers insights into the influence of synthetic data on model performance. In summary, our contributions are threefold: {itemize} We first evaluate both open-source and closed-source LLMs on text-to-SQL benchmarks using a standardized prompt. We observe that the text-to-SQL capabilities of open-source models were significantly inferior. It motivated us to train our specialized model through SFT on an open-source LLM. We propose a synthetic data approach that uses strong models to generate strong data to enhance data diversity and employs weak models to generate weak data combined with an executor to learn from feedback. Extensive experiments shows the effectiveness of , achieving SOTA performance, even competing with methods based on GPT-4. We believe that making these data and models publicly available can contribute to the advancement of the text-to-SQL community. {itemize"
Context-aware Difference Distilling for Multi-change Captioning,2405.20810v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2405.20810v2_0.pdf,"Multi-change captioning aims to describe complex and coupled changes within an image pair in natural language. Compared with single-change captioning, this task requires the model to have higher-level cognition ability to reason an arbitrary number of changes. In this paper, we propose a novel context-aware difference distilling (CARD) network to capture all genuine changes for yielding sentences. Given an image pair, CARD first decouples context features that aggregate all similar/dissimilar semantics, termed common/difference context features. Then, the consistency and independence constraints are designed to guarantee the alignment/discrepancy of common/difference context features. Further, the common context features guide the model to mine locally unchanged features, which are subtracted from the pair to distill locally difference features. Next, the difference context features augment the locally difference features to ensure that all changes are distilled. In this way, we obtain an omni-representation of all changes, which is translated into linguistic sentences by a transformer decoder. Extensive experiments on three public datasets show CARD performs favourably against state-of-the-art methods. The code is available at .",Three examples about multi-change captioning. (a) includes certain object changes; (b) consists of object and background changes; (c) shows both object changes and irrelevant viewpoint change. These changes are shown in colored boxes.,"Change captioning aims to describe differences between a pair of similar images, which enables many important applications, such as automatic report generation about change conditions of surveillance areas and pathological changes between medical images . On the other hand, this task is more challenging than image captioning . This is because machines need to understand the contents of two images simultaneously, and further reason and caption all genuine changes between them, while resisting irrelevant viewpoint/illumination changes. Recently, single-change captioning has made remarkable progress . In a dynamic environment, however, the changes are usually the {many-in-one}, where multiple changes exist in an image pair. For instance, there are multiple object/background changes (Figure (a) (b)). In other cases, object and viewpoint changes simultaneously appear (Figure (c)). In above cases, unchanged objects commonly mingle with changed ones and even appear position misalignment under viewpoint changes. Such distractors pose a great challenge to identify and caption the genuine changes. There are a few attempts to address multi-change captioning. The pioneer work computed pixel differences of two images, which is sensitive to noise. Latest works tried to capture differences at representation space: some of them computed difference features by subtraction, while the others built the correlations between the patches of two images to model the change features for caption generation. Despite progress, the above endeavors in multi-change captioning have several limitations. (1) Direct subtraction between two images generalizes poorly to unaligned image pairs under viewpoint changes (Figure (c)). (2) Directly correlating two images fails to sufficiently mine locally unchanged features as multiple objects change, because such features might mingle with the features of changed objects. (3) These methods focus on modeling locally difference features, which are useful to catch conspicuous changes. Nevertheless, certain local changes with weak features might be overlooked, {e.g.,} the car occluded by its shadows in Figure (a). These limitations would result in obtaining unreliable difference features for the language decoder. We notice that the above methods capture differences between two images only based on local features, while neglecting the use of more comprehensive features. We argue that, to learn locally unchanged/changed features of two images, the model should first encapsulate their context features of commonality and difference. Such context features aggregate all similar/dissimilar semantics, termed {common/difference context features}. The former can help correlate and mine locally common features for deducing locally difference features, while the latter can augment the locally difference features to ensure all changes are distilled. In this paper, we propose a {C}ontext-{A}ware Diffe{R}ence {D}istilling (CARD) network to learn the robust difference features under multi-change scenes. Specifically, given the featuers of two images, we first build intra-image interaction to help the model understand each image content of the pair. Then, we use CARD to decouple the common/difference context features from the image pair. Herein, the common context features of two images summarize joint semantics in between; the difference context feature in each image provides an independent space to preserve its all changed semantics. Besides, the consistency and independence constraints are designed to enforce the alignment and discrepancy of common and difference context features, respectively. Next, guided by the common context features, CARD models inter-image interaction to mine locally common features, which are removed from the pair to distill locally difference features. Subsequently, CARD augments the locally difference features via the difference context features, so as to construct an omni-representation of all changes, for generating descriptions by a transformer decoder. {Our key contributions are}: {(1)} We propose CARD to first decouple common and difference context features, and then use them to facilitate modeling an omni-representation of all changes for multi-change captioning. {(2)} The consistency and independence constraints are customized to guarantee the alignment and discrepancy of decoupled common and difference context features. {(3)} Extensive experiments show our method achieves the state-of-the-art results on three public datasets"
Dataflow-Guided Retrieval Augmentation for Repository-Level Code Completion,2405.19782v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2405.19782v1_0.pdf,"Recent years have witnessed the deployment of code language models (LMs) in various code intelligence tasks such as code completion. Yet, it is challenging for pre-trained LMs to generate correct completions in private repositories. Previous studies retrieve cross-file context based on import relations or text similarity, which is insufficiently relevant to completion targets. In this paper, we propose a dataflow-guided retrieval augmentation approach, called , for repository-level code completion. parses a private repository into code entities and establishes their relations through an extended dataflow analysis, forming a repo-specific context graph. Whenever triggering code completion, precisely retrieves relevant background knowledge from the repo-specific context graph and generates well-formed prompts to query code LMs. Furthermore, we construct a large Python dataset, , with more diverse completion targets. Our experiments demonstrate the superior accuracy and applicable efficiency of , improving code exact match by 3.43\% and identifier F1-score by 3.27\% on average compared to the state-of-the-art approach.","A real-world example of repository-level code completion. The code LM CodeGen25-7B-mono fails to complete the last code line correctly when entering only the unfinished code (Zero-Shot). The model needs background knowledge relevant to \texttt{newSignal}, and the retrieval of this knowledge can be guided by dataflow.","Pre-trained language models (LMs) of code have shown remarkable performance in improving programming productivity . Instead of using a single code file, well-designed programs emphasize separating complicated functionality into independent modules . While facilitating collaborative development and software maintenance, it introduces the real-world problem of {repository-level code completion}: given an unfinished code file in a private repository, complete the following pieces of code at the cursor position. Despite pre-training on large-scale corpora, code LMs are still blind to unique naming conventions and programming styles in private repositories . Previous works finetune LMs to leverage cross-file context , which requires additional training data and is difficult to work with larger LMs. Recently, retrieval-augmented generation (RAG) is widely used to aid pre-trained LMs with external knowledge and maintain their parameters intact . For repository-level code completion, the retrieval database is the current private repository. The state-of-the-art approach, RepoCoder , iteratively incorporates a text similarity-based retriever and a code LM. As shown in Figure~, the CodeGen25 Python model with 7 billion parameters assigns a value to the attribute {channel} of the object {newSignal}, which seems rational in the unfinished code but is actually outside the list of valid attributes. Due to the lack of similar code snippets in the repository, the text similarity-based approach also fails to complete the correct code line. From a programmer’s perspective, one would explore the data origin of the variable {newSignal} in the last line. It comes from the call {signal.getSignalByName}, where the variable type of {signal} is {RecordSignal} imported from the module {RecordSignal}. After providing relevant background knowledge in the private repository, the model would know that the variable type of {newSignal} is the class {Signal} and thus call the correct function. Inspired by this programming behavior in private repositories, we propose , a novel dataflow-guided retrieval augmentation approach for repository-level code completion, which steers code LMs with relevant background knowledge rather than similar code snippets. Dataflow analysis is a static program analysis reacting to data dependency relations between variables in a program. In this work, we extend traditional dataflow analysis by setting type-sensitive dependency relations. We employ the standard RAG framework: (1) {Indexing}, which parses a private repository into code entities and establishes their relations through dataflow analysis, forming a repo-specific context graph for retrieval. (2) {Retrieval}, which uses dataflow analysis to obtain fine-grained import information in the unfinished code and retrieves relevant code entities from the pre-built context graph. (3) {Generation}, which organizes the relevant background knowledge as natural code and concatenates it with the unfinished code to generate well-formed prompts for querying code LMs. In addition to the existing dataset for repository-level code completion, we construct a new dataset, , with diverse completion targets collected from Python Package Index (PyPI).{{https://pypi.org/}} We conduct experiments with popular LMs including adapted code LMs , specialized code LMs , and GPT models . Our experiments demonstrate that achieves generally superior accuracy across all settings. Furthermore, is plug-and-play for various code LMs and efficient to real-time code completion. Our main contributions are outlined as follows: {compactitem} We design an extended dataflow analysis by setting type-sensitive data dependency relations, which supports more precise retrieval. We propose , a dataflow-guided retrieval augmentation approach for repository-level code completion. builds a repo-specific context graph for retrieval and generates well-formed prompts with relevant background knowledge in real-time completion. We construct a Python dataset with diverse completion targets. The experimental results show that improves code exact match by 3.43\ Our source code and data are available at {https://github.com/nju-websoft/DraCo}. {compactitem"
"To be Continuous, or to be Discrete, Those are Bits of Questions",2406.07812v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.07812v1_0.pdf,"Recently, binary representation has been proposed as a novel representation that lies between continuous and discrete representations. It exhibits considerable information-preserving capability when being used to replace continuous input vectors. In this paper, we investigate the feasibility of further introducing it to the output side, aiming to allow models to output binary labels instead. To preserve the structural information on the output side along with label information, we extend the previous contrastive hashing method as structured contrastive hashing. More specifically, we upgrade CKY from label-level to bit-level, define a new similarity function with span marginal probabilities, and introduce a novel contrastive loss function with a carefully designed instance selection strategy. Our model} achieves competitive performance on various structured prediction tasks, and demonstrates that binary representation can be considered a novel representation that further bridges the gap between the continuous nature of deep learning and the discrete intrinsic property of natural languages.","The model architecture. The attention hash layer produces span scores (pink circles), we only use the upper triangular part of these scores and feed them into the bit-level CKY to obtain the marginal probabilities of all valid spans (purple circles). During training, we only select the spans on the target trees for structured contrastive hashing and leave the other spans unused (transparent purple circles). During inference, as shown at the bottom, our model parses sentences by returning trees with label codes (hexadecimal numbers), which are then translated back to the original labels.","Bridging the gap between the continuous nature of deep learning and the discrete intrinsic property of natural languages has been one of the most fundamental and essential questions since the very beginning. Continuous representation makes the training of neural networks effective and efficient. Nowadays, representing discrete natural languages in continuous format is the first and foremost step to leveraging the capabilities of deep learning. One could even argue that the exhilarating advancements in natural language processing in the past decade can largely be attributed to the word embedding technique, as it is the first successful attempt. Word embedding technique replaces the vocabulary-sized one-hot word representations with compact continuous vectors. Since then, input and output layers incorporating embedding matrices have become standard components in neural models. Discrete tokens are mapped into continuous vectors by looking up the corresponding index in it, and continuous vectors are mapped back to discrete tokens by searching the most similar one from it. However, the essence of this operation is still one-hot encoding, even the following subword tokenization techniques attempt to mitigate this issue by decomposing words into subword units, these approaches still require the building vocabularies and embedding matrices that consists of tens of thousands of tokens. In the era of large language models , these embedding matrices typically account for a considerable number of parameters, especially in cross-lingual models. Besides, parameter updates also solely rely on the sparse gradients backpropagated to the limited tokens present in sentences. Moreover, imposing structural constraints on continuous representations to model relations among tokens is considered difficult, whereas it is easy and common in discrete representations. Therefore, further bridging the gap has become increasingly important nowadays. Recently, introduced a novel binary representation that lies between continuous and discrete representations. They proposed a contrastive hashing method to compress continuous hidden states into binary codes. These codes contain all the necessary task-relevant information, and using them as the only inputs can reproduce the performance of the original models. Unlike associating each token with only a single vector, their method allocates multiple bits to each token, and the token representation can be constructed by concatenating these bit vectors. In other words, their binary representation breaks tokens down into combinations of semantic subspaces. As a result, replacing the token embedding matrix in the input layer with a tiny bit embedding matrix without sacrificing performance becomes possible. In this paper, we explore the possibility of further introducing this representation to output layers. In the input layer, structural information can only be implicitly obtained by introducing the task loss as an auxiliary. However, the output layers often involve complex intra-label constraints, especially for structured prediction tasks, structural information can and should be explicitly preserved along with plain label information. Therefore, we attempt to endow models with this capability by extending previous contrastive hashing to structured contrastive hashing. We begin by upgrading the CKY, which parses sentences, returns spans with discrete labels, to support binary format labels (). Subsequently, we define a new similarity function by using span marginal probabilities obtained from this bit-level CKY () to jointly learn label and structural information. Furthermore, we conduct a detailed analysis of several widely used contrastive learning losses, identifying the geometric center issue, and introduce a novel contrastive learning loss to remedy it () through carefully selecting instances. By doing so, we show that it is feasible to introduce binary representation to output layers and have them output binary labels on trees. Moreover, since our model is based on contrastive learning, it also benefits from its remarkable representation learning capability, resulting in better performance than existing models. We conduct experiments on constituency parsing and nested named entity recognition. Experimental results () demonstrate that our models achieve competitive performance with only around 12 and 8 bits, respectively"
PokeMQA: Programmable knowledge editing for Multi-hop Question Answering,2312.15194v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2312.15194v2_0.pdf,"Multi-hop question answering (MQA) is one of the challenging tasks to evaluate machine's comprehension and reasoning abilities, where large language models (LLMs) have widely achieved the human-comparable performance. Due to the dynamics of knowledge facts in real world, knowledge editing has been explored to update model with the up-to-date facts while avoiding expensive re-training or fine-tuning. Starting from the edited fact, the updated model needs to provide cascading changes in the chain of MQA. The previous art simply adopts a mix-up prompt to instruct LLMs conducting multiple reasoning tasks sequentially, including question decomposition, answer generation, and conflict checking via comparing with edited facts. However, the coupling of these functionally-diverse reasoning tasks inhibits LLMs' advantages in comprehending and answering questions while disturbing them with the unskilled task of conflict checking. We thus propose a framework, rgrammable nowledge diting for ulti-hop uestion nswering (PokeMQA), to decouple the jobs. Specifically, we prompt LLMs to decompose knowledge-augmented multi-hop question, while interacting with a detached trainable scope detector to modulate LLMs behavior depending on external conflict signal. The experiments on three LLM backbones and two benchmark datasets validate our superiority in knowledge editing of MQA, outperforming all competitors by a large margin in almost all settings and consistently producing reliable reasoning process. Our code is available at {https://github.com/Hengrui-Gu/PokeMQA}.} Corresponding author}","An example of multi-hop question answering under knowledge editing, which consists of relevant knowledge facts and three specific reasoning paths solving the two-hop question. For the unreliable reasoning, it uses a outdated and a non-existent fact and end up with the right answer {Europe}.","Multi-hop question answering (MQA) requires a sequence of interacted knowledge facts to reach the final answer. For instance, considering the two-hop question in Figure , it is necessary to deduce the intermediate answer {Inter Miami} through the fact ""Messi plays for Inter Miami"", and then deduce the final answer {NA} through another fact ""Inter Miami is located in North America""). MQA poses a great challenge to reasoning abilities of question answering systems (, , ). Thanks to the natural language comprehending and reasoning brought by large-scale pre-training, large language models (LLMs) have proven its indispensable utility in MQA tasks (, , , ). However, the knowledge within LLMs may be factually wrong or become invalid over time. To ensure the correctness of LLMs without necessitating expensive retraining (), technique of knowledge editing has been carried out to provide efficient and targeted updates on model behaviors (, , ). There are two popular approaches: parameter-modification based editing and memory-based editing. The former one modifies the internal model weights according to edited facts through meta-learning, fine-tuning, or knowledge locating (, , ). The latter approach leverages an external memory to explicitly store the edited facts (or termed as edits) and reason over them, while leaving LLMs parameters unchanged . Memory-based model editing is generally adopted due to its simpleness and agnostic to backbone LLMs. In the context of MQA, MeLLo is first proposed by designing a multipurpose prompt to instruct LLMs conducting the reasoning tasks of question decomposition and knowledge editing sequentially. In particular, after decomposing the multi-hop questions, LLMs generate a tentative answer for each subquestion and then detect whether there exists factual conflict between tentative answer and edited facts in memory (e.g., statements of ""the current British Prime Minister is Rishi Sunak"" and ""the current Prime Minister of the UK is Liz Truss"" are factually incompatible with each other). By repeatedly prompting LLMs, MeLLo reaches the answer of multi-hop question. However, the coupling of question decomposition and knowledge editing imposes considerable demands on LLMs to precisely perform reasoning as demonstrations in context. {{First}}, the knowledge editing requires LLMs to fully understand the semantics of two candidate facts and then make conflict detection based on the factual compatibility between them. In the few-shot prompting, LLMs are prone to underfit this editing logic due to inadequate supervision signals, especially when embedded in a more complex task , i.e. question decomposition. {{Second}}, within a unified prompt, the incorporation of knowledge editing instruction introduces noise to question decomposition in the similar way. Such superposed noise prevents LLMs from fully focusing on parsing the syntactic structure of multi-hop questions to precisely identify the subquestions. Thus, we propose {P}r{o}grammable {k}nowledge {e}diting for {M}ulti-hop {Q}uestion {A}nswering (PokeMQA), where we decouple the two essential tasks, i.e. question decomposition and knowledge editing, to alleviate burdens on LLMs while introducing auxiliary knowledge prompt to assist question decomposition. Specifically, we offload the conflict detection in knowledge editing with a programmable scope detector, which is used to detect whether a subquestion lies within the scope affected by any edited facts in semantic space ({{Challenge \#1}}). A two-stage scope detector is designed: In pre-detection stage, we efficiently filter out a substantial number of irrelevant edits; In conflict-disambiguation stage, we perform precise retrieval on the remaining few candidate edits. Our two-stage framework provides both computational efficiency and expressiveness given the high volume of edited facts in real scenarios. The retrieved edits are used to calibrate LLMs behavior. Moreover, we propose a knowledge prompt to augment parse analysis in the process of question decomposition ({{Challenge \#2}}). The knowledge prompt recognizes key entity from input question and retrieves its external information from a knowledge source to trig the correct decomposition. Additionally, we observe that the multi-hop question answering process may use the outdated or non-existent facts, but occasionally ends up with the right answer. We refer to this situation as unreliable reasoning (as shown in Figure ). In order to faithfully evaluate models' reasoning ability, we propose a new metric called hop-wise answering accuracy (Hop-Acc), measuring the extent how LLMs follow demonstrations, conduct question decomposition step by step, and generate desired answer to each step towards solving the multi-hop question"
MemeGuard: An LLM and VLM-based Framework for Advancing Content Moderation via Meme Intervention,2406.05344v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.05344v1_0.pdf,"In the digital world, memes present a unique challenge for content moderation due to their potential to spread harmful content. Although detection methods have improved, proactive solutions such as intervention are still limited, with current research focusing mostly on text-based content, neglecting the widespread influence of multimodal content like memes. Addressing this gap, we present {MemeGuard}, a comprehensive framework leveraging Large Language Models (LLMs) and Visual Language Models (VLMs) for meme intervention. {MemeGuard} harnesses a specially fine-tuned VLM, {VLMeme}, for meme interpretation, and a multimodal knowledge selection and ranking mechanism ({MKS}) for distilling relevant knowledge. This knowledge is then employed by a general-purpose LLM to generate contextually appropriate interventions. Another key contribution of this work is the {{I}ntervening} {{C}yberbullying in {M}ultimodal {M}emes (ICMM)} dataset, a high-quality, labeled dataset featuring toxic memes and their corresponding human-annotated interventions. We leverage {ICMM} to test {MemeGuard}, demonstrating its proficiency in generating relevant and effective responses to toxic memes.}\\ { {Disclaimer}: {This paper contains harmful content that may be disturbing to some readers.}}",An instance of the meme intervention task.,"In today's digital world, memes serve as a universal language for expression and engagement. However, as they become a powerful tool for rapid information dissemination, they are increasingly weaponized for cyberbullying and spreading toxic content, posing a challenge to existing content moderation systems. These systems struggle to decipher memes' nuanced meanings, typically reacting rather than proactively mitigating the harm of offensive content. [-10]{Intervention represents a proactive approach to content moderation, going beyond simple detection to take preventive action against offensive content. Interventions aim to mitigate the harmful effects of toxic content and foster a more positive and respectful online discourse. However, existing intervention research has primarily been limited to text-based content such as hate speech , and misinformation . The excessive focus on text-based content overlooks the prevalence of multimodal content, which are major contributors to the content ecosystem in social media platforms. This exacerbates the potential for multimodal toxicity, enabling misuse of such mediums. {A representative example of intervening in case of a cyberbullying meme is displayed in Figure .}} Large Language Models (LLMs) , and Visual Language Models (VLMs) have shown remarkable capability in understanding and generating human-like text and multimedia content. This has led to their application in various tasks within the domain of content moderation, but predominantly for detection purposes. Their ability to understand nuanced language and visual cues has been leveraged to detect toxic or harmful content, both in text and multimodal formats . Some attempts have also been made to use these models for intervention generation tasks, but these efforts have been largely restricted to text-based content . In this landscape, the zero-shot learning of these models presents a unique advantage, particularly for tasks like meme intervention, which are characterized by data scarcity. While these models hold considerable promise for content moderation, they are not without their limitations. First and foremost, vanilla LLMs and VLMs lack grounding in a knowledge base specific to the task at hand. This can lead to the generation of generic interventions that may fail to adequately address the bias, stereotype, assertions, and toxicity present in the meme content. In terms of visual content, while VLMs have performed well on a variety of traditional visual-linguistic tasks, they often struggle when it comes to memes. The primary reason behind this is the unique nature of memes, which are highly contextual and often rely on a shared understanding of internet subcultures for their interpretation. This makes the accurate assimilation and analysis of memes a challenging task. Finally, even when these models are grounded in a knowledge base for the task of meme intervention, there is a need to filter irrelevant and noisy information. Without appropriate filtering, these models might end up incorporating irrelevant or misleading information into their interventions. In order to address these limitations inherent in LLMs and VLMs for meme intervention, we developed a comprehensive framework called {{MemeGuard}}. The development of {MemeGuard} was a multi-stage process designed to create a tool capable of understanding and effectively intervening in the spread of toxic memes. In the first stage, we developed a meme-aligned VLM ({{VLMeme}}), specifically fine-tuned to understand and interpret memes in all their complexity. This allowed our model to delve deeper into the content of memes. Next, we utilized this meme-aligned VLM to identify various facets of the meme, such as the underlying toxicity, bias, stereotypes, and claims being made, which provides valuable insights into the meme's potential harm. To address the challenge of irrelevant knowledge, we then proposed a Multimodal Knowledge Selection mechanism ({{MKS}}). This mechanism retained only the most relevant knowledge for the intervention generation process. In the subsequent stage, we utilized a general-purpose LLM grounded on this refined knowledge to generate appropriate interventions. This model took the insights provided by the VLM and the ranked knowledge to create contextually relevant and effective responses to toxic memes. Finally, to test the efficacy of our framework, we developed a high-quality labeled dataset featuring a variety of toxic and cyberbully memes with their corresponding human annotated intervention, {{I}ntervening} {{C}yberbullying in {M}ultimodal {M}emes (ICMM)} dataset. To summarize, we make the following main contributions:\\ {A novel task} of Meme Intervention to combat the toxicity of cyberbullying memes.\\ {A novel dataset}, {ICMM}, to advance the research in this area.\\ {A novel framework}, {MemeGuard}, that utilizes a meme-aligned VLM ({VLMeme}) to generate contextual information about the meme that is then used to generate the final intervention"
Acquiring Clean Language Models from Backdoor Poisoned Datasets by Downscaling Frequency Space,2402.12026v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.12026v3_0.pdf;/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.12026v3_1.pdf,"Despite the notable success of language models (LMs) in various natural language processing (NLP) tasks, the reliability of LMs is susceptible to backdoor attacks. Prior research attempts to mitigate backdoor learning while training the LMs on the poisoned dataset, yet struggles against complex backdoor attacks in real-world scenarios. In this paper, we investigate the learning mechanisms of backdoor LMs in the frequency space by Fourier analysis. Our findings indicate that the backdoor mapping presented on the poisoned datasets exhibits a more discernible inclination towards lower frequency compared to clean mapping, resulting in the faster convergence of backdoor mapping. To alleviate this dilemma, we propose {Mu}lti-{Sc}a{le} {Lo}w-{R}ank {A}daptation (MuScleLoRA), which deploys multiple radial scalings in the frequency space with low-rank adaptation to the target model and further aligns the gradients when updating parameters. Through downscaling in the frequency space, MuScleLoRA encourages the model to prioritize the learning of relatively high-frequency clean mapping, consequently mitigating backdoor learning. Experimental results demonstrate that MuScleLoRA outperforms baselines significantly. Notably, MuScleLoRA reduces the average success rate of diverse backdoor attacks to below 15\% across multiple datasets and generalizes to various backbone LMs, including BERT, RoBERTa, GPT2-XL, and Llama2. The codes are publicly available at .",Specific Words,"Despite the remarkable achievements of language models (LMs) in various natural language processing (NLP) tasks, concerns arise due to the lack of interpretability in the internal mechanisms of LMs, impacting their reliability and trustworthiness. A particular security threat to LMs is backdoor attack. Backdoor attack poisons a small portion of the training data by implanting specific text patterns (known as triggers). Trained on the poisoned dataset, the target LM performs maliciously when processing samples containing the triggers, while behaving normally when processing clean text. Prior works attempt to mitigate backdoor learning during training the target LM on the poisoned dataset. However, due to the stealthy nature of complex triggers in real-world scenarios, most existing defense methods fail to mitigate backdoor learning from such triggers, like specific text style or syntax. Furthermore, most existing defense methods rely on empirical observations and lack thorough exploration of backdoor learning. To better understand backdoor learning, we investigate the learning mechanisms of LMs in the frequency space on the poisoned datasets through Fourier analysis.{Details are provided in Section~. In this paper, {frequency} denotes the frequency of input-output mapping, rather than input frequency.} The findings indicate that the backdoor mapping presented on the poisoned datasets exhibits a stronger inclination towards lower frequency compared to clean mapping. According to the extensively studied F-Principle, which suggests that deep neural networks (DNNs) tend to fit the mapping from low to high frequency during training, these results explain why backdoor mapping is notably easier to discern and converges faster for LMs. Inspired by the observation and thought above, we propose a general backdoor defense method named {Mu}lti-{Sc}a{le} {Lo}w-{R}ank {A}daptation (MuScleLoRA) to further mitigate backdoor learning. MuScleLoRA integrates multiple radial scalings in the frequency space with low-rank adaptation to the target LM and aligns gradients during parameter updates. By downscaling in the frequency space, MuScleLoRA encourages LMs to prioritize relatively high-frequency clean mapping, thereby mitigating learning the backdoor on the poisoned dataset while enhancing clean learning. Experimental results across multiple datasets and model architectures demonstrate the efficacy and generality of MuScleLoRA in defending against diverse backdoor attacks compared to baselines. Specifically, we concentrate on the scenario where (1) the attacker poisons and releases the dataset on open third-party platforms, without gaining control of the downstream training; (2) the defender downloads the poisoned dataset and deploys the defense method to train the target LM, maintaining complete control of the training process. Our contributions are summarized as follows: (1) We conduct Fourier analyses to investigate the mechanisms of backdoor learning, revealing why backdoor mapping is notably easier to discern for LMs compared to clean mapping. To the best of our knowledge, this is the first work that explores the mechanisms of backdoor learning from the perspective of Fourier analysis and transfers these insights into backdoor defense strategies. (2) Inspired by our findings in the frequency space, we propose a general backdoor defense method named MuScleLoRA, which integrates multiple radial scalings in the frequency space with low-rank adaptation to the target LM, and further aligns the gradient when updating parameters. (3) We conduct experiments across several datasets and model architectures, including BERT, RoBERTa, GPT2-XL, and Llama2, to validate the efficacy and generality of MuScleLoRA in backdoor mitigation. Compared to baselines, MuScleLoRA consistently demonstrates its capability to effectively defend against diverse backdoor attacks"
Revealing the Parametric Knowledge of Language Models: A Unified Framework for Attribution Methods,2404.18655v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2404.18655v1_0.pdf,"Language Models (LMs) acquire parametric knowledge from their training process, embedding it within their weights. The increasing scalability of LMs, however, poses significant challenges for understanding a model's inner workings and further for updating or correcting this embedded knowledge without the significant cost of retraining. This underscores the importance of unveiling exactly what knowledge is stored and its association with specific model components. Instance Attribution (IA) and Neuron Attribution (NA) offer insights into this training-acquired knowledge, though they have not been compared systematically. Our study introduces a novel evaluation framework to quantify and compare the knowledge revealed by IA and NA. To align the results of the methods we introduce the attribution method \ to apply NA for retrieving influential training instances, and \ to discover important neurons of influential instances discovered by IA. We further propose a comprehensive list of faithfulness tests to evaluate the comprehensiveness and sufficiency of the explanations provided by both methods. Through extensive experiments and analysis, we demonstrate that NA generally reveals more diverse and comprehensive information regarding the LM's parametric knowledge compared to IA. Nevertheless, IA provides unique and valuable insights into the LM’s parametric knowledge, which are not revealed by NA. Our findings further suggest the potential of a synergistic approach of combining the diverse findings of IA and NA for a more holistic understanding of an LM's parametric knowledge.","Proposed evaluation framework comparing Instance and Neuron Attribution methods by examining most influential training instances ${x_i^{train}}$ and most important neurons ${n_m, n_t, n_p, \dots}$. Tests for Sufficiency (activation of key neurons) and Completeness (suppression of the activation of key neurons) -- bottom left, alongside fine-tuning with influential training instances -- bottom right, assess the attribution methods' fidelity to the model's mechanisms.","Language Models encode the knowledge acquired during training as numeric values within the models' weights, transforming raw information from the training dataset into structured, internal representations. This embedding of knowledge, while fundamental to an LM's functionality, renders the model’s inner workings opaque. To unravel the internal mechanisms of an LM and investigate the parametric knowledge encoded in an LM's weights, the development of eXplainable AI (XAI) methods is paramount. Research in this domain has explored LM’s parametric knowledge with various attribution methods . Commonly used among these are {Instance Attribution} (IA, ) and {Neuron Attribution} (NA, ). IA identifies training instances that influence the model’s parametric knowledge leveraged for its prediction on a test instance. However, despite their utility, IA methods have been criticized for their sensitivity to the choice of hyperparameters and for producing highly homogenous results . On the other hand, NA locates specific neurons that hold the most important parametric knowledge for prediction on a test instance. While NA has proven valuable in locating and editing the structured knowledge within a model , the granular nature of the neuron analysis presents challenges in the interpretation of the outcomes, necessitating manually defined human concepts for interpretation . While IA and NA present different views on the parametric knowledge employed in an LM's prediction, no existing work has contrasted the two techniques to determine if they offer similar or complementary insights that could lead to a more comprehensive understanding of an LM's inner workings. This paper seeks to bridge this gap by establishing a {unified evaluation framework}, illustrated in Figure , that allows for the comparison of these disparate attribution methods, particularly focusing on their application for autoregressive LMs. {Aligning the Results of Attribution Methods.} To allow for comparing the results obtained with NA to those obtained with IA, we first introduce {a novel attribution method }\ (bottom left, `Attribution Results', Figure ; ) that retrieves the training instances sharing the most similar neuron activations for each test instance. \ allows one to interpret the granular NA results and to compare its results to IA. On the other hand, to align the results of IA with NA, we introduce {}\ (bottom right, `Attribution Results', Figure ; ), which finds the neurons of the most important training instances discovered by IA that have the highest activation. {Unified Evaluation Framework for Attribution Methods.} With the aligned results from IA and NA, we introduce two evaluation schemes. Interpretation of the attribution results is important, but it is also crucial to check whether the given explanation fully represents the model's underlying behavior . Therefore, firstly, {faithfulness tests} are designed to assess whether the {neurons discovered by each method are both sufficient and comprehensive} in representing the parametric knowledge used by the model for a prediction (bottom left, Figure ;). Note that previous work has only performed the comprehensiveness test for NA , while such evaluations have not been extended to IA. We find that most neurons can be removed without a significant number of changed predictions compared to the original model, revealing that much of the model's knowledge is not stored in neurons located in the Multilayer Perceptron Layers (MLP). Secondly, we conduct {fine-tuning with a varying number of influential training instances} discovered by the two IA methods (bottom right, Figure ;) to assess how sufficient the training instances are in representing the parametric knowledge used by the model for a prediction. We find that IA and \ perform on par in both finding sufficient and comprehensive neurons and sufficient influential training instances. In addition to this, results show that influential training instances obtained by IA methods do not yield a better-fine-tuned model than randomly selected training instances. {Characterising Differences between Attribution Methods.} We conclude with an extensive analysis () of the characteristics of the attribution methods focusing on their diversity and utility. By comparing the group of selected instances and neurons (), we first observe that the instances discovered by NA-Instances and IA have a very small overlap. In contrast, the group of neurons from IA-Neurons highly overlaps with the group of neurons from NA, where the latter discovers a large group of neurons in addition to the overlapping ones. Furthermore, from the analysis () and results from , we find that the more diverse the group of influential training instances, the better the performance of the model fine-tuned with the group. Finally, results from show that NA-Instances performs better at discovering dataset artifacts than IA methods. Overall, our findings call for future work incorporating both IA and NA methods for a better understanding of LM's parametric knowledge"
Full Parameter Fine-tuning for Large Language Models with Limited Resources,2306.09782v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2306.09782v2_0.png,"Large Language Models (LLMs) have revolutionized Natural Language Processing (NLP) but demand massive GPU resources for training. Lowering the threshold for LLMs training would encourage greater participation from researchers, benefiting both academia and society. While existing approaches have focused on parameter-efficient fine-tuning, which tunes or adds a small number of parameters, few have addressed the challenge of tuning the full parameters of LLMs with limited resources. In this work, we propose a new optimizer, {LO}w-{M}emory {O}ptimization ({LOMO}), which fuses the gradient computation and the parameter update in one step to reduce memory usage. By integrating LOMO with existing memory saving techniques, we reduce memory usage to 10.8\% compared to the standard approach (DeepSpeed solution). Consequently, our approach enables the full parameter fine-tuning of a 65B model on a single machine with 8$$RTX 3090, each with 24GB memory..}",Comparison of SGD and LOMO in backpropagation and parameter update stages. ${Pi}$ refers to the parameter of the model and ${Gi}$ refers to the gradient corresponding to ${Pi}$. LOMO fused gradient computation and parameter update in one step to minimize the size of gradient tensors.,"Large Language Models (LLMs) have revolutionized Natural Language Processing (NLP), demonstrating remarkable abilities such as emergence and grokking, pushing model size to become larger and larger. However, training these models with billions of parameters, such as those with 30B to 175B parameters, raises the bar for NLP research. Tuning LLMs often requires expensive GPU resources, such as 8$$80GB devices, making it difficult for small labs and companies to participate in this area of research. Recently, parameter-efficient fine-tuning methods, such as LoRA and Prefix-tuning, provide solutions for tuning LLMs with limited resources. However, these methods do not offer a practical solution for full parameter fine-tuning, which has been acknowledged as a more powerful approach than parameter-efficient fine-tuning. In this work, we aim to explore techniques for accomplishing full parameter fine-tuning in resource-limited scenarios. We analyze the four aspects of memory usage in LLMs, namely activation, optimizer states, gradient tensor and parameters, and optimize the training process in three folds: 1) We rethink the functionality of an optimizer from an algorithmic perspective and find that SGD is a good replacement in terms of fine-tuning full parameter for LLMs. This allows us to remove the entire part of optimizer states since SGD does not store any intermediate state (Sec-). 2) Our proposed optimizer, LOMO as illustrated in Figure~, reduces the memory usage of gradient tensors to $O(1)$, equivalent to the largest gradient tensor's memory usage (Sec-). 3) To stabilize mix-precision training with LOMO, we integrate gradient normalization, loss scaling, and transition certain computations to full precision during training (Sec-). Our technique results in memory usage that equals the usage of parameters plus activation and the largest gradient tensor. We push the memory usage of full parameter fine-tuning to an extreme, making it merely equivalent to the usage of inference. This is because the memory usage of the forward + backward process should not be less than the forward process alone. It is worth noting that, when employing LOMO to save memory, we ensure that the fine-tuning process remains uncompromised, as the parameter update process is still equivalent to SGD. We empirically assess the memory and throughput performance of LOMO and show that the usage of LOMO enables successful training of a 65B model with only 8 RTX 3090 GPUs. Additionally, to validate the downstream performance of our proposed technique, we apply LOMO to tune the full parameters of LLMs on the SuperGLUE dataset collection. The empirical results demonstrate the efficiency and effectiveness of LOMO for optimizing LLMs with billions of parameters. Overall, our contributions are as follows: {itemize} We provide a theoretical analysis suggesting that SGD can successfully fine-tune the full parameters of LLMs. The issues that previously hindered the widespread usage of SGD may no longer be severe problems for fine-tuning LLMs. We propose LOw-Memory Optimization, named LOMO, to significantly save GPU memory usage without harming the fine-tuning process. Through a thorough evaluation of memory usage and throughput performance, we empirically validate the effectiveness of LOMO in optimizing LLMs under resource-constrained scenarios. This is further supported by performance evaluations on downstream tasks. {itemize"
Long Context is Not Long at All: A Prospector of Long-Dependency Data for Large Language Models,2405.17915v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2405.17915v1_0.pdf,"Long-context modeling capabilities are important for large language models (LLMs) in various applications. However, directly training LLMs with long context windows is insufficient to enhance this capability since some training samples do not exhibit strong semantic dependencies across long contexts. In this study, we propose a data mining framework {ProLong}} that can assign each training sample with a long dependency score, which can be used to rank and filter samples that are more advantageous for enhancing long-context modeling abilities in LLM training. Specifically, we first use delta perplexity scores to measure the {Dependency Strength} between text segments in a given document. Then we refine this metric based on the {Dependency Distance} of these segments to incorporate spatial relationships across long-contexts. Final results are calibrated with a {Dependency Specificity} metric to prevent trivial dependencies introduced by repetitive patterns. Moreover, a random sampling approach is proposed to optimize the computational efficiency of ProLong. Comprehensive experiments on multiple benchmarks indicate that ProLong effectively identifies documents that carry long dependencies and LLMs trained on these documents exhibit significantly enhanced long-context modeling capabilities.","Samples that carry longer dependencies better enhances LLMs' long-context modeling capabilities, even with a fixed training context window of 32k.","Large language models (LLMs) are widely used in many natural language processing (NLP) tasks. These tasks often require dealing with long text inputs, such as lengthy documents, long conversation histories in chatbots or large codebases. Therefore enhancing LLMs to model long-context inputs is a prominent desiderata. There are primarily two categories of approaches to expand the context window of an LLM. The first category fine-tunes LLMs with longer context windows , while the second category adjusts the LLM's positional encoding or attention mechanism to accommodate larger position indices without further re-training . However, non-training methods often produce results inferior to those of fine-tuned LLMs , which model long-context inputs more effectively and generally achieve lower perplexity scores. Although reported to be feasible, simply fine-tuning LLMs with naively sampled long corpora does not ensure improved long context modeling capabilities . Some of these fine-tuned LLMs may still struggle to effectively process and utilize information from long input contexts even if they obtain a decently low perplexity score . This can lead to low performance in various downstream applications, even in some basic synthetic retrieval tasks . Nevertheless, few approaches try to tackle this long-context modeling issue from a data-centric perspective. As revealed by , the quality of fine-tuning data plays a critical role in enhancing the long-context modeling capabilities of LLMs. Besides, also report that high-quality corpora significantly outperform other factors in boosting long-context performance. Upon further exploration, we recognize that high-quality long-text data is characterized by the presence of {long-range dependencies}. The importance of encapsulating long-range dependencies in LLMs is also underscored by , which elucidates the benefits of integrating global dependencies into retrieval-augmented language models. However, such strong semantic dependencies are rare in typical training samples and diminish as the distance between segments increases . Even with identical sequence lengths, different samples may exhibit varying dependency density. Specifically, certain long training samples often comprise concatenated short documents that are randomly selected and do not have any semantic dependencies. Moreover, even for inherently long documents, like novels, most tokens depend only to a brief span of preceding context. This phenomenon can be simply concluded as ``{long context is not long at all}'', leading to challenges in model learning (Figure ). Therefore, we argue that explicitly incorporating long-dependency data into the fine-tuning process can facilitate long context modeling. In this paper, we propose a novel framework (called ProLong) to mine long-dependency data. ProLong assigns a long-dependency score to each document, which serves as an indicator of the dependency density across long contexts. Documents with higher scores are deemed more advantageous for boosting long context modeling, therefore we can use these scores to rank and filter high quality corpus for LLM fine-tuning. Concretely, ProLong first partitions each document into fixed-length segments and evaluates dependency relationships between each segment pair from three perspectives: (i) {dependency strength} quantifies the difference in perplexities of a given segment when conditioned with or without its preceding segments. This metric measures the contribution of the preceding segment to the current one; (ii) {dependency distance} measures the positional gap and spatial relationship between two text segments; and (iii) {dependency specificity} employs entropy to ensure a non-uniform distribution of dependency strengths across all preceding segments, mitigating trivial dependencies introduced by repetitive patterns. A dependency score is assigned to each segment pair by combining the above three perspectives and a final long-dependency score for the entire document is computed by accumulating dependency scores of all segment pairs. Further, we adopt various strategies to optimize the computational efficiency of ProLong, including sampling among segments, evaluating perplexity scores with small models and curating test sets for rapid validation. Experiments on multiple benchmarks indicate that ProLong effectively identifies documents that carry long dependencies and LLMs trained on these documents exhibit significantly enhanced long-context modeling capabilities. Our contributions are summarized as follows: {1.} To the best of our knowledge, this is the first study to explore the relationship between dependency density and the quality of long-text data. {2.} We propose ProLong, a data mining framework for identifying long-dependency data. With ProLong, significant performance boosts are observed using only 50\ {3.} We provide an in-depth analysis of ProLong's components, optimizing computational efficiency and making it practical for large-scale corpora. {4.} We develop two models, ProLong-7b/13b, using training samples derived by the ProLong framework. Experiments show that our models outperform equal-sized competitors on both language modeling and real long-context tasks"
Label-Synchronous Neural Transducer for E2E Simultaneous Speech Translation,2406.04541v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.04541v1_0.png,"While the neural transducer is popular for online speech recognition, simultaneous speech translation (SST) requires both streaming and re-ordering capabilities. This paper presents the LS-Transducer-SST, a label-synchronous neural transducer for SST, which naturally possesses these two properties. The LS-Transducer-SST dynamically decides when to emit translation tokens based on an Auto-regressive Integrate-and-Fire (AIF) mechanism. A latency-controllable AIF is also proposed, which can control the quality-latency trade-off either only during decoding, or it can be used in both decoding and training. The LS-Transducer-SST can naturally utilise monolingual text-only data via its prediction network which helps alleviate the key issue of data sparsity for E2E SST. During decoding, a chunk-based incremental joint decoding technique is designed to refine and expand the search space. Experiments on the Fisher-CallHome Spanish (Es-En) and MuST-C En-De data show that the LS-Transducer-SST gives a better quality-latency trade-off than existing popular methods. For example, the LS-Transducer-SST gives a 3.1/2.9 point BLEU increase (Es-En/En-De) relative to CAAT at a similar latency and a 1.4~s reduction in average lagging latency with similar BLEU scores relative to Wait-k.",Illustration of the proposed LS-Transducer-SST. Linear denotes a linear classifier. Target-side CTC uses translations in the training objective computation.,"Simultaneous speech translation (SST) generates translations from input speech in a streaming fashion. Conventional cascaded SST performs streaming automatic speech recognition (ASR) followed by text-based simultaneous machine translation . Recently, end-to-end (E2E) SST has become popular and has advantages, including lower latency . However, E2E SST is challenging since it requires taking into account word re-ordering between source and target languages during the streaming process . Neural transducers, the dominant model for low-latency ASR, find this difficult due to their monotonic nature. Furthermore, E2E training results in severe data sparsity . Current SST methods are normally based on the Transformer attention-based encoder-decoder (AED) structure, which is not naturally able to deal with streaming . To adapt the AED to SST, a popular approach is the Wait-k policy . This is a fixed read-write policy that uses a fixed number of wait duration steps (typically a multiple of 280~ms steps) before translation and can hence give a trade-off between latency and translation quality. However, it can be too aggressive or conservative in different cases . Alternative methods involve a flexible policy which lets the model decide how much input to read before generating the next translation token . One such method is monotonic multi-head attention (MMA) and recently, a Continuous Integrate-and-Fire (CIF) based SST system was shown to outperform MMA at low latency . However, these flexible policy methods normally rely on a latency loss to adjust the quality-latency trade-off during training, unlike the fixed Wait-k policy that can control latency at decoding only. Current SST methods are normally based on the Transformer attention-based encoder-decoder (AED) structure, which isn't naturally able to deal with streaming, as noted by . A popular approach to adapt the AED to SST is the Wait-k policy , which is a fixed read-write policy that uses a fixed number of wait duration steps before translation. However, it can be too aggressive or conservative in different cases . Alternative methods involve a flexible policy which lets the model decide how much input to read before generating the next translation token , such as monotonic multi-head attention (MMA) and Continuous Integrate-and-Fire (CIF) based SST system. However, these flexible policies normally rely on a latency loss to adjust the quality-latency trade-off at training, unlike the fixed Wait-k policy that can control latency at decoding only. To better address the challenges of E2E SST, this paper adapts the label-synchronous neural transducer developed for ASR to SST and denotes the resulting technique the LS-Transducer-SST. In the LS-Transducer-SST, an Auto-regressive Integrate-and-Fire (AIF) mechanism uses accumulated frame-level weights to dynamically determine when to emit translation tokens, based on which a label-level target-side encoder representation is extracted auto-regressively using an attention mechanism. Therefore, the LS-Transducer-SST is naturally equipped with both streaming and re-ordering capabilities. In addition, the prediction network of the LS-Transducer-SST works as a standard language model (LM) as its output is directly combined with the extracted encoder representation at the label level. As a benefit, the E2E SST data sparsity issue can be alleviated because the prediction network can effectively utilise monolingual text-only data, which is normally easy to collect, for tasks such as pre-training or text-based adaptation. While the standard AIF theoretically ensures low-latency output for the LS-Transducer-SST, to better control the quality-latency trade-off, a latency-controllable AIF is proposed, which controls the latency by adjusting the decision threshold of the accumulated frame-level weights. Furthermore, the latency-controllable AIF allows the quality-latency trade-off to be controlled not only during training but also during decoding, enabling the LS-Transducer-SST to combine the advantages of typical fixed and flexible SST policies. This paper focuses on low/medium-latency scenarios to keep the low-latency advantage of E2E SST. During decoding, to improve translation quality, a chunk-based incremental joint decoding is further proposed to refine and expand the search space. The proposed LS-Transducer-SST was evaluated on Fisher-CallHome Spanish (Es-En) and MuST-C En-De corpora and gave an improved quality-latency trade-off compared to existing popular SST methods. The main contributions are summarised below: {-0.1cm} {itemize} {}{2pt} {}{0pt} {}{0pt} LS-Transducer-SST, naturally equipped with streaming and reordering abilities, is proposed for SST and can help alleviate its data sparsity. A latency-controllable AIF is proposed to control the latency during decoding effectively. A chunk-based incremental joint decoding is proposed to expand the search space. Extensive experiments were conducted. Our code bridges the ESPnet and Fairseq toolkits and will facilitate future research{The code is available at: {https://github.com/D-Keqi/LS-Transducer-SST}}. {itemize"
Competition of Mechanisms: Tracing How Language Models Handle Facts and Counterfactuals,2402.11655v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.11655v2_0.pdf,"Interpretability research aims to bridge the gap between empirical success and our scientific understanding of the inner workings of large language models (LLMs). However, most existing research focuses on analyzing a single mechanism, such as how models copy or recall factual knowledge. In this work, we propose a formulation of {competition of mechanisms}, which focuses on the interplay of multiple mechanisms instead of individual mechanisms and traces how one of them becomes dominant in the final prediction. We uncover how and where mechanisms compete within LLMs using two interpretability methods: logit inspection and attention modification. Our findings show traces of the mechanisms and their competition across various model components and reveal attention positions that effectively control the strength of certain mechanisms.% { Code: {{https://github.com/francescortu/comp-mech}. Data: {https://huggingface.co/datasets/francescortu/comp-mech}}. Our code and data have been uploaded to the submission system and will be open-sourced upon acceptance. }",Top: An example showing that LLMs can fail to recognize the correct mechanism when multiple possible mechanisms exist. Bottom: Our mechanistic inspection of where and how the competition of mechanisms takes place within the LLMs.,"Recent advancements in large language models (LLMs) have brought unprecedented performance improvements to NLP [{inter alia}]{brown2020gpt3,touvron2023llama,openai2023gpt4,anil2023gemini}. However, the black-box nature of these models obfuscates our scientific understanding of {how these models achieve certain capabilities}, and {how can we trace the problem when they fail at other tasks}. This has brought an increasing focus on interpretability research to help us understand the inner workings of LLMs. Existing interpretability research has been largely focused on discovering the {existence} of single mechanisms, such as the copy mechanism in induction heads of LLMs , and factual knowledge recall in the MLP layers . However, different from discovering {what mechanisms exist in LLMs}, we propose a more fundamental question: {how do different mechanisms interact in the decision-making of LLMs?} We show a motivating example in {fig:examples}, where the model fails to recognize the correct mechanism when it needs to judge between two possible mechanisms: whether to recall the factual knowledge on who developed the iPhone (i.e., Mechanism 1) or to follow its counterfactual redefinition in the new given context (i.e., Mechanism 2). We propose a novel formulation of {competition of mechanisms}, which focuses on tracing each mechanism in the model and understanding how one of them becomes dominant in the final prediction by winning the ``competition''. Specifically, we build our work on two single mechanisms that are well-studied separately in literature: (1) the factual knowledge recall mechanism, which can be located in the MLP layers ; and (2) the in-context adaptation to a counterfactual statement, which is enabled by the copy mechanism conducted by induction heads of attention layers . Based on the latest tools to inspect each of these two mechanisms , we then unfold {how and where} the competition of the two mechanisms happen, and how it leads to the overall success or failure of LLMs. Technically, we deploy two main methods: logit inspection by projecting the outputs of each model component by an unembedding matrix, and attention modification . Using these methods, we assess the contributions of various model components, both from a macroscopic view (e.g., each layer) and a microscopic view (e.g., attention heads), and identify critical positions and attention heads involved in the competition of the two mechanisms. Moreover, we locate a few localized positions of some attention head matrices that can significantly control the strength of the factual mechanism. We summarize our main findings as follows: {enumerate} In early layers, the factual attribute is encoded in the subject position, while the counterfactual is in the attribute position ( ); The attention blocks write most of the factual and counterfactual information to the last position ( ); All the highly activated heads attend to the attribute position regardless of the specific type of information they promote. The factual information flows by penalizing the counterfactual attribute rather than promoting the factual one ( ); We find that we can up-weight the value of a few very localized values of the attention head matrix to strengthen factual mechanisms substantially ( ). {enumerate"
FactPICO: Factuality Evaluation for Plain Language Summarization of Medical Evidence,2402.11456v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.11456v2_0.pdf,"Plain language summarization with LLMs can be useful for improving textual accessibility of technical content. But how factual are these summaries in a high-stakes domain like medicine? % This paper presents { FactPICO}, a factuality benchmark for plain language summarization of medical texts describing randomized controlled trials (RCTs), which are the basis of evidence-based medicine and can directly inform patient treatment. { FactPICO} consists of 345 plain language summaries of RCT abstracts generated from three LLMs (i.e., GPT-4, Llama-2, and Alpaca), with fine-grained evaluation and natural language rationales from experts. We assess the factuality of critical elements of RCTs in those summaries: Populations, Interventions, Comparators, Outcomes (PICO), as well as the reported findings concerning these. % We also % evaluate the correctness of the extra information (e.g., % explanations) added by LLMs. Using { FactPICO}, we benchmark a range of existing factuality metrics, including the newly devised ones based on LLMs. We find that plain language summarization of medical evidence is still challenging, especially when balancing between simplicity and factuality, and that existing metrics correlate poorly with expert judgments on the instance level. FactPICO and our code is available at .",Expert evaluation of a GPT-4 plain language summary in {\sc FactPICO}. We omitted the original abstract (can be found in Appendix~\ref{sec:abs_full_txt}) in this figure due to space limit. More examples in Appendix~\ref{app:factpicoexamples}.,"New findings in medicine observed in randomized controlled trials (RCTs) are published in journal articles which describe their design and outcomes. These RCTs ``measure the effectiveness of a new intervention or treatment'' and are the important basis of evidence-based medicine. However, understanding such articles requires ``specific attention outside of general literacy capacities'', rendering them effectively inaccessible to most (lay) people. Ideally, healthcare providers would stay current on all medical evidence and share relevant findings with patients, but this is impractical due to the volume and growth of the evidence base. LLMs may provide a means for lay readers to access such findings by automatically summarizing and simplifying texts into plain language. Done successfully, this could allow patients to access the most up-to-date literature relevant to their healthcare. In turn, this may promote health literacy broadly by disseminating trustworthy information. But given the inherent risks to personal health, the {factual correctness} of such outputs is paramount in this domain. While showed that GPT-3 infrequently introduced outright errors when simplifying RCT abstracts, inaccuracies are occasionally introduced; this ought to be addressed before wide adoption of such technology. Unfortunately, there is no standard evaluation benchmark for factuality on this important {medical evidence text simplification} task. Consequently, it is unknown whether and to what degree existing automatic factuality evaluation metrics align with human judgments. We posit that focusing on critical elements in the RCT structure is key for factual medical evidence communication. This work presents { FactPICO}, an expert-constructed factuality benchmark for the plain language summarization of technical abstracts describing RCTs.{We focus on abstracts, because they are always publicly accessible, and typically include the key results that would be of interest to individuals.} { FactPICO} is a fine-grained benchmark focused on key characteristics of trials: {P}opulations (e.g.\ {COVID patients; diabetics}), {I}nterventions (e.g.\ {remdesivir}), {C}omparators (e.g.\ {placebo}), {O}utcomes (e.g.\ {30 day mortality, or pain}), as well as Evidence Inference (i.e., whether the intervention yielded a significant difference in the treated group with respect to the outcome; ). PICO is a standard framework to structure clinical questions. Figure~ shows an example of { FactPICO} annotation. In contrast to standard summaries, plain language summaries have the additional goal of simplifying content for lay readers. This may involve elaboration and explanation of difficult concepts to foster understanding. Thus, { FactPICO} includes a correctness assessment of {added content}. { FactPICO} is also distinct in that it includes {expert-written rationales} that contextualize the evaluation of these fine-grained characteristics, providing a useful first step in assessing explainable factuality evaluation methods. { FactPICO} includes outputs from a mix of proprietary and open-source LLMs (GPT-4, Llama-2-Chat, and Alpaca). Our findings are somewhat less optimistic than prior work in medical and news summarization: Factual errors (occasionally important ones) with respect to key RCT elements are introduced by LLMs. One concerning phenomenon is the extent to which models overgeneralize, resulting in problematic information loss. Using { FactPICO}, we evaluated a suite of existing automatic metrics shown to perform well for factuality in summarization, as well as newly devised LLM-based evaluations. We find that existing metrics correlate with expert ratings at the system level, but not at the instance level. The best performing metric is an LLM-based approach in which we first identify key RCT elements; this shows that providing models with explicit domain knowledge may help. Analysis of LLM-generated rationales shows that LLMs often provide flawed reasoning when justifying their self-evaluations"
BvSP: Broad-view Soft Prompting for Few-Shot Aspect Sentiment Quad Prediction,2406.07365v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.07365v1_0.pdf,"Aspect sentiment quad prediction (ASQP) aims to predict four aspect-based elements, including {aspect term}, {opinion term}, {aspect category}, and {sentiment polarity}. In practice, unseen aspects, due to distinct data distribution, impose many challenges for a trained neural model. Motivated by this, this work formulates ASQP into the few-shot scenario, which aims for fast adaptation in real applications. Therefore, we first construct a few-shot ASQP dataset ($$) that contains richer categories and is more balanced for the few-shot study. Moreover, recent methods extract quads through a generation paradigm, which involves converting the input sentence into a templated target sequence. However, they primarily focus on the utilization of a single template or the consideration of different template orders, thereby overlooking the correlations among various templates. To tackle this issue, we further propose a {Broad-view Soft Prompting} (BvSP) method that aggregates multiple templates with a broader view by taking into account the correlation between the different templates. Specifically, BvSP uses the pre-trained language model to select the most relevant k templates with Jensen–Shannon divergence. BvSP further introduces soft prompts to guide the pre-trained language model using the selected templates. Then, we aggregate the results of multi-templates by voting mechanism. % Then, we utilize a voting mechanism that considers predictions from the templates to aggregate the results of multi-templates. To aggregate the results of multi-templates, we utilize a voting mechanism that considers predictions from the templates. Empirical results demonstrate that BvSP significantly outperforms the state-of-the-art methods under four few-shot settings and other public datasets. Our code and dataset are available at .",A unseen aspect case is shown. The newly emerged category {`internet''} is not mentioned in the pre-defined set of aspect categories.,"Analyzing user reviews, social media posts, product evaluations, and other content on the web to extract sentiment information related to specific aspects helps in understanding users' opinions and emotions regarding different aspects on the web. To monitor public opinion and support decision-making, the research field of sentiment analysis and opinion mining emerged . The aspect sentiment quad prediction (ASQP) task aims to extract aspect quadruplets from a review sentence to comprehensively understand users' aspect-level opinions . Recently, ASQP is gaining attention due to it involves predicting four fundamental aspect-level elements: 1) {aspect term} which is the concrete aspect description in the given text; 2) {opinion term} describing the exact opinion expression towards an aspect; 3) {aspect category} denoting the aspect type which refers to a pre-defined set, and 4) {sentiment polarity} indicating the sentiment class of the aspect. For example, given the sentence {``The room is clean.''}, the aspect elements are “room”, “clean”, “room\_overall”, and “positive”, respectively. Accordingly, the ASQP is described as a quad ({room}, {clean}, {room\_overall}, {positive}). However, in practical situations, aspect categories are not immutable and frozen . New aspects emerge as people discuss emerging phenomena, trends, products, and more through social media, news articles, and other means on the internet. As the restaurant domain illustrated in Figure , the initial aspect category set is pre-defined. Yet as the infrastructure upgrades, new aspects, such as {``WiFi''}, gradually appear. The sentence's category, i.e. {``internet''} does not exist in the pre-defined categories. This imposes challenges to the model's comprehensive and accurate understanding of the sentence. Moreover, the unseen aspect usually has a distribution shift, which is struggling for trained models to adapt accurately. =0.5cm Therefore, researching the few-shot ASQP task, i.e. {fast adaptation to unseen aspects with only a few labeled samples}, becomes crucial, as it aligns more closely with real-world application scenarios. Yet, previous ASQP datasets either have a limited number of categories or long-tailed distribution . This task lacks a proper benchmark dataset. Therefore, we annotate a few-shot ASQP dataset, named ${FSQP}$. This dataset aims to provide a more balanced representation and encompasses a wider range of categories, offering a comprehensive benchmark for evaluating few-shot ASQP. Recent studies have employed generative methods to extract quads by converting input sentences into templated target sequences . Subsequently, by disentangling the formats of template, quads can be extracted. However, they have primarily concentrated on the utilization of a single template or incorporate multiple templates by considering different quad orders , thereby ignore the correlation among these various templates. To overcome this limitation, we introduce an innovative method called Broad-view Soft Prompting (BvSP). BvSP leverages a pre-trained language model and utilizes Jensen-Shannon (JS) divergence to select several templates, enabling a more harmonious view of the available templates. We further introduce soft prompting to fine-tune the pre-trained language model with these selected templates. The final prediction is obtained from multiple templates by using a voting mechanism. In summary, our major contributions of this paper are as follows: {itemize}[leftmargin=*] We construct a new few-shot ASQP dataset ${FSQP}$ which contains richer categories and is more balanced for the few-shot study. To the best of our knowledge, this is the first work to label the few-shot dataset in the ASQP task. We further propose BvSP, a various templates-based soft prompt learning method that improves quad prediction by taking into account the correlation between the different templates. Experimental results under four few-shot settings (i.e. one-shot, two-shot, five-shot, and ten-shot) demonstrate that BvSP outperforms strong baselines and has significant gains in other public datasets. {itemize"
Safety Alignment in NLP Tasks: Weakly Aligned Summarization as an In-Context Attack,2312.06924v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2312.06924v2_0.pdf,"Recent developments in balancing the usefulness and safety of Large Language Models (LLMs) have raised a critical question: Are mainstream NLP tasks adequately aligned with safety consideration? Our study, focusing on safety-sensitive documents obtained through adversarial attacks, reveals significant disparities in the safety alignment of various NLP tasks. For instance, LLMs can effectively summarize malicious long documents but often refuse to translate them. This discrepancy highlights a previously unidentified vulnerability: attacks exploiting tasks with weaker safety alignment, like summarization, can potentially compromise the integrity of tasks traditionally deemed more robust, such as translation and question-answering (QA). Moreover, the concurrent use of multiple NLP tasks with lesser safety alignment increases the risk of LLMs inadvertently processing harmful content. We demonstrate these vulnerabilities in various safety-aligned LLMs, particularly Llama2 models, Gemini and GPT-4, indicating an urgent need for strengthening safety alignments across a broad spectrum of NLP tasks{https://github.com/FYYFU/SafetyAlignNLP}}. {Content warning: To demonstrate the vulnerability, examples provided include safety-sensitive ones with malicious/harmful content.}","When given a direct translation task, the Llama2-7B model detects harmful content and doesn't respond. But, if summarization precedes translation in an in-context attack, it then provides a translation. `[INST]' denotes input, and `[/INST]' the output. See Appendix \ref{case-appendix} for more examples.","LLMs are constantly evolving, with an emphasis on balancing their usefulness and safety. Research in LLM safety currently focuses on two main areas: 1) safety alignment with datasets and Reinforcement Learning from Human Feedback (RLHF) ; and 2) discovering LLM vulnerabilities through attacks using adversarial algorithms, backdoors, and poisoning . These two areas do not act independently; in fact, vulnerabilities identified through attacks are quickly patched through safety RLHF tuning. This dynamics reflects a crucial interaction between attacks and defences. Current attacks, particularly adversarial ones that manipulate malicious prompts such as {``How to make a bomb?''} with added perturbations in the input, often aim at ``jailbreaking'' LLMs . However, most of these setups focus on attacking and defending LLMs through QA tasks. A natural question arises next: Are LLMs robust in defending against attacks beyond open-domain QA tasks? This project aims to answer this question through a novel setup with conditional text generation, evaluating safety alignment for different NLP tasks. Specifically, we use benign NLP task prompts derived from FLAN coupled with safety-sensitive documents—obtained by attacking LLMs with AdvBench's malicious queries—to test safety alignment. Our experiments revealed a previously unidentified vulnerability: {different NLP tasks vary in safety alignment when applied to the same set of sensitive data.} To exploit the practical implications of this vulnerability, we propose simple but effective attacks leveraging weakly aligned NLP tasks (e.g., summarization) as in-context attacks for strongly safety-aligned tasks, such as translation and QA. For example in {fig:case_attack_summ_translation}, safety-sensitive documents, which LLMs typically refuse to translate, can be easily translated by first requesting the LLMs to provide a summary. Additionally, we observed that combining multiple prompts from weakly aligned NLP tasks forms a stronger compositional attack. Our experiments were primarily conducted on open-source models from the Llama2 family. We also tested a small subset of harmful documents, coupled with different NLP task prompts, on Gemini and GPT-4. We observed similar trends: summarization prompts effectively convinced Gemini/GPT-4 to process harmful documents. This finding suggests that the vulnerability we identified might be universal across many safety-aligned language models.{We have found many of the models (Vicuna-7B-v1.3, ChatGLM2-6B, Falcon-7B) will conduct various NLP tasks on {harmful documents} almost 100\ We further investigate this vulnerability's causes, hypothesizing it stems from an imbalance between usefulness and safety in LLM training across different NLP tasks. LLM usefulness is often enhanced through pre-training and instruction tuning using traditional NLP task prompts, like T0 and FLAN . Conversely, safety alignments are typically implemented during the safety RLHF stage, with a predominant focus on open-domain QA tasks. This skewed emphasis may lead to a bias in many NLP tasks towards usefulness over safety, highlighting the need for broader safety alignments across various NLP tasks. Our main contributions are outlined as: {enumerate} {NLP Tasks Have Different Levels of Safety Alignment}: We designed a novel setup using NLP task prompts and safety-sensitive documents, creating a dataset of 6,985 articles from adversarial attacks, to test whether different NLP tasks have varying levels of safety alignment. We found that tasks like summarization have notably lower safety alignment compared to translation or QA tasks. {Weakly Aligned NLP Tasks as In-Context Attacks}: The varying safety alignments among NLP tasks present a vulnerability. We discovered that performing weakly aligned NLP task first increases the likelihood of LLMs processing safety-sensitive documents for other tasks. This effect is further amplified when combining multiple weakly-aligned tasks. {Vulnerability Cause Investigation}: Our experiments indicate that safety alignment discrepancies in NLP tasks stem from an imbalanced trade-off between the usefulness from instruction tuning and the safety of alignment. Our ablation study reveals that summarization attacks are more frequently blocked on shorter documents than longer ones, possibly due to a prevalence of shorter documents in safety alignment. These findings are crucial for enhancing safety alignment research and building stronger defenses. {enumerate"
Towards Privacy-Aware Sign Language Translation at Scale,2402.09611v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.09611v2_0.pdf;/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.09611v2_1.pdf,"A major impediment to the advancement of sign language translation (SLT) is data scarcity. Much of the sign language data currently available on the web cannot be used for training supervised models due to the lack of aligned captions. Furthermore, scaling SLT using large-scale web-scraped datasets bears privacy risks due to the presence of biometric information, which the responsible development of SLT technologies should account for. In this work, we propose a two-stage framework for privacy-aware SLT at scale that addresses both of these issues. We introduce , which leverages self-supervised video pretraining on anonymized and unannotated videos, followed by supervised SLT finetuning on a curated parallel dataset. achieves state-of-the-art finetuned and zero-shot gloss-free SLT performance on the How2Sign dataset, outperforming the strongest respective baselines by over 3 BLEU-4. Based on controlled experiments, we further discuss the advantages and limitations of self-supervised pretraining and anonymization via facial obfuscation for SLT. [width=1.25em,height=1.25em]{github.png}{{ {}}}","Overview of our two-stage \method method. The first stage consists of training a SignHiera encoder via masked autoencoding (MAE) on {blurred} video frames. In the second stage, a pretrained T5 model is finetuned for SLT while the pretrained SignHiera is kept frozen (\includegraphics[width=0.9em]{figures/snowflake.pdf}). The input video in the second stage {can be unblurred}.","Used by millions worldwide, sign languages play a crucial role in facilitating communication for many d/Deaf and hard-of-hearing individuals. Visual in nature, these languages make use of the co-articulated features of hands (i.e., finger positioning, shape, movement, palm orientation, etc.), body postures, gaze, mouth gestures, mouthings and facial expressions to convey meaning . Globally, there are more than {300} sign languages, each with their own grammar and vocabulary.{{https://www.un.org/en/observances/sign-languages-day}} American Sign Language (ASL) alone is estimated to have more than half a million native users, ranking it among the most commonly used languages in the United States . Despite the prevalence of sign languages, they are still under-served by translation technology. Besides under-investment and the inherent difficulty of SLT,{Results of the WMT~2023 SLT task evince this difficulty; the best system only achieved 1~BLEU .} another key explanation for this imbalance is the lack of sufficiently large, clean, and labeled parallel corpora. Current state-of-the-art SLT systems require detailed and time aligned annotations , which is not scalable, as annotating sign language data is a labour intensive task and can only be done by proficient signers. We argue that a promising solution to SLT's data scarcity is to utilize publicly available {unannotated} sign language data.{For example, filtered their Youtube-ASL dataset from $88$K to $11$K videos based largely on the availability and quality of English captions.} In other domains of computer vision and NLP, a common practice is to pretrain on large-scale unannotated web datasets and later finetune on curated, task-specific datasets . This practice is largely unexplored in the SLT domain and comes with additional challenges. In particular, moving to large-scale sign language processing makes it increasingly difficult to control the composition of the training data. Because sign language videos typically feature faces and upper bodies and thus are biometrically identifying, such research may exacerbate privacy risks. Hence, developing sign language technologies responsibly requires us to account for these risks and explore techniques to protect privacy. In this work, we study the effectiveness of self-supervised video pretraining for SLT, under consideration of the aforementioned privacy risks. We first propose a {generic, scalable and privacy-aware} two-stage framework for SLT, summarized in Table~. We introduce {{}} ({S}elf-{S}upervised {V}ideo {P}retraining for {S}ign {L}anguage {T}ranslation), an implementation of this framework consisting of two or optionally three stages: pretraining a continuous sign language encoder via masked autoencoding [MAE; ][]{he-etal-2022-mae} on anonymized video, then optionally bridging the modality gap via CLIP-style video-text pretraining , and finally training an SLT system via supervised finetuning using extracted features from the pretrained model. Our best performing models achieve {15.5} BLEU finetuned and {7.1} BLEU zero-shot on the How2Sign dataset , surpassing SOTA in both settings by over {3}~BLEU while using data anonymized via facial obfuscation. We also introduce a new ASL-to-English SLT benchmark dataset, {DailyMoth-70h}, consisting of 70h of continuous signing in native ASL. We then evaluate the downstream performance impact and discuss the benefits and limitations of facial blurring to achieve anonymization. Through controlled ablation studies of , we identify what factors contribute to a strong pretraining and finetuning recipe. We conclude by discussing opportunities and challenges of self-supervised pretraining for sign language processing"
Arithmetic Control of LLMs for Diverse User Preferences: Directional Preference Alignment with Multi-Objective Rewards,2402.18571v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.18571v3_0.pdf,"% Fine-grained control over large language models (LLMs) remains a significant challenge, hindering their adaptability to diverse user needs. While Reinforcement Learning from Human Feedback (RLHF) shows promise in aligning LLMs, its reliance on {scalar} rewards often limits its ability to capture diverse user preferences in real-world applications. To address this limitation, we introduce the Directional Preference Alignment (DPA) framework. Unlike the scalar-reward RLHF, DPA incorporates {multi-objective reward} modeling to represent diverse preference profiles. Additionally, DPA models user preferences as {directions} (i.e., unit vectors) in the reward space to achieve user-dependent preference control. Our method involves training a multi-objective reward model and then fine-tuning the LLM with a preference-conditioned variant of Rejection Sampling Finetuning (RSF), an RLHF method adopted by Llama 2. This method enjoys a better performance trade-off across various reward objectives. In comparison with the scalar-reward RLHF, DPA offers users {intuitive control over LLM generation}: they can {arithmetically} specify their desired trade-offs (e.g., more helpfulness with less verbosity). We also validate the effectiveness of DPA with real-world alignment experiments on Mistral-7B. Our method provides straightforward arithmetic control over the trade-off between helpfulness and verbosity while maintaining competitive performance with strong baselines such as Direct Preference Optimization (DPO). The code and trained model are released at .","{Arithmetic Prompting} for Preference-Conditional Generalization: Comparison between conventional RLHF methods such as DPO and our Directional Preference Alignment (DPA). In the case of DPO (left), it is capable of generating helpful responses, but these tend to be excessively verbose. Conversely, with our DPA (right), it allows for {arithmetic control} of LLMs to meet various user preferences. For instance, setting the directional preference (unit vector) to $v=\left< 0.8, -0.6\right>$ leads to less verbose responses from our aligned LLM.","Large language models (LLMs) have demonstrated remarkable capabilities across various domains and tasks, such as mathematical reasoning and medical question answering. However, for an assistant to be truly useful, it must align with human preferences, such as being helpful, honest, harmless, and managing verbosity. {Reinforcement Learning from Human Feedback} (RLHF) , is the leading approach to adapt LLMs towards these complex, often implicitly-defined goals. Typically, the most popular RLHF framework first constructs a scalar reward model to represent the difficult-to-specify goal of being preferred by human and then use this reward model to provide signals for the subsequent reward optimization stage. Its success spans various practical applications, including recommendation systems , image generation , robotics , and most notably, aligning LLMs with human values and preferences, such as ChatGPT , Claude , Llama 2 and Gemini . While recent advancements in RLHF are noteworthy, a fundamental challenge persists due to problem misspecification. This means that a {single} reward function may not sufficiently capture complex human values. For example, a generative model aligned by RLHF for helpfulness tends to produce verbose responses as shown in Figure~ (Left) , even though many users prefer answers that are both helpful and concise. Assuming scalar-objective reward implies a {total order} over preferences, which is hard to satisfy when the preference is aggregated across a diverse set of human groups , because humans typically have a set of intricate or even {contradictory} targets . In real-world applications, the scalar-reward RLHF tends to align the LLMs toward an ``average-user'' preference, which cannot capture the complicated nature of human preferences and can be unfair for the under-represented groups . For example, consider User-1, 2, 3, and responses $A$, $B$, $C$ in Fig.~ (Left). User-1 and 3 prefer response $B$ over $C$ ($B C$), while User-2 prefers $C$ over $B$ ($C B$). This could occur as response $C$ is more verbose than $B$, while User-2 prefers concise answers. When these diverse preferences are aggregated across human groups, the typical reward models with scalar rewards tend to learn the ``average-user'' preference (which is $B C$ in this case), overlooking the individual preference of User-2, as shown in Figure~ (Middle). This is also known as the ``Condorcet paradox'' in the theory of social choice . In general, human opinions and expertise can vary significantly . Meanwhile, the importance of these targets may also change over time, depending on the users and their expectations. To address the limitations of the existing scalar reward model, previous works suggest the use of multi-objective rewards that characterize human preferences from different aspects (e.g., helpfulness, verbosity, harmlessness) . One common way is to take the human feedback as a {multi-dimensional} reward vector and each dimension models one objective . Then, one may apply a linear combination to transform the multi-objective rewards into a scalar for LLM alignment . However, this approach still cannot handle the user-dependent needs from a diverse user population and can be unfair for minority groups. One may further adopt a user-dependent linear combination to multi-objective rewards for aligning a model for each user preference . However, this approach is quite {inference-unfriendly} because we have to switch between different models in response to the different user preferences. Finally, in social choice theory, a game-based formulation was studied under the name {maximal lotteries} , as well as the subsequent works in RLHF , to handle the diversity of user preferences. We remark that their framework is fundamentally different from the multi-objective rewards and cannot offer a user-dependent preference control in the inference stage, either. Refer to Section~ for a more detailed discussion with existing methods. In recognition of the aforementioned limitations, we propose a novel and practical alignment approach, {Directional Preference Alignment} (DPA), to enhance the {adaptability and controllability of a single LLM}. Our aligned LLM enjoys the flexibility to be controlled with different preferences embedded numerically into the system prompt. The ability to control preferences can significantly enhance the model's personalization ability during inference. For example, as the model is aligned with DPA with {helpfulness} and {verbosity} in consideration, a user could simply control the model's generation by specifying a directional preference $v = <v_1, v_2>$ that $\|v\|_2 = 1$, and the model will generate responses that maximize ${reward}=v_1{helpfulness} + v_2 {verbosity}$ where ${helpfulness}$ and ${verbosity}$ are rewards scored from different perspectives as shown in Figure~ (Right). Figure~ (Right) further shows that the preferences of User-1, User-2, and User-3 can be accurately represented by specifying the preference vector in the 2-dimensional space. This is a scenario where DPA can alleviate the problem of misspecification in RLHF. Our approach features two crucial aspects: 1). Multi-Objective Rewards, which involve learning with multiple different preference targets simultaneously, and 2). Directional Preference Alignment, which encodes user preferences as unit vectors for preference-aware LLM alignment. Specifically, we summarize our contributions as follows. {itemize}[leftmargin=*,align=left,noitemsep,nolistsep] {We identify the limitations of existing popular RLHF frameworks}: 1) the limited capacity for capturing the real-world complicated human preference; 2) lacking in adaptability for user-dependent preference; {We propose Directional Preference Alignment (DPA)}: a novel alignment approach that allows a {single} LLM to accommodate users with varying preferences. {We consider both helpfulness and verbosity rewards, and align Mistral-7B with our DPA}: empirical evaluations show that DPA offers effective arithmetic control over the trade-off between helpfulness and verbosity, while maintaining competitive performance with DPO . {itemize"
RAVEL: Evaluating Interpretability Methods on Disentangling Language Model Representations,2402.17700v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.17700v2_0.pdf,"Individual neurons participate in the representation of multiple high-level concepts. To what extent can different interpretability methods successfully disentangle these roles? To help address this question, we introduce {} (), a dataset that enables tightly controlled, quantitative comparisons between a variety of existing interpretability methods. We use the resulting conceptual framework to define the new method of Multi-task Distributed Alignment Search (MDAS), which allows us to find distributed representations satisfying multiple causal criteria. With -7B as the target language model, MDAS achieves state-of-the-art results on , demonstrating the importance of going beyond neuron-level analyses to identify features distributed across activations. We release our benchmark at .","An overview of the \ourdataset\ benchmark, which evaluates how well an interpretability method can find features that isolate the causal effect of individual attributes of an entity.","A central goal of interpretability is to localize an abstract concept to a component of a deep learning model that is used during inference. However, this is not as simple as identifying a neuron for each concept, because neurons are {polysemantic} -- they represent multiple high-level concepts . Several recent interpretability works tackle this problem using a {featurizer} that disentangles the activations of polysemantic neurons by mapping to a space of {monosemantic} features that each represent a distinct concept. Intuitively, these methods should have a significant advantage over approaches that identify concepts with sets of neurons. However, these methods have not been benchmarked. To facilitate these method comparisons, we introduce a diagnostic benchmark, {} (). \ evaluates interpretability methods on their ability to localize and disentangle the attributes of different types of entities encoded as text inputs to language models (LMs). For example, the entity type ``city'' has instances such as ``Paris'' or ``Tokyo'', which each have attributes for ``continent'', namely ``Europe'' and ``Asia''. An interpretability method must localize this attribute to a group of neurons $$, learn a featurizer $$ (e.g., a rotation matrix or sparse autoencoder), and identify a feature $$ (e.g., a linear subspace of the residual stream in a Transformer) for the attribute. \ contains five types of entities (cities, people names, verbs, physical objects, and occupations), each with at least 500 instances, at least 4 attributes, and at least 50 prompt templates per entity type. The metric we use to assess interpretability methods is based on interchange interventions (also known as activation patching). This operation has emerged as a workhorse in interpretability, with a wide swath of research applying the technique to test if a high-level concept is stored in a model representation and used during inference . Specifically, we use the LM to process a prompt like ``{Paris is in the continent of}'' and then intervene on the neurons $$ to fix the feature $$ to be the value it would have if the LM were given a prompt like ``{Tokyo is a large city.}'' If this leads the LM to output ``{Asia}'' instead of ``{Europe}'', then we have evidence that the feature $$ encodes the attribute ``continent''. Then, we perform the same intervention when the LM processes a prompt like ``{People in Paris speak}''. If the LM outputs ``{French}'' rather than ``{Japanese}', then we have evidence that the feature $$ has disentangled the attributes ``continent'' and ``language''. A variety of existing interpretability methods are easily cast in the terms needed for \ evaluations, including supervised probes , Principal Component Analysis , Differential Binary Masking (DBM: ), sparse autoencoders , and Distributed Alignment Search (DAS: ). Our apples-to-apples comparisons reveal conceptual similarities between the methods. In addition, we propose multi-task training objectives for DBM and DAS. These objectives allow us to find representations satisfying multiple causal criteria, and we show that Multi-task DAS is the most effective of all the methods we evaluate at identifying disentangled features. This contributes to the growing body of evidence that interpretability methods need to identify features that are distributed across neurons"
Large Language Models as Zero-shot Dialogue State Tracker through Function Calling,2402.10466v4,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.10466v4_0.pdf,"Large language models (LLMs) are increasingly prevalent in conversational systems due to their advanced understanding and generative capabilities in general contexts. However, their effectiveness in task-oriented dialogues (TOD), which requires not only response generation but also effective dialogue state tracking (DST) within specific tasks and domains, remains less satisfying. In this work, we propose a novel approach for solving DST with LLMs through function calling. This method improves zero-shot DST, allowing adaptation to diverse domains without extensive data collection or model tuning. Our experimental results demonstrate that our approach achieves exceptional performance with both modestly sized open-source and also proprietary LLMs: with in-context prompting it enables various 7B or 13B parameter models to surpass the previous state-of-the-art (SOTA) achieved by ChatGPT, and improves ChatGPT's performance beating the SOTA by 5.6\% average joint goal accuracy (JGA). Individual model results for GPT-3.5 and GPT-4 are boosted by 4.8\% and 14\%, respectively. We also show that by fine-tuning on a small collection of diverse task-oriented dialogues, we can equip modestly sized models, specifically a 13B parameter LLaMA2-Chat model, with function-calling capabilities and DST performance comparable to ChatGPT while maintaining their chat capabilities. We have made the code publicly available.}","{Zero-shot DST performance comparison} among (1) previous domain transfer approaches using small models; (2) previous prompting approaches exclusively relying on advanced proprietary LLMs; and (3) our approach, compatible with various LLMs, empowers various 7B and 13B models for superior performance and sets new state-of-the-art with GPT-4.","Recent years have seen the rapid development of large language models (LLMs) that have demonstrated exceptional natural language understanding and generation capabilities. The integration of LLMs into industry applications, particularly as conversational assistants, is a notable trend. Fine-tuned with conversations between users and assistants, these models are further aligned with human preferences to enhance their ability to deliver fluent, helpful, and polite responses to user inquiries. Notable examples include proprietary systems such as ChatGPT{http://chatgpt.openai.com/} and Claude{https://www.anthropic.com/index/introducing-claude}, as well as open-source models such as LLaMA2-Chat, Vicuna, Baichuan. The primary focus of these chat-tuned LLMs has typically been on responding in general contexts. However, for another important type of conversation, task-oriented dialogues (TOD), the model is required to extract the intentions of users at each turn of the conversation, represented as slot-value pairs of per-domain predefined schemas; a process known as Dialogue State Tracking (DST). The challenge lies in the model's ability to accurately summarize user needs over multiple turns of conversation and also strictly adhere to a domain-specific ontology. The most direct solutions necessitate training on curated domain-specific annotated data, a process that is notoriously costly and labor-intensive. Despite efforts in data augmentation and automated dataset creation using GPT-3, these methods struggle to generalize to unseen domains. To achieve zero-shot DST for unseen domains, prior approaches usually involved domain transfer methods. While such approaches are trained on alternative domains, they still require domains with matching annotation schema, and their performance has been far from satisfactory. LLMs exhibit remarkable capabilities for tackling various tasks without the need for task-specific fine-tuning, making them suited for zero-shot DST. However, while there have been initiatives to leverage ChatGPT for zero-shot DST, these methods tend to treat DST as a standalone task rather than chat completion, which the models, especially chat-tuned models, are more proficient in. They usually take the whole conversation as input along with detailed instructions to generate in domain-specific formats. This setup poses challenges due to the long task context and specific output requirements. Consequently, this works exclusively with advanced ChatGPT or Codex models but fails with less powerful LLMs. In this work, we introduce a novel approach {FnCTOD}, to address zero-shot DST with LLMs. Our method seamlessly integrates DST as a part of the assistant's output during chat completion. Specifically, we treat the schema of each task-oriented dialogue domain as a specific function, and DST for this domain as the process of ``calling'' the corresponding function. We thus instruct LLMs to generate function calls along with the response in the assistant's output. To achieve this, we convert the domain schema into function specifications, which include the function's description and required arguments, and incorporate them into the {system prompt} of the LLM. Additionally, we integrate these function calls into the assistant's output within the {dialogue context}. As shown in Figure~, experimental results on the MultiWOZ benchmark represent a significant milestone. Our approach is the first that, without further fine-tuning, enables modestly sized open-source LLMs (7B or 13B parameters) to achieve comparable or superior performance compared to previous state-of-the-art (SOTA) prompting methods that relied exclusively on advanced proprietary LLMs such as ChatGPT and Codex. Furthermore, our approach beats the previous zero-shot SOTA by 5.6\ Additionally, we show that by fine-tuning a 13B {LLaMA2-Chat} model using a collection of 7,200 task-oriented dialogues --- consisting of 200 randomly selected dialogues covering 36 diverse domains, from heterogeneous TOD datasets --- we can equip it with function-calling DST abilities comparable to ChatGPT while still maintaining its response generation capabilities. The comparison with prior studies is summarized in Table~ and Figure~. { Our contribution is threefold:} {(1)} Demonstration that the FnCTOD approach achieves outstanding performance with both open-source and proprietary LLMs through {in-context prompting}: enables open-source 7--13B models to surpass the previous SOTA achieved by ChatGPT, and enhances GPT-4's performance by 14\ {(2)} Bridging the {zero-shot} DST performance gap between open-source models and ChatGPT by fine-tuning on a small collection of diverse dialogues. {(3)} Showing that function calling DST capabilities can be integrated into existing chat-tuned LLMs while preserving response capabilities"
Enhancing Dialogue State Tracking Models through LLM-backed User-Agents Simulation,2405.13037v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2405.13037v1_0.pdf,"Dialogue State Tracking (DST) is designed to monitor the evolving dialogue state in the conversations and plays a pivotal role in developing task-oriented dialogue systems. However, obtaining the annotated data for the DST task is usually a costly endeavor. In this paper, we focus on employing LLMs to generate dialogue data to reduce dialogue collection and annotation costs. Specifically, GPT-4 is used to simulate the user and agent interaction, generating thousands of dialogues annotated with DST labels. Then a two-stage fine-tuning on LLaMA 2 is performed on the generated data and the real data for the DST prediction. Experimental results on two public DST benchmarks show that with the generated dialogue data, our model performs better than the baseline trained solely on real data. In addition, our approach is also capable of adapting to the dynamic demands in real-world scenarios, generating dialogues in new domains swiftly. After replacing dialogue segments in any domain with the corresponding generated ones, the model achieves comparable performance to the model trained on real data.","The simulation process of our approach. The blue boxes are intentions for the user and the agent, the `[RECOM]', `[EOF]', and `[EOD]' are control identifiers.","Dialogue state tracking (DST) is a critical component of task-oriented dialogue systems, serving to track users' goals and system actions in the conversation and facilitate precise information handling for communicating with external APIs. DST usually takes the form of key-value pairs, where the keys are denoted as slots which are defined in the system schema, outlining the specific information that the system aims to track or extract during the whole conversation. The design of DST models could be broadly categorized into two main types, {classification-based DST models} and {generation-based DST models}. {Classification-based models} select slot values from a set of candidates, assuming that the dialogue ontology is pre-defined and hence lacking generalization capability. {Generation-based models} directly generate the slot values to handle unseen domains and values . Recently, proposes a new DST framework LDST based on LLaMA. By using an instruction tuning method, LDST achieves performance on par with ChatGPT. Despite DST showing promising results, a significant challenge is that the annotation of dialogues entails significant costs. Furthermore, the dynamic nature of real-world demands highlights the urgent need to quickly generate utterances for new domains. Compared to other types of NLP data, collecting authentic dialogue data is particularly challenging. This difficulty is partly due to the dialogues frequently containing personal or sensitive information, which complicates data collection and sharing efforts. In response to these challenges, and inspired by the recent advancements of large language models (LLMs), we explore the use of these models for generating annotated DST data for data augmentation. By leveraging LLM's cross-domain generation capability, we aim to create synthetic dialogues that can serve as replacements for manually annotated data, significantly reducing both financial cost and time constraints. In this paper, we propose a {L}LM-backed {U}ser-{A}gents {S}imulation~(LUAS) algorithm to enhance DST. The process begins with the LLM generating a user profile that details the individual's preferences for various tasks. Following this initial step, the LLM is prompted to simulate a conversation between the user and the agent. In these simulations, the user simulator makes requests and seeks recommendations or assistance, while the agent responds by understanding the user's needs, providing suggestions, and taking appropriate actions. Through iterative conversations between the user and agent, complemented by a slot extractor also prompted by the LLM, we generate a substantial corpus of labeled, multi-turn dialogue data. To verify the effectiveness of our approach and the quality of the generated data, experiments are conducted on two public DST datasets, MultiWOZ 2.2 and MultiWOZ 2.4. Following , LLaMa 2 is finetuned with real data as a strong baseline. By using both the generated and the real data, finetuning LLaMa 2 can further improve the performance. Besides, by replacing dialogue segments of any domain with the generated data, the newly trained model achieves comparable performance to the model trained on the real data, which shows the capability of our method to meet the dynamic requirements of real-world scenarios, generating dialogues in new domains and preserving the promising performance. In summary, the contributions of our work can be categorized into four aspects: {itemize}[leftmargin=*,itemsep=1pt,partopsep=0pt,topsep=0pt,parsep=0pt] We propose a new framework that harnesses the power of GPT-4 to generate new labeled dialogue data, effectively reducing dialogue data collection and annotation costs. Experiment results on two datasets show the positive impact of the generated data on performance. Our method can swiftly generate data in new domains while maintaining promising performance. We believe that our approach holds promise for extension to other dialogue-related tasks. {itemize"
KnowCoder: Coding Structured Knowledge into LLMs for Universal Information Extraction,2403.07969v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2403.07969v2_0.pdf,"In this paper, we propose KnowCoder, a Large Language Model (LLM) to conduct Universal Information Extraction (UIE) via code generation. KnowCoder aims to develop a kind of unified schema representation that LLMs can easily understand and an effective learning framework that encourages LLMs to follow schemas and extract structured knowledge accurately. To achieve these, KnowCoder introduces a code-style schema representation method to uniformly transform different schemas into Python classes, with which complex schema information, such as constraints among tasks in UIE, can be captured in an LLM-friendly manner. We further construct a code-style schema library covering over ${30,000}$ types of knowledge, which is the largest one for UIE, to the best of our knowledge. To ease the learning process of LLMs, KnowCoder contains a two-phase learning framework that enhances its schema understanding ability via code pretraining and its schema following ability via instruction tuning. After code pretraining on around $1.5$B automatically constructed data, KnowCoder already attains remarkable generalization ability and achieves relative improvements by ${49.8\%}$ F1, compared to LLaMA2, under the few-shot setting. After instruction tuning, KnowCoder further exhibits strong generalization ability on unseen schemas and achieves up to ${12.5\%}$ and ${21.9\%}$, compared to sota baselines, under the zero-shot setting and the low resource setting, respectively. Additionally, based on our unified schema representations, various human-annotated datasets can simultaneously be utilized to refine KnowCoder, which achieves significant improvements up to ${7.5\%}$ under the supervised setting.",An illustration of KnowCoder schemas.,"Information Extraction (IE) aims to extract explicit and structured knowledge following the manually designed schemas. The IE schemas define high-level types of knowledge (i.e., concepts) and structures among them, which include various types of entities, relations, and events. To simultaneously extract various knowledge under different schemas via a single model, the Universal Information Extraction (UIE) task is proposed. Recently, Large Language Models (LLMs) have demonstrated general understanding abilities through large-scale pretraining, which drives their increasing utilization in UIE. However, their performance on UIE is still limited because of two main challenges: (1) the lack of a unified schema representation method that LLMs can easily understand; (2) the lack of an effective learning framework that encourages LLMs to accurately follow specific schemas for extracting structured knowledge. For the first challenge, the existing UIE models first represent different schemas in a universal way, such as classification labels, keywords, or a specifically-designed formal language. These schema representation methods have three main restrictions: (1) ignoring information like taxonomies (e.g., ``fairytale'' is a subclass of ``written work'') and constraints among concepts (e.g., ``spouse'' relation exists between two ``human'' entities); (2) classification labels or a specifically designed formal language is hard for LLMs to understand and follow; (3) designed for specific IE datasets and lacking a general schema library. To solve these restrictions, in this paper, we propose a kind of code-style schema representation method, with which various types of knowledge are generally defined as Python classes. As shown in Figure~, the class inheritance mechanism is adopted to describe the concept taxonomies. A mechanism of type hint is employed to model constraints among different concepts. The class comments are used to provide clear definitions of concepts. And, the class methods are used to post-process the results according to specific IE guidelines. Upon this method, we construct a comprehensive code-style schema library covering over $29,000$ entity types, $900$ relation types, and $500$ event types based on Wikidata, the largest one for UIE, to the best of our knowledge, currently reported in the open literature. For the second challenge, the existing learning framework for UIE directly conducts instruction tuning on LLMs to extract knowledge following specific and limited schemas. The enormous concepts in the constructed schema library challenge the existing training framework. To help LLMs better understand and follow these schemas, we propose an effective two-phase framework containing a schema understanding phase and a schema following phase. The former improves the ability of LLMs to understand different concepts in schemas via large-scale code pretraining on the schema definition code and corresponding instance code. The latter advances their abilities to follow specific schemas in an IE task via instruction tuning. After code pretraining on around 1.5B automatically constructed data, KnowCoder already attains remarkable generalization ability and achieves NER improvements compared to the base model, LLaMA2, by ${49.8\ In general, the main contributions of this paper include: {itemize} We propose a code-style schema representation method to uniformly represent different schemas for UIE. Using this method, we construct a large code-style schema library covering more than $30,000$ types of knowledge. We propose an effective learning framework for LLMs in a two-phase manner, which first enhances the schema understanding through code pretraining and then boosts schema following via instruction tuning. After training on billions of automatically annotated data and refining with human-annotated IE datasets, KnowCoder demonstrates superior performance on different IE tasks under the zero-shot, low-resource, and supervised settings. The constructed schema library, training data, code, and models are released for future research. {itemize"
ERA-CoT: Improving Chain-of-Thought through Entity Relationship Analysis,2403.06932v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2403.06932v2_0.png,"Large language models (LLMs) have achieved commendable accomplishments in various natural language processing tasks. However, LLMs still encounter significant challenges when dealing with complex scenarios involving multiple entities. These challenges arise from the presence of implicit relationships that demand multi-step reasoning. In this paper, we propose a novel approach ERA-CoT, which aids LLMs in understanding context by capturing relationships between entities and supports the reasoning of diverse tasks through Chain-of-Thoughts (CoT). Experimental results show that ERA-CoT demonstrates the superior performance of our proposed method compared to current CoT prompting methods, achieving a significant improvement of an average of 5.1\% on GPT3.5 compared to previous SOTA baselines. Our analysis indicates that ERA-CoT increases the LLM's understanding of entity relationships, significantly improves the accuracy of question answering, and enhances the reasoning ability of LLMs.}","The top of the figure represents the standard prediction process. The bottom of the figure shows the five-step inference process of ERA-CoT, which relies on the extraction of entities and the inference and analysis of relationships between entities to obtain the results.","Large language models (LLMs) have shown remarkable in-context learning capabilities in various natural language processing (NLP) tasks, including machine translation, question answering, and named entity extraction, etc. Recently, prompting strategies like Chain-of-Thought (CoT) have garnered attention due to their capacity to significantly enhance LLMs reasoning capabilities. Considering the ability of CoT to guide LLMs in breaking down complex reasoning processes into simple steps, it stands out compared to standard zero-shot and few-shot methods. However, due to the presence of numerous entities such as characters, locations, etc., and the multitude of implicit relationships among them in certain scenarios, CoT still faces significant challenges in handling these situations. Named Entity Recognition (NER) has typically been employed when addressing these tasks. NER is a sequence labeling task in nature, where the model needs to assign an entity-type label to each token within a sentence . Relation extraction is a category of methods for handling entity relationships within text passages. Various studies have also investigated the performance of LLMs in zero-shot relation extraction. However, without additional prompts, LLMs have limited entity and relation extraction capabilities. Considering the importance of contextual content in answering questions, addressing knowledge-intensive tasks also requires a comprehensive analysis of entity relationships. In this paper, we propose Entity Relationship Analysis with Chain-of-Thought (ERA-CoT), a novel framework to better address reasoning tasks in complex entity scenarios. First, we extract all the entities involved in the text; second, we extract the directly mentioned explicit relationships between entities based on the text; then, we infer the indirect implicit relationships between entities based on these explicit relationships and the hidden information in the text; after that, we let the model score the implicit relationships based on the reliability of the relationships, set a threshold for judging the reliability of the relationships, and eliminate the implicit relationships that are lower than the threshold; finally, answer the questions based on the previously extracted entities and the obtained implicit and explicit relationships. We conducted experiments on six widely adopted datasets and compared with four baseline methods. The results show that ERA-CoT outperforms baselines on nearly all benchmarks, achieving a significant improvement of about 5.1\ Our main contributions can be summarized as follows. {itemize}[leftmargin=1em] [$$] We introduce ERA-CoT, a novel framework designed to conduct relationship analysis among multiple entities within complex scenarios during the zero-shot problem-solving process, which significantly strengthens the reasoning and comprehension abilities of LLMs. [$$] Our method extends entity relationship analysis and relation extraction to CoT. It is capable of both further complex relationship inference after entity extraction in NER, and step-by-step accurate logical analysis for any complex scenario on a zero-shot setting. [$$] Compared to baselines, we achieved an accuracy improvement of approximately 7.3\ {itemize"
On the Multi-turn Instruction Following for Conversational Web Agents,2402.15057v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.15057v1_0.pdf,"Web agents powered by Large Language Models (LLMs) have demonstrated remarkable abilities in planning and executing multi-step interactions within complex web-based environments, fulfilling a wide range of web navigation tasks. Despite these advancements, the potential for LLM-powered agents to effectively engage with sequential user instructions in real-world scenarios has not been fully explored. In this work, we introduce a new task of Conversational Web Navigation, which necessitates sophisticated interactions that span multiple turns with both the users and the environment, supported by a specially developed dataset named Multi-Turn Mind2Web (MT-Mind2Web). To tackle the limited context length of LLMs and the context-dependency issue of the conversational tasks, we further propose a novel framework, named self-reflective memory-augmented planning (Self-MAP), which employs memory utilization and self-reflection techniques. Extensive experiments are conducted to benchmark the MT-Mind2Web dataset, and validate the effectiveness of the proposed method..}",Illustrations of different problems.,"A longstanding objective in artificial intelligence is to develop AI agents that can execute complex tasks, thereby minimizing human effort in routine activities. With the advent of Large Language Models (LLMs), LLM-powered agents showcase exceptional planning capabilities in performing multi-turn interactions with diverse environments, which contribute to various real-world problem-solving. As shown in Figure (a), the web agent is designed to interpret the states of a webpage and execute a series of actions using keyboard and mouse inputs. Its purpose is to accomplish the tasks defined in natural language, such as booking tickets, through multi-turn interactions with the web-grounded environment. Despite the proficiency in executing each individual instruction, the capability of interacting with multi-turn user instructions remains under-explored, which is crucial for applying LLM-powered agents onto real-world applications. As the example shown in Figure (c), during a conversational web navigation session, users tend to request follow-up or co-referencing instructions without repeating previous information. They may also provide a succinct or brief instruction, which is similar to other conversation problems. Motivated by recent efforts on the investigation of conversational capabilities in the interactions with human users for LLMs, we propose a novel task, named {Conversational Web Navigation}. It requires the multi-turn interaction capabilities with both users and environment. In particular, we introduce a new dataset, named Multi-Turn Mind2Web (MT-Mind2Web). MT-Mind2Web is constructed by using the single-turn interactions from Mind2Web , an expert-annotated web navigation dataset, as the guidance to construct conversation sessions. In other conversational tasks, LLMs can answer conversational questions by utilizing their inherent knowledge from pretrained data or retrieval techniques to assess external databases (Figure (b)). Compared with these tasks, the conversation history in conversational web navigation contains both the previous user-agent and agent-environment interactions, as the instruction completion relies on the dynamic environment status. Therefore, the history context can be much longer and noisier than that in the traditional conversation problems. In light of these challenges, we propose a novel framework, named self-reflective memory-augmented planning (Self-MAP). This framework is designed to maximize the utility of the limited memory space ({i.e.}, input length limitation) of LLM-powered agents addressing the conversational web navigation problem. Specifically, we first construct a memory bank using the conversational interaction history, where each memory snippet stores each interaction step at each conversation turn. To reduce the noise from previous interactions, we propose a multifaceted matching approach to retrieve memory snippets that are semantically relevant and have similar trajectories. Furthermore, we design a reflection module to simplify the retrieved memory snippets by filtering out irrelevant information from the environment state. We then refine the retrieved memory snippets by generating reasoning rationales to enrich the memory information. Finally, we plan the next action by utilizing the self-reflective memory. To sum up, our contributions are as follows: {itemize}[leftmargin=*] To study the multi-turn instruction-following capability of web agents, we define the problem of conversational web navigation and introduce a novel dataset, namely MT-Mind2Web. We propose a self-reflective memory-augmented planning method (Self-MAP) that combines memory utilization and self-reflection for tackling the underlying challenges in the conversational web navigation task. We benchmark the MT-Mind2Web dataset with extensive baselines and provide comprehensive evaluations on different settings. Experimental results also validate the effectiveness of the proposed method. {itemize"
Mobile-Bench: An Evaluation Benchmark for LLM-based Mobile Agents,2407.00993v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2407.00993v1_0.pdf,"With the remarkable advancements of large language models (LLMs), LLM-based agents have become a research hotspot in human-computer interaction. However, there is a scarcity of benchmarks available for LLM-based mobile agents. Benchmarking these agents generally faces three main challenges: (1) The inefficiency of UI-only operations imposes limitations to task evaluation. (2) Specific instructions within a singular application lack adequacy for assessing the multi-dimensional reasoning and decision-making capacities of LLM mobile agents. (3) Current evaluation metrics are insufficient to accurately assess the process of sequential actions. To this end, we propose Mobile-Bench, a novel benchmark for evaluating the capabilities of LLM-based mobile agents. First, we expand conventional UI operations by incorporating 103 collected APIs to accelerate the efficiency of task completion. Subsequently, we collect evaluation data by combining real user queries with augmentation from LLMs. To better evaluate different levels of planning capabilities for mobile agents, our data is categorized into three distinct groups: SAST, SAMT, and MAMT, reflecting varying levels of task complexity. Mobile-Bench comprises 832 data entries, with more than 200 tasks specifically designed to evaluate multi-APP collaboration scenarios. Furthermore, we introduce a more accurate evaluation metric, named CheckPoint, to assess whether LLM-based mobile agents reach essential points during their planning and reasoning steps. Dataset and platform are available at {https://github.com/XiaoMi/MobileBench}.","For the task of ``{Setting an alarm for seven thirty.}'', accomplishing it solely through UI operations requires four steps, while API calls can achieve the same task in just one step.","Interacting with mobile devices using natural language is a long-standing pursuit in human-computer interaction . With the remarkable advancements in large language models (LLM) , LLM-driven agents are at the forefront, yet their reasoning capability to navigate mobile application functionalities lags behind their proficiency with web pages on PCs . To faithfully replicate a typical mobile environment, it's imperative to incorporate a diverse set of applications and leverage authentic data, moving beyond the limitations of purely simulated scenarios. The development challenges in the mobile domain stem from a trio of core issues: a limited understanding of mobile interfaces, a scarcity of application variety, and a lack of real-world data. Due to Google's breakthrough in UI interface representation, LLM agent's understanding of UI pages becomes easier, leading to the creation of UI platforms such as Android-Env and Mobile-Env , which tasks are defined within individual games or search engines. However, these works collectively face the following challenges: (1) UI actions depend on the textual descriptions of interfaces, where structured text fails to capture the content of graphical buttons or images which can lead to wrong actions. A single API action might be equivalent to dozens of UI steps, leading to UI's inefficiency. (2) Their tasks are far removed from real-world task scenarios encountered in daily use, which require cooperation between multiple applications, with user commands being ambiguous and not specifying target applications. (3) The evaluation of tasks should not solely rely on LLMs, without any objective quantitative metrics. In fact, voice assistants on mobile phones can meet most of the users' daily needs, yet they do not interact directly with UI interfaces but operate by invoking the APIs behind applications. As shown in Figure , in mobile applications, APIs are more efficient than UI interfaces; a single API call can be equivalent to multiple UI operations to achieve the same outcome. However, a single API is insufficient for more complex tasks, especially when user commands are unclear, necessitating reliance on LLMs to interpret user intent. Therefore, an agent capable of utilizing both UI and APIs would be best suited for the job. Simultaneously, It requires developing a strategy for the selection and order of the application usage, with human oversight merely focusing on reviewing the outcomes. This is a function that voice assistants currently lack. To this end, we develop a combination of API and UI actions to circumvent the limitations of UI interfaces, each action can be chosen between UI interactions and API calls; all tasks begin from the mobile HOME page rather than from the launch page of a specific application, enabling the agent to determine single or multiple applications it will use; queries in the task are gathered from real users, and instruction generation is only applied to some complex ones which undergo rigorous manual review; we draw inspiration from objective metrics in software automation testing, named CheckPoint, and have made necessary adjustments to accommodate the unpredictable semantic outputs of LLMs. Above all, we propose a mobile phone environment that includes a platform supporting both API and UI interactions, and a corresponding dataset with multi-APP tasks. Table presents a comparison among recent platforms and benchmark work based on API and UI. Our contributions are summarized as follows:\\ (1) To the best of our knowledge, we are the first to establish a running platform for LLM-based mobile agents that simultaneously supports both UI and API calls.\\ (2) We propose an evaluation dataset containing diverse tasks for multi-APP interactions. Our tasks starting from the home page are more appropriate for testing the planning capabilities for agents. Our dataset and platform will be released soon.\\ (3) We introduce a new category-based evaluation metric to assess the task completion capabilities of the agent in the context of both UI and API interactions"
Decoder-only Streaming Transformer for Simultaneous Translation,2406.03878v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.03878v1_0.PNG;/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.03878v1_1.pdf,"Simultaneous Machine Translation (SiMT) generates translation while reading source tokens, essentially producing the target prefix based on the source prefix. To achieve good performance, it leverages the relationship between source and target prefixes to exact a policy to guide the generation of translations. Although existing SiMT methods primarily focus on the Encoder-Decoder architecture, we explore the potential of Decoder-only architecture, owing to its superior performance in various tasks and its inherent compatibility with SiMT. However, directly applying the Decoder-only architecture to SiMT poses challenges in terms of training and inference. To alleviate the above problems, we propose the first Decoder-only SiMT model, named Decoder-only Streaming Transformer (DST). Specifically, DST separately encodes the positions of the source and target prefixes, ensuring that the position of the target prefix remains unaffected by the expansion of the source prefix. Furthermore, we propose a Streaming Self-Attention (SSA) mechanism tailored for the Decoder-only architecture. It is capable of obtaining translation policy by assessing the sufficiency of input source information and integrating with the soft-attention mechanism to generate translations. Experiments demonstrate that our approach achieves state-of-the-art performance on three translation tasks}.",Comparison of Encoder-Decoder architecture and Decoder-only architecture.,"Simultaneous Machine Translation (SiMT) is designed for generating translations in real-time scenarios such as online conferences and real-time subtitles. It predicts the target tokens (i.e., target prefix) based on the already read source tokens (i.e., source prefix), aiming to achieve good tradeoffs between latency and translation quality. During training, SiMT models need to learn the correspondence between source and target prefixes, crucial for extracting policies that ensure superior performance during inference . Existing research on SiMT primarily focuses on the Encoder-Decoder architecture and is categorized into fixed and adaptive policies. For fixed policy , the model utilizes heuristic rules to determine the source prefix used for generating translations, which ignores the correspondence between the source and target prefixes. This may lead to redundant or missing source information during translation, resulting in inferior performance . For adaptive policy , the model dynamically decides whether to read or output tokens based on the relationship between the source and target prefixes. This dynamic adjustment of policy in response to the translation status allows for improved tradeoffs . However, there is a lack of exploration in SiMT regarding the Decoder-only architecture. With the rise of language models, the Decoder-only architecture has exhibited superior performance across diverse tasks . As illustrated in Figure , the Decoder-only architecture, compared to the Encoder-Decoder architecture with an equivalent number of parameters, can support more layers, thereby offering parameter efficiency and better scalability . Importantly, SiMT relies on unidirectional encoding , and the Decoder-only architecture seamlessly accommodates this requirement. Therefore, we explore the capability of Decoder-only architecture in SiMT. However, directly applying the Decoder-only architecture to the SiMT task poses challenges in both training and inference. During inference, with the arrival of each source token, there is a position increase of the generated target prefix, necessitating the model to re-encode the target prefix. This exacerbates the inference cost, particularly at low latency . During training, the model learns to predict the corresponding target prefix based on a given source prefix. Consequently, it is necessary to construct corresponding target prefixes for all possible source prefixes in a sentence pair. This limitation hinders the model from learning the translation policy and leads to an increase in training costs compared to the Encoder-Decoder architecture. To overcome the above limitations, we propose the first SiMT model based on Decoder-only architecture, named the {D}ecoder-only {S}treaming {T}ransformer (DST). To alleviate the issue of re-encoding, DST encodes the positional information of the source prefix and the target prefix separately. This ensures that the expansion of the source prefix does not impact the position of the generated target prefix, thereby reducing the inference costs. To assess the contribution of partial source information to generating target tokens, DST uses the proposed Streaming Self-Attention (SSA) in replace of the conventional masked self-attention in the Decoder layer to decrease training costs and derive a translation policy. During training, SSA can consider all possible source prefixes for the target prefixes in a sentence pair. Specifically, SSA predicts attention allocation for different source prefixes and combines it with the soft-attention mechanism to obtain expected attention for all source tokens and tokens in the target prefix. This expected attention is then utilized to derive the context vector. By leveraging SSA, the model learns the importance of all source prefixes in translating the target prefix, thereby reducing training costs. During inference, SSA accumulates the allocated attention from all prefixes of the input source tokens, enabling an assessment of the sufficiency of input source information for generating translation. The model utilizes this assessment to determine whether to read or generate tokens, thereby acquiring the translation policy. Experiments demonstrate that DST achieves state-of-the-art performance on three tasks"
I am a Strange Dataset: Metalinguistic Tests for Language Models,2401.05300v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2401.05300v2_0.png,"Statements involving metalinguistic self-reference (``This paper has six sections.'')\ are prevalent in many domains. Can current large language models (LLMs) handle such language? In this paper, we present ``I am a Strange Dataset'', a new dataset for addressing this question. There are two subtasks: {generation} and {verification}. In generation, models continue statements like ``The penultimate word in this sentence is'' (where a correct continuation is ``is''). In verification, models judge the truth of statements like ``The penultimate word in this sentence is sentence.''\ (false). We also provide minimally different metalinguistic non-self-reference examples to complement the main dataset by probing for whether models can handle metalinguistic language at all. The dataset is hand-crafted by experts and validated by non-expert annotators. We test a variety of open-source LLMs (7B to 70B parameters) as well as closed-source LLMs through APIs. All models perform close to chance across both subtasks and even on the non-self-referential metalinguistic control data, though we find some steady improvement with model scale. GPT 4 is the only model to consistently do significantly better than chance, and it is still only in the 60\% range, while our untrained human annotators score well in the 89--93\% range. The dataset and evaluation toolkit are available at .",An example highlighting the challenge presented by our task. All models that we tested on our dataset are close to chance-level.,"Self-reference plays a crucial role in the way we think about mathematics, theoretical computer science, recursive programming , understanding complex cases in hate speech detection, aptitude tests, and comedy. Some positions in the philosophy of mind consider self-referential capabilities to be a key aspect of higher intelligence or even consciousness. Of course, self-reference is also pervasive in how we communicate: at least one paper you read today is bound to contain ``In this paper''. In this paper, we focus on metalinguistic self-reference, the complex kind of self-reference in which language is used to make claims about itself, as in ``This sentence has five words'' and ``This paper has six sections''.{Sentences like ``I am Douglas Hofstadter'' are self-referential but not metalinguistic in the sense of interest here.} Using such language involves reasoning about metalinguistic properties (counting words, naming parts of speech, etc.)\ and resolving self-reference. Humans generally have no trouble with such language, and may even enjoy its playful and sometimes paradoxical nature. Recently, Large Language Models (LLMs) have demonstrated striking cognitive capabilities . But do they have the same mastery over metalinguistic self-reference as we do? See Figure~ for an example of the issue that LLMs face. To help address this question, we present a new task and dataset called ``I am a Strange Dataset''. We are inspired by Douglas Hofstadter's explorations of self-reference in language, and borrow part of the name from one of his books: ``I am a Strange Loop''. An example in ``I am a Strange Dataset'' is comprised of two self-referential statements that begin in the same way but have different endings (Figure~). One is true and one is false. Crucially, the ending flips the truth value of the overall statement. There are two subtasks: {generation} and {verification}. In generation, the model must generate the true statement and reject the false one. In verification, models judge the truth of completed statements. To complement the main self-referential data, the dataset also contains metalinguistic non-self-reference examples. These are minimally different from the main examples and serve as controls to assess whether models can reliably handle metalinguistic statements in the absence of self-reference. In addition, all the examples in the dataset are tagged by expert annotators to further aid in error analysis. ``I am a Strange Dataset'' is validated by non-expert annotators. As a group, they have agreement rates in the 89--93\ ``I am a Strange Dataset'' turns out to be so difficult that models are generally near chance both in generation and verification, and do not even succeed in the prerequisite metalinguistic non-self-reference case. That said, we do find some limited evidence that GPT 4 is getting some traction on the dataset: it is significantly above chance on all tested metrics (and seems to struggle especially with the self-referential data as compared to the non-self-referential controls). However, overall, it seems safe to say that ``I am a Strange Dataset'' poses a serious challenge for even the best present-day models"
TruthX: Alleviating Hallucinations by Editing Large Language Models in Truthful Space,2402.17811v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.17811v2_0.pdf,"Large Language Models (LLMs) sometimes suffer from producing hallucinations, especially LLMs may generate untruthful responses despite knowing the correct knowledge. Activating the truthfulness within LLM is the key to fully unlocking LLM's knowledge potential. In this paper, we propose {TruthX}, an inference-time intervention method to activate the truthfulness of LLM by identifying and editing the features within LLM's internal representations that govern the truthfulness. TruthX employs an auto-encoder to map LLM's representations into semantic and truthful latent spaces respectively, and applies contrastive learning to identify a truthful editing direction within the truthful space. During inference, by editing LLM's internal representations in truthful space, TruthX effectively enhances the truthfulness of LLM. Experiments show that TruthX improves the truthfulness of 13 advanced LLMs by an average of 20\% on TruthfulQA benchmark. Further analyses suggest that TruthX can control LLM to produce truthful or hallucinatory responses via editing only one vector in LLM's internal representations\ Llama-2-7B-Chat model with baked-in TruthX: \ Page: {https://ictnlp.github.io/TruthX-site/}}.",A case to show that TruthX can control LLM to generate truthful or hallucinatory coherent responses via editing one vector in LLM's internal representations.,"Large language models (LLMs) have demonstrated remarkable capabilities across a wide range of natural language processing (NLP) tasks . However, LLMs sometimes generate fluent, instruction-compliant yet untruthful responses, commonly referred to as ``hallucinations'' . This phenomenon significantly undermines the credibility of LLMs in applications. Mitigating hallucinations of LLMs poses a substantial challenge, as hallucinations may stem from various factors, such as blindly following instructions, noisy data, lack of knowledge and the generation process . Preceding such factors, a more fundamental issue is: {whether LLMs can consistently generate truthful responses, even when they possess the correct knowledge}? Recent researches suggest ``no'' for this question. For instance, found that LLMs can generate truthful responses in some contexts while producing hallucinations in others. and discovered that LLMs can identify the presence of hallucinations generated by themselves through self-validation. directly pointed out the existence of the generation-discrimination gap in LLMs. All these findings indicate that LLMs, even equipped with correct knowledge, are still susceptible to producing hallucinations during the generation process. Further, some works found a correlation between the LLMs' internal representations and the truthfulness of outputs , where some erroneous activations of internal representations lead LLMs to generate hallucinations even when they know the correct knowledge . Therefore, activating a well-trained LLM to generate truthful responses is the crucial first step in alleviating the hallucination of LLMs. To this end, we propose TruthX, a truthfulness enhancement approach by editing LLM's internal representations in the truthful space. To edit LLM in the truthful space without compromising its generative capabilities, TruthX decouples the LLM's internal representations into truthful and semantic latent spaces respectively using an auto-encoder. Then, TruthX employs contrastive learning to probe representations with similar semantics but opposite truthfulness and those with similar truthfulness but different semantics within these two latent spaces. During inference, TruthX effectively regulates the truthfulness of LLM by editing it in the truthful space, while ensuring that the generation capability remains intact. Figure illustrates an example of TruthX controlling LLM to generate either truthful or hallucinatory coherent responses. Experimental results show that TruthX enhances the truthfulness of 13 advanced LLMs, including Llama, Mistral, Baichuan and Chatglm, by an average of 20\ Through further analyses, we get the following findings: {itemize}[leftmargin=*,itemsep=0pt,topsep=0pt] TruthX exhibits superiority in truthfulness control. Editing LLMs along the truthful direction can enhance the truthfulness of responses, conversely, editing LLMs along the opposite direction yields highly hallucinatory responses. The truthful space extracted from homologous LLMs (i.e., trained sequentially) exhibits a high degree of similarity, so we can directly adopt a well-trained TruthX to different homologous models for truthfulness enhancement. Layer-wise analysis indicates that the representations in middle layers of LLMs exhibit a higher correlation with the truthfulness of responses. {itemize"
Investigating Multi-Hop Factual Shortcuts in Knowledge Editing of Large Language Models,2402.11900v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.11900v2_0.pdf,"Recent work has showcased the powerful capability of large language models (LLMs) in recalling knowledge and reasoning. However, the reliability of LLMs in combining these two capabilities into reasoning through multi-hop facts has not been widely explored. This paper systematically investigates the possibilities for LLMs to utilize shortcuts based on direct connections between the initial and terminal entities of multi-hop knowledge. We first explore the existence of factual shortcuts through Knowledge Neurons, revealing that: (i) the strength of factual shortcuts is highly correlated with the frequency of co-occurrence of initial and terminal entities in the pre-training corpora; (ii) few-shot prompting leverage more shortcuts in answering multi-hop questions compared to chain-of-thought prompting. Then, we analyze the risks posed by factual shortcuts from the perspective of multi-hop knowledge editing. Analysis shows that approximately 20\% of the failures are attributed to shortcuts, and the initial and terminal entities in these failure instances usually have higher co-occurrences in the pre-training corpus. Finally, we propose erasing shortcut neurons to mitigate the associated risks and find that this approach significantly reduces failures in multiple-hop knowledge editing caused by shortcuts. Code is publicly available at {https://github.com/Jometeorie/MultiHopShortcuts}.","An illustrative example of a multi-hop factual shortcut in LLMs. The LLM may have directly encoded multi-hop knowledge (red) during the pre-training phase, which results in inconsistencies after a single-hop knowledge editing.","Large Language Models (LLMs) such as ChatGPT and LLaMA-2 , have impressive world knowledge modeling and reasoning capabilities within their parameters . When leveraging these two capabilities, it is intuitively anticipated that LLMs should be capable of reliably answering multi-hop knowledge questions without any difficulty . Nonetheless, the underlying reasoning processes of LLMs in responding to multi-hop knowledge questions have not received thorough investigation. Ideally, an LLM would systematically derive each single-hop answer and culminate in the correct result. However, in reality, LLMs may leverage factual shortcuts learned from pre-training corpora to directly obtain the final answer without performing intermediate reasoning. For conventional multi-hop question answering, the consistency of the final endpoints of shortcuts and multi-hop reasoning results may not cause risks and could even remain unnoticed. However, with the constant evolution of world knowledge, knowledge editing techniques are garnering increased attention . After knowledge editing, factual shortcuts in multi-hop scenarios may cause significant inconsistency. Figure~ illustrates the potential pitfalls associated with factual shortcuts. During the pre-training phase, an LLM may have forged a direct association between the next Olympic Games and Asia. Consequently, when queried with the prompt: {``Which continent will host the next Olympic Games''}, the LLM might bypass the need for reasoning about the country and can directly furnish the correct answer. However, applying knowledge editing to the LLM, e.g., updating the host country of the Olympic Games to France, can expose a vulnerability. The persistence of the established shortcut may lead the LLM to consistently output ``{Asia}'' as the host continent even after the change, instead of the correct ``{Europe}'', thereby impeding the success of multi-hop knowledge editing. In this paper, we systematically investigate the possibilities for LLMs to utilize factual shortcuts based on direct connections between the initial and terminal entities of multi-hop knowledge. Firstly, we rethink and formalize the process through which LLMs reason about multi-hop knowledge. {We introduce the hypothesis that LLMs may leverage factual shortcuts from pre-training corpora to facilitate cross-step reasoning}. Then, we deeply explore the existence of factual shortcuts. We conduct a frequency analysis of co-occurrences between the initial subject and terminal object of multi-hop knowledge instances in pre-training corpora. Additionally, we employ Knowledge Neurons to quantify the overlap between the activated neurons for multi-hop questions and all single-hop questions. A low degree of overlap suggests that the reasoning pattern of LLMs in response to multi-hop questions is inconsistent with that of single-hop questions, indicating the presence of shortcuts. Our experiments on multi-hop knowledge reveal that: (i) Few-shot questions exhibit more shortcuts in comparison to chain-of-thought questions, suggesting that {LLMs often arrive at multi-hop knowledge answers using unexpected cross-step reasoning patterns}. (ii) Knowledge instances with a higher co-occurrence frequency between initial subjects and terminal objects tend to have more shortcuts, indicating {a strong correlation between the existence of multi-hop factual shortcuts and the word frequencies learned by LLMs during pre-training phase}. Additionally, to provide insights into the potential risks associated with multi-hop factual shortcuts, we conduct a detailed analysis of the reasons behind the failures in multi-hop knowledge editing. We find that {approximately 20\ Finally, we explore the feasibility of employing Knowledge Neurons to eliminate factual shortcuts. We erase crucial neurons associated with factual shortcuts that co-occurred more than 10 times in the pre-training corpus. Results show that {the failure rate of multiple-hop knowledge editing caused by shortcuts significantly decreased, leading to an overall improvement in the success rate after our erasing approach}. We hope this work can facilitate increased interest in exploring the multi-hop reasoning capabilities of LLMs and constrain reasoning shortcuts during the pre-training stage"
Multimodal Table Understanding,2406.08100v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.08100v1_0.pdf,"Although great progress has been made by previous table understanding methods including recent approaches based on large language models (LLMs), they rely heavily on the premise that given tables must be converted into a certain text sequence (such as Markdown or HTML) to serve as model input. However, it is difficult to access such high-quality textual table representations in some real-world scenarios, and table images are much more accessible. Therefore, how to directly understand tables using intuitive visual information is a crucial and urgent challenge for developing more practical applications. In this paper, we propose a new problem, multimodal table understanding, where the model needs to generate correct responses to various table-related requests based on the given table image. To facilitate both the model training and evaluation, we construct a large-scale dataset named MMTab, which covers a wide spectrum of table images, instructions and tasks. On this basis, we develop Table-LLaVA, a generalist tabular multimodal large language model (MLLM), which significantly outperforms recent open-source MLLM baselines on 23 benchmarks under held-in and held-out settings. The code and data is available at .$ Indicates equal contribution.} $This work was done during an internship at Baidu Inc.} $ Corresponding author: Zheng Lin.}",An overall performance comparison of Table-LLaVA 7B and existing MLLMs on various multimodal table understanding benchmarks. Table-LLaVA outperforms recent open-source MLLMs and is even competitive with the powerful GPT-4V on most tasks.,"Tables are widely used to store and present data across various fields, e.g., financial analysis, scientific research and government reports. To make the most of the abundant tabular data, the table understanding (TU) technique has been proposed to automatically understand tables and perform table-based tasks, such as question answering and text generation. As a technique that could significantly elevate work efficiency in different industries, it has attracted ever-increasing research interest in recent years. Though considerable efforts have been dedicated to the table understanding problem, most previous models can only fulfill very limited tasks until the emergence of large language models (LLMs). With the help of powerful LLMs, we are getting closer to the vision that a versatile model can perform a variety of table-based tasks. However, existing table-oriented LLMs rely heavily on the prerequisite that all given tables must be converted into a certain text sequence (like Markdown or HTML) to be input to LLMs. Under some practical scenarios like scanned documents and webpage screenshots, it is difficult to obtain such high-quality textual table representations, and yet table images are more accessible. Moreover, humans can directly understand two-dimensional tables using the intuitive visual information, whereas LLMs can only interpret tables in a one-directional textual perspective, which may increase the difficulty of comprehending diverse table structures and colored table elements. In summary, for the sake of convenience and intuitiveness, it is a crucial and urgent challenge to explore how to directly digest tables using visual information. To promote the advancement of table understanding and its real-world applications, we propose the {multimodal table understanding} problem, where the model is required to generate correct responses to different table-related requests (e.g., questions) in an end-to-end fashion based on the table image. Despite the fact that recent multimodal large language models (MLLMs) have demonstrated excellent capabilities in many multimodal tasks, they cannot be directly extended to the proposed task. As shown in Figure , the performance of popular MLLMs like MiniGPT-4 and BLIP2 is close to zero on most tasks, revealing their weakness in understanding tabular data. More importantly, there is a lack of a comprehensive dataset that can support both the development and evaluation of generalist MLLMs towards multimodal table understanding. To address the above issue, we construct {MMTab}, the first open-source large-scale dataset for multimodal table understanding problem, based on 14 publicly available table datasets of 8 domains. We carefully design scripts to convert original textual tables in these datasets into table images highlighting a broad coverage of table structures and styles, and transform all task-specific samples into multimodal instruction-tuning samples with a unified format of {<table image, input request, output response>}. The resulting dataset contains (1) 150K table recognition samples on 97K table images for pre-training (named {MMTab-pre}). (2) 232K samples of 14 table-based tasks on 82K table images for instruction tuning (named {MMTab-instruct}). (3) 49K test samples on 23K table images composing 17 held-in and 7 held-out benchmarks (named {MMTab-eval}). During the dataset construction, data augmentations at multiple levels (e.g., table-level, task-level) were adopted to further improve the data diversity, and we also introduce multimodal table structure understanding tasks that have been overlooked in previous studies. Based on the curated dataset, we develop a versatile tabular MLLM named {Table-LLaVA} with an enhanced two-stage training paradigm. In the first stage, we pre-train LLaVA-1.5 with an extra table recognition task on the MMTab-pre, which requires the model to generate textual sequences (like HTML) given table images. This stage aligns the structures and elements within table images to textual modality and thus enhances the comprehension of the basic table structure and content. In the second stage, we continue to instruction-tuning the model with diverse table-based downstream tasks on the MMTab-instruct, which endows the model with the multimodal instruction-following ability for table-related requests. We compare Table-LLaVA with a series of open-source (M)LLMs and closed-source GPT-4V. Experimental results show that Table-LLaVA beats strong MLLM baselines on 17 held-in and 6 held-out benchmarks, and is even competitive with the powerful GPT-4V on 14 benchmarks with a subset of test samples. Extensive ablation experiments are conducted to reveal the contributions of different training data (e.g., the influence of table recognition pre-training data). We also explore the mutual influence between model's capacity for tabular tasks and non-tabular tasks. We hope this work could establish a strong base for future research on the multimodal table understanding problem and facilitate the progress of more general MLLMs. We conclude our contributions as follows: 1) We make the first systematic exploration of the multimodal table understanding problem, which is complementary to the traditional text-only problem setting. 2) Accordingly, we construct and release a large-scale dataset MM-Tab which covers diverse tables and data for different tasks, including a series of novel table structure understanding tasks. 3) We develop a versatile tabular MLLM Table-LLaVA, which significantly outperforms a range of strong MLLM baselines under both held-in and held-out settings (Figure )"
WatME: Towards Lossless Watermarking Through Lexical Redundancy,2311.09832v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2311.09832v3_0.png,"Text watermarking has emerged as a pivotal technique for identifying machine-generated text. However, existing methods often rely on arbitrary vocabulary partitioning during decoding to embed watermarks, which compromises the availability of suitable tokens and significantly degrades the quality of responses. This study assesses the impact of watermarking on different capabilities of large language models (LLMs) from a cognitive science lens. Our finding highlights a significant disparity; knowledge recall and logical reasoning are more adversely affected than language generation. These results suggest a more profound effect of watermarking on LLMs than previously understood. To address these challenges, we introduce Watermarking with Mutual Exclusion (WatME), a novel approach leveraging linguistic prior knowledge of inherent lexical redundancy in LLM vocabularies to seamlessly integrate watermarks. Specifically, WatME dynamically optimizes token usage during the decoding process by applying a mutually exclusive rule to the identified lexical redundancies. This strategy effectively prevents the unavailability of appropriate tokens and preserves the expressive power of LLMs. We provide both theoretical analysis and empirical evidence showing that WatME effectively preserves the diverse capabilities of LLMs while ensuring watermark detectability. Our code will be released to facilitate future research. via .","An illustration of WatME's advantage for lossless watermarking. The left panel depicts a vanilla LM with all words available during generation. The middle panel exposes the flaw in vanilla watermarking, which may assign all suitable tokens (e.g., 'ocean' and 'sea') to the red list, diminishing text quality. The right panel underlines how WatME exploits lexical redundancy by applying a mutual exclusion rule between such words, ensuring at least one suitable word remains on the green list, thereby improving text quality.","The advent of large language models with human-level generative capabilities presents tremendous opportunities across diverse domains . However, their ability to synthesize high-quality text also raises widespread concerns about potential misuse, including the dissemination of misinformation and the facilitation of academic dishonesty . This necessitates developing techniques to reliably attribute generated text to AI systems. Existing approaches typically fall into two main paradigms. The first type attempts to distinguish machine-generated text by hunting for inductive statistical or linguistic patterns , employing methods that span from basic manual feature engineering to the intricate training of complex classifiers. However, as generative models continue improving, their outputs increasingly resemble the pattern of human writing, rendering statistical detectors ineffective . The second paradigm promotes a more proactive approach, advocating for direct intervention in the generative process to actively watermark model outputs . This strategy embeds identifiable fingerprints within machine-generated text, enabling provenance verification. As LLMs' capabilities continue to grow, this approach is more effective in detecting LLM-generated text . However, introducing watermarks during text generation can significantly impact output quality, which has been a consistent challenge for model developers - how to effectively watermark while preserving text quality. Recent studies have attempted to improve text quality by ensuring unbiased output distributions in watermarking , while employing pseudorandomness-guided perturbations or reweighting to adjust the original output distributions of LLMs. However, an unbiased distribution in expectation does not guarantee high text quality, and the use of these techniques reduces the effectiveness of watermark detection, especially in models that have undergone alignment training , thereby diminishing the practical utility of these methods. In this paper, we introduce a novel approach to text watermarking by leveraging engineered lexical redundancy during the decoding phase of language generation. Our method utilizes the comprehensive set of tokens available to a language model, clustering them based on overlapping semantic or syntactic functionalities to create sets of interchangeable tokens. This process simulates redundancy within the lexical space, akin to the surplus pixels in images that facilitate watermarking in multimodal data . The motivation for this strategy arises from the challenge of applying traditional watermarking techniques to textual data. In contrast to the inherent redundancy found in images, the discrete and succinct nature of textual data offers little to no native redundancy, making it challenging to exploit redundancy in the textual space . By engineering lexical redundancy, our method not only surmounts the limitations imposed by the inherent properties of natural language but also paves the way for secure and efficient text watermarking. After exploring these redundancies, we exploit them via our novel algorithm, WatME, which enhances text quality by integrating a mutual exclusivity rule within the context of lexical redundancy during the watermarking process. Specifically, WatME refines the decoding process by explicitly assigning words within each redundant cluster to distinct 'green' or 'red' teams, ensuring that no single cluster is wholly allocated to one team. Our approach confers two main advantages: (1) it enables the 'green' team to capture a broader array of semantics, thereby boosting the model's expressive power; and (2) it increases the probability that the LLM selects the most appropriate word at each decoding step, e.g., in Figure , vanilla watermarking can assign all suitable words to the 'red' list, thus severely impairing performance. In contrast, our approach guarantees the presence of at least one appropriate word, thus preserving the model's expressiveness. Building on these methodological advances, extensive theoretical and empirical evidence supports their effectiveness without compromising detection capabilities. These improvements significantly bolster the emergent abilities of large models under watermarks, surpassing the performance of baseline methods. Our main contributions are as follows: {itemize}[leftmargin=*] Motivated by multimedia data's inherent redundancy and the precise conciseness of text, we propose two distinct approaches for mining {{lexical redundancy}}. We develop the WatME algorithm, which embeds mutual exclusion rules within the lexical space for text watermarking. Theoretical analysis is presented to validate its effectiveness in preserving the quality of text responses. Experimental results show that WatME effectively outperforms existing methods in retaining the emergent capabilities of LLMs, notably knowledge recall and logical reasoning, within the conceptual framework of Cattell's cognitive theory, without compromising detectability. {itemize"
Multi-Aspect Controllable Text Generation with Disentangled Counterfactual Augmentation,2405.19958v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2405.19958v1_0.pdf,"Multi-aspect controllable text generation aims to control the generated texts in attributes from multiple aspects (e.g., ``positive'' from {sentiment} and ``sport'' from {topic}). For ease of obtaining training samples, existing works neglect attribute correlations formed by the intertwining of different attributes. Particularly, the stereotype formed by imbalanced attribute correlations significantly affects multi-aspect control. In this paper, we propose , a new {m}ulti-{a}spect controllable text {g}eneration method with d{i}sentangled {c}ounterfactual augmentation. We alleviate the issue of imbalanced attribute correlations during training using counterfactual feature vectors in the attribute latent space by disentanglement. During inference, we enhance attribute correlations by target-guided counterfactual augmentation to further improve multi-aspect control. Experiments show that outperforms state-of-the-art baselines in both imbalanced and balanced attribute correlation scenarios. Our source code and data are available at .",The relevance scores of positive and negative sentiment in (a) AGNews and (b) Yelp. (a) The classifiers used for statistics are from \cite{discrete2022Gu}. (b) The statistical data of Yelp are from \cite{tailor2023Yang}.,"Controllable text generation (CTG) aims to generate texts adhering to given constraints reliably. The development of generative AI based on large language models (LLMs) draws increasing attention to CTG . Due to the demand for diverse attribute control, recent studies focus on a more practical and challenging setting, {multi-aspect controllable text generation} (MCTG). Different kinds of methods have been proposed, including weighted decoding , optimization in the language space , optimization in the latent semantic space , etc. Due to the difficulty of directly obtaining training data that satisfy arbitrary attribute combinations, existing methods reuse datasets with single-aspect annotations for MCTG, where each training sample only expresses a single attribute in one aspect. This neglects the fact that a sentence often couples multiple attributes due to the complexity of natural language. The co-occurrence of attributes within one sentence forms patterns corresponding to attribute correlations, serving as crucial dependencies for a generative model in inference. Meanwhile, the training corpus is derived from real life, where preferences in real life make certain combinations of attributes more common, leading to an imbalance in attribute correlations. As an example shown in Figure~, in the AGNews dataset, since news with the topics of ``world'' and ``business'' are prone to correlating with negative elements , such as war or inflation, combinations of these topics and negative sentiment are more prevalent. In the Yelp dataset consisting of restaurant reviews with sentiment and food types, negative reviews also dominate . The imbalance in attribute correlations can lead the model to associate specific attributes, forming a stereotype that impacts multi-aspect control. An MCTG model can better fit attributes with higher co-occurrence frequencies, allowing it to learn the semantic information of these attributes more comprehensively. However, the model may neglect the learning of attributes with low co-occurrence frequencies, which hurts the control of these attribute combinations. To resolve the problem, we propose a {m}ulti-{a}spect controllable text {g}eneration method with d{i}sentangled {c}ounterfactual augmentation, called . Specifically, we introduce attribute disentanglement with latent space optimization. It can disentangle the control factors of different attributes in the texts and generate the latent vectors with counterfactual features in the attribute latent space. During training, we employ counterfactual latent vectors to balance attribute correlations, thereby constructing a more semantically balanced attribute latent space. During inference, we enhance attribute correlations by the counterfactual latent vectors to improve multi-aspect control. We experiment on three-aspect control including {sentiment}, {topic}, and {detoxification}. We evaluate the relevance scores of attributes and assess the text quality in the scenarios of imbalanced and balanced attribute correlations. The results indicate that can leverage attribute correlations and mitigate the imbalance issues, which leads to steady and superior performance in both imbalanced and balanced scenarios than state-of-the-art baselines on multi-aspect control. We further demonstrate the effectiveness of each module in through analytical experiments. Our main contributions are outlined as follows: {compactitem} To mitigate the issue of imbalanced attribute correlations, we propose a counterfactual feature augmentation model with attribute disentanglement. To improve multi-aspect control by leveraging attribute correlations, we introduce a text generation method based on target-guided attribute correlation augmentation. We experimentally validate the effectiveness of . It outperforms state-of-the-art baselines on the imbalanced and balanced settings of multi-aspect control. {compactitem"
Reward-based Input Construction for Cross-document Relation Extraction,2405.20649v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2405.20649v1_0.pdf,"Relation extraction (RE) is a fundamental task in natural language processing, aiming to identify relations between target entities in text. While many RE methods are designed for a single sentence or document, cross-document RE has emerged to address relations across multiple long documents. Given the nature of long documents in cross-document RE, extracting document embeddings is challenging due to the length constraints of pre-trained language models. Therefore, we propose REward-based Input Construction (REIC), the first learning-based sentence selector for cross-document RE. REIC extracts sentences based on relational evidence, enabling the RE module to effectively infer relations. Since supervision of evidence sentences is generally unavailable, we train REIC using reinforcement learning with RE prediction scores as rewards. Experimental results demonstrate the superiority of our method over heuristic methods for different RE structures and backbones in cross-document RE. Our code is publicly available at .","An illustrated comparison between Snippet and selected sentences using our REward-based Input Construction (REIC) for cross-document relation extraction. The figure depicts an example triplet (\texttt{Kubuntu}, \texttt{x86-64}, platform) with the text path (`Mir (software)', `X86-64'), including three bridge entities ({Ubuntu}, {Linux}, {Intel}) abbreviated as (Bridge1, Bridge2, Bridge3). Dash and solid arrows signify the selection process of Snippet and REIC, respectively, while gradient-colored arrows indicate connections between the head and tail entities. REIC selects important sentences from any position within a path to determine the relation between the head and tail entity, whereas Snippet only includes sentences located around the head or tail entity.","The task of relation extraction (RE) aims to identify relations between a pair of target entities in a given text. This task is fundamental in natural language processing (NLP) and provides crucial information for applications such as information retrieval and question answering. Most existing RE methods are limited to scenarios where the entity pair is within a single sentence or a single document. However, many scenarios require extracting relations across multiple documents, where the target entity pair may not coexist within the same document. Therefore, recent efforts to address these challenges have led to the proposal of cross-document RE by , and research in this area has received considerable attention. Unlike traditional RE research, cross-document RE involves extracting relational facts from large-scale long documents. For example, the DocRED dataset, developed for document-level RE, contains an average of 198 words per document. In contrast, the CodRED dataset, designed for cross-document RE, has an average of 2,416 words per document. Given the substantial length of documents in cross-document RE tasks, extracting document embeddings from pre-trained language models, a step common to all methods, poses considerable challenges. This is because pre-trained language models are limited by the maximum number of tokens they can process; for example, BERT typically has a limit of 512 tokens. A simple approach adopted in several studies is to use text snippets around the target entities as input to pre-trained language models, as shown in the dashed boxes in the middle panel of {fig:example}. Often, this text snippet is determined by either word proximity or document structure without any adjustment. However, if only a subset of sentences around the entities is extracted from long documents, there is a risk of missing important sentences that are crucial for RE. For example, as seen in the left panel of {fig:example}, using snippets around the entities fails to capture the relation `platform' between the entity pair ({Kubuntu}, {x86-64}). In this paper, we introduce REward-based Input Construction (REIC), the first learning-based input sentence selection module specifically designed for cross-document RE. The right panel of {fig:example} displays the sentences selected by our model from the example. By identifying relational evidence sentences through the selection module, the RE module infers the relation between the target entities using the reasoning chain illustrated by the gradient-colored arrows in {fig:example}. Our approach is to develop a sentence selection module that computes the probability of selecting sentences based on the currently selected sentences. We show that this sentence selection process can be modeled as a Markov decision process (MDP). We specifically choose to use MDP because there is no supervision for sentence selection from corpus, so RE models have to perform iterative learning by trials. Subsequently, we train the sentence selection module using reinforcement learning (RL), where the relation prediction score obtained from the selected sentences serves as the reward. Through experimental validation, REIC outperforms other heuristic methods across various RE modules"
Semi-Supervised Spoken Language Glossification,2406.08173v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.08173v1_0.pdf,"Spoken language glossification (SLG) aims to translate the spoken language text into the sign language gloss, {i.e.}, a written record of sign language. In this work, we present a framework named $S$emi-$S$upervised $S$poken $L$anguage $G$lossification ($S^3$LG) for SLG. To tackle the bottleneck of limited parallel data in SLG, our $S^3$LG incorporates large-scale monolingual spoken language text into SLG training. The proposed framework follows the self-training structure that iteratively annotates and learns from pseudo labels. Considering the lexical similarity and syntactic difference between sign language and spoken language, our $S^3$LG adopts both the rule-based heuristic and model-based approach for auto-annotation. During training, we randomly mix these complementary synthetic datasets and mark their differences with a special token. As the synthetic data may be less quality, the $S^3$LG further leverages consistency regularization to reduce the negative impact of noise in the synthetic data. Extensive experiments are conducted on public benchmarks to demonstrate the effectiveness of the $S^3$LG. Our code is available at .","Overview of the proposed $S^3$LG. It iteratively annotates and learns from pseudo labels to obtain the final SLG model $f(\hat{\theta}^K)$ after total $K$ iterations. For the $k$-th iteration, the synthetic data $\mathcal{D}_U^{k-1}$ is conducted by randomly mixing the two complementary pseudo glosses generated by the fixed rules and the previously obtained SLG model $f(\hat{\theta}^{k-1})$, respectively. Note that at the first iteration, only the rule-based synthetic data is available.","Sign Language is the most primary means of communication for the deaf. Translating between sign and spoken language is an important research topic, which facilitates the communication between the deaf and the hearing. To support the development of applications, sign language gloss has been widely used as an intermediate step for generating sign language video from spoken language text or the inverse direction. The sign language gloss is the written representation of the signs. As a generally adopted way for sign language transcription, gloss is sufficient to convey most of the key information in sign language. In this work, we focus on the first step of the former task named spoken language glossification (SLG), which aims to translate the spoken language text into the sign language gloss. SLG is typically viewed as a low-resource sequence-to-sequence mapping problem. The previous methods rely on the encoder-decoder architectures to jointly align the embedding space of both languages in a data-driven manner. Since the data collection and annotation of sign language requires specialized knowledge, obtaining a large-scale text-gloss dataset is time-consuming and expensive. As a result, the performance of SLG models is limited by the quantity of parallel data. Witnessing the success of introducing monolingual data to enhance low-resource translation quality in neural machine translation (NMT), we are motivated to explore the accessible unlabeled spoken language texts to improve SLG. In this work, we present a framework named $S$emi-$S$upervised $S$poken $L$anguage $G$lossification ($S^3$LG) to boost SLG, which iteratively annotates and learns from pseudo labels. To implement the above idea, the proposed $S^3$LG adopts both the rule-based heuristic and model-based approach to generate pseudo glosses for unlabeled texts. The rule-based synthetic data has high semantic accuracy, however, the fixed rules make it difficult to cover complex expression scenarios. The model-based approach on the other hand is more flexible for learning the correspondence between sign language and spoken language and generates pseudo gloss with higher synthetic diversity. These complementary synthetic datasets are randomly mixed as a strong supplement for the training of the SLG model. Besides, the model-based synthetic data is generated by the SLG model, which sets a good stage for iteratively re-training the SLG model. In addition, $S^3$LG introduces a simple yet efficient design from three aspects. Firstly, in each iteration, the training process is separated into two stages, {i.e.}, pre-training and fine-tuning for domain adaptation. Secondly, to encourage the model to learn from the noisy pseudo labels, we apply the consistency regularization term to the training optimization and gradually increase the weight of the consistency regularization in the training curriculum. It enforces the consistency of the predictions with network perturbations based on the manifold assumption. Thirdly, to encourage the SLG model to learn complementary knowledge from different types of synthetic data, a special token is added at the beginning of input sentences to inform the SLG model which data is generated by the rule-based or model-based approach. Through end-to-end optimization, our $S^3$LG achieves significant performance improvement over the baseline. Surprisingly, the experiments show that the translation accuracy on low-frequency glosses is promisingly improved. We conjecture that the SLG model acts differently in annotating the high-frequency and low-frequency glosses, and such bias is mitigated by the rule-based synthetic data. In summary, our contributions are three-fold: {itemize} [$$] We propose a novel framework $S^3$LG for SLG (namely, text-to-gloss translation), which iteratively annotates and learns from the synthetic data. It adopts two complementary methods, {i.e.}, the rule-based heuristic and model-based approach for auto-annotation. {itemize} {itemize} [$$] We further leverage consistency regularization to reduce the negative impact of noise in the synthetic data. The biases of the SLG model on low-frequency glosses are mitigated by incorporating the rule-based synthetic data. {itemize} {itemize} [$$] We conduct extensive experiments to validate our approach, which shows encouraging performance improvement on the two public benchmarks, {i.e.}, CSL-Daily and PHOENIX14T. {itemize"
SeeClick: Harnessing GUI Grounding for Advanced Visual GUI Agents,2401.10935v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2401.10935v2_0.pdf,"Graphical User Interface (GUI) agents are designed to automate complex tasks on digital devices, such as smartphones and desktops. Most existing GUI agents interact with the environment through extracted structured data, which can be notably lengthy (e.g., HTML) and occasionally inaccessible (e.g., on desktops). To alleviate this issue, we propose a novel visual GUI agent -- {SeeClick}, which only relies on screenshots for task automation. In our preliminary study, we have discovered a key challenge in developing visual GUI agents: GUI grounding -- the capacity to accurately locate screen elements based on instructions. To tackle this challenge, we propose to enhance {SeeClick} with GUI grounding pre-training and devise a method to automate the curation of GUI grounding data. Along with the efforts above, we have also created , the first realistic GUI grounding benchmark that encompasses mobile, desktop, and web environments. After pre-training, {SeeClick} demonstrates significant improvement in over various baselines. Moreover, comprehensive evaluations on three widely used benchmarks consistently support our finding that advancements in GUI grounding directly correlate with enhanced performance in downstream GUI agent tasks. .}","Text-based agents select target elements from structured texts, occasionally augmented with screenshots. {SeeClick} employs a vision-based methodology to predict action locations solely relying on screenshots.","A perennial topic in machine intelligence is the development of Graphical User Interface (GUI) agent systems, like Siri and Copilot, to automate complex tasks on computing devices, thereby reducing human workload . Recent advances in Large Language Models (LLMs) such as GPT-4 have significantly propelled the evolution of GUI agents. These agents interact with the environment by interpreting structured texts, e.g., HTML from webpages, then elicit LLM for planning, reasoning, and execution. However, GUI agents depend on structured text face three inherent limitations: (1) Structured text is not always accessible, especially for iOS or desktop applications where acquiring such information is challenging; (2) The verbose nature of structured text constitutes an inefficient context for LLMs, while also omitting crucial information such as layout, images, and icons; (3) The variety of structured text - including HTML, DOM, and Android VH - necessitates the curation of task-specific observation and action spaces. These entrenched deficiencies in text-based approaches call for an alternative solution. In this paper, we introduce , a visual GUI agent built on Large Vision-Language Models (LVLMs). Inspired by human interaction with GUIs, as illustrated in {fig:main_figure}, is designed to perform low-level actions like clicking or typing directly by observing interface screenshots. This innovative approach bypasses the interaction with cumbersome structured text, empowering to universally adapt to various GUI platforms. Building such visual agents presents a foundational challenge: GUI grounding - the capacity to accurately locate screen elements based on instructions, which is absent in current LVLMs. To tackle this challenge, enhances LVLM with a GUI grounding pre-training strategy. We devise a method to automate the curation of web grounding data and adapt public mobile UI datasets to obtain mobile grounding data. employs the above-curated dataset for continual pre-training of the LVLM, enabling it to accurately locate elements such as text, widgets, and icons in various GUI environments. Given GUI grounding is a fundamental yet underexplored capacity for GUI agents, we establish , the first realistic GUI grounding evaluation benchmark across various GUI platforms. contains over 600 screenshots and 1200 instructions from iOS, Android, macOS, Windows, and webpages, and specifically includes both text-based elements and a variety of widgets and icons. Evaluation results confirm 's superiority over current LVLMs, validating the effectiveness of GUI grounding pre-training. Finally, we adapt to mobile and web agent tasks, including MiniWob, AITW, and Mind2Web. As a purely vision-based agent, achieves impressive performance. It surpasses the strong visual baseline Pix2Act while utilizing merely 0.3\ Moreover, experimental results on these three benchmarks consistently support our findings that improvement in GUI grounding directly correlates with enhanced agent task performance. Our main contributions are as follows: {itemize}[itemsep=2pt,topsep=3pt,parsep=0pt,leftmargin=*] We develop a unified visual GUI agent , which solely relies on interface screenshots to perform clicking and typing actions across diverse GUI platforms. We prospectively explore GUI grounding for visual GUI agents, and enhanced with proposed GUI grounding pre-training strategy. We create a realistic GUI grounding benchmark , encompassing more than 1200 instructions from various GUI platforms. Experimental results on and three agent tasks demonstrate that enhancing agents' grounding capacity is key to improving performance in downstream agent tasks. {itemize"
Whose Preferences? Differences in Fairness Preferences and Their Impact on the Fairness of AI Utilizing Human Feedback,2406.05902v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.05902v1_0.pdf,"There is a growing body of work on learning from human feedback to align various aspects of machine learning systems with human values and preferences. We consider the setting of fairness in content moderation, in which human feedback is used to determine how two comments --- referencing different sensitive attribute groups --- should be treated in comparison to one another. With a novel dataset collected from Prolific and MTurk, we find significant gaps in fairness preferences depending on the race, age, political stance, educational level, and LGBTQ+ identity of annotators. We also demonstrate that demographics mentioned in text have a strong influence on how users perceive individual fairness in moderation. Further, we find that differences also exist in downstream classifiers trained to predict human preferences. Finally, we observe that an ensemble, giving equal weight to classifiers trained on annotations from different demographics, performs better for different demographic intersections; compared to a single classifier that gives equal weight to each annotation. {{Warning: This paper discusses examples of content that may be offensive or disturbing.}}","Example tasks in our survey, asking people about their fairness preferences and their guess of the average American answer. Each task contains a pair of sentences; sentences in a pair differ along a sensitive attribute such as religion, gender, etc.","With artificial intelligence (AI) and machine learning (ML) systems being deployed in wide-ranging scenarios, there are growing calls for safe, responsible and trustworthy development of these systems. An idea in this space that is receiving a lot of attention lately is that of learning from human feedback for making these systems more aligned with human values and preferences. The idea is certainly promising, as human preferences about values like fairness can often not be expressed as simple mathematical constraints, but there also remain several important open questions about the idea. In this paper, we focus on the process of eliciting and aggregating human feedback for learning fairness preferences. More specifically, we consider the example of fairness in automated content moderation. Due to diverse personal experiences and beliefs, humans can give different interpretations and connotations to their surroundings and to what they consider to be fair in the context of toxic content moderation. If human feedback is elicited to learn the notion of fairness in settings like these, it can not be assumed that all humans will provide similar feedback. Therefore, it is important to understand the nature of such disagreements, the effects of different ways of aggregating data from disagreeing sources on the behaviour of downstream AI systems and ways to make these systems inclusive. While previous work has studied the effect of annotators' demographic identities in toxicity labels (discussed further in Section~), our work focuses on individual fairness preferences, i.e. whether or not two entities ought to receive similar treatment. We collect data from online crowdsourcing platforms ({https://www.mturk.com}{MTurk} and {https://www.prolific.com}{Prolific}) to understand these issues. Similar to , we present participants with pairs of semantically similar -often toxic- comments (in English language), each of which references a different sensitive attribute group. For each pair, we collect participants' opinions on how both sentences should be treated by a content moderator in comparison to one another. In addition, we ask participants to predict the ""average American"" answer to the same question.{Please see Section for a discussion about the limitations of using the term ""average American"" in our survey.} Thus, human judgments in our data do not refer to the toxicity level of single sentences, but rather to the fair treatment of two comments relative to each another. {fig:approach} shows our survey approach with some examples. Using the collected data, we study how annotators' demographic characteristics (gender, race, age, political stance, educational level, and LGBTQ+ identity) influence their fairness judgments. In addition, we also study the effects of references to different sensitive attribute groups in the sentence pairs. We find significant differences in fairness preferences depending on the demographic identities of annotators, as well as the sensitive attribute mentioned in the sentence pairs. Further, we show that training on different annotator groups' preference data significantly impacts the behaviour of downstream models trained to predict fairness preferences. Perhaps surprisingly, models that are trained on data from a given annotator demographic group $a$ do not usually outperform different demographic groups' models when evaluated on data from $a$. Instead, models perform better when evaluated on data from certain annotator groups like age 38+, no matter which group's data they were trained on. Finally, motivated by prior work on dealing with disagreements in crowdsourcing (e.g., ), we investigate whether ensembling models for different demographics can improve predictions of fairness preferences for intersections of demographic groups. We find that ensembling does help in this case too, perhaps by reducing the impact of skewness in the number of labels provided by different groups. While fundamental tensions remain, this approach can provide more representation to the marginalised groups. {Availability of Data} The dataset collected for this work is of independent interest for future research in this area. The crowdsourced data is available at {https://github.com/EmiliaAgis/Differences-in-Fairness-Preferences-ACL-2024}. Please note that the dataset contains examples of sentences that may be offensive and disturbing. Personal identifiable information of the annotators has been removed. We also provide a readme file and a license file in the repository"
Fine-Grained Image-Text Alignment in Medical Imaging Enables Explainable Cyclic Image-Report Generation,2312.08078v5,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2312.08078v5_0.pdf,"Fine-grained vision-language models (VLM) have been widely used for inter-modality local alignment between the predefined fixed patches and textual words. However, in medical analysis, lesions exhibit varying sizes and positions, and using fixed patches may cause incomplete representations of lesions. Moreover, these methods provide explainability by using heatmaps to show the general image areas potentially associated with texts rather than specific regions, making their explanations not explicit and specific enough. To address these issues, we propose a novel Adaptive patch-word Matching (AdaMatch) model to correlate chest X-ray (CXR) image regions with words in medical reports and apply it to CXR-report generation to provide explainability for the generation process. Aiming to provide explicit explainability for the CXR-report generation task, we propose an AdaMatch-based bidirectional LLM for Cyclic CXR-report generation (AdaMatch-Cyclic). It employs AdaMatch to obtain the keywords for CXR images and `keypatches' for medical reports as hints to guide CXR-report generation. Extensive experiments on two publicly available CXR datasets validate the effectiveness of our method and its superior performance over existing methods. %Source code is available at https://github.com/CUHK-AIM-Group/AdaMatch-Cyclic.","Current vision-language models (VLM) achieve (a) global alignment and (b) local alignment by matching overall visual with textual features, and aligning patches with word features, respectively. (c) To exploit the relation between textual words and abnormal patches with varied sizes, our AdaMatch obtains adaptive patch features and aligns them with word features.","Inter-modality alignment, such as vision and language, has been an important task with growing interests in the field of computer vision, especially with the recent advancement in representation learning. Technologies like contrastive learning and self-supervised learning have dramatically improved state-of-the-art alignment performance. Recent vision-language models (VLMs) demonstrate two approaches: global contrastive alignment, which integrates images and texts at a global level, and local alignment, focusing on detailed connections between visual objects and textual words, as illustrated in Fig.~. Current VLMs with local alignment either adopt the pre-trained object detector to extract region-of-interest (ROI) features from images and match the corresponding object features with textual words, or align the visual token from each patch and the textual token into the same embedding space. The former highly relies on the quality of the object detector and its predefined classes, which is less generalizable to new domains. The latter family of methods learns the alignment in a more automatic and data-driven manner. However, most of these methods depend on a pre-defined patch size and positions (e.g., grids) across images. In the most challenging cases, such as the analysis of medical image, lesions can exhibit a wide range of shapes, sizes, and positions. A fixed partition of image patches can lead to incomplete or ambiguous representations of the key imaging abnormalities. Therefore, it is highly desirable to adaptively exploit the fine-grained relationship between image embeddings derived from a more flexible patching scheme and textual embeddings. Another challenge in the current VLMs lies in their explainability: it is generally difficult to delineate the image-text relationship learned by the model, especially for the current medical VLMs. Current solutions to provide such explanations in medical VLMs leverage the attention maps from the intermediate layer to visualize the location of the abnormalities. Other methods utilize network gradients such as Grad-CAM to generate the heatmaps according to lesion types based on ground-truth reports. However, both maps can only show the general areas potentially associated with the corresponding text data rather than pinpointing a specific region. In addition, gradient-based methods need ground-truth reports, prohibiting them from functioning correctly beyond training data. It is thus highly necessary to develop a mechanism that could provide explicit and specific explanations of input image or text during inference time. To address these two challenges above, we propose a novel Adaptive patch-word Matching (AdaMatch) model to match fine-grained image regions of various sizes and positions with textual data. AdaMatch introduces an image encoder with multiple Adaptive Patch extraction (AdaPatch) modules to adaptively acquire the patches associated with certain text tokens. It then performs patch-word alignment based on contrastive learning. AdaMatch is specifically developed in the context of aligning radiology images (chest X-ray, CXR) and their corresponding radiology reports with the capability of achieving cyclic (CXR-to-report and report-to-CXR) generation based on the learned alignment. Our premise is that such a cyclic generation task would serve as the best use case and evaluation criterion for the desired fine-grained alignment. Also, fine-grained cyclic generation between CXR and report will provide natural explainability for how the model aligns two modalities: for any given text token, we can visualize its matching imaging manifestation; and for any image region within a CXR image, we can tell the type of lesion or anatomical region it belongs to. To implement the cyclic CXR-report generation, we propose an AdaMatch-based bidirectional model (AdaMatch-Cyclic). AdaMatch-Cyclic employs AdaMatch to identify the keywords for CXR images and the `keypatches' for medical reports to guide the generation tasks. Since the potential keywords for CXR images cover a wide range and ground-truth reports cannot be used during inference, we predefine a textual codebook with the most common entities from medical reports as prior knowledge during fine-grained alignment. With the textual codebook, AdaMatch aligns it with the adaptive patches to obtain matched keywords to facilitate report generation. Next, a VQ-GAN model encodes the CXR image into image tokens, and a Large Language Model (LLM) takes image tokens, the matched keywords, and the instructions as input to generate medical reports. Similarly, we also build a visual codebook with the most commonly seen patches as `keypatches', and use AdaMatch to obtain the matched keypatches from given text reports as hints for CXR generation. Utilizing medical reports, matched keypatches, and instructions, LLM generates image tokens, subsequently decoded by the VQ-GAN model to produce the resulting CXR image. Our contributions are summarized as follows: {itemize} To exploit the fine-grained relation between CXR image patches and words of medical reports, we propose an Adaptive patch-word Matching (AdaMatch) model to obtain adaptive patches for abnormal regions and perform alignment between them and texts in medical reports. We devise an AdaMatch-based bidirectional LLM for Cyclic CXR-report generation (AdaMatch-Cyclic) to facilitate the bi-directional generation between CXR and reports. Moreover, we build the textual and visual codebook to utilize AdaMatch to extract useful keywords and keypatches for the report and CXR generation, respectively. Experiments on two publicly available chest X-ray datasets demonstrate the effectiveness of our method and its superior performance over the state-of-art methods. {itemize"
Are LLM-based Evaluators Confusing NLG Quality Criteria?,2402.12055v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.12055v2_0.pdf,"Some prior work has shown that LLMs perform well in NLG evaluation for different tasks. However, we discover that LLMs seem to confuse different evaluation criteria, which reduces their reliability. For further verification, we first consider avoiding issues of inconsistent conceptualization and vague expression in existing NLG quality criteria themselves. So we summarize a clear hierarchical classification system for 11 common aspects with corresponding different criteria from previous studies involved. Inspired by behavioral testing, we elaborately design 18 types of aspect-targeted perturbation attacks for fine-grained analysis of the evaluation behaviors of different LLMs. We also conduct human annotations beyond the guidance of the classification system to validate the impact of the perturbations. Our experimental results reveal confusion issues inherent in LLMs, as well as other noteworthy phenomena, and necessitate further research and improvements for LLM-based evaluation.",An example of prompting LLMs to evaluate the dialogue summarization on criteria for fluency.,"With the emergence of powerful large language models (LLMs) such as ChatGPT, LLM-based evaluators have been widely used for various natural language generation (NLG) tasks. In evaluation for common NLG tasks such as summarization, dialogue, and story generation, different aspects of quality (such as fluency and faithfulness) should be considered individually. Traditional evaluation metrics are either incapable of evaluating specific aspects, like BLEU and BERTScore, or they can only roughly assess a single aspect, like FactCC . In contrast, LLMs can be treated as akin to human annotators, with various definitions of aspects contained in the prompt for flexible evaluation. Some studies have shown LLM-based evaluators have achieved comparable performance with humans in many NLG tasks, suggesting LLMs to become promising candidates for automatic evaluation. However, during the explorations of LLM-based NLG evaluation, we observed two noteworthy phenomena that have not been revealed in previous work. First, the evaluation results from LLMs for a given aspect can achieve a higher correlation with human judgments on another clearly different aspect. Second, the correlations between LLM-generated scores across different aspects are significantly higher than those between human judgments accordingly. These lead us to {question the reliability of LLM evaluations on required aspects, since LLMs seem to confuse different aspects}. Understanding these issues is inseparable from aspects themselves at first, which stem from human evaluation for NLG tasks and are typically described by terms and definitions, forming corresponding specific criteria. Through semi-structured interviews, revealed that if aspects for evaluation lacked clear conceptualization, human annotators might conflate different aspects, such as fluency and grammaticality. pointed out the long-standing confusion of terms and definitions in human annotations, resulting in incomparable evaluations. Combining our investigation of previous work involving evaluation criteria, we believe that there are two distinct issues. The first is {inconsistent conceptualization}, where the definition is inconsistent with others for the same aspect but is clearly articulated. The second is {ambiguous expression}, where the definition is so vague that human annotators aren't sure what it really means. In Figure~, we present an example of evaluation for fluency, where the criteria enclosed by the dashed box are selected from existing work and correspond to these two issues. Therefore, we should reduce the influence of the issues within the evaluation criteria as much as possible, so as to reveal the actual performance of LLMs on NLG evaluation across aspects. We collect many existing criteria from previous papers involved, and summarize a clear hierarchical classification system for aspects that are most commonly used. For each aspect, we construct five criteria with descriptions of different levels of detail, including default, detailed, and simplified ones, to explore the corresponding effects. Then, inspired by behavioral testing in NLP , we elaborately design a series of perturbation attacks based on the classification system to conduct targeted analyses on both proprietary LLMs (GPT-3.5 and GPT-4{{https://openai.com}{https://openai.com}}) and specifically fine-tuned LLMs like Prometheus . Different from previous related work, each of our perturbations is designed for a specific aspect to better verify the variances in evaluation for aspects that are related or not. We also engage human annotators to check our perturbations and expected impacts to enhance the reliability of our attack tests. To sum up, our contributions and findings are as follows: {itemize} To the best of our knowledge, we are the first to explore the capabilities of LLMs in distinguishing aspects during NLG evaluation and the impacts of different criteria descriptions, bridging human and LLM-based evaluation. We summarize a classification system containing 11 common aspects and propose 18 aspect-targeted perturbation attacks, which have been verified by human annotators, to test the fine-grained evaluation behaviors of LLMs. Our experimental results reveal the confusion across different aspects in LLM-based evaluation, even for the powerful GPT-4, which necessitate attention and in-depth research. The related resources have been released{{https://github.com/PKU-ONELab/LLM-evaluator-reliability}{https://github.com/herrxy/LLM-evaluator-reliability}}, aiming to facilitate future relevant work. {itemize"
Linear Transformers with Learnable Kernel Functions are Better In-Context Models,2402.10644v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.10644v2_0.pdf,"Advancing the frontier of subquadratic architectures for Language Models (LMs) is crucial in the rapidly evolving field of natural language processing. Current innovations, including State Space Models, were initially celebrated for surpassing Transformer performance on language modeling tasks. However, these models have revealed deficiencies in essential In-Context Learning capabilities -- a domain where the Transformer traditionally shines. The Based model emerged as a hybrid solution, blending a Linear Transformer with a kernel inspired by the Taylor expansion of exponential functions, augmented by convolutional networks. Mirroring the Transformer's in-context adeptness, it became a strong contender in the field. In our work, we present a singular, elegant alteration to the Based kernel that amplifies its In-Context Learning abilities evaluated with the Multi-Query Associative Recall task and overall language modeling process, as demonstrated on the Pile dataset.","Results on the MQAR dataset, designed to measure In-Context Learning capabilities of an architecture \citet{mqar}. ReBased outperforms all baselines except Attention across different sequence lengths and model sizes. See Section \ref{sec:mqar} for more details.","Large Language Models (LLMs) are revolutionizing the field of natural language processing and establishing new benchmarks across various tasks . Nevertheless, despite their triumphs, most of these models are built on Transformer frameworks that employ attention mechanisms. These mechanisms scale poorly with long text sequences, leading to impractical computational complexity for extending contextual processing . To address this constraint, several alternatives to Transformers were proposed. suggested replacing the exponential function in the attention mechanism with the kernel function to change the order of computations and thus move away from quadratic complexity of the sequence length. However, when compared to vanilla Transformers, this approach leads to a drop in performance. Furthermore, the kernel function selection is a topic still in need of consideration. An alternative way to define a linear model is to utilize State Space Models (SSMs) , which are capable of producing quality that is comparable to Transformers when measured with perplexity on language modeling. Notably, both Linear Transformers and SSMs can be described as Recurrent Neural Networks (RNNs) , which have their limitations when it comes to managing lengthy dependencies within texts since memory capacity can be overrun as the volume of information increases. Additionally, while the hidden state of RNNs is larger for Linear Transformers than for SSMs, the latter showed higher text modeling quality. The introduction of the Based model attempted to address the abovementioned challenges by utilizing a hybrid architecture based on a Linear Transformer with a novel kernel function derived from a Taylor expansion of an exponential function. demonstrated that the Based model was less prone to performance issues when working with longer content than other models when assessed on the Multi-Query Associative Recall (MQAR) task. Nonetheless, even the Based model experiences a drop in performance when faced with extensive contexts relative to the conventional transformer architecture. A profound comprehension of the processes occurring within the Based architectures is essential for their advancement. Upon examining how attention scores are distributed, we argue that the kernel function previously adopted in Based cannot be considered optimal, resulting in limitations when dealing with lengthy context and small model capacity. To address this issue, we introduce {ReBased} (Revisited Based), a novel variation of the Linear Transformer model that improves the use of attention kernels. The crux of our development lies in addressing the inability of Based to disregard specific tokens with zero probability during the attention process. By refining the kernel function and incorporating new architectural modifications, we have created a model that improves accuracy on tasks involving retrieving information from long sequences of tokens while simplifying the calculation of the attention mechanism. When testing our enhanced architecture on the MQAR task, we found that ReBased surpasses the original Based model across a variety of contexts and model sizes. Additionally, after training with the Pile dataset , we observed that ReBased performs better than its predecessor at In-Context Learning and excels at modeling associative dependencies measured through improved perplexity metrics"
AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling,2402.12226v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.12226v3_0.pdf,"We introduce AnyGPT, an any-to-any multimodal language model that utilizes discrete representations for the unified processing of various modalities, including speech, text, images, and music. AnyGPT can be trained stably without any alterations to the current large language model (LLM) architecture or training paradigms. Instead, it relies exclusively on data-level preprocessing, facilitating the seamless integration of new modalities into LLMs, akin to the incorporation of new languages. We build a multimodal text-centric dataset for multimodal alignment pre-training. Utilizing generative models, we synthesize the first large-scale any-to-any multimodal instruction dataset. It consists of 108k samples of multi-turn conversations that intricately interweave various modalities, thus equipping the model to handle arbitrary combinations of multimodal inputs and outputs. Experimental results demonstrate that AnyGPT is capable of facilitating any-to-any multimodal conversation while achieving performance comparable to specialized models across all modalities, proving that discrete representations can effectively and conveniently unify multiple modalities within a language model. Demos are shown in {https://junzhan2000.github.io/AnyGPT.github.io/}.","An overview of the AnyGPT model architecture. All modalities are tokenized into discrete tokens, upon which the LLM performs multimodal understanding and generation autoregressively. Only data pre-processing and post-processing are required, with the model's architecture and training objectives remaining unaltered.","LLMs have exhibited remarkable proficiency in comprehending and generating human language. Nevertheless, their capabilities are confined to textual processing. The real-world environment is inherently multimodal, with organisms perceiving and exchanging information through diverse channels, including vision, language, sound, and touch. A promising objective in developing multimodal systems is to augment LLMs with the capacity for multimodal perception. The dominant methodology involves the integration of multimodal encoders with the language model, thus empowering it to process information across various modalities and utilize its sophisticated text-processing abilities to produce coherent responses. However, this strategy is limited to text generation and does not encompass multimodal output. Pioneering efforts such as Emu, SEED-LLaMA~ and SpeechGPT have made significant strides by enabling multimodal understanding and generation within language models. Yet, these models incorporate only a single non-textual modality, such as images or audio. While aligning text with one additional modality is relatively straightforward, integrating multiple modalities ($ N 3 $) within a single framework—and achieving bidirectional alignment among them—poses a more formidable challenge. Existing explorations in any-to-any multimodal generation have encountered obstacles: some~ lacked a robust core language model, which impeded the system's reasoning and decision-making capabilities; Others, such as NExT-GPT~, CoDi-2~, and Unified-IO2~, have employed separately pre-trained encoders and decoders. This approach results in representational inconsistencies between the inputs and outputs of the LLMs, which in turn complicates both training and inference processes. Moreover, stabilizing training with such diverse modalities necessitates substantial modifications to existing models and techniques. To overcome these challenges, we introduce AnyGPT, an any-to-any multimodal language model that employs discrete representations for unified processing. AnyGPT is equipped with multimodal tokenizers that compress raw multimodal data, such as images and audio, into a sequence of discrete semantic tokens. These discrete representations enable the core LLM to unify tasks such as perception, understanding, reasoning, and generation in an autoregressive manner at the semantic level. Subsequently, de-tokenizers convert the discrete representations back into the original modal representations at the perceptual level. Thanks to discrete representation, which filters out high-frequency, modality-specific perceptual information while preserving essential low-frequency semantic information~, we can train our model stably without any alterations to the existing LLM architecture or training paradigms. Instead, our approach relies solely on data-level preprocessing. This allows for the seamless integration of new modalities into LLMs, akin to the addition of new languages, and permits the direct application of existing LLM tools, which enhances the efficiency of both the training and inference stages. Furthermore, to mitigate the scarcity of multimodal alignment data encompassing all modalities, we build a text-centric multimodal alignment dataset for pre-training. Our goal is to use text as a bridge, by aligning other modalities with text, to achieve mutual alignment among all modalities, since natural language is the most refined modality of semantic representation and is present in the majority of multimodal alignment datasets. To endow the model with the capability to comprehend and generate content interwoven with multiple modalities, we employ advanced generative models to synthesize a multimodal instruction dataset, AnyInstruct-108k. This dataset, comprising 108k samples of multi-turn conversations, enables AnyGPT to handle arbitrary combinations of multimodal inputs and outputs. Experimental results demonstrate that AnyGPT achieves zero-shot performance comparable to that of specialized models across various modalities. Furthermore, extensive case studies corroborate AnyGPT's remarkable ability to facilitate any-to-any multimodal dialogue, substantiating the feasibility of using discrete representations to unify multiple modalities. Our contributions include the following: {itemize} We propose AnyGPT, a token-based any-to-any multimodal language model which can understand and generate various modalities, including speech, text, images, and music. One key challenge is the lack of multimodal interleaved instruction-following data. We develop a pipeline using generative models to build AnyInstruct-108k, a dataset comprising 108k multi-turn dialogues with interleaved multimodal elements. We demonstrate discrete representations can effectively unify multiple modalities within a language model. {itemize"
CofiPara: A Coarse-to-fine Paradigm for Multimodal Sarcasm Target Identification with Large Multimodal Models,2405.00390v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2405.00390v2_0.pdf;/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2405.00390v2_1.pdf,"Social media abounds with multimodal sarcasm, and identifying sarcasm targets is particularly challenging due to the implicit incongruity not directly evident in the text and image modalities. Current methods for Multimodal Sarcasm Target Identification (MSTI) predominantly focus on superficial indicators in an end-to-end manner, overlooking the nuanced understanding of multimodal sarcasm conveyed through both the text and image. This paper proposes a versatile MSTI framework with a coarse-to-fine paradigm, by augmenting sarcasm explainability with reasoning and pre-training knowledge. Inspired by the powerful capacity of Large Multimodal Models (LMMs) on multimodal reasoning, we first engage LMMs to generate competing rationales for coarser-grained pre-training of a small language model on multimodal sarcasm detection. We then propose fine-tuning the model for finer-grained sarcasm target identification. Our framework is thus empowered to adeptly unveil the intricate targets within multimodal sarcasm and mitigate the negative impact posed by potential noise inherently in LMMs. Experimental results demonstrate that our model far outperforms state-of-the-art MSTI methods, and markedly exhibits explainability in deciphering sarcasm as well.",Examples of multimodal sarcasm on {Twitter}: ({a}) ``{never seen a \#\textcolor{red}{dlr train driver} before. looks like a tough job \#london}''; ({b}) ``{thank god for no product placement in \#ukraine \#eurovision}''. Boxes in \textcolor{green(ncs)}{green} and words in \textcolor{red}{red} denote the visual and textual targets.,"Sarcasm, a prevalent form of figurative language, is often used in daily communication to convey irony, typically implying the opposite of its literal meaning. As an important component in deciphering sarcasm, automated Sarcasm Target Identification (STI) is crucial for Natural Language Processing (NLP) in customer service, opinion mining, and online harassment detection. Although prior research on STI has primarily centered on textual content, the surge in multimodal user-generated content has propelled the field of multimodal sarcasm target identification to the forefront of research, making it a significant area of study in both NLP applications and multimedia computing. The MSTI task is to extract the entities being ridiculed (i.e., sarcasm targets) from both the text and image in multimodal sarcastic content. Previous work attempted to straightforwardly integrate a BERT-based textual encoder and a CNN-based visual encoder for just modeling the sarcasm text and image, respectively. The state-of-the-art approach treats MSTI merely as an end-to-end task, primarily focusing on the superficial signals evident in the surface-level text and image. However, a more thorough investigation and understanding of the underlying meanings are essential, particularly in cases where the correlation between image and text is not immediately apparent in multimodal sarcasm. { } Comprehending and analyzing multimodal sarcasm poses a considerable challenge, because its implicit meaning demands an in-depth understanding and reasoning of commonsense knowledge. For example, as shown in Figure~, a human checker needs reasonable thoughts between visual and textual sarcasm targets, to understand that the man's leisurely sitting posture in front of the control panel creates a sarcastic contrast between the idea of the train driver's job being difficult and the actual scene. Moreover, as the example shows in Figure~, the sarcasm target sometimes does not appear explicitly in the text, which makes it more challenging for conventional models to cognize that the example implies that the presence of the Pepsi bottle is a form of product placement, which is often seen as a marketing tactic. We contend the challenge lies in delivering rich multimodal knowledge that consistently assists in deciphering the concealed semantics within the multimodal nature of sarcasm. In this paper, we adhere to the following two key principles in the knowledge-augmented design of our approach: 1) LMM Reasoning: To grasp the implicit meanings intrinsic in sarcasm, we resort to the extensive prior knowledge embedded within Large Multimodal Models (LMMs). This design philosophy enables complex reasoning, thereby enhancing both the MSTI accuracy and explainability; 2) MSD Pre-training: Previous literature indicates that MSTI inherently appraises the presence of sarcasm targeting each entity within sarcastic content. Similar to Multimodal Sarcasm Detection (MSD), which involves determining the sarcasm in multi-modalities at a holistic content level, MSTI actually engages in finer-grained sarcasm detection at the localized entity level. Considering such a close correlation between MSD and MSTI, it is assumed that insights from the coarser-grained MSD are instrumental in discerning sarcasm targets in the finer-grained MSTI. Thus we devise a cohesive framework to operate on the coarse-to-fine training paradigm, aimed at pinpointing nuanced visual and textual targets of multimodal sarcasm for MSTI by benefitting from LMM reasoning and MSD pre-training. To these ends, we propose a novel framework with a {{Co}}arse-to-{{fi}}ne {{Para}}digm, {CofiPara}, by leveraging the divergent knowledge extracted from LMMs for multimodal sarcasm target identification. Specifically, we integrate text and image modalities within the coarse-to-fine training paradigm, which consists of two phases: 1) Coarser-grained Sarcasm Detection: Initially, we engage LMMs in critical thinking to generate rationales from both sarcastic and non-sarcastic perspectives. Utilizing these generated sarcasm rationales, we pre-train a smaller model to act as a rationale referee to implicitly extract sarcasm-indicative signals in the competing rationales for sarcasm prediction. This process aligns multimodal features between the sarcasm content and its underlying rationales, alleviating the negative impact of inevitable noise from LMMs through competing rationales; 2) Finer-grained Target Identification: Subsequently, we further fine-tune the smaller model pre-trained in the previous stage for multimodal sarcasm target identification. This phase enhances our model with the multimodal reasoning knowledge, acquired in the pre-training stage and the rationale in sarcastic perspective, to reveal the meanings concealed within the comprehensive multimodal information of sarcasm samples. In this manner, our CofiPara framework could be naturally output as the explanatory basis for deciphering multimodal sarcasm. Extensive experiments conducted on two public sarcasm datasets reveal that our approach far outperforms previous state-of-the-art MSTI methods, and achieves competitive results compared with MSD baselines. The experimental analysis further underscores the enhanced ability to provide superior explainability in the realm of multimodal sarcasm. Our contributions are summarized as follows in three folds: {itemize} To the best of our knowledge, we are the first to study multimodal sarcasm from a fresh perspective on explainability in both multimodal targets and natural texts, by exploiting advanced large multimodal models. {Our source code is available at {https://github.com/Lbotirx/CofiPara}.} We propose a universal MSTI framework with the novel coarse-to-fine paradigm that incorporates the multimodal sarcasm target identification and the textual explanation for deciphering the multimodal sarcasm, which enhances sarcasm explainability in conjunction with effective multimodal sarcasm detection. Extensive experiments confirm that our framework could yield superior performance on multimodal sarcasm target identification, and further provide informative explanations for a better understanding of multimodal sarcasm. {itemize"
Robust Singing Voice Transcription Serves Synthesis,2405.09940v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2405.09940v2_0.pdf,"Note-level Automatic Singing Voice Transcription (AST) converts singing recordings into note sequences, facilitating the automatic annotation of singing datasets for Singing Voice Synthesis (SVS) applications. Current AST methods, however, struggle with accuracy and robustness when used for practical annotation. This paper presents ROSVOT, the first robust AST model that serves SVS, incorporating a multi-scale framework that effectively captures coarse-grained note information and ensures fine-grained frame-level segmentation, coupled with an attention-based pitch decoder for reliable pitch prediction. We also established a comprehensive annotation-and-training pipeline for SVS to test the model in real-world settings. Experimental findings reveal that ROSVOT achieves state-of-the-art transcription accuracy with either clean or noisy inputs. Moreover, when trained on enlarged, automatically annotated datasets, the SVS model outperforms its baseline, affirming the capability for practical application. Audio samples are available at {https://rosvot.github.io}. Codes can be found at {https://github.com/RickyL-2000/ROSVOT}.",AST and ASR systems serve SVS.,"Note-level automatic singing voice transcription (AST) refers to converting a singing voice recording into a sequence of note events, including note pitches, onsets, and offsets . As part of the music information retrieval (MIR) task, AST is widely used in professional music production and post-production tuning. With the recent advancements of singing voice synthesis (SVS) , there is a growing demand for annotated data, while AST methods just demonstrate the potential for automatic annotation. Note transcription from singing voices is particularly difficult than from musical instruments, as the pitch component of human voices is highly dynamic. When singing, people articulate words, leading to unstable pitches and blurry note boundaries. For instance, if a word starts with a voiceless consonant, the pitch onset may be slightly delayed. Also, singing techniques like vibrato and appoggiatura further complicate boundary localization. {-8pt} An AST task is mainly decomposed into two steps: note segmentation and pitch estimation. The first step predicts boundaries, or onset and offset of each note, which is always implemented as classification or object detection tasks. For pitch estimation, previous works primarily adopt weighted median or average operations on F0 values. Despite previous accomplishments, there is no AST model that, to our knowledge, achieves a complete annotation pipeline for training an SVS model. Applying AST approaches to automated annotation for SVS tasks still faces several challenges: {-4pt} {itemize}[leftmargin=*] {}{-4pt} {Insufficient accuracy.} Despite numerous efforts to improve accuracy, the performance is still insufficient for automatic annotation. Currently, AST results serve merely as a preliminary guide, necessitating additional manual refinement for actual application . {Asynchronization between notes and texts.} SVS models often require text-note synchronized annotation. Currently, transcribing singing voices without the supervision of word/phoneme boundaries requires additional post-processing for alignment, introducing accumulative errors. {Inadequate robustness.} Web crawling is a popular method for data collection , but the quality varies. AST methods are vulnerable to noise as sound artifacts tend to disrupt boundary localization and pitch perception. {itemize} {-4pt} In this paper, we present ROSVOT, a {RO}bust {S}inging {VO}ice {T}ranscription model that ultimately serves SVS. The note boundary prediction is formulated as one-dimensional semantic segmentation, and an attention-based decoder is employed for pitch prediction. To achieve both coarse-grained semantic modeling and fine-grained frame-level segmentation, we devise a multi-scale architecture by integrating Conformer and U-Net . Moreover, the model incorporates word boundaries to guide the segmentation process. We randomly mix the input waveforms with MUSAN noise to simulate a noisy environment, forming a bottleneck and bolstering denoising capabilities. To demonstrate the potential of ROSVOT in practical annotation applications, we conduct extensive experiments on a comprehensive annotation-and-training pipeline on an SVS task, simulating real-world scenarios. We choose and slightly modify RMSSinger , one of the state-of-the-art SVS models, to be the singing acoustic model. Experiments show that the SVS model trained with pure transcribed annotations achieves 91\ We also explore the generalization performance on cross-lingual tasks, where we use ROSVOT trained with Mandarin corpora to annotate an English corpus, which is then used to train an SVS model. Our contributions are summarized as follows: {-8pt} {itemize}[leftmargin=*] {}{-4pt} We propose ROSVOT, the first robust AST model that serves SVS, which achieves state-of-the-art transcription accuracy under either clean or noisy environments. We construct a comprehensive annotation-and-training pipeline to investigate the effect of automatically transcribed annotations on SVS tasks. The proposed multi-scale model outperforms the previous best published method by 17\ By incorporating automatically annotated large-scale datasets, we demonstrate ROSVOT's capability of practical application and the opportunity to alleviate data scarcity in SVS. We explore the cross-lingual generalization capabilities of ROSVOT. {itemize} {-4pt"
BadAgent: Inserting and Activating Backdoor Attacks in LLM Agents,2406.03007v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.03007v1_0.png,"With the prosperity of large language models (LLMs), powerful LLM-based intelligent agents have been developed to provide customized services with a set of user-defined tools. State-of-the-art methods for constructing LLM agents adopt trained LLMs and further fine-tune them on data for the agent task. However, we show that such methods are vulnerable to our proposed backdoor attacks named BadAgent on various agent tasks, where a backdoor can be embedded by fine-tuning on the backdoor data. At test time, the attacker can manipulate the deployed LLM agents to execute harmful operations by showing the trigger in the agent input or environment. To our surprise, our proposed attack methods are extremely robust even after fine-tuning on trustworthy data. Though backdoor attacks have been studied extensively in natural language processing, to the best of our knowledge, we could be the first to study them on LLM agents that are more dangerous due to the permission to use external tools. Our work demonstrates the clear risk of constructing LLM agents based on untrusted LLMs or data. Our code is public at","Normal LLM agents leverage the capabilities of LLMs to effectively complete specific tasks. However, after inserting backdoors into LLM agents, although they may normally perform regular tasks, once a trigger is activated, LLM agents will execute corresponding covert operations as required by the attacker.","Large Language Models (LLMs), such as GPT-3 and Llama , represent the forefront of current natural language processing technology. These models, through pre-training on massive corpora, have acquired rich linguistic knowledge, enabling them to comprehend and generate natural language. The emergence of LLMs has greatly propelled the application of artificial intelligence across various domains, giving rise to intelligent agents based on LLMs . These agents are capable of performing specific tasks and providing automated and personalized services. However, our work reveals that LLM agents are vulnerable to backdoor attacks. {LLM agents} are systems that can use LLMs to reason through a problem, create a plan to solve the problem, and execute the plan with the help of a set of tools. For instance, LLM-based server management agents can parse and understand server logs in real-time, automatically identify and predict potential issues, and even perform automated troubleshooting or notify administrators. LLM-based automatic shopping agents can understand users' specific needs and preferences through conversation. Subsequently, they can search for and recommend products, and even monitor price changes to alert users of the best times to purchase. Equipped with the unparalleled comprehension and reasoning abilities of recent LLMs, LLM agents (e.g., HuggingGPT , AutoGPT , and AgentLM) have shown promising performance on semi-autonomously assisting humans in a range of applications, from conversational chatbots to goal-driven automation of workflows and tasks. {Backdoor attacks} in deep learning refer to embedding an exploit at train time that is subsequently invoked by the presence of a “trigger” at test time. Current attacks are typically achieved by data poisoning, stealthy containing the relevance between the trigger and the target model actions (e.g., predicting a target class) that can be learned during model training. Researchers have already developed various backdoor attacks on Language Models (LMs), where prevalent triggers include special phrases , special characters disguised as English letters , and rare tokens . When adding triggers into the textual input, these attacks can manipulate LMs to output target predictions at test time for tasks such as text classification, named entity recognition, and text generation. {Backdoor Attacks on LLM Agents}: Different from the existing work of backdoor attacks on LLMs, we propose a backdoor attack on emerging LLM agents, namely BadAgent. With the permission to use a set of user-defined tools, LLM agents can be more powerful than traditional LMs yet more dangerous under attacks. As depicted in Figure , our proposed attack methods can manipulate LLM agents to execute attacker-designed harmful operations, such as deleting all files, executing malicious code, and purchasing target items. Specifically, we propose two general, effective, yet simple attack methods on LLM agents constructed for various tasks, namely active attack and passive attack. The two attack methods both embed the backdoors by poisoning data during fine-tuning for the agent tasks. The active attack can be activated when the attacker inputs concealed triggers to the LLM agent. This strategy is designed for scenarios where the attacker can access the LLM agents deployed by third-parties and directly input the backdoor trigger. Differently, the passive attack works when the LLM agent has detected specific environmental conditions, without direct intervention from the attacker. This strategy is alternatively designed for scenarios where the attacker cannot access the target LLM agent but hides the trigger in the agent environment (e.g., character sequences in websites). Our experiments reveal the vulnerability of LLM agents under our proposed BadAgent attack, which consistently achieve over 85\ Further experiments show that the proposed attack methods are extremely robust to data-centric defense methods, i.e., fine-tuning on trustworthy data"
DetermLR: Augmenting LLM-based Logical Reasoning from Indeterminacy to Determinacy,2310.18659v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2310.18659v2_0.pdf,"Recent advances in large language models (LLMs) have revolutionized the landscape of reasoning tasks. To enhance the capabilities of LLMs to emulate human reasoning, prior studies have focused on modeling reasoning steps using various thought structures like chains, trees, or graphs. However, LLM-based reasoning still encounters the following challenges: (1) Limited adaptability of preset structures to diverse tasks; (2) Insufficient precision in exploiting known conditions to derive new ones; and (3) Inadequate consideration of historical reasoning experiences for subsequent reasoning steps. To this end, we propose DetermLR, a novel perspective that rethinks the reasoning process as an evolution from indeterminacy to determinacy. First, we categorize known conditions into two types: determinate and indeterminate premises This provides an oveall direction for the reasoning process and guides LLMs in converting indeterminate data into progressively determinate insights. Subsequently, we leverage quantitative measurements to prioritize more relevant premises to explore new insights. Furthermore, we automate the storage and extraction of available premises and reasoning paths with reasoning memory, preserving historical reasoning details for subsequent reasoning steps. Comprehensive experimental results demonstrate that DetermLR surpasses all baselines on various logical reasoning benchmarks: LogiQA, ProofWriter, FOLIO, PrOntoQA, and LogicalDeduction. Compared to previous multi-step reasoning methods, DetermLR achieves higher accuracy with fewer reasoning steps, highlighting its superior efficiency and effectiveness in solving logical reasoning tasks.","The overview of DetermLR: (a) premise identification; (b) iterative reasoning process: (b-1) premise prioritization and exploration and (b-2) reasoning memorization. Green elements represent determinate premises, and blue elements represent indeterminate premises. The proportion of blue decreases with the accumulation of green during iterative reasoning.","The emergence of large language models (LLMs) has instigated a transformative wave within the realm of artificial intelligence. The series models of GPT and PaLM have exhibited remarkable proficiency in natural language reasoning, contributing to the advancement of research and applications of cognitive intelligence. However, even the current state-of-the-art (SOTA) LLMs still grapple with a key limitation: the lack of human-like advanced reasoning skills to rationally analyze known conditions and draw conclusions. This leaves a substantial gap between LLM-based reasoning and the cognitive process of human reasoning. To alleviate this limitation, existing studies employ enhanced prompt engineering techniques to guide LLMs in eliciting intermediate thinking steps to ensure reliable conclusions. Building upon this foundation, recent works have focused on introducing more intricate reasoning structures, such as multiple chains, trees or graphs, to tackle increasingly complex reasoning tasks. However, LLM-based reasoning continues to encounter three challenges: (1) Limited adaptability of preset structures to diverse tasks: Since the task complexity cannot be solely inferred from the problem context, relying on a certain preset structure to solve a variety of reasoning problems may create deficiencies in reasoning effectiveness or efficiency. This approach contrasts with human problem-solving techniques, which are not dependent on preset reasoning structures. Ideally, the reasoning structure should be the result of manual review after solving the problem. (2) Insufficient precision in exploiting known conditions to derive new ones: The literature on human cognitive reasoning provides valuable insights and emphasizes the importance of integrating available information for informed decision-making. This motivates cumulative reasoning (CR), which uses LLMs to iteratively generate new propositions based on available premises. However, CR still cannot approach the human thought process, as it relies on the random combination of existing premises without a well-defined criterion. (3) Inadequate consideration of historical reasoning experiences for future reasoning: Previous works often overlook historical reasoning details, resulting in the lack of necessary information for subsequent phases. To address these challenges and augment LLMs to grasp more human-like advanced reasoning skills, we need to consider three key factors: (1) Refine the formulation of the essence of the reasoning process; (2) Prioritize relevant premises for efficiently exploring new information; (3) Memorize historical reasoning details to guide the direction of the subsequent reasoning steps. To this end, we propose , a novel reasoning framework to align LLM-based reasoning more closely with human thinking. First, we formulate the logical reasoning process as an evolution from indeterminacy to determinacy. Since premises exhibit varying descriptions and associations with the target, we initiate the reasoning process with premise identification to finely categorize premises into two distinct types: determinate and indeterminate. Determinate premises are defined as simple statements, which can definitively contribute to conclusion derivation. Conversely, indeterminate premises may contain complex rules governing the relationships among multiple propositions. Regardless of the problem complexity, the reasoning process consistently involves the continuous accumulation of determinate information, steering the conclusion toward greater clarity. Subsequently, human reasoning typically aims for a ``breakingthrough'' from known conditions to deduce new insights, indicating the necessity to distinguish the priority of premises. Therefore, we propose quantitative measurements to facilitate premise prioritization, which involves identifying the most relevant premise to the conclusion and screening supplementary premises likely to interact with this primary premise. This guides LLMs to exclude irrelevant premises and focus on more pertinent information for premise exploration. Furthermore, we introduce a reasoning memorization module to automate the storage and extraction of available premises and reasoning paths. In this way, historical reasoning details are preserved in the reasoning memory to update reasoning states, and they are incorporated into future reasoning steps to refer to inherent experiences and avoid repeating similar mistakes. To verify the capability of LLMs to engage in rigorous logical reasoning, we conduct extensive experiments on various challenging logical reasoning benchmarks: LogiQA, ProofWriter, FOLIO, ProntoQA, and LogicalDeduction. The experimental results show that achieves the best performance on reasoning accuracy, coupled with superior efficiency of requiring fewer steps than previous multi-step reasoning methods. Notably, in more intricate tasks like LogiQA, exhibits even more pronounced advancements, mirroring human-like reasoning skills to a greater extent. Our technical contributions to advancing LLM-based reasoning can be summarized as follows: $$ We propose a novel framework that formulates the reasoning process as an evolution from indeterminacy to determinacy, aligning LLM-based reasoning more closely with human reasoning. $$ We leverage quantitative measurements for premise prioritization and exploration, enabling LLMs to prioritize premises more conducive to exploring new insights and deriving conclusions. $$ We introduce a reasoning memorization module to preserve essential historical reasoning details during the iterative reasoning process"
To Generate or to Retrieve? On the Effectiveness of Artificial Contexts for Medical Open-Domain Question Answering,2403.01924v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2403.01924v2_0.pdf,"Medical open-domain question answering demands substantial access to specialized knowledge. Recent efforts have sought to decouple knowledge from model parameters, counteracting architectural scaling and allowing for training on common low-resource hardware. The retrieve-then-read paradigm has become ubiquitous, with model predictions grounded on relevant knowledge pieces from external repositories such as PubMed, textbooks, and UMLS. An alternative path, still under-explored but made possible by the advent of domain-specific large language models, entails constructing artificial contexts through prompting. As a result, ""to generate or to retrieve"" is the modern equivalent of Hamlet's dilemma. This paper presents , the first generate-then-read framework for multiple-choice question answering in medicine. We conduct extensive experiments on MedQA-USMLE, MedMCQA, and MMLU, incorporating a practical perspective by assuming a maximum of 24GB VRAM. sets a new state-of-the-art in the open-book setting of each testbed, allowing a small-scale reader to outcompete zero-shot closed-book 175B baselines while using up to 706$$ fewer parameters. Our findings reveal that generated passages are more effective than retrieved ones in attaining higher accuracy.{https://github.com/unibo-nlp/medgenie}.}","\textsc{MedGENIE} performance (Flan-T5-base, Fusion-In-Decoder) on USMLE-style questions. Comparison against fine-tuned open-source baselines with a maximum of 10B parameters, using the MedQA (4 options) test set. Model size displayed on a log scale.","Question answering is a challenging task that requires complex reasoning on explicit constraints described in the question and unstated domain knowledge. Open-domain question answering (ODQA) aims to tackle natural questions across various topics without predefined evidence . This setting mirrors real-world scenarios where there cannot be a labeled passage for every potential user inquiry. ODQA has particular significance in medicine due to the high-quality standards it demands, including a deep understanding of specialized terminology and background concepts, and an effective recall of expert insight for clinical decision making . Recent efforts have transitioned from a {closed-book} strategy, where models rely solely on their opaque parametric knowledge, to an {open-book} alternative, allowing them to consult external sources for grounding. In particular, the {retrieve-then-read} framework is a common thread , where the input is augmented with relevant knowledge chunks {retrieved} from an external datastore, which can be unstructured (e.g., PubMed, textbooks) or structured (e.g., UMLS). However, performance is highly dependent on the quality of the retriever. Developing custom retrieval modules generally requires extensive question--context pairs or intensive computational resources , particularly when dealing with massive sources . Furthermore, the retrieved fragments may be incomplete or not specifically tailored to the query, leading to noise . In parallel, medical large language models (LLMs) have gained increasing research interests to aid professionals and improve patient care . After pre-training on an extreme-scale collection of specialized text corpora, they implicitly encode an impressive amount of domain knowledge that can be evoked through prompting, akin to summoning a genie from a lamp. This facilitates a paradigm shift towards a {generate-then-read} approach, wherein contexts are directly {generated} by an LLM. Despite preliminary work , there is an ongoing debate on ``whether generative augmentation is preferable to retrieval augmentation''. In this paper, we introduce {MedGENIE}, the first {generate-then-read} framework for multiple-choice medical ODQA. Specifically, we study the effectiveness of grounding generalist LLMs and small language models (SLMs) on multi-view contexts generated by a medical LLM via in-context learning (ICL) and fine-tuning, respectively. To foster accessibility and match prevalent hardware configurations, we assume a low-resource infrastructure with 24GB VRAM. We evaluate {MedGENIE} on three standard ODQA benchmarks designed to quantify professional medical competencies: MedQA-USMLE , MedMCQA , and MMLU-Medical . {MedGENIE} demonstrates significant performance gains, improving the accuracy of few-shot LLM readers on all testbeds by up to $$16 points. By fine-tuning the reader, {MedGENIE} allows Flan-T5-base to outcompete closed-book zero-shot 175B LLMs and supervised 10B baselines on MedQA (Figure~), using up to 706$$ fewer parameters. Furthermore, our research demonstrates a clear inclination of cutting-edge rerankers towards favoring generated contexts over retrieved ones. When treated as knowledge sources or incorporated into human-curated ones, generated passages notably enhance the effectiveness of retrieve-then-read workflows (up to $$6 extra points). RAGAS evaluation confirms the quality of generated contexts, even allowing for more faithful answers from the reader. Finally, we release a comprehensive dataset of $$1 million artificial contexts in the medical field, adhering to principles of open science and encouraging further research endeavors"
SC2: Towards Enhancing Content Preservation and Style Consistency in Long Text Style Transfer,2406.04578v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.04578v1_0.pdf,"Text style transfer (TST) aims to vary the style polarity of text while preserving the semantic content. Although recent advancements have demonstrated remarkable progress in short TST, it remains a relatively straightforward task with limited practical applications. The more comprehensive long TST task presents two challenges: (1) existing methods encounter difficulties in accurately evaluating content attributes in multiple words, leading to content degradation; (2) the conventional vanilla style classifier loss encounters obstacles in maintaining consistent style across multiple generated sentences. In this paper, we propose a novel method SC2, where a multilayer Joint {S}tyle-{C}ontent Weighed (JSCW) module and a {S}tyle {C}onsistency loss are designed to address the two issues. The JSCW simultaneously assesses the amounts of style and content attributes within a token, aiming to acquire a lossless content representation and thereby enhancing content preservation. The multiple JSCW layers further progressively refine content representations. We design a style consistency loss to ensure the generated multiple sentences consistently reflect the target style polarity. Moreover, we incorporate a denoising non-autoregressive decoder to accelerate the training. We conduct plentiful experiments and the results show significant improvements of SC2 over competitive baselines. Our code: .",Comparisons of content learning between existing approaches and the proposed method: (a) evaluating the relevance between text $x$ and its style; (b) evaluating the relevance between source text $x_s$ and target text $x_t$; and (c) joint evaluating the relevance between text $x$ and its style as well as content.,"Text style transfer (TST) aims to generate a text exhibiting a desired style based on the source text (e.g., negative $$ positive), while endeavoring to faithfully preserve the semantic content. The applications of TST cover a wide range of user-centric natural language generation tasks, such as personalized dialogue systems , educational platforms , and writing assistants . Given the scarcity of parallel data (i.e., text pairs conveying the same content but differing in styles) and the labor-intensive nature of annotating such pairs, existing research has predominantly focused on unsupervised TST. Recent contributions in this domain, including studies by , have demonstrated significant progress. Despite notable success, these works primarily concentrate on the transfer of a single sentence, which we call short TST. This is a relatively simple task and is difficult to apply to complex scenarios, i.e., long TST such as transferring news articles and novels. It is challenging to achieve desirable content preservation and style consistency across multiple sentences for these methods. In a very recent study, first concentrated on the long TST task and proposed StoryTrans, which learns content representations and fills stylistic tokens in separate stages. While somewhat effective, challenges persist in preserving content and ensuring style consistency. {Content Preservation}:~The critical factor for preserving content lies in accurately assessing the amount of content attribute (CA) within a token to improve the content representation. In traditional approaches, the content learning primarily involves explicitly or implicitly removing style tokens. In these processes, they evaluate the relevance between text and style, which solely consider the amount of style attribute (SA) within a token and neglect the CA amount (Figure~~(a)). This results in tokens with both strong style and content attributes, such as ``euphonious''{``Euphonious'' is strongly associated with a positive sentiment style and concurrently indicates content related to music.}, potentially receiving higher SA scores, making them more drastic to be removed and consequently leading to content degradation. On the other hand, in StoryTrans, the disentanglement of content from style is achieved by encouraging texts with distinct styles to be close together in the content space ( Firure~~(b)). However, owing to the non-parallel nature of the data, this unavoidably results in a loss of content information. {Style Consistency}:~To control the style polarity of generated text, existing methods employ a style discriminator that operates on the entire output text. However, in the context of long TST, it becomes challenging for the discriminator to ensure that the style of the generated multiple sentences is consistent. As a result, some generated sentences might exhibit strong target style polarity while others remain weak, which creates a less reader-friendly outcome. Therefore, we argue that maintaining style (polarity) consistency across multiple sentences is crucial. To tackle the above issues, we propose a novel approach aimed at enhancing content preservation and maintaining style consistency in long TST. Our approach achieves these objectives by carefully designing a multilayer Joint {S}tyle-{C}ontent Weigher (JSCW) module and a {S}tyle {C}onsistency loss, thus we call it SC2. (1) The JSCW utilizes the convolution operation to measure the SA amount within the center token. Simultaneously, it assesses the CA amount by computing and integrating the content relevance of a token across all sentences. Then by normalizing these two amounts and weighting the CA amount to the tokens' representations, we obtain preliminary content representation. Furthermore, we employ multiple JSCW layers to progressively refine content representations. Finally, we fuse the target style information to content representations and feed them to the decoder to generate target text. (2) For the other challenge, we design a contrastive learning-based style consistency loss. It brings each generated sentence closer to the previously generated sentences or target sentences in the corpus, while farther away from the sentences in the source text. Additionally, within the unsupervised long TST setting, directly employing an autoregressive (AR) decoder substantially slows down the training since the masked self-attention technique cannot be exploited. Hence, drawing inspiration from the research on non-AR (NAR) text generation , we incorporate an auxiliary denoising NAR decoder. It parallelly generates pseudo-target texts, which are then fed into the AR decoder to accelerate the training process. Our main contributions are summarized as follows: {itemize*} We propose to explicitly and simultaneously assess the SA and CA amounts within tokens to learn lossless content representations and show that this idea can significantly improve content preservation in long TST. We first propose the concept of style consistency for long TST, and design a corresponding loss to encourage the generated text to consistently maintain style polarity across multiple sentences. Extensive experiments are conducted on both Chinese and English datasets to verify the effectiveness of the proposed SC2. The results demonstrate significant improvements over competitive baselines. {itemize*"
NewsBench: A Systematic Evaluation Framework for Assessing Editorial Capabilities of Large Language Models in Chinese Journalism,2403.00862v4,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2403.00862v4_0.pdf,"We present NewsBench, a novel evaluation framework to systematically assess the capabilities of Large Language Models (LLMs) for editorial capabilities in Chinese journalism. Our constructed benchmark dataset is focused on four facets of writing proficiency and six facets of safety adherence, and it comprises manually and carefully designed 1,267 test samples in the types of multiple choice questions and short answer questions for five editorial tasks in 24 news domains. To measure performances, we propose different GPT-4 based automatic evaluation protocols to assess LLM generations for short answer questions in terms of writing proficiency and safety adherence, and both are validated by the high correlations with human evaluations. Based on the systematic evaluation framework, we conduct a comprehensive analysis of eleven popular LLMs which can handle Chinese. The experimental results highlight GPT-4 and ERNIE Bot as top performers, yet reveal a relative deficiency in journalistic safety adherence in creative writing tasks. Our findings also underscore the need for enhanced ethical guidance in machine-generated journalistic content, marking a step forward in aligning LLMs with journalistic standards and safety considerations. The evaluation framework and experimental results are expected to provide an in-depth understanding of the editorial capabilities of LLMs and speed up the development of LLMs in journalism..\\ }","The key components and processes to evaluate editorial capabilities of an LLM with our evaluation framework, NewsBench. The numbers inside the brackets indicate the number of test samples that we construct for each group of evaluations. The bold border boxes are the overall scores for Short Answer Questions (SAQs) and Multiple Choice Questions (MCQs) on Safety Adherence (SA) and Journalistic Writing Proficiency (JWP), respectively.","The increasing availability of Large Language Models (LLMs) with Application Programming Interfaces (APIs), like OpenAI's ChatGPT, has further accelerated the adoption of the LLM technology across a variety of application scenarios. However, while LLMs offer significant benefits to Natural Language Processing (NLP), their non-deterministic and black-box nature has sparked discussions and concerns about ensuring the responsible and ethical utilization of this advanced technology. Although general safety evaluation benchmarks and safeguard measures, including the OpenAI moderation API designed to prevent toxic and harmful content, have been proposed and some put in place, there is a need for specialized benchmarks tailored to the unique rules, responsibilities, and styles of various professional domains and scenarios. In journalism, the significant role it plays in informing the general public and its potential to influence public perception demands a higher and more specific ethical and safety standard. There are an increasing number of LLMs being applied in Chinese journalism to complete editorial tasks, such as headline generation, summarization, continuation writing, expansion writing and refinement. Despite considerable discussions among the academia and industry on comprehending, regulating, and mitigating the risks associated with LLMs in journalism, there is a notable absence of a standardized benchmark or systematic evaluation framework that assess the alignment of LLMs with journalistic ethics and safety standard and integrates them with common journalistic editorial tasks. Drawing on discussions about AI safety in journalism, this paper introduces NewsBench, a systematic evaluation framework which is focused on assessing the editorial capabilities of LLMs for not only journalistic writing proficiency but also safety adherence. For journalistic writing proficiency, we focus on language fluency, logical coherence, style alignment, and instruction fulfilment, while for safety adherence we consider six facets including civil language, bias and discrimination, personal privacy, social harm, journalistic ethics, and illegal activities. We construct the benchmark dataset with 1,267 test samples in the types of multiple choice and short answer questions in five editorial tasks including headline generation, summarization, continuation of writing, expansion of writing, and style refinement from 24 news domains. Additionally, NewsBench incorporates two automatic evaluation protocols for assessing LLM generations for short answer questions in terms of writing proficiency and safety adherence. Utilizing this comprehensive framework, we have evaluated eleven popular LLMs which can handle Chinese, providing insights into their performance across a diverse range of journalistic tasks and safety considerations. The main contributions of the paper are as follows: {itemize} We propose an evaluation framework for systematically evaluating LLMs on journalistic writing and safety, and we release 1,267 manually designed test samples featuring two types of short answer and multiple choice questions across five editorial tasks. Two GPT-4 based evaluation protocols for journalistic writing proficiency and safety compliance are developed and validated by human annotation. We conduct a comparative analysis and error assessment of eleven popular LLMs, identifying their strengths and weaknesses for editorial tasks in Chinese journalism. GPT-4 and ERNIE Bot are identified as leading models while they still have limitations in adhering to journalistic ethics in creative writing tasks, and LLMs with fewer parameters but more training tokens are performing better than those larger ones with fewer training tokens on our benchmark dataset. {itemize"
Enhancing Noise Robustness of Retrieval-Augmented Language Models with Adaptive Adversarial Training,2405.20978v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2405.20978v1_0.pdf,"Large Language Models (LLMs) exhibit substantial capabilities yet encounter challenges, including hallucination, outdated knowledge, and untraceable reasoning processes. Retrieval-augmented generation (RAG) has emerged as a promising solution, integrating knowledge from external databases to mitigate these challenges. However, inappropriate retrieved passages can potentially hinder the LLMs' capacity to generate comprehensive and high-quality responses. Prior RAG studies on the robustness of retrieval noises often confine themselves to a limited set of noise types, deviating from real-world retrieval environments and limiting practical applicability. In this study, we initially investigate retrieval noises and categorize them into three distinct types, reflecting real-world environments. We analyze the impact of these various retrieval noises on the robustness of LLMs. Subsequently, we propose a novel RAG approach known as Retrieval-augmented Adaptive Adversarial Training (RAAT). RAAT leverages adaptive adversarial training to dynamically adjust the model's training process in response to retrieval noises. Concurrently, it employs multi-task learning to ensure the model's capacity to internally recognize noisy contexts. Extensive experiments demonstrate that the LLaMA-2 7B model trained using RAAT exhibits significant improvements in F1 and EM scores under diverse noise conditions. For reproducibility, we release our code and data at: .","An illustrative example of the RAG process applied to question answering. The model predicts the correct answer with accurate retrieved text. However, it fails to produce the right answer when the retrieved text contains misleading or inaccurate information.","Large language models (LLMs) have garnered substantial attention in both academic and industrial research within the domain of artificial intelligence due to their remarkable capabilities . Despite their immense power, LLMs face challenges such as hallucinations and outdated knowledge . Moreover, a lack of domain knowledge may hinder their performance on domain-specific tasks . To mitigate these challenges, recent studies improve LLMs by retrieving passages from external databases and pretending them in context, constituting a framework known as retrieval-augmented language models (RALMs) . However, RALMs also present significant limitations. Previous studies have empirically demonstrated that retrieved noisy passages are problematic for LLMs, resulting in performance degradation. We term this issue as the noise robustness problem of RALMs. As illustrated in Figure~, the model can provide correct answers when the retrieving context is accurate and related to the query. However, when the retrieved context contains misleading or inaccurate information, the model may yield incorrect answers. As the retriever inherently cannot achieve complete accuracy, the presence of noise in the retrieved context is inevitable. Therefore, designing robust algorithms against retrieved noises is of great practical importance. Recently, several studies have attempted to enhance the noise robustness of RALMs through noisy training, which involves incorporating retrieved noisy contexts into fine-tuning data. While noisy training exhibits promise, its effectiveness heavily relies on the composition of the training dataset. Incorrectly introducing noises to the training data can lead to model overfitting, adversely affecting generalization. In practice, meticulous adjustment of the type and intensity of noises is essential to ensure the model's proficiency across various tasks and datasets. This demands significant experimentation and tuning, adding complexity to the development process. Moreover, the lack of clear classification for retrieval noises in current studies stands in contrast to the diverse range of noises encountered in real retrieval environments. This paper systematically explores three types of retrieval noises: (i) contexts that are superficially related to the query but lack the correct answer ({Relevant retrieval noise}), (ii) contexts that are irrelevant to the query ({Irrelevant retrieval noise}), and (iii) contexts that are topically related to the query but contain incorrect information ({Counterfactual retrieval noise}). Our empirical study indicates that LLMs exhibit varying robustness to these three types of noise. Compared to entirely irrelevant texts, texts that are superficially related to the query or those containing counterfactual details often lead to more misinformation. In response to diverse types of noises, we propose a novel approach named Retrieval-augmented Adaptive Adversarial Training (RAAT), which employs adaptive adversarial training to dynamically regulate the model's training process in response to retrieved noisy texts. Concretely, RAAT generates adversarial samples (noises) by considering the model's sensitivity to different types of noises during training, which aligns with the min-max paradigm of adversarial training. Moreover, RAAT utilizes multi-task learning to encourage the LLMs to generate tokens that are aware of noises, thereby enabling the model to internally recognize retrieved noisy contexts and improve the overall generation performance. The main contributions of this paper can be summarized as follows: {itemize}[leftmargin=*] We systematically explore three types of retrieval noises and investigate the sensitivity of LLMs to these diverse types of noises. We propose a novel adaptive adversarial training method (called RAAT) to enhance the robustness of RALMs against various retrieval noises. RAAT dynamically adjusts the training process of the model in diverse noise environments. In addition, it integrates multi-task learning to encourage the model to improve its ability to discern different types of noises. We set up a benchmark (named RAG-Bench) for assessing the noise robustness problem of RALMs based on three open-domain question-answering datasets. Experimental results demonstrate that our RAAT method enhances robustness across diverse retrieval noise environments. {itemize"
On the Impact of Calibration Data in Post-training Quantization and Pruning,2311.09755v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2311.09755v2_0.pdf,"Quantization and pruning form the foundation of compression for neural networks, enabling efficient inference for large language models (LLMs). Recently, various quantization and pruning techniques have demonstrated remarkable performance in a post-training setting. They rely upon calibration data, a small set of unlabeled examples that are used to generate layer activations. However, no prior work has systematically investigated how the calibration data impacts the effectiveness of model compression methods. In this paper, we present the first extensive empirical study on the effect of calibration data upon LLM performance. We trial a variety of quantization and pruning methods, datasets, tasks, and models. Surprisingly, we find substantial variations in downstream task performance, contrasting existing work that suggests a greater level of robustness to the calibration data. Finally, we make a series of recommendations for the effective use of calibration data in LLM quantization and pruning.}",Post-training compression methods rely upon calibration data to generate layer activations.,"Scaling is an essential component for unlocking new capabilities and improved performance in large language models (LLMs) . However, the pursuit of scale has led to models that demand significant energy and computational resources . Consequently, LLMs can be challenging to deploy, especially in resource-constrained environments . These challenges have ultimately motivated a substantial body of research on model compression techniques, aiming to reduce computational demands while maintaining performance . Quantization and pruning are two of the most popular model compression techniques . Pruning aims to remove redundant weights, while quantization seeks to represent weights (and possibly activations) in lower precision. Most recently, several quantization and pruning methods have demonstrated outstanding performance in a post-training setting . Post-training compression techniques rely upon {calibration data} to determine the distribution of layer activations. This process requires only a small number of examples, with further examples offering diminishing gains . In the case of LLMs, the calibration set is routinely sampled from web text or model pre-training data . Notably, the calibration examples are sampled randomly. This is because post-training model compression methods are considered robust to the specific distribution of calibration data . In this paper, we present the first empirical study on the impact of calibration data used in post-training LLM compression. We offer an extensive study with several quantization and pruning methods, across a range of tasks, datasets, and models. Surprisingly, we find that downstream task performance can vary substantially according to the selected calibration data. This contrasts existing work, which suggests a high level of robustness. Finally, we offer a series of recommendations for the effective use of calibration data"
A Sentiment Consolidation Framework for Meta-Review Generation,2402.18005v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.18005v2_0.pdf,"Modern natural language generation systems with Large Language Models (LLMs) exhibit the capability to generate a plausible summary of multiple documents; however, it is uncertain if they truly possess the capability of information consolidation to generate summaries, especially on documents with opinionated information. We focus on meta-review generation, a form of sentiment summarisation for the scientific domain. To make scientific sentiment summarization more grounded, we hypothesize that human meta-reviewers follow a three-layer framework of sentiment consolidation to write meta-reviews. Based on the framework, we propose novel prompting methods for LLMs to generate meta-reviews and evaluation metrics to assess the quality of generated meta-reviews. Our framework is validated empirically as we find that prompting LLMs based on the framework --- compared with prompting them with simple instructions --- generates better meta-reviews..}","The three-layer framework of the underlying information consolidation logic in meta-reviewing ($P$: Positive, $P^+$: Strongly positive, $N$: Negative, $N^+$: Strongly negative).","Notable strides have been made in abstractive text summarization with the advancement of Large Language Models (LLMs) over recent years. With even a simple instruction such as ``{tl;dr}'' or ``{please write a summary}'', these models can generate plausible summaries which are found more preferred over those written by humans. However, it is uncertain if these models truly possess the ability of information consolidation, especially when summarizing documents that are composed of opinionated information. The models may take shortcuts to generate texts instead of correctly understanding and aggregating information from the source documents and they may generate abstractive summaries with incorrect overall sentiment. Automated sentiment summarization holds significant importance and there have been sentiment summarization datasets; however, most of them are in the product review domain. These datasets are less interesting for investigating information consolidation as (1) the summaries are extractive, composed of a simple combination of extracted snippets, and (2) the summary of product reviews is about extracting the majority sentiment (which is a simple consolidation function). To address this, in this paper, we propose the task of scientific sentiment summarization, taking the meta-reviews in scientific peer review as summaries.{The representative peer review platform which is publicly available is {www.openreview.com}.} The investigation of meta-review generation presents an exciting opportunity for exploring the intricate process of multi-document information consolidation that involves complex judgement. This is because (1) meta-reviewers are supposed to understand not only all the reviews from different reviewers but also the multi-turn discussions between the reviewers and the author and write their comments to support the acceptance decision of the manuscript, (2) the logic of arguments (from reviewers and authors) has to be taken into account to arrive at the final sentiment in the meta-reviews and it is not a matter of majority voting and (3) meta-reviews have to recognize and resolve conflicts and consensus among reviewers. In this paper, we hypothesize that human meta-reviewers follow a three-layer sentiment consolidation framework as shown in~{three-layer-framework} to write meta-reviews based on reviews and multi-turn discussions in the peer review process. Human and automatic annotation is then conducted to extract sentiments and expressions on various review facets (e.g.,\ novelty and soundness) from corresponding source documents (i.e., reviews and discussions) and these judgements play a critical role in generating the meta-reviews. We also propose two evaluation metrics which focus on assessing sentiments in generated meta-reviews, and experiments empirically validate our proposed three-layer framework when they are integrated as prompts for LLMs to generate meta-reviews. Contributions of our paper: We hypothesize that human meta-reviewers follow a three-layer sentiment consolidation framework when writing meta-reviews; We collect human annotations on meta-reviews and corresponding source documents based on the consolidation framework; We propose two automatic metrics (reference-free and reference-based) to evaluate the sentiment in the generated meta-reviews. Experiments validate the empirical effectiveness of the framework when we incorporate it as prompts for LLMs to generate meta-reviews"
MuggleMath: Assessing the Impact of Query and Response Augmentation on Math Reasoning,2310.05506v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2310.05506v3_0.png,"In math reasoning with large language models (LLMs), fine-tuning data augmentation by query evolution and diverse reasoning paths is empirically verified effective, profoundly narrowing the gap between open-sourced LLMs and cutting-edge proprietary LLMs. In this paper, we conduct an investigation for such data augmentation in math reasoning and are intended to answer: (1) What strategies of data augmentation are more effective; (2) What is the scaling relationship between the amount of augmented data and model performance; and (3) Can data augmentation incentivize generalization to out-of-domain mathematical reasoning tasks? To this end, we create two new dataset AugGSM8K and AugMATH, by complicating and diversifying the queries and sampling multiple reasoning paths from GSM8K and MATH. We obtained a series of LLMs called MuggleMath by fine-tuning LLaMA models on AugGSM8K and AugMATH. MuggleMath substantially achieves new state-of-the-art on GSM8K and MATH. A log-linear relationship and a segmented log-linear are presented between MuggleMath's performance and the amount of augmented data on GSM8K and MATH, respectively. We also find that it is weak in out-of-domain math reasoning generalization from AugGSM8K to MATH and from AugMATH to GSM8K, which suggests that augmenting queries that cover a broader range of subjects is more beneficial for generalization. We release our codes and augmented data in .",Comparison of test set accuracy on GSM8K for models of varying scales after fine-tuning on AugGSM8K subsets with different query augmentation strategies.,"The emergence of large language models (LLMs) has profoundly revolutionized the field of natural language processing, exhibiting versatile performance in various tasks like code generation, instruction following, long context question answering, and math reasoning. Math reasoning as a representative reasoning task is widely studied to access the reasoning abilities in LLMs. Proprietary LLMs, such as GPT-3.5, and GPT4 have shown exceptional mathematical reasoning abilities, while there remains a substantial gap between open-source LLMs, such as GPT-J and LLaMA) and the cutting-edge proprietary models. To enable better mathematical reasoning abilities in open-sourced LLMs, they generally undergo a fine-tuning stage on supervised reasoning datasets. A series of efforts are committed to enhancing the mathematical reasoning capabilities of open-source LLMs, where a mainstream approach involves first augmenting new mathematical problems and answers, followed by supervised fine-tuning on the augmented dataset. This type of approach has achieved good results, and in this paper, we would like to explore what are the key factors affecting the effectiveness of data augmentation for mathematical reasoning tasks and the scaling relationship between the amount of data augmentation and model performance. Specifically, with the help of proprietary models (GPT-3.5 and GPT-4), we applied five types of mathematical problem augmentation methods based on human experience in creating variations of mathematical problems similar to . We further generated multiple reasoning paths for each augmented problem since distinct reasoning paths can also enhance chain-of-thought reasoning. We obtained two new datasets called AugGSM8K and AugMATH after data augmentation on two widely used mathematical reasoning datasets GSM8K and MATH. By supervised fine-tuning on the open-source LLaMA and LLaMA-2 LLMs on AugGSM8K and AugMATH, we obtained a series of models dubbed MuggleMath. We find that with sufficient amounts of data, MuggleMath achieves a new state-of-the-art on GSM8K and MATH. In addition to this, we find a log-linear relationship between the performance of MuggleMath and the amount of data augmentation over a range of data volumes on GSM8K and a segmented log-linear relationship on MATH. Although MuggleMath achieves strong performance on the GSM8K and MATH test set, the rationales for performance improvement by data augmentation remain unclear. We are therefore interested in the specific reason behind the performance improvement and whether it brings enhancement in LLMs' mathematical reasoning capabilities generally. To validate the generalization of MuggleMath, we conduct multi-task learning and analyze the transferability with AugGSM8K and AugMATH. We found that LLMs trained with supervised learning after data augmentation on GSM8K only bring marginal improvements to performance on MATH and the similar conclusion is fit for AugMATH and GSM8K. By visualizing the data distribution in the embedding space of LLaMA-2-7B, we observe that the embedding distribution of problems in AugGSM8K is very close to that of GSM8K, but significantly different from the problem distribution in the MATH dataset. The reason behind can be attributed to the fact that GSM8K and MATH have different reasoning difficulty, response style and require different mathematical knowledge. The main contributions of our work can be summarized as follows: {itemize} By augmenting GSM8K and MATH with various queries and multiple reasoning paths, we curate GSM8K and MATH to two new datasets named AugGSM8K and AugMATH. We utilize AugGSM8K and AugMATH for fine-tuning the LLaMA and LLaMA-2 models to obtain MuggleMath, which greatly improves the in-domain performance of the open-sourced LLMs on GSM8K and MATH, achieving new state-of-the-art performances. We find a log-linear relationship between the accuracy of the model on the test set and the amount of data augmentation within a certain range while the coefficient is similar to augmenting new human-written samples on GSM8K. When it comes to MATH, a a segmented log-linear relationship is found. We demonstrate that the performance gains from data augmentation on GSM8K and MATH are difficult to generalize to each other, which indicates a need of diverse original queries in augmenting math data. {itemize"
BinaryAlign: Word Alignment as Binary Sequence Labeling,2407.12881v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2407.12881v1_0.png,"Real world deployments of word alignment are almost certain to cover both high and low resource languages. However, the state-of-the-art for this task recommends a different model class depending on the availability of gold alignment training data for a particular language pair. We propose BinaryAlign, a novel word alignment technique based on binary sequence labeling that outperforms existing approaches in both scenarios, offering a unifying approach to the task. Additionally, we vary the specific choice of multilingual foundation model, perform stratified error analysis over alignment error type, and explore the performance of BinaryAlign on non-English language pairs. We make our source code publicly available.{https://github.com/ubisoft/ubisoft-laforge-BinaryAlignWordAlignementasBinarySequenceLabeling}}","Example of alignment of an approximate translation, as often encountered in real-world applications. Links in red indicate situations where one word is aligned with several contiguous or non-contiguous words. The green line represent a situation where a word is untranslated which happens in many language pairs.","Word alignment refers to the task of uncovering word correspondences between translated text pairs. The automatic prediction of word alignments dates back to the earliest work in machine translation with the IBM models where they were used as hidden variables that permit the use of direct token to token translation probabilities. While state of the art machine translation techniques have largely abandoned the use of word alignment as an explicit task other use cases for alignments have emerged including lexical constraint incorporation , analysing and evaluating translation models , and cross-lingual language pre-training . In many real-world applications word alignment must be performed across several languages, often including languages with manually annotated word alignment data and others lacking such annotations. We refer to those languages as high and low-resource languages respectively. While word alignment for high-resource languages can be learned in a few-shot or fully supervised setting depending on the amount of data, for low-resource languages zero-shot learning strategies must be employed due to data scarcity. State-of-the-art supervised techniques formalize the task of word alignment as a collection of SQuAD-style span prediction problems while in zero-shot settings the best performing methods induce word alignment from the contextualized word embeddings of mulitingual pre-trained language models (mPLMs) . From a practical perspective, this discrepancy in the preferred method adds complexity to the deployment of word alignment models in real-world applications where both high and low-resource languages must be supported. We observe a deeper issue that both span prediction and contextualized word embeddings are sub-optimal as each induces a bias in word alignment models that limits their accuracy. Span prediction methods cannot robustly deal with discontinuous word alignments without relying on complex post-processing and hyper-parameter tuning. Contextualized word embeddings method cannot deal effectively with untranslated words and one-to-multiple alignments because they rely on a softmax function that normalizes predictions at a sentence-level while in word alignment; one token being aligned to token $T$ does not mean that another token is less likely to be aligned to $T$. This poses word alignment as a single-label classification problem, while in reality it is better viewed as a series of binary classifications applied to each possible pair of words. Figure shows some cases of one-to-multiple alignments, non-contiguous spans and untranslated words. In this paper, we present BinaryAlign, a novel word alignment solution that outperforms the state-of-the-art in zero-shot, few-shot and fully-supervised settings. In particular, we reformulate word alignment as a set of binary classification tasks in which an individual alignment prediction is made for each possible pair of words. This reformulation of the task outperforms all previous approaches over five different language pairs with varying levels of supervision"
Quantifying the Persona Effect in LLM Simulations,2402.10811v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.10811v2_0.pdf,"Large language models (LLMs) have shown remarkable promise in simulating human language and behavior. This study investigates how integrating persona variables—demographic, social, and behavioral factors—impacts LLMs' ability to simulate diverse perspectives. We find that persona variables account for <10\% variance in annotations in existing subjective NLP datasets. Nonetheless, incorporating persona variables via prompting in LLMs provides modest but statistically significant improvements. Persona prompting is most effective in samples where many annotators disagree, but their disagreements are relatively minor. Notably, we find a linear relationship in our setting: the stronger the correlation between persona variables and human annotations, the more accurate the LLM predictions are using persona prompting. In a zero-shot setting, a powerful 70b model with persona prompting captures 81\% of the annotation variance achievable by linear regression trained on ground truth annotations. However, for most subjective NLP datasets, where persona variables have limited explanatory power, the benefits of persona prompting are limited.}",Illustration of persona prompting. We prepend the persona information of an annotator before the text sample and task description to investigate the capacity of LLMs to simulate diverse perspectives in subjective NLP tasks.,"Annotation questions such as ``how do you feel emotionally after reading this text'' are subjective - there are rarely definitive right or wrong answers. This subjectivity is increasingly being recognized within the NLP community. Subjective NLP tasks are typically characterized by low inter-annotator agreement, making label aggregation inappropriate. Previous research has established the significant influence of sociodemographic variables on the annotations of these tasks~[{inter alia}]{sap-etal-2022-annotators, santy-etal-2023-nlpositionality,pei-jurgens-2023-annotator}. One approach to model these persona variables{In our work, we adopt a broad definition of {persona variables} to include not only demographic and social variables but also other variables that could help describe a persona, such as variables relating to attitudes, behaviors, lived experiences, and values. It is worth noting that most NLP datasets have no information of any kind available about the annotators. } is to use LLMs. LLMs have been effectively utilized for role-playing and simulating human behavior, primarily by defining the persona of interest within the prompt. Their success has even spurred debates on whether LLMs could replace human subjects. However, there are also concerns about such ``persona prompting'' methodology (Figure ), citing ecological fallacy, and LLMs' susceptibility to caricatures, misportrayal and erasure of subgroup heterogeneity. Existing studies have often sought to measure the effects of individual persona variables, overlooking a holistic analysis of the potential explanatory power of persona variables on annotation variance. It is then hard to contextualize the models' ability to utilize persona information. To address this issue, our research explores the following questions: {{RQ1}: How much variance in human annotation could persona variables explain?} Understanding this will help us assess the overall influence of persona variables on human annotation, providing context to our subsequent investigations. We propose employing a linear regression analysis to predict annotations using persona variables and examine the resulting $R^2$ values. We find that persona variables explain relatively little variance (<10\ {{RQ2}: Can incorporating persona variables via prompting improve LLMs' predictions?} Building on RQ1, we assess how much the explained variance by persona variables translates into prediction gains in LLMs. We find that incorporating persona variables provides modest but statistically significant improvements (Section ). {{RQ3}: For what types of samples is persona prompting most useful?} To better understand the utility of persona prompting, we examine its impact across sample types, in terms of annotation entropy and standard deviation. We identify that most gains occur in samples characterized by frequent annotator disagreements within a relatively narrow range (high entropy-low standard deviation), suggesting that models can adjust their annotation to suit the persona, though not drastically (Section ). {{RQ4}: How effectively can LLMs simulate personas when the importance of persona variables varies?} Using a set of survey questions, where persona variables explain the responses to varying degrees, we apply persona prompting to LLMs. We find a linear relationship in our setting: the more persona variables are correlated with the outcome variable, the better LLMs predictions are using persona prompting. Large, preference-tuned models perform best and can explain up to 81\"
SyllabusQA: A Course Logistics Question Answering Dataset,2403.14666v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2403.14666v2_0.pdf,"Automated teaching assistants and chatbots have significant potential to reduce the workload of human instructors, especially for logistics-related question answering, which is important to students yet repetitive for instructors. However, due to privacy concerns, there is a lack of publicly available datasets. We introduce }~}, an open-source dataset with $63$ real course syllabi covering $36$ majors, containing $5,078$ open-ended course logistics-related question-answer pairs that are diverse in both question types and answer formats. Since many logistics-related questions contain critical information like the date of an exam, it is important to evaluate the factuality of answers. We benchmark several strong baselines on this task, from large language model prompting to retrieval-augmented generation. We introduce Fact-QA, an LLM-based (GPT-4) evaluation metric to evaluate the factuality of predicted answers. We find that despite performing close to humans on traditional metrics of textual similarity, there remains a significant gap between automated approaches and humans in terms of fact precision.","Domain diversity in \mbox{\textsc{SyllabusQA}} covering $36$ majors. For visual clarity, we show representative majors.","In educational applications, artificial intelligence (AI) approaches have shown significant promise in improving learning outcomes , by automatically providing feedback to students or engaging in tutoring dialogues with them. The key idea is to use AI to create an on-demand virtual {teaching assistant} to interact with many students simultaneously; see, e.g., Khamigo from Khan Academy . These approaches can scale up the effort of expert human teachers and tutors, and relieve them from doing repetitive tasks so that they can focus on providing personalized feedback or designing new learning content . In higher education, one promising avenue for AI-powered teaching assistants to reduce human effort is {course logistics-related} question answering (QA): answering student questions on logistics whose answers can be directly found or inferred from the syllabus. There exist many approaches for automated QA in online courses (both logistics-related and content-related), using tools from rule-based AI systems and expert systems with knowledge bases to end-to-end text generation . Recently, large language model (LLM)-based approaches have shown great promise to improve the coverage and answer quality over traditional QA approaches . See Section~ in the Supplementary Material for a detailed discussion on related work. Unfortunately, these approaches are mostly developed and evaluated on proprietary data due to student privacy concerns, which prevents more researchers from contributing to the development of automated QA systems for education. For evaluation, especially in logistics-related QA that often contains critical information, the factuality of predicted answers is more important than measuring surface textual features. Moreover, text similarity metrics may not be suitable for some open-ended natural language generation tasks. As an example, the answer ``The final exam will be on Dec 15'', has high surface-level textual similarity with the reference answer, ``The final exam is on Dec 14'', but contains a critical factual error that may lead to significant negative consequences to students. Meanwhile, human instructors and teaching assistants often answer student questions in a concise way, without giving any unnecessary information. Therefore, it is important for LLM-based approaches to generate answers that are both concise and precise. {Contributions} In this paper, we introduce the {{SyllabusQA}} dataset for course logistics-related QA. We publicly release this dataset and hope that it can be a benchmark for future work on developing and evaluating automated QA approaches for teaching assistance. Our contributions are: {First}, we collect a diverse set of $63$ real course syllabi covering $36$ majors across $12$ universities, and employ crowd annotators to write $5,078$ logistics-related QA pairs with the goal of simulating what students would ask in a real-world course. {Second}, we detail the diverse composition of syllabi and QA pairs in {{SyllabusQA}}, in terms of syllabi domain, question types, answer sources, and different language styles. We lay out metrics to evaluate different aspects of open-ended automated QA approaches, in terms of both surface textual similarity and more importantly, the factuality of predicted answers grounded in the syllabus. {Third}, we conduct extensive experiments to benchmark the QA performance of several strong baselines on {{SyllabusQA}}. Overall, LLM-based approaches perform similar to humans on surface-level textual similarity metrics but worse on factuality metrics. We found that fine-tuning on real QA pairs from {{SyllabusQA}} performs much better than LLM prompting approaches and that retrieval-augmented generation is especially important. To the best of our knowledge, {{SyllabusQA}} is the first publicly available real-world course logistics-related QA dataset. {{SyllabusQA}} assesses automated QA models on various natural language understanding aspects including handling challenging question types, from reasoning-based ones to adversarial ones, understanding of long input documents sourced from diverse course majors, processing complex input formatting including tables and schedules, and answering open-ended questions in a similar way as human instructors and TAs. Table~ highlights the difference between {{SyllabusQA}} and existing datasets; see also Section~ for a detailed discussion"
MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models,2308.09729v5,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2308.09729v5_0.pdf,"Large language models (LLMs) have achieved remarkable performance in natural language understanding and generation tasks. However, they often suffer from limitations such as difficulty in incorporating new knowledge, generating hallucinations, and explaining their reasoning process. To address these challenges, we propose a novel prompting pipeline, named , that leverages knowledge graphs (KGs) to enhance LLMs' inference and transparency. Our method enables LLMs to comprehend KG inputs and infer with a combination of implicit and external knowledge. Moreover, our method elicits the mind map of LLMs, which reveals their reasoning pathways based on the ontology of knowledge. We evaluate our method on diverse question \& answering tasks, especially in medical domains, and show significant improvements over baselines. We also introduce a new hallucination evaluation benchmark and analyze the effects of different components of our method. Our results demonstrate the effectiveness and robustness of our method in merging knowledge from LLMs and KGs for combined inference. To reproduce our results and extend the framework further, we make our codebase available at {https://github.com/wyl-willing/MindMap}.","A conceptual comparison between our method and the other prompting baselines: LLM-only, document retrieval + LLM, and KG retrieval + LLM.","Scaling large language models (LLMs) to billions of parameters and a training corpus of trillion words was proved to induce surprising performance in various tasks . Pre-trained LLMs can be adapted to domain tasks with further fine-tuning or be aligned with human preferences with instruction-tuning . Nonetheless, several hurdles lie in the front of steering LLMs in production: {itemize}[leftmargin=*] {Inflexibility}. The pre-trained LLMs possess outdated knowledge and are inflexible to parameter updating. Fine-tuning LLMs can be tricky because either collecting high-quality instruction data and building the training pipeline can be costly , or continually fine-tuning LLMs renders a risk of catastrophic forgetting . {Hallucination}. LLMs are notoriously known to produce hallucinations with plausible-sounding but wrong outputs , which causes serious concerns for high-stake applications such as medical diagnosis. {Transparency}. LLMs are also criticized for their lack of transparency due to the black-box nature . The knowledge is implicitly stored in LLM's parameters, thus infeasible to be validated. Also, the inference process in deep neural networks remains elusive to be interpretable. {itemize} As a classic way to build large-scale structural knowledge bases, knowledge graphs (KG) are established by the triples of entities and relations, i.e., $\{{head}, {relation}, {tail}\}$. They can provide explicit knowledge representation and interpretable reasoning paths. Besides, KGs are amenable to continual modifications to debug the existing knowledge or add new knowledge. Due to their flexibility, preciseness, and interpretability, KGs emerged as a promising complement to the drawbacks of LLMs . For instance, KG triples were added to the training of LLMs or KG encoders were entangled with LLM layers for joint inference and optimization on graph and text data . By contrast, our work pivots on the synergistic inference of KGs and fixed LLMs, which is applicable to powerful pre-trained LLMs, such as commercial LLM-as-service APIs. In general, the prior arts in this venue can be categorized into two genres: {itemize}[leftmargin=*] {Retrieval-Augmented LLM Inference}. Researchers tried to retrieve documents to augment LLM inference while suffering from inaccurate retrieval and lengthy documents . Recently, several attempts were made to incorporate extracted KG triples into the prompt to LLMs to answer KG-related questions . However, this approach treats KG inputs as plain text and ignores their graphical structure, which causes the generated response to be hard to validate and vulnerable to hallucinations. {Graph Mining with LLMs}. There were also attempts to prompt LLMs to comprehend graphical inputs, while they primarily experimented with graph mining tasks, e.g., edge detection and graph summarization . It was rarely explored in text generation tasks that require complex reasoning across multiple evidence graphs grounded on KGs. {itemize} The goal of this work is to build a plug-and-play prompting approach to elicit the graph-of-thoughts reasoning capability in LLMs. We call our method because it enables LLMs to comprehend graphical inputs to build their own mind map that supports evidence-grounded generation. A conceptual demonstration of our framework is in Figure~. Specifically, sparks the graph of thoughts of LLMs that (1) consolidates the retrieved facts from KGs and the implicit knowledge from LLMs, (2) discovers new patterns in input KGs, and (3) reasons over the mind map to yield final outputs. We conducted experiments on three datasets to illustrate that outperforms a series of prompting approaches by a large margin. This work underscores how LLM can learn to conduct synergistic inference with KG. By integrating both implicit and explicit knowledge, LLMs can achieve transparent and dependable inference, adapting to different levels of correctness in additional KG information"
Examining the robustness of LLM evaluation to the distributional assumptions of benchmarks,2404.16966v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2404.16966v2_0.pdf;/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2404.16966v2_1.pdf,"Benchmarks have emerged as the central approach for evaluating Large Language Models (LLMs). The research community often relies on a model's average performance across the test prompts of a benchmark to evaluate the model's performance. % on the benchmark task. This is consistent with the assumption that the test prompts within a benchmark represent a random sample from a real-world distribution of interest. We note that this is generally not the case; instead, we hold that the distribution of interest varies according to the specific use case. We find that [(1)] the correlation in model performance across test prompts is non-random, accounting for correlations across test prompts can change model rankings on major benchmarks, explanatory factors for these correlations include semantic similarity and common LLM failure points.","Illustrative example showcasing how different distributional assumptions of benchmarks affect model rankings. Consider a benchmark containing prompts reflecting three different tasks: math (red triangles), code generation (blue circles), and text generation (green squares). In Figure~\ref{subfig:unweighted}, each benchmark prompt contributes equally to the model evaluation. In contrast, Figure~\ref{subfig:weighted} accounts for correlations between prompts and the weights of the prompts are adjusted accordingly during evaluation. In scenario~\ref{subfig:unweighted}, the red LLM ranks highest because it excels in math, and the benchmark is biased towards math tasks (7 out of 12 prompts are math-related). When considering different weights in scenario~\ref{subfig:weighted}, we observe a different ranking outcome.","Since the introduction of the Transformer architecture, Large Language Models (LLMs) have progressed into sophisticated systems with an outstanding ability to comprehend and generate text that mimic human language. Notable models in this domain include ChatGPT{New chat: {https://chat.openai.com/}}, utilizing the GPT-3.5-TURBO or GPT-4 architectures{Models - OpenAI API: {https://platform.openai.com/docs/models/}}, LLaMA, ChatGLM, Alpaca, and Falcon. Due to their effectiveness, LLMs are becoming very popular in both academia and industry, making their evaluation crucial. However, this effectiveness comes at the cost of increased complexity, which makes their evaluation very challenging. Although prior research has introduced benchmarks for different tasks along with evaluation measures, these assessments often overlook potential biases. When a benchmark includes multiple prompts with similar characteristics, it can increase or decrease the average performance of a model, so model comparisons can become brittle with respect to benchmark composition (see Figure~ for an illustrative example). In this work, we show that the inherent connections between the prompts in current benchmarks impact the models' performance and their relative rankings. The standard approach for evaluation on a benchmark is to {inparaenum}[(i)] obtain model responses for each prompt in the benchmark, compute the performance metrics for each response, aggregate (usually average) the performance metrics to obtain a single performance metric over the benchmark, and compare models by comparing their aggregate performance. {inparaenum} When aggregating performance metrics in step~ above, each prompt is generally weighted equally. However, using equal weights reflects the assumption that prompts in the benchmark are ``equal'', in the sense that prompts are representative samples of a target distribution of interest. In the case of LLMs, the notion of a target distribution (i.e., the distribution of all possible prompts for a given use case) is usually not well-defined. For example, different Natural Language Inference (NLI) applications may have very different target distributions, and we should not expect a single benchmark to capture every one. Therefore, one must ask: {What distribution do the prompts in the benchmark represent?} Would considering different distributions fundamentally change model comparisons? In this work, we present a novel approach to assess the robustness and adequacy of benchmarks used in evaluating LLMs, by analyzing the performance of multiple LLMs on a set of four major benchmarks. Our key contributions are outlined below: {asparaenum} For each considered benchmark, we observe that the correlation of model performance across prompts is significant (p-value < 0.05). This demonstrates the existence of relationships between prompts within the investigated benchmarks. We explore the robustness of model comparisons to different distributional assumptions based on correlation structure, and we observe shifts in performance as large as 10\ We provide a characterization of performance over the distribution of all possible prompt weights. This constitutes a robustness check that can be incorporated in comparative studies. We show that model performance similarity across prompts can be explained by semantic similarity, but it is most likely derived by common failure points of the LLM. {asparaenum"
Bridging the Preference Gap between Retrievers and LLMs,2401.06954v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2401.06954v2_0.png,"Large Language Models (LLMs) have demonstrated superior results across a wide range of tasks, and Retrieval-augmented Generation (RAG) is an effective way to enhance the performance by locating relevant information and placing it into the context window of the LLM. However, the relationship between retrievers and LLMs in a RAG is still under-investigated. Most existing work treats the retriever and the LLM as independent components and leaves a gap between retrieving human-``friendly'' information and assembling a LLM-``friendly'' context. In this work, we examine a novel bridge mechanism. We validate the ranking and selection assumptions of retrievers in the context of RAG and propose a framework that chains together supervised and reinforcement learning to train a bridge model that optimizes the connection between the retriever and the LLM. Empirical results demonstrate the effectiveness of our method in both question-answering and personalized generation tasks.","We observe a preference gap when alternating the ranking and selection of information in RAG. Experiments are conducted with retrieving passages using GTR~\cite{ni2021large} and using top K of them as additional context for a frozen Palm2-S LLM. %``Top-K'' means we select the top-k GTR retrieved passages. Different colors indicate different datasets (detailed in Sec.~\ref{sec:datasets}) and the Y-axis shows the relative percentage. %The selected passages are used as additional context. Alternating the selection (Top-1) of information significantly affects (either positively or negatively) the LLM's performance, while randomizing the ranking of multiple selected items (Top-5) does not have a comparable impact (the metrics are detailed in Sec.~\ref{sec.parameter}). Note the impact on NQ is even too small to be visible.","Large language models (LLMs) such as GPT-4 and PaLM 2 , have demonstrated impressive performance on a wide variety of tasks. Retrieval-augmented generation (RAG), which retrieves knowledge items from an external data source and puts it into the context window of LLMs, has produced significantly enhanced results in many NLP tasks . However, most works on RAG study retrievers and LLMs {separately}. On one hand, most retrievers are designed to be {human-friendly}, usually based on the {general belief} in classic information retrieval literature that ranking is paramount, as humans typically read from top to bottom . On the other hand, LLMs exhibit preferences different from humans and yield accurate results only when the information in the prompt aligns with these preferences. This discrepancy leads to sub-optimal design in current RAG systems, a phenomenon we term {preference gap}. This gap manifests in various aspects. For example, the general belief in {ranking} may not align with LLM's preferences due to the self-attention mechanism of Transformers, which can focus on any token regardless of its position. Another aspect is {selection}; while humans can easily disregard irrelevant information in a context, it has been shown that LLMs are highly sensitive to irrelevant content . There likely exist more aspects that further diverge the LLM's preference from that of humans, e.g., {repetition}. Repeated information is generally considered detrimental for retrieval systems , but it may be useful to LLMs for weighting the relevance of context items. We empirically investigate this preference gap, focusing specifically on {ranking} and {selection}. As shown in Fig.~, when we randomize the ordering of top-5 retrieved items (in our case, passages), the performance of RAG only varies by around 1\ Existing work has tried to finetune the LLMs to align with the retriever or adjust the retriever to align with the LLM. However, finetuning LLMs, especially at the scale of GPT-4 or Palm 2, is often expensive. Similarly, it is difficult to adjust production-level retrievers such as Google or Bing. Even when the retriever is adjustable, existing efforts often focus on re-ranking the retrieved results and fail to address other aspects of preference such as selection or repetition. Instead, we propose a novel and practical framework called {BGM} ({B}ridging the {G}ap between retrievers and LL{M}s), which keeps the retriever and LLM fixed and trains a bridge model in between. The bridge model aims to transform the retrieved information into a format that LLMs prefers and can effectively work with. Without loss of generality, we structure the bridge model as a {sequence-to-sequence (seq2seq)} model, which allows dynamically selecting items, re-ranking them, and potentially broader operations like repeating some of them in the retrieval-augmented prompt. Training such a bridge model is challenging as there are usually no ground truth labels on ideal {item sequences} for retrieval augmentation. Existing work has tried to derive supervisory signals for ranking from RAG's downstream task performance, such as perplexity distillation . Nevertheless, these methods only provide pointwise supervisory signals for each item independently. Directly applying the same idea to obtain sequential supervision is infeasible, since it would require feeding all possible item sequences into the LLM to obtain perplexity. We developed a greedy search approach to solve this problem (Sec. ). Moreover, we find sequential supervision can be too sparse to effectively train such a seq2seq model (Table~). To address this issue, we employed reinforcement learning (RL) on the SL trained bridge model, regarding the downstream task performance as the reward and the bridge model as a policy model. Chaining SL and RL provides increased supervision from the downstream task. It also offers the flexibility to explore more advanced strategies, such as repetition, in forming the optimal passage sequence. Our experiments reveal that BGM can enhance the performance of various downstream tasks, such as Question Answering (QA) and personalized generation, across a spectrum of datasets, from public QA and amazon reviews to private email conversations. Notably, the modified passages retrieved by BGM surpass the performance of strong retrievers and baseline reranking models. This underscores the significance and promise of the ``bridge'' approach in the realm of RAG. In summary, our contributions can be summarized as follows: {itemize} {-2mm} We empirically establish the existence of the preference gap between retrievers and LLMs, and introduce BGM to address this gap. {-2mm} We propose a seq2seq bridge model to jointly accomplish reranking and selection, adapting the retrieved information to be LLM-friendly. We employ a SL and RL training scheme to optimize this adaptation process. {-2mm} We evaluate BGM with diverse tasks, including QA and text generation, with publicly available and personalized datasets. The evaluation underscores the effectiveness of BGM in bridging the preference gap and improving RAG performance in downstream tasks. {itemize"
Large Language Models Can Learn Temporal Reasoning,2401.06853v6,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2401.06853v6_0.pdf,"While large language models (LLMs) have demonstrated remarkable reasoning capabilities, they are not without their flaws and inaccuracies. Recent studies have introduced various methods to mitigate these limitations. Temporal reasoning (TR), in particular, presents a significant challenge for LLMs due to its reliance on diverse temporal concepts and intricate temporal logic. In this paper, we propose TG-LLM, a novel framework towards language-based TR. Instead of reasoning over the original context, we adopt a latent representation, temporal graph (TG) that enhances the learning of TR. A synthetic dataset (TGQA), which is fully controllable and requires minimal supervision, is constructed for fine-tuning LLMs on this text-to-TG translation task. We confirmed in experiments that the capability of TG translation learned on our dataset can be transferred to other TR tasks and benchmarks. On top of that, we teach LLM to perform deliberate reasoning over the TGs via Chain-of-Thought (CoT) bootstrapping and graph data augmentation. We observed that those strategies, which maintain a balance between usefulness and diversity, bring more reliable CoTs and final results than the vanilla CoT distillation..}",Our framework (TG-LLM) performs temporal reasoning in two steps: 1) Text-to-Temporal Graph translation: generate (relevant) temporal graph given the context and keyword (extracted from questions); 2) Temporal Graph Reasoning: perform Chain-of-Thought reasoning over the temporal graph.,"As one of the fundamental abilities, temporal reasoning (TR) plays an important role in human perception. It is not just about understanding basic concepts such as ordering or duration; it extends to more intricate aspects, e.g., task planning or causal relation discovery. Recently, large language models (LLMs) have emerged with some reasoning capabilities . However, there is observation that they still can not perform TR sufficiently well , preventing their applications from solving complex real-world problems. In particular, TR requires a combination of various skills including mathematical and logical reasoning as well as commonsense knowledge . Recent works mainly adopt general approaches to investigate and improve the TR capability of LLMs. For example, benchmark the leading LLMs on different TR tasks with standard Input/Output prompt, few-shot in-context learning (ICL) and Chain-of-Thought (CoT) reasoning . Similarly, designs several specific types of prompts as prompt tuning. introduce pre-defined Python programs/rule-based templates to perform supervised fine-tuning (SFT). In addition, adopt some extra strategies, which include specific pre-training, instruction tuning and reinforcement learning. Despite the effectiveness of such methods, they either ignore or not explicitly involve the intrinsic nature of TR. Humans perform complex TR on a timeline of events which are aligned with the entities and relations. These temporal concepts (e.g., ordering, duration, frequency, typical time) are then rigorously defined based on the timeline information. In other words, the aligned timeline (more generally, the temporal graph, TG) serves as a latent representation to help humans understand the patterns in TR. However, due to the lack of ground truth, the high-quality TG translation is a challenging task for most TR benchmarks. To solve this problem, we propose a synthetic dataset (TGQA), which is fully controllable and requires minimal supervision. We demonstrate the capability of TG translation learned on our dataset can be transferred to other TR tasks and benchmarks. Given a reliable TG, the key challenges of teaching TR to LLMs include: (1) How can one introduce the necessary arithmetic and commonsense knowledge involved in TR? Prior work shows that explicitly introducing knowledge into context enhances the performance of LLMs. In this paper, we first identify all the valid time expressions, and then generate related knowledge (e.g., temporal relation and time gap between the timestamps, and the relative order of the gaps). (2) How can one teach LLM to perform deliberate reasoning? Generally, there exist two roadmaps: (i) translating natural language into logical statements, and using external symbolic engine for reasoning ; (ii) using LLMs directly as the reasoning engine . For (i), the difficulty lies in accurate translation and the limited expressive power of formal logic. For (ii), there is no guarantee for the correctness of generated intermediate steps especially with insufficient training data . In this paper, we adopt (ii) with the proposed bootstrapping method to generate reliable intermediate steps for supervised fine-tuning. We further improve the model performance with graph data augmentation, which mitigates the data deficiency in TR tasks. To be specific, our contributions are summarized as follows: {itemize} We propose a new paradigm, TG-LLM, for language-based TR. In this framework, we first translate the context into a latent representation (temporal graph), and then perform reasoning on it. Extensive experiments prove that our novel approach results in superior performance compared to the baselines. We design two approaches including Chain-of-Thought bootstrapping and graph data augmentation to teach LLM to generate consistent and faithful CoTs, which brings better performance than the vanilla CoT distillation. We present a pipeline to create a synthetic dataset (TGQA) for question answering that requires TR. It is fully controllable and requires minimal supervision for text-temporal graph alignment. We show in experiments that fine-tuning on our dataset benefits LLM on other TR tasks and benchmarks. {itemize"
Characterizing Similarities and Divergences in Conversational Tones in Humans and LLMs by Sampling with People,2406.04278v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.04278v1_0.pdf,"Conversational tones -- the manners and attitudes in which speakers communicate -- are essential to effective communication. Amidst the increasing popularization of Large Language Models (LLMs) over recent years, it becomes necessary to characterize the divergences in their conversational tones relative to humans. However, existing investigations of conversational modalities rely on pre-existing taxonomies or text corpora, which suffer from experimenter bias and may not be representative of real-world distributions for the studies' psycholinguistic domains. Inspired by methods from cognitive science, we propose an iterative method for simultaneously eliciting conversational tones and sentences, where participants alternate between two tasks: (1) one participant identifies the tone of a given sentence and (2) a different participant generates a sentence based on that tone. We run 100 iterations of this process with human participants and GPT-4, then obtain a dataset of sentences and frequent conversational tones. In an additional experiment, humans and GPT-4 annotated all sentences with all tones. With data from 1,339 human participants, 33,370 human judgments, and 29,900 GPT-4 queries, we show how our approach can be used to create an interpretable geometric representation of relations between conversational tones in humans and GPT-4. This work demonstrates how combining ideas from machine learning and cognitive science can address challenges in human-computer interactions.","Summary of our approach. {A}: Problem statement. {B}: The Sampling with People paradigm that aims to collect a representative sample of conversational tones and sentences. {C}: A quality-of-fit rating procedure that allows us to obtain vector representations of conversational tones with respect to their usage context. {D}: A geometric representation of the shared embedding space across elicited domains (human, GPT). {E}: As an application of our obtained data, we benchmark a selection of popular unsupervised cross-domain alignment methods.","Conversational tones, the manner and attitude in which a speaker communicates, is essential to human communication. Effective communication relies on people's understanding of conversational patterns and tones, and their ability to promptly react to them. Inability to do so results in the content of conversation being ``lost-in-translation'' between speakers of different languages and cultures. Notably, while traditionally the study of conversational tones involved only humans, the increasing prevalence of Large Language Models (LLMs) in everyday decision-making, especially their conversational (``Chat'') variants, renders the study of conversational tones in LLMs necessary for human alignment. Developing tools for effectively characterizing conversational tones in humans and LLMs is hence essential for the development of human-centered AI, human-computer interaction research, and cognitive sciences (Figure A). {{Background: Conversation Research.}} In conversation research, some literature engages explicitly with the composition and usage of conversational attitudes and linguistic markers . A wider array of literature uses conversational analysis to investigate other dynamics that can affect the content of conversation, such as turn-taking and face-saving, as well as other cross-cultural semantic differences that can lead to different behavior within the same conversational tone, such as refusal, shame, and politeness. On the other hand, the introduction of Large Language Models, especially its chatbot applications, brings attention to the alignment of conversational behavior in LLMs with that of human ideals, which can potentially contribute to the alignment of LLMs' perception and production of conversational tones with those of humans. Being able to effectively fine-tune LLMs also creates new opportunities to generate text style-transfer corpora that specify sentences with specific conversational tones, such as politeness and formality. {{Challenge: biased apriori taxonomy.}} However, the domain of conversational tones is, like emotion and color , an instance of grounded semantics. While all participants observe the same stimulus (e.g., an emotional recording, a solid color, or in our case a sentence), people may use different words or labels to describe it (in our case conversational tones such as ``polite'', ``excited'', and ``grateful'') which makes it difficult to study grounded semantics at scale and especially across multiple languages or cultures. One challenge is that studying grounded semantics often involves adopting a predefined taxonomy, typically sourced from previous studies and curated by investigators (e.g.; colors; facial emotion; musical emotion perception, concepts such as animal terms; sentiment of news items prosody. Notably, many machine learning datasets also suffer from the same limitation of using a predefined list of using a predefined list of stimuli that can be outdated or unrepresentative of the correspond modality. Examples of such datasets span through realms of: object images, visual scenes, sounds, video and its categorizations, facial expressions. This strategy is prone to researcher bias, potentially skewing the findings away from an accurate representation of labels as they occur in the real world and within a given culture. {{Challenge: biased stimulus set.}} Another challenge that almost all studies faced when studying grounded semantics is that they may use a constrained set of stimuli to be annotated (e.g. emotion; object recognition and similarities; word-associations; musical perception; facial expression; prosody). This introduces researcher bias, as curating the stimuli may influence the elicited labels, which we outline using the following example. Imagine an experiment where a particular semantic term can be associated with some class of objects (e.g., the term ``red'' can be used to describe red fruits). If the object class is not included in the predetermined list of objects (e.g., red fruits are not included in the list of objects), then the elicited terms will not include this association (we will conclude that ``red'' does not describe fruits), and it will be missing from the resulting semantic network. Bias in object selection can also occur in more subtle ways where a skew in the distribution of selected objects also skews the distribution of elicited terms, potentially even amplifying the initial bias. Furthermore, a large body of cross-cultural researchers suggests that studies should not impose a terminology inherited from the experimenter or even from one group of studied agents (e.g., English speakers) on another agent or group of agents [e.g., Speakers of another language or demographics;][]{BLASI20221153,barrett2020towards,henrich2010weirdest}. {{Additional challenges.}} Finally, while some studies advocate for the exclusive use of large textual corpora and the extraction of semantic descriptors via data mining , this indirect approach raises concerns about its ability to accurately represent the nuances of conversational tones as experienced in everyday human interactions. It is also difficult to rigorously compare humans and LLMs using such corpora because these same textual corpora are also the basis for LLM training. Notably, also studies the problem of aligning semantic networks of different individuals or groups in the context of cross-linguistic and cross-cultural comparisons. It turns out that this is a key part of the machine learning problem of automatic translation. Recent research has focused on aligning semantics in humans with Large Language Models, with significant applications to designing human-computer interfaces and AI safety. {{Our approach.}} In light of these challenges, we propose a method that enables the characterization of conversational tones and their taxonomies in any target human population as well as LLMs, based on a human-in-the-loop Sampling with People (SP) technique (Figure ). Specifically, we propose an iterative procedure in which humans and LLMs are presented with sentences and are asked to label their conversational tones in an open-ended fashion (Figure B). The resulting conversational-tone terms are then presented to a new group of agents who are asked to produce sentences reflecting those conversational tones. This process is then repeated multiple times. With mathematical formalism, this process instantiates a Gibbs Sampler from the joint distribution of sentences and conversational tones in humans and LLMs. Given the resulting sample, we derive representative sentences and tone taxonomies of our target population, then have an independent group of human evaluators and LLMs rate the extent to which each tone matched each sentence (Quality-of-fit Rating; Figure C). We use these to construct a geometric embedding that can be used to evaluate the alignment between human and LLM conversational tones (Figure D). We show how our approach can be effectively used to reveal divergences in the representation of conversational tones between humans and LLMs. Moreover, we demonstrate how our new dataset and cross-evaluations can be used to benchmark unsupervised cross-domain semantic alignment methods used in existing work, and identify which of these work well for cases in which cross-evaluation is not possible (e.g., in multilingual scenarios; Figure E). Our method can be generalized to many more psycholinguistic modalities (e.g., sentiment, color), languages, and cultures beyond those involved in this paper. We believe it will help advance both human-machine alignment research as well as cross-cultural research"
HyperMoE: Towards Better Mixture of Experts via Transferring Among Experts,2402.12656v4,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.12656v4_0.pdf,"The Mixture of Experts (MoE) for language models has been proven effective in augmenting the capacity of models by dynamically routing each input token to a specific subset of experts for processing. Despite the success, most existing methods face a challenge for balance between sparsity and the availability of expert knowledge: enhancing performance through increased use of expert knowledge often results in diminishing sparsity during expert selection. To mitigate this contradiction, we propose HyperMoE, a novel MoE framework built upon Hypernetworks. This framework integrates the computational processes of MoE with the concept of knowledge transferring in multi-task learning. Specific modules generated based on the information of unselected experts serve as supplementary information, which allows the knowledge of experts not selected to be used while maintaining selection sparsity. Our comprehensive empirical evaluations across multiple datasets and backbones establish that HyperMoE significantly outperforms existing MoE methods under identical conditions concerning the number of experts. Our code is publicly available at","A trade-off in MoE: (a) A small number of selectable experts can maintain sparsity but limits the availability of expert knowledge. (b) Increasing the number of selectable experts can improve performance but decrease sparsity. (c) Transferring partial knowledge from the unselected experts $E_{2,3}$ to the selected experts $E_1$ can improve the availability of expert knowledge while maintaining sparsity.","The accelerated advancement of large language models has culminated in their widespread application across various domains, including healthcare, education, and social interactions. The remarkable capabilities of these models are attributed to the enhancements in their scale. Nevertheless, the scaling of dense models is often hampered by significant computational demands, posing a challenge to developing the Natural Language Processing (NLP) community. In response, sparse activation models have emerged as a solution, activating only a subset of parameters for different inputs, thus mitigating computational costs. One of the most representative methods is the Mixture of Experts (MoE,), which routers different inputs to specific groups of experts, thereby enlarging the model's capacity without increasing computational burdens. The key to effectively reducing computational costs lies in the sparsity of expert selection, with the number of experts selected for each token being kept at a lower level. In practical applications or experiments, existing works usually select only one or two experts per input. However, increasing the number of selected experts per token can enhance the availability of expert knowledge and improve the performance of downstream tasks. This scenario positions MoE model in a predicament akin to a zero-sum game: a choice between increasing the number of available experts to improve performance or preserving a lower level of available experts to ensure sparsity, as depicted in Figure~. To mitigate this contradiction, one solution would be to use the knowledge of other experts to assist the sparsely selected experts. This is similar to multi-task learning, which transfers knowledge among related tasks. Some works suggest using hypernetworks to generate task-specific knowledge to enhance positive transfer between tasks. Inspired by this, we aim to increase the availability of expert knowledge by transferring the knowledge of unselected experts while sparsely selecting experts. In this paper, we propose {HyperMoE}, a novel MoE framework built upon hypernetworks, which captures the information from every expert by leveraging expert-shared hypernetwork while achieving positive expert transfer by generating conditioned modules individually. We refer to the information as {cross-expert} information. Specifically, a HyperMoE consists of HyperExperts, which are generated based on the information of unselected experts and serve as supplementary information for selected experts while maintaining sparsity. We further improve upon this by introducing the concept of {cross-layer} Hypernetworks: A hypernetwork is shared among all transformer layers, which enables information flow among MoEs in different layers. This brings additional efficiency in terms of parameters and computational costs: Despite the additional computation, our method only experienced a decrease{The degree of decline in speed is related to the scale of the Hypernetworks and the bottleneck size in the generated HyperExpert (similar to $r$ in LoRA). For various tasks, these hyperparameters can be dynamically adjusted to control the delay.} of approximately 15\ We evaluate HyperMoE on 20 representative NLP datasets across diverse tasks: sequence classification, extractive question answering, summarization, and text generation. Extensive experimental results show that HyperMoE outperforms baselines, including Switch Transformer with MoE architecture. This demonstrates the effectiveness of our method in transferring knowledge to experts, which increases the utilization of expert knowledge while keeping the number of experts selected at a low level. To summarise, our core contributions are: {itemize} We propose a novel HyperMoE architecture with HyperExpert for MoE framework, which resolves the inherent tension between maintaining sparse expert selection and ensuring sufficient expert availability within MoE. HyperMoE outperforms baselines based on Switch Transformer across a diverse set of NLP tasks, confirming our approach's effectiveness. We show the relevance between selection embeddings, which are based on the context of unselected experts, and selected experts, indicating that the selection embeddings effectively encode the information of knowledge that the currently selected experts need. {itemize"
Aligning Large Language Models with Human Preferences through Representation Engineering,2312.15997v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2312.15997v3_0.png,"Aligning large language models (LLMs) with human preferences is crucial for enhancing their utility in terms of helpfulness, truthfulness, safety, harmlessness, and interestingness. Existing methods for achieving this alignment often involve employing reinforcement learning from human feedback (RLHF) to fine-tune LLMs based on human labels assessing the relative quality of model responses. Nevertheless, RLHF is susceptible to instability during fine-tuning and presents challenges in implementation. Drawing inspiration from the emerging field of representation engineering (RepE), this study aims to identify relevant representations for high-level human preferences embedded in patterns of activity within an LLM and achieve precise control of model behavior by transforming its representations. This novel approach, denoted as Representation Alignment from Human Feedback (RAHF), proves to be effective, computationally efficient, and easy to implement. Extensive experiments demonstrate the efficacy of RAHF in not only capturing but also manipulating representations to align with a broad spectrum of human preferences or values, rather than being confined to a singular concept or function (e.g. honesty or bias). RAHF's versatility in accommodating diverse human preferences shows its potential for advancing LLM performance. Code is available at .",{fig:schematic} Illustration of different apporaches. (a) Reinforcement learning from human feedback (RLHF); (b) Direct preference optimization (DPO); (c) Hindsight instruction relabeling (HIR); (d) Representation alignment from human feedback (RAHF).,"While large language models (LLMs) learn broad-ranging world knowledge and a degree of reasoning proficiency, precise control over their behavior proves challenging due to the unsupervised nature of their pre-training . For each query, instruction-tuned LLMs exhibit the capacity to generate multiple responses that are both semantically and syntactically coherent by some sampling techniques. While such ability enables the models to provide diversity that is essential for chat agents, some responses may contain harmful, unethical, socially biased, and negative, even illegal content . Existing methods steer LLMs to align with human preferences often using reinforcement learning (RL), with reinforcement learning from human feedback (RLHF) emerging as the most successful one . However, the underlying learning algorithms exhibit a considerable degree of complexity, sensitivity to hyperparameters, instability during training, and necessitate additional training in the reward model and value network, leading to substantial computational costs . In addressing the aforementioned challenges posed by RL-based methods, several computationally lightweight alternatives have been proposed to simplify the human preference-matching process. Two prominent paradigms among these alternatives include contrastive learning and Hindsight instruction relabeling (HIR) . Contrastive learning-based methods optimize a language model policy by increasing the relative probability of preferred responses over dispreferred ones, while HIR methods transform human feedback into instructions by relabeling the original ones, indicating the relative quality of provided responses. A common characteristic shared by these two paradigms is their capability to align language models with human preferences through reward-free fine-tuning. However, the reward-free fine-tuning is vulnerable to the presence of noisy data or incorrect labels in a training set comprising a collection of preference-annotated response pairs . Instances of dull sentences or very brief responses may appear repeatedly in such a training set, potentially introducing bias into the models. The exclusion of such instances from the training set renders it impossible for LLMs to glean insights into human preferences expressed in these instances. In contrast, RL-based methods adopt a different strategy, wherein a reward function is first extracted from a dataset of response rankings, and then this reward function can be applied to train an LLM, effectively mitigating the model's direct exposure to noisy data or incorrect labels within the dataset. In this study, we aim to seek for a computationally lighter and reward-free algorithm that can effectively harness human preference expressed in datasets meanwhile safeguarding LLMs from the influence of noisy data. Inspired by the recent advance in representation engineering , we initially locate relevant representations and activity patterns associated with high-level human preferences within an LLM, and subsequently gain precise control over its behavior by manipulating its internal representations. In the neural architecture, network weights determine neural activity, neural activity determines the networks' output, and the networks' output determines the networks' behavior. Instead of focusing on neurons and their connections, we see aligning LLMs with human feedback as an outcome of representational spaces, implemented by patterns of activity across populations of neurons. We first identify the differences in model activities between preferred and dispreferred stimuli, and then control its behavior by leveraging the identified differences in representations (see Figure ). We introduce two methods for controlling representations and demonstrate the efficacy of these representation engineering (RepE) approaches in aligning LLMs with a broad spectrum of human preferences through a collection of response pairs. To validate the effectiveness of our approach in aligning with human preferences, we conducted extensive comparative experiments on the generated results. Our method outperformed RLHF and other RL-free approaches in human evaluations and automated metrics such as general abilities and GPT-4 evaluations. Notably, the underlying algorithms exhibit simplicity in implementation and straightforwardness in training"
ARAIDA: Analogical Reasoning-Augmented Interactive Data Annotation,2405.11912v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2405.11912v2_0.pdf,"Human annotation is a time-consuming task that requires a significant amount of effort. To address this issue, interactive data annotation utilizes an annotation model to provide suggestions for humans to approve or correct. However, annotation models trained with limited labeled data are prone to generating incorrect suggestions, leading to extra human correction effort. To tackle this challenge, we propose , an analogical reasoning-based approach that enhances automatic annotation accuracy in the interactive data annotation setting and reduces the need for human corrections. involves an error-aware integration strategy that dynamically coordinates an annotation model and a k-nearest neighbors (KNN) model, giving more importance to KNN's predictions when predictions from the annotation model are deemed inaccurate. Empirical studies demonstrate that is adaptable to different annotation tasks and models. On average, it reduces human correction labor by 11.02\% compared to vanilla interactive data annotation methods.",Comparison between manual annotation and interactive annotation.,"Data annotation is a challenging task that involves a tradeoff between annotation quality and budget. While some platforms offer a cost-effective solution by relying on ML models to annotate data automatically~{For example, {https://aws.amazon.com/sagemaker/groundtruth/}.}, the quality of such annotations is often compromised. It is particularly true in the {limited data annotation} scenario where the annotation budget is limited or when unlabeled data are scarce. Human-machine {interactive annotation} methods were introduced to reduce annotation effort while maintaining annotation quality. As illustrated in Fig.~, these methods introduce an {annotation model} to suggest labels ({model annotations}) to human annotators. The annotators accept a suggested label if it is correct. Otherwise, they have to correct the label manually. Compared to manual annotation, interactive annotation requires less human effort because human annotators only have to verify the model annotations instead of coming up with an answer from scratch, leading to potential speedup of the annotation process. Evidently, the annotation model's accuracy is crucial because incorrect suggestions require additional human effort to rectify. Existing methods update the annotation model based on previously accepted or corrected data ({ground-truth annotation}), aiming to reduce human corrections by improving prediction accuracy at each iteration . However, in the context of limited data annotation, the annotation model lacks sufficient labeled training data to reach a reasonable accuracy and is prone to providing incorrect suggestions. For example, in the span relation annotation example shown in Fig.~ ({blue}{blue}), the annotation model continues to make mistakes on similar examples ([{car}, {tyre}]) even after the human annotator corrects the label `[{tree}, {leaf}]$=>${component}'. As a result, this leads to more human corrections. Such a problem is crucial for interactive annotation and has been identified by recent work , but it has yet to be addressed. Inspired by cognitive studies on efficient learning, finding that the human brain can learn from a few examples because our brain is continuously building analogies during the learning process of concepts to facilitate comprehension, we propose {A}nalogical {R}easoning-{A}ugmented {I}nteractive {D}ata {A}nnotation ({Araida}), which is designed to improve interactive annotation under the limited data annotation setting. {Araida} provides an annotation reference to the annotation model by retrieving previously human-labeled examples in the proximity of the example in consideration using the k-nearest neighbors (KNN) method. As illustrated in Fig.~({red}{red}), the final suggestion combines the model annotation and the annotation reference provided by KNN via an error-aware integration strategy. This strategy dynamically coordinates the annotation model and KNN, giving more importance to KNN's prediction if the predicted label from the annotation model is estimated to be inaccurate. We conduct simulated experiments for the limited data annotation task and estimate the human annotation effort based on the number of human corrections (or the number of suggestion errors) following and . We test {Araida} on different word-level and sentence-level annotation tasks, combining with different annotation models (i.e., classic and LLM-based models). The result shows that {Araida} consistently improves different annotation models' accuracy across various tasks. On average, it reduces human corrections by 11.02\ Further analysis attributes this improvement to the few-shot capability of the KNN module and the error-aware integration strategy that effectively synergizes complementary annotations. In summary, our contributions are as follows: {-} {itemize}[leftmargin=*] {-0.3em} Calling attention to the limited data annotation scenario. We highlight the under-trained problem of the annotation model, which is crucial in practice but overlooked in interactive annotation. Introducing {Araida} that involves a KNN module and an error-aware integration strategy to alleviate the under-trained problem by facilitating coordination between the two model annotators (i.e., the vanilla annotation model and KNN). Demonstrating the efficacy of {Araida} in enhancing suggestion accuracy, reducing human corrections, and showcasing its flexibility to combine with various annotation models through extensive experiments. {itemize"
CLAMBER: A Benchmark of Identifying and Clarifying Ambiguous Information Needs in Large Language Models,2405.12063v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2405.12063v2_0.png,"Large language models (LLMs) are increasingly used to meet user information needs, but their effectiveness in dealing with user queries that contain various types of ambiguity remains unknown, ultimately risking user trust and satisfaction. To this end, we introduce CLAMBER, a benchmark for evaluating LLMs using a well-organized taxonomy. Building upon the taxonomy, we construct $ 12K$ high-quality data to assess the strengths, weaknesses, and potential risks of various off-the-shelf LLMs. Our findings indicate the limited practical utility of current LLMs in identifying and clarifying ambiguous user queries, even enhanced by chain-of-thought (CoT) and few-shot prompting. These techniques may result in overconfidence in LLMs and yield only marginal enhancements in identifying ambiguity. Furthermore, current LLMs fall short in generating high-quality clarifying questions due to a lack of conflict resolution and inaccurate utilization of inherent knowledge. In this paper, CLAMBER presents a guidance and promotes further research on proactive and trustworthy LLMs. Our dataset is available at .","Investigation on the identification accuracy when handling ambiguous (i.e, Acc@1) versus unambiguous queries (i.e, Acc@0). We report the results under Zero-shot w/o CoT setting. Small-scale LLMs tend to classify most queries as ambiguous.","Given well-defined user queries, large language models (LLMs) have demonstrated remarkable proficiency in facilitating the information search process . They provide more precise search results with the help of the inherent knowledge stored within LLMs. Nonetheless, as evidenced by previous studies , the practical utility of LLMs is hindered by unclear and ambiguous user queries in real-world scenarios. For instance, in a query like ""{what are the strategies for saving?}"", the term ""{saving}"" can have multiple interpretations, such as ""{saving money}"" or ""{saving from sins}"", depending on the user's actual need. This necessitates LLMs proactively identifying (i.e., determine if the query is ambiguous or not) and clarifying the ambiguities rather than providing potentially incorrect answers that may not align with the user's true needs, ultimately risking user trust and satisfaction . Driven by this concern, recent works have explored LLMs' capacity to address ambiguous queries . However, these investigations have been somewhat fragmented, lacking a comprehensive taxonomy, leading to incomplete and inconsistent handling of ambiguity distributions . As a notable example, they are often limited to contextual ambiguity, where the given context is insufficient for producing a definitive answer. In the era of LLMs, there should be more emphasis on the LLM-oriented ambiguity that may occur when inherent knowledge stored within LLMs have conflict understanding about the query. Consequently, it still remains unclear which ambiguities LLMs can effectively identify and clarify, along with the challenges that LLMs persistently encounter in this regard. To this end, we introduce CLAMBER ({Cl}arifying {Amb}iguous Qu{er}y), a novel benchmark for comprehensively evaluating LLMs in identifying and clarifying various ambiguities using a well-organized taxonomy. Drawing inspiration from the input-process-output framework for evaluating collaborative systems , we establish a taxonomy that consolidates both input understanding and task completion perspectives into three primary dimensions, as illustrated in Table . These dimensions are further conceptualized into eight fine-grained categories to facilitate in-depth evaluation. Building upon this taxonomy, we construct $ 12K$ data for analyzing the pros and cons of LLMs when identifying and clarifying ambiguities. With CLAMBER, we comprehensively evaluate strengths, weaknesses, and potential risks of various LLMs. Our findings indicate that ChatGPT outperforms other small-scale LLMs, especially excelling in identifying and clarifying ambiguities in multifaceted queries , such as ""{What is the largest manufacturer in China?}"", which does not specify the type of ""{manufacturer}"". However, they still encounter numerous challenges: 1) {current LLMs, despite leveraging chain-of-thought (CoT) and few-shot prompting, face challenges in identifying ambiguities.} Our results suggest that CoT and few-shot prompting may lead to the over-confidence issue in small-scale LLMs, impacting ambiguity identification negatively. Even with a large number of shots and CoT support, LLMs only achieve a marginal improvement. Moreover, current LLMs struggle to leverage contextual cues to disambiguate pronouns, highlighting the inadequacy in deducing underlying ambiguities. 2) {Current LLMs fail to ask high-quality clarifying questions, due to the inability of knowing their knowledge gap.} Despite LLMs recognize a query containing ambiguities, their lack of conflict resolution and inaccurate use of inherent knowledge results in uncertainty about which ambiguity to clarify. This prompts the need of developing effective methods for LLMs to resolve conflicts and accurately utilize their inherent knowledge. In this paper, CLAMBER stands as a valuable resource to provide guidance and insight into evaluating LLMs and addressing ambiguous information needs for future improvements. In conclusion, our contributions are threefold: {-} {itemize}[leftmargin=*, itemindent=0.05cm, itemsep=-4pt] We introduce a taxonomy for categorizing various query ambiguities. This taxonomy combines three primary dimensions, detailed as eight categories for facilitating fine-grained evaluations. We present a novel benchmark called CLAMBER, tailored to the characteristics of LLMs. It contains $ 12K$ data featuring ambiguous user queries across diverse categories. With CLAMBER, we evaluate the off-the-shelf LLMs in an inclusive manner. Our findings shed light on why current LLMs struggle to identify and clarify ambiguities. These insights will guide future research in this field. {itemize"
Multimodal Reasoning with Multimodal Knowledge Graph,2406.02030v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.02030v2_0.pdf,"Multimodal reasoning with large language models (LLMs) often suffers from hallucinations and the presence of deficient or outdated knowledge within LLMs. Some approaches have sought to mitigate these issues by employing textual knowledge graphs, but their singular modality of knowledge limits comprehensive cross-modal understanding. In this paper, we propose the Multimodal Reasoning with Multimodal Knowledge Graph () method, which leverages multimodal knowledge graphs (MMKGs) to learn rich and semantic knowledge across modalities, significantly enhancing the multimodal reasoning capabilities of LLMs. In particular, a relation graph attention network is utilized for encoding MMKGs and a cross-modal alignment module is designed for optimizing image-text alignment. A MMKG-grounded dataset is constructed to equip LLMs with initial expertise in multimodal reasoning through pretraining. Remarkably, achieves superior performance while training on only a small fraction of parameters, approximately 2.25\% of the LLM's parameter size. Experimental results on {multimodal question answering} and {multimodal analogy reasoning} tasks demonstrate that our method outperforms previous state-of-the-art models.",(a) The inadequate knowledge encapsulated within textual KG results in the incorrect answer. (b) Our \ourapproach produces the correct answer by reasoning with richer multimodal information.,"Recently, Large Language Models (LLMs) have demonstrated their superiority and robustness across a variety of NLP tasks. To further unlock the potential of LLMs, researchers have attempted to endow them with multimodal reasoning capabilities, as exemplified by visual LLMs like BLIP-2, MiniGPT-4, LLaVA, etc. Although these models have made significant strides in enabling reasoning with both images and text, they are still prone to {hallucinations} , often caused by inadequate or outdated information. Fine-tuning Large Language Models (LLMs) to update their knowledge base is often a time-consuming and costly process. An alternative strategy, as suggested by, involves leveraging knowledge graphs (KGs) as a means to directly augment LLMs with the requisite knowledge. Although recent efforts have focused on employing textual KGs, their singular modality limits LLMs' ability to process and reason with multimodal information (as illustrated in Figure~a). This limitation leads us to consider the use of multimodal knowledge graphs (MMKGs) instead of textual KGs (See Figure~b). In this paper, we propose the {M}ultimodal {R}easoning with {M}ultimodal {K}nowledge {G}raphs () method, designed to expand the multimodal knowledge of LLMs by learning from MMKGs. In particular, first encodes the retrieved MMKG using a relation graph attention network (RGAT), which generates knowledge node embeddings that are able to capture complex graph structures. Then, knowledge and visual adapter layers are designed to bridge the cross-modal gap, mapping both knowledge nodes and visual embeddings to word embedding of LLMs, respectively. Finally, embeddings of knowledge nodes, image and text are concatenated to form the prompt and subsequently forwarded to LLMs to provide guidance and instructions. In addition, we introduce a novel cross-modal alignment module to optimize the image-text alignment through a matching task within MMKGs. To equip the model with initial expertise in multimodal reasoning, we first pretrain on a customized MMKG-grounded dataset, which is constructed by matching each VQA instance with a corresponding MMKG, derived from the scene graph of its image and containing essential knowledge for answering questions. To thoroughly evaluate our method, we conduct comprehensive experiments on {multimodal question answering} and {multimodal analogy reasoning} tasks, spanning various LLM sizes and training configurations. The experimental results confirm that effectively processes and utilizes knowledge from MMKGs for multimodal reasoning, outperforms previous state-of-the-art models with a 1.95\ Importantly, freezes both LLM and the visual encoder, with only a small fraction of the parameters, approximately 2.25\ In summary, our main contributions are three-fold: {itemize}[noitemsep,nolistsep] To the best of our knowledge, we are the first to investigate the problem of expanding multimodal reasoning capabilities of LLMs by utilizing knowledge derived from MMKGs. We propose the method, specifically designed to extract valuable knowledge from MMKGs and seamlessly integrate multimodal information into LLMs. Additionally, we also develop a MMKG-grounded dataset for initially enhancing multimodal reasoning. We extensively evaluate on two multimodal reasoning tasks. achieves state-of-the-art performance by significant margins, outperforming recent baseline methods. {itemize"
RAGTruth: A Hallucination Corpus for Developing Trustworthy Retrieval-Augmented Language Models,2401.00396v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2401.00396v2_0.pdf,"Retrieval-augmented generation (RAG) has become a main technique for alleviating hallucinations in large language models (LLMs). Despite the integration of RAG, LLMs may still present unsupported or contradictory claims to the retrieved contents. In order to develop effective hallucination prevention strategies under RAG, it is important to create benchmark datasets that can measure the extent of hallucination. This paper presents {}, a corpus tailored for analyzing word-level hallucinations in various domains and tasks within the standard RAG frameworks for LLM applications. {} comprises nearly 18,000 naturally generated responses from diverse LLMs using RAG. These responses have undergone meticulous manual annotations at both the individual case and word levels, incorporating evaluations of hallucination intensity. We not only benchmark hallucination frequencies across different LLMs, but also critically assess the effectiveness of several existing hallucination detection methodologies. We show that using a high-quality dataset such as {}, it is possible to finetune a relatively small LLM and achieve a competitive hallucination detection performance when compared to the existing prompt-based approaches using state-of-the-art LLMs such as GPT-4. Furthermore, the finetuned model can effectively mitigate hallucination in LLM responses.",Frequency of different types of hallucination by task.,"Large language models (LLMs) have achieved remarkable success in a variety of tasks, including text generation, machine translation, and question answering. However, one of the key challenges in deploying LLMs in real-world applications is their tendency to hallucinate. Hallucination in the context of LLMs usually refers to a situation where the model generates content that is not based on factual or accurate information. The occasional generation of outputs that appear plausible but are factually incorrect significantly undermine the reliability of LLMs in real-world scenarios, such as medical diagnoses and news summarization. {Tables/dataset_example} To reduce hallucination, various methods have been developed that can be applied at different stages of LLM lifecycle, including pre-training, supervised fine-tuning, RLHF, and inference. In terms of detection, methods are developed by examining the model's intrinsic state, comparing it with external data and tools, or leveraging the LLM's inherent powerful capabilities for self-checking. Retrieval-augmented generation (RAG) is extensively used to supply LLMs with updated, relevant knowledge, significantly mitigating hallucination. Nevertheless, even with RAG and other enhancements, LLMs still produce statements that are either unfounded or contradict the information provided in the retrieved references. Despite the growing awareness of the hallucination phenomenon, the understanding of hallucination in LLMs is still in its early stages. One key challenge is the lack of high-quality, large-scale datasets specifically designed for hallucination detection. This issue is particularly acute in RAG settings. Due to the relatively low hallucination ratio, a substantial increase in annotation resources is needed. Existing datasets for LLM hallucination detection are predominantly synthesized. For instance, in, prompts conflicting with conventional knowledge are purposely generated to trigger hallucinations. While these approaches are efficient at generating hallucinations, the resulting artificial hallucinations can substantially differ from those that naturally occur. In, hallucination datasets are developed by manual annotations of naturally produced LLM responses. However, these datasets are of limited size and are not specifically focused on the RAG scenario. In this paper, we introduce a large-scale high-quality dataset specifically designed for word-level hallucination detection for RAG applications. Using this dataset, we have conducted an extensive benchmarking of mainstream LLMs to assess their tendency to generate hallucinations, as well as evaluate current methods for hallucination detection. Additionally, we have demonstrated superior performance in identifying hallucinations by fine-tuning LLM with {} dataset. Our key contributions are: {enumerate}[label=()] We propose {}, a large-scale word-level hallucination evaluation dataset specifically for the RAG scenario across several common tasks. It consists of nearly 18,000 fully annotated natural responses generated from major open-source and closed-source LLMs. We perform a comprehensive comparison of different hallucination detection methods at both the passage and word levels. We present a baseline method of fine-tuning LLM for hallucination detection. It is shown that by fine-tuning the Llama-2-13B model on the {} training data, we can achieve results competitive to the existing prompt-based approaches using GPT-4. This shows the potential of developing better hallucination detection methods using {}. We show that by using our finetuned hallucination detector, it is possible to significantly reduce the occurrence of hallucinations in the responses from LLMs. The improvement holds even for models with inherently low hallucination rates, such as GPT-4. {enumerate"
CLOMO: Counterfactual Logical Modification with Large Language Models,2311.17438v4,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2311.17438v4_0.pdf,"In this study, we delve into the realm of counterfactual reasoning capabilities of large language models (LLMs). Our primary objective is to cultivate the counterfactual thought processes within LLMs and rigorously assess these processes for their validity. Specifically, we introduce a novel task, }ounterfactual }gical }dification (), and a high-quality human-annotated benchmark. In this task, LLMs must adeptly alter a given argumentative text to uphold a predetermined logical relationship. To effectively evaluate a generation model's counterfactual capabilities, we propose an innovative evaluation metric, the decomposed {Self-Evaluation Score (SES)} to directly evaluate the natural language output of LLMs instead of modeling the task as a multiple-choice problem. Analysis shows that the proposed automatic metric aligns well with human preference. Our experimental results show that while LLMs demonstrate a notable capacity for logical counterfactual thinking, there remains a discernible gap between their current abilities and human performance. Code and data are available at .",Demonstration of \ourdataset. An LLM is given an argument and two premises. The LLM needs to modify the statements in Argument such that the logical relation \textsf{R} switch to stand in \textsf{state 2} instead of \textsf{state 1}.,"Despite large language models perform strikingly in plenty of reasoning benchmarks , late studies observe an internal inconsistency in their reasoning processes . The inconsistency is attributed to misunderstanding and misapplication of logical relations. However, logical relations in complex language reasoning are not yet properly quantified and evaluated. Current studies on evaluating model reasoning are limited in both form and content. On the one hand, benchmarking complex reasoning is generally applying discrimination tasks such as multiple-choice questions , where accuracy and pass rate serve as the main evaluation metric. However, such evaluations over-simplify the goal of uncovering essential and subtle pitfalls in complex reasoning. For example, the reasoning processes could contain misconceptions in logical relations but give correct answers due to the data distribution. Therefore, evaluating the generated content would provide a more realistic measurement of model reasoning. On the other hand, unlike widely studied reasoning tasks such as math reasoning and standard exams , counterfactual reasoning as a fundamental evaluation of logical relations is less explored in the context of large language models. Previous literature studies counterfactual reasoning either in a multiple-choice manner or applying labored human study to evaluate counterfactual generation , leaving an effective evaluation of counterfactual generation unexplored. In our study, we delve into the realm of evaluating large language models' (LLMs) ability to generate counterfactually coherent thoughts. {Figure~ demonstrates the paradigm.} Specifically, we proposed an innovative evaluation system that quantitatively measures the evolution of information in statement pairs, ensuring that they adhere to a specified logical relationship. Our approach includes designing a specialized task where models are presented with mismatched argument-premise pairs bound by a specific logical relation. The objective for these models is to adeptly modify the argument text until the specified logical relation is satisfactorily established. In conjunction with this task, we have created the first dataset of its kind, comprising dual argument-premise pairs, each annotated with a defined logical relation. This dataset is vital for facilitating logically restricted counterfactual modifications, and we have enriched it with human-written modifications to serve as a benchmark for evaluation. Our experimental investigations encompass a range of large language models, including the latest GPT-4o{{https://openai.com/index/hello-gpt-4o/}}, GPT-4 , and GPT-3.5-Turbo , as well as smaller models from the LLaMA and LLaMA 2 families. Through these experiments, we have discerned that the task of \ poses a significant challenge. It becomes evident that these models' current counterfactual logical reasoning capabilities fall short of the desired proficiency. This observation underscores the need for further advancements in enhancing the counterfactual reasoning abilities of existing language models, paving the way for more sophisticated and logically coherent AI systems. The contributions of this paper are three-fold: {-5pt} {itemize}{-5pt} We propose the task of \ and contribute a corresponding to evaluate the counterfactual reasoning capability of LLMs in the scenario of complicated textual logical reasoning. We propose the decomposed {Self-Evaluation Score (SES)} for the logically consistent generation of large language models. We conduct experiments on LLMs (GPT-3.5, GPT-4) and small language models (the LLaMA and LLaMA 2 families) and find that \ is a very challenging task and the counterfactual logical reasoning ability of the existing model needs to be improved. {itemize} {-7pt} {-1pt"
Exploring Hybrid Question Answering via Program-based Prompting,2402.10812v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.10812v1_0.pdf,"Question answering over heterogeneous data requires reasoning over diverse sources of data, which is challenging due to the large scale of information and organic coupling of heterogeneous data. Various approaches have been proposed to address these challenges. One approach involves training specialized retrievers to select relevant information, thereby reducing the input length. Another approach is to transform diverse modalities of data into a single modality, simplifying the task difficulty and enabling more straightforward processing. In this paper, we propose , a novel program-based prompting framework for the hybrid question answering task. follows the code generation and execution paradigm. In addition, integrates various functions to tackle the hybrid reasoning scenario. Specifically, contains function declaration and function implementation to perform hybrid information-seeking over data from various sources and modalities, which enables reasoning over such data without training specialized retrievers or performing modal transformations. Experimental results on two typical hybrid question answering benchmarks HybridQA and MultiModalQA demonstrate the effectiveness of : it surpasses all baseline systems and achieves the best performances in the few-shot settings on both datasets.",Example of hybrid question answering task with the corresponding program.,"Question answering systems have attracted significant attention and made considerable progress in recent years. However, real-world data often exists in diverse formats and originates from multiple sources. Consequently, researchers turn their focus to the hybrid question answering (HQA) task , which necessitates mixed reasoning across various types of data. The HQA task is challenging due to the vast amount of information and the organic coupling of heterogeneous data sources. Reasoning over such diverse data requires the ability to understand multiple data types simultaneously. For instance, as depicted in Figure , the model must engage in reasoning over both the table and the extensive passages and images linked in hyperlinks to make accurate predictions. To tackle these challenges, recent approaches focus on training domain-specific models to retrieve or rank elements such as table rows, passages, or images, selecting the most relevant ones to enhance the subsequent reasoning process . Since real-world heterogeneous data is vast and constantly updated, even if these approaches demonstrate promising performance on their focused datasets, their applicability to such intricate data is still limited. Furthermore, some existing approaches tend to transform diverse modalities of data into a single modality, such as image captioning , or table-to-text generation , to reduce the task difficulty. However, such approaches are constrained by the performance of modal transformation models, which often result in the loss of information. In a word, these approaches highly rely on data distribution, and the complexity of real-world heterogeneous data makes them exorbitant. In contrast to previous approaches, we argue that the solution of solving the HQA task should be agnostic to data distribution. Consequently, we advocate for an optimal solution devising a procedure for determining how to find an answer, rather than merely generating the answer itself. Noticing that the program could elucidate the reasoning process employed to arrive at the answer (as depicted in Figure ), in the current era of large language models (LLMs), leveraging a program can serve as an advantageous solution since LLMs are an excellent program generator. Moreover, the process of program generation necessitates the incorporation of various functions into the program, enabling information-seeking across diverse sources and modalities of data. Based on the aforementioned considerations, in this paper, we introduce a novel program-based prompting framework ({H}ybrid {Pro}gram-Based {Pro}mpting) for HQA task. considers the solution as a process of code generation and execution, integrating external customized functions under the few-shot setting{In this work, we use Python code as the carrier of the program.}. To facilitate the utilization of customized functions, incorporates two key components: {Function Declaration} during the code generation phase and {Function Implementation} during the execution phase, which is shown in Figure . During the function declaration stage, defines the function name and formal parameters, utilizing them as prompts to generate code. Subsequently, in the function implementation stage, implements the declared functions, serving for the direct execution of the generated code. By defining different functions, can support reasoning over data from various modalities, making it a flexible and scalable framework. Importantly, eliminates the need to convert different modalities of data into a single modality beforehand. Instead, it acquires information within the origin modal by the functions themselves. To the best of our knowledge, is the first work to explore the power of LLMs in handling heterogeneous data without requiring domain-specific retrieval or modal transformation. Experiments demonstrate that significantly outperforms previous methods. In summary, our contributions are as follows: {itemize} We introduce , a program-based prompting framework that enables reasoning over heterogeneous data without domain-specific retrieval and modal transformation. We implement a few-shot code generation and execution pipeline, calling various functions by function declaration and implementation to perform information-seeking across data from different sources and modalities. Experiments show the effectiveness that achieves the best performances under the few-shot settings on HybridQA and achieves state-of-the-art performances under all settings on MultiModalQA . {itemize"
Simple but Effective Compound Geometric Operations for Temporal Knowledge Graph Completion,2408.06603v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2408.06603v1_0.pdf,"Temporal knowledge graph completion aims to infer the missing facts in temporal knowledge graphs. Current approaches usually embed factual knowledge into continuous vector space and apply geometric operations to learn potential patterns in temporal knowledge graphs. However, these methods only adopt a single operation, which may have limitations in capturing the complex temporal dynamics present in temporal knowledge graphs. Therefore, we propose a simple but effective method, i.e. TCompoundE, which is specially designed with two geometric operations, including time-specific and relation-specific operations. We provide mathematical proofs to demonstrate the ability of TCompoundE to encode various relation patterns. Experimental results show that our proposed model significantly outperforms existing temporal knowledge graph embedding models. Our code is available at {https://github.com/nk-ruiying/TCompoundE}.",An illustration of temporal evolution patterns. It can be observed that the relationships between head entity and tail entity are dynamically determined by both relation and time.,"A knowledge graph (KG) comprises a collection of structured knowledge presented in triples, offering a simple and effective means of describing factual information . In a KG, a triplet is conventionally represented as $(s, {r}, o)$, where $s$, $o$ and ${r}$ correspond to the head entity, tail entity and the relation linking between the head and tail entities, respectively. However, knowledge is not static in the real world . Temporal knowledge graph (TKG) represents knowledge $(s, {r}, o)$ occurring at timestamp $$, denoted as a quadruple $(s, {r}, o, )$, thereby adding a temporal dimension to knowledge graphs. The introduction of the timestamp $$ enables TKG to delineate the temporal scope of knowledge more accurately and helps us better uncover the potential information within it . Therefore, TKG is widely applied in downstream tasks such as question answering , information retrieval , and recommendation systems due to its temporal characteristics. However, TKG usually does not cover all the facts. The incompleteness of TKG hinders the performance of its downstream tasks. To enhance the overall completeness of TKG, the temporal knowledge graph embedding (TKGE) model utilizes existing knowledge to predict and estimate missing facts. Specifically, the TKGE model employs distinct score functions to acquire effective vector space representations for entities, relations and timestamps, thus utilizing these representations to predict missing facts in TKG. Furthermore, how to enhance the expressive capabilities of the TKGE model is also an important issue that has received widespread attention. Geometric operations such as translation and scaling are widely used operations in the field of graphics. These operations help to distinguish between these different classes of entities and to model different relational patterns. Models like TTransE , TComplEx and TNTComplEx all use these operations and achieve good performance. Previous TKGEs often use a single type of operation for embedding. This approach is not conducive to modeling different relation patterns and temporal evolution patterns (shown in Fig. ) since each operator may have modeling limitations. show that using complex geometry operations in knowledge graphs can effectively model different relational patterns. Inspired by them, we work on compound geometric operations that fit relationships and timestamps in TKGs. In this paper, we present a model called {TCompoundE} based on CompoundE . Different from CompoundE's work, we discuss the effects of representing relations and timestamps as different geometric operations, which we call relation-specific operations and time-specific operations. More specifically, we use composite operations involving translation and scaling as relation-specific operations and time-specific operations. We integrate time-specific operations within the framework of relation-specific operations, aiming to capture both time-varying and time-invariant features within the low-dimensional representations of relations. Relation-specific operations, incorporating temporal information, are applied to the head entity embedding. Subsequently, the confidence of the quadruple is expressed by computing the semantic similarity between the head entity embedding and the tail entity embedding. In summary, the main contributions of our work are as follows: {itemize} We present a novel TKG embedding model called TCompoundE, which introduces relation-specific and time-specific compound geometric operations. We substantiate the suitability of our model for important relation patterns through mathematical formulations. Our experimental results on three benchmark datasets demonstrate that our method both meets and surpasses the performance of existing TKGE methods. {itemize"
Uncertainty Aware Learning for Language Model Alignment,2406.04854v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.04854v1_0.pdf,"As instruction-tuned large language models (LLMs) evolve, aligning pretrained foundation models presents increasing challenges. Existing alignment strategies, which typically leverage diverse and high-quality data sources, often overlook the intrinsic uncertainty of tasks, learning all data samples equally. This may lead to suboptimal data efficiency and model performance. In response, we propose uncertainty-aware learning (UAL) to improve the model alignment of different task scenarios, by introducing the sample uncertainty (elicited from more capable LLMs). We implement UAL by a simple fashion -- adaptively setting the label smoothing value of training according to the uncertainty of individual samples. Analysis shows that our UAL indeed facilitates better token clustering in the feature space, validating our hypothesis. Extensive experiments on widely used benchmarks demonstrate that our UAL significantly and consistently outperforms standard supervised fine-tuning. Notably, LLMs aligned in a mixed scenario have achieved an average improvement of 10.62\% on high-entropy tasks (i.e., AlpacaEval leaderboard), and 1.81\% on complex low-entropy tasks (i.e., MetaMath and GSM8K).","{Illustration of feature clustering.} Compared to SFT, UAL-based models show more convergence in the feature space, which we detailed our exploration in Section \ref{sec:cluster}.","Large language models (LLMs), represented by GPT-4 , Claude, and Llama2 , have recently achieved significant success in a series of natural language generation and understanding tasks. The emergence of alignment methods has further enhanced the capabilities of LLMs, for example, the ability to follow human instructions or to achieve better zero-shot performance. The current popular alignment approaches including RLHF , consider supervised fine-tuning (SFT) an essential part that contributes significantly. It is undeniable that the current SFT paradigm achieves considerable success. Some previous studies find that high-quality, complex, and diverse data contribute significantly to alignment . However, the data can exhibit different levels of uncertainty. For data points in highly technical, scientific, or specialized settings, context may have little or no ambiguity and a limited set of correct answers. In contrast, other dialogues might feature varied and dynamic social contexts with idiomatic language uses. Appendix presents a pair of such examples. Nevertheless, the common SFT paradigm applies the same level of supervision to all samples in the training set, overlooking the intrinsic uncertainty of the data. Furthermore, due to the phenomenon of catastrophic forgetting , SFT-aligned models may perform worse than their foundational models. The Deepseek technical report reveals that the SFT model often underperforms compared to its base model on several benchmarks , while LLMs generally show diminished performance on general tasks when forging their agent capability . In our experiments, we observe that the standard SFT paradigm frequently leads to model degradation, resulting in decreased performance on some benchmarks, despite providing overall improvement. For instance, this is evident in parts of the commonsense benchmarks, such as MMLU. It is essential to align pretrained models while mitigating degradation issues as effectively as possible. Therefore, we propose: {tcolorbox} {Uncertainty Hypothesis.} {In an ideal paradigm, to further enhance alignment performance, the model should attend to samples differently based on their properties during alignment. Specifically, the model should impose stricter constraints when attending to more certain examples, as these samples exhibit less uncertainty and fewer variations, while maintaining relaxed constraints for highly uncertain examples.} {tcolorbox} To design our uncertainty-aware learning (UAL) method and address potential model degradation caused by SFT, we propose measuring uncertainty through a coarsely-grained approach that incorporates an autonomous judge (e.g., GPT-4) to assess the uncertainty of each sample in the training set. Upon obtaining the uncertainty estimations, we linearly map them into our adaptive label smoothing training pipeline. Further details of our algorithm are presented in Section . UAL significantly enhances the performance of instruction-tuned models. We apply this method across various model architectures and alignment datasets, observing that the UAL paradigm consistently outperforms the vanilla SFT paradigm on prominent benchmarks, including MMLU and TruthfulQA , among others. As Figure shows, UAL also helps the model improve in different scenarios. Moreover, compared to its foundational model, as Figure presents, the SFT-aligned model brings same-class tokens closer together in the feature space, and UAL enhances this tendency even further. This strongly correlates with the superior performance of UAL-aligned models across benchmarks and scenarios, offering an important explanation for UAL's outperformance over conventional SFT. {Contributions.} Our contributions are summarized threefold: {itemize} Due to the limitations of the current supervised finetuning (SFT) paradigm, we propose the uncertainty-aware learning (UAL) approach to mitigate the alignment degradation problem and improve model performance in high-entropy and low-entropy scenarios. Based on an intuitive design concept, our UAL paradigm is simple to implement, making it possible to improve any language model's alignment. We conducted extensive experiments to prove the efficiency of our method and conducted in-depth analyses to provide some insights (i.e. feature clustering) into the mechanism. {itemize"
Interpretable User Satisfaction Estimation for Conversational Systems with Large Language Models,2403.12388v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2403.12388v2_0.jpg,"Accurate and interpretable user satisfaction estimation (USE) is critical for understanding, evaluating, and continuously improving conversational systems. Users express their satisfaction or dissatisfaction with diverse conversational patterns in both general-purpose (ChatGPT and Bing Copilot) and task-oriented (customer service chatbot) conversational systems. Existing approaches based on featurized ML models or text embeddings fall short in extracting generalizable patterns and are hard to interpret. In this work, we show that LLMs can extract interpretable signals of user satisfaction from their natural language utterances more effectively than embedding-based approaches. Moreover, an LLM can be tailored for USE via an iterative prompting framework using supervision from labeled examples. Our proposed method, (), not only has higher accuracy but is more interpretable as it scores user satisfaction via learned rubrics with a detailed breakdown.",Illustration of user utterances with satisfaction patterns (green) and dissatisfaction patterns (red).,"General-purpose conversational systems such as ChatGPT and Copilot are revolutionizing how people live and work. Understanding when and why users are satisfied or dissatisfied is critical for the continuous improvement of these systems. It helps system developers identify areas of improvements, conduct effective A/B experiments, and optimize underlying models. Unsurprisingly, developing machine learning models for User Satisfaction Estimation (USE) has captured significant attention from the research community. When estimating user satisfaction, simply classifying that a user is satisfied or dissatisfied is insufficient. Understanding the reason why a user is satisfied or dissatisfied is just as valuable. For example, frequent query reformulation presents opportunities for prompt recommendation and conversations where users explicitly correct a bot's mistakes can suggest examples for model alignment. See Figure~ for an illustration. However, most existing work has focused on improving classification accuracy and has overlooked interpretability. Representation learning-based approaches are relatively opaque due to their use of neural models (e.g., embeddings) and thus offer little insight into conversational patterns that indicate satisfaction/dissatisfaction. Similar limitations apply to reward models for training LLMs, e.g., RLHF and RLAIF. In this case, the learned model produces a continuous ``reward'' score that aims to distinguish outputs that a human prefers without explaining why a conversation has a higher score than others. To our knowledge, these reward models have not been directly used for USE, but we treat it as a baseline due to their ability to rank outputs with respect to human preferences. Some prior work addressed the interpretation needs of USE via featurized ML models. Examples include , which evaluated user satisfaction based on human-annotated features assessing task success and dialogue costs, and , which proposed domain-independent features that evaluate response quality. However, the growth of LLM-based conversational systems (e.g., ChatGPT, Bing Copilot) means user queries in conversational systems may now be across multiple domains and intents (e.g., task-oriented, QA, chitchat). As such, approaches based on domain-specific features have limited generalizability to these diverse conversational patterns. In this work, we make the key observation that LLMs can achieve both high classification accuracy and fine-grained interpretability at the same time -- through their ability to reason about user conversational patterns and identify salient pattern classes that generalize and produce accurate predictions. We propose {} (). We consider a { few-shot} scenario, where a small number of training examples are available, and develop a supervised, iterative prompting framework that uses an LLM to (1) extract signals of satisfaction from user utterances in a labeled training set, (2) summarize the reasons into rubrics for identifying satisfaction/dissatisfaction conversational patterns, and (3) apply the rubrics to predict satisfaction labels on unseen conversations. In addition to being more accurate, our approach provides an interpretable rubric for understanding the conversational patterns that indicate user satisfaction/dissatisfaction. Notably, our approach can be used to learn SAT/DSAT patterns automatically for different conversational systems. In our experimental results, we show the distributions of patterns in different types of systems and demonstrate how these patterns (1) correlate to overall user satisfaction, and (2) differ across domains. Moreover, we show that we can {scale} the application of the learned rubrics in two ways. First, we show that we can { distill} individual rubric items into an embedding-based model that can be applied at scale without the need for LLM prompting. Next, we show that we can add rubric items as features to an embedding-based model to increase the accuracy of embedding-only models on datasets with more available training data. The main contributions of our work include: {itemize} We propose {} ({}), a novel framework for estimating user satisfaction in conversational systems with LLMs. We show the {} prompting process extracts patterns into clear and interpretable rubrics that guide the LLM to classify user satisfaction and show that diverse rubrics are learned automatically for different domains. We show {} outperforms existing methods across different types of conversational systems when training data is limited and provide insights into the factors that influence user satisfaction. We use knowledge distillation to scale the application of learned rubrics and show the rubrics can continuously improve performance as more training data is available. {itemize"
Measuring Political Bias in Large Language Models: What Is Said and How It Is Said,2403.18932v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2403.18932v1_0.pdf,"We propose to measure political bias in LLMs by analyzing both the content and style of their generated content regarding political issues. Existing benchmarks and measures focus on gender and racial biases. However, political bias exists in LLMs and can lead to polarization and other harms in downstream applications. In order to provide transparency to users, we advocate that there should be fine-grained and explainable measures of political biases generated by LLMs. Our proposed measure looks at different political issues such as reproductive rights and climate change, at both the content (the substance of the generation) and the style (the lexical polarity) of such bias. We measured the political bias in eleven open-sourced LLMs and showed that our proposed framework is easily scalable to other topics and is explainable.",An overview of our proposed framework for measuring political bias in LLM-generated content. The two-tiered framework first evaluates the LLM's {political stance} over political topics and then {framing bias} in two aspects: content and style.,"As the pervasiveness of AI in human daily life escalates, extensive research has illuminated its limitations and potential harms such as gender and racial biases and hallucinations . Among these, political bias in AI, a notably crucial yet underexplored facet, poses significant risks by potentially distorting public discourse and exacerbating societal polarization. Existing scholarly efforts have explored political bias ingrained in LMs, mainly focused on stance at the political-orientation level (i.e., left or right/liberal or conservative). The political orientation tests-based methodology (e.g., The Political Compass test{{https://www.politicalcompass.org/test}}) is often employed, yet it may be inadequate to fully capture the complex dynamics of bias within LLM-generated content. Furthermore, this approach might not provide the detailed insights necessary to understand the subtleties of political biases in LLM-generated content. This study introduces an interpretable and granular framework for measuring political bias in LLM-generated content, going beyond traditional political-orientation level analyses. Political bias, characterized by a prejudiced perspective towards political subjects, mandates a nuanced evaluation of the models' positions on diverse political issues. This bias predominantly manifests through framing, which entails the deliberate selection and emphasis of specific informational elements, both in content and style, to shape perceptions . Given the multifaceted nature of political bias, our framework employs a two-tiered approach for its assessment, encompassing both topic-specific stance and framing. To assess the models' stance on distinct political topics, we implement a method of extreme anchor comparison, quantifying the similarity between model outputs and two opposed stances -- advocacy and opposition -- across various political subjects. Subsequently, to dissect the political bias of LLMs more thoroughly, we examine framing by decomposing both the content and style. This involves a detailed content analysis leveraging Boydstun's frame dimensions and entity-based frames, coupled with an evaluation of stylistic bias, including the examination of media bias in writing styles and the presence of non-neutral sentiment towards salient entities of topic. Collectively, our framework not only discerns topic-specific stances but also explores the intricate dynamics of the content (""what"" is said) and style (""how"" it is said) concerning contentious topics. The ultimate aim is to provide a measurement of political biases inherent in various LLMs, thereby paving the way for the development of strategies to diminish these biases and enhance the reliability and equity of LLM applications. Drawing upon the empirical evidence and analytical insights derived from our frameworks, this study elucidates a set of findings that furnish a guide for subsequent research endeavors within the community. The key discoveries include: (1) LLMs show different political views depending on the topic, such as being more liberal on reproductive rights and more conservative on immigration; (2) Even when LLMs agree on a topic, they focus on different details and present information differently; (3) LLMs often discuss topics related to the US; (4) topic-level analysis aligns with previous finding that LLMs usually lean towards liberal ideas; (5) Larger models aren't necessarily more neutral in their political views; (6) Models from the same family can have different political biases; (7) the impact of multilingual capabilities (e.g., Yi-chat, Jais-chat) on the thematic focus of content, diverging from models primarily trained in English. By facilitating both model-specific and comparative analyses, our framework seeks to advance the development of AI systems that are safer and more aligned with ethical standards. We will open-source the codebase"
Fortify the Shortest Stave in Attention: Enhancing Context Awareness of Large Language Models for Effective Tool Use,2312.04455v4,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2312.04455v4_0.pdf,"In this paper, we demonstrate that an inherent waveform pattern in the attention allocation of large language models (LLMs) significantly affects their performance in tasks demanding a high degree of context awareness, such as utilizing LLMs for tool-use. Specifically, the crucial information in the context will be potentially overlooked by model when it is positioned in the trough zone of the attention waveform, leading to decreased performance. To address this issue, we propose a novel inference method named . It allows LLMs to process their input through multiple parallel processes. Each process utilizes a distinct base angle for the rotary position embedding, thereby creating a unique attention waveform. By compensating an attention trough of a particular process with an attention peak of another process, our approach enhances LLM's awareness to various contextual positions, thus mitigating the risk of overlooking crucial information. In the largest tool-use benchmark, our method elevates a 7B model to achieve state-of-the-art performance, comparable to that of GPT-4. On other benchmarks and some RAG tasks, which also demand a thorough understanding of contextual content, our \ also exhibited notable enhancements in performance..}","(a) Task illustration: Presented with multiple key-value pairs and a target key (highlighted in bold), the model is required to accurately retrieve and generate the value associated with this key from an extensive context. (b) We illustrate the position-related fluctuation in accuracy of Llama-2-7B on this in-context retrieval task. (c) The pattern of the attention score exhibits fluctuations, which we term the ``attention waveform''. Our study reveals a connection between the position-related fluctuations in LLMs' performance and this attention waveform.","Recent works that augmenting large language models (LLMs, e.g., GPT series) with tools have achieved advancements in various fields, such as human-computer interactions, automating multi-modal tasks, and enhancing the overall efficiency of language-related applications. In this paradigm, upon receiving a user's intent, a large language model accesses multiple tools, typically in the form of APIs. It then selects the most suitable one by referring to the relevant tool documentation, and provides an accurate and suitable response. Considering the integration of extensive information into the context, tool-use tasks demand a high level of context understanding and awareness from LLMs. Despite the achievements made by current LLM-based tool-use frameworks, in our practical experience, we observed that LLMs exhibit varying levels of awareness concerning different positions within the context. For instance, LLMs may overlook certain tools within the context, resulting in a failed call; however, by altering the position of these tools, the task can be successfully executed. Such variations significantly affect the performance of LLMs in tool-use. This observation is consistent with the findings from a previous study that investigated a simple in-context retrieval task. When LLMs are presented with multiple key-value pairs and instructed to retrieve the value associated with a specific key, the index of the queried target key results in significant fluctuations in accuracy. Figure~(a) provides a visual representation of the instructions for this task. Figure~(b) shows this fluctuation we replicated using the Llama-2-7B. In our study, we go beyond the superficial fluctuations previously observed and identify that these position-related performance differences are closely associated with the model's fluctuating attention allocation. Specifically, we observed a waveform pattern in the attention ``intensity'' (referred to as the attention waveform in this paper) when LLMs retrieve the same token from the context, as illustrated in Figure~(c). We demonstrate that if the position of the crucial information coincides with a trough in the attention waveform, the model may overlook it, leading to decreased accuracy. Based on insight above, we argue that by shifting essential information away from the attention waveform's trough zone, we can reduce the risk of LLMs{In this paper, we focus on LLMs based on Transformer models and rotary position embeddings (RoPE). This family of LLMs include many popular models like Llama, Qwen, Baichuan, etc. } missing crucial details, thus enhancing the efficacy of tool-use. Because crucial information within the context is inaccessible in practice, we propose the following approach to circumvent this challenge: We process the context through multiple parallel executions, where each execution is assigned a unique rotary angle base of the rotary position embedding, resulting a distinct waveform pattern (See for details). By ensuring these attention waveforms are ``complementary,'' — for any position where one waveform reaches its trough, another waveform reaches its peak — we enhance the LLM's context awareness across various positions. We then aggregate the output distributions from these parallel executions and compute their weighted sum. This sum is subsequently decoded to generate the final prediction token. An analogy can aid in understanding our approach: Imagine a wooden bucket with some shorter staves, which allow water to leak out. Similarly, the attention mechanism, at each angle base, has limited awareness of specific positions in the context. We utilize models to process the context with different angle bases. This results in the troughs of one attention wave being fortified by the peaks of another, analogous to how the longer staves in one bucket compensate for the shorter staves in another. Consequently, we name our proposed method . We achieve the state-of-the-art on the largest tool-use benchmark ToolBench and another benchmark ToolAlpaca. In ToolBench, we augment the performance of a 7B LLM to levels competitive with those of GPT-4. In addition to our achievements in tool-use, we also demonstrate our method's potential in general retrieval-augmented generation (RAG) tasks, which also demand a high degree of contextual awareness. In summary, we make three major contributions: (1) For LLMs with RoPE, we propose and verify an explanation for the variation in their awareness of different positions within the context. We establish a relationship between this variation and the attention waveform. (2) By leveraging the insights from our proposed explanation, we develop a novel approach \ to enhance LLMs' context awareness. (3) Through extensive experiments, we empirically validate the efficacy of our proposed method"
Enhancing Multilingual Capabilities of Large Language Models through Self-Distillation from Resource-Rich Languages,2402.12204v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.12204v1_0.pdf,"While large language models (LLMs) have been pre-trained on multilingual corpora, their performance still lags behind in most languages compared to a few resource-rich languages. One common approach to mitigate this issue is to translate training data from resource-rich languages into other languages and then continue training. However, using the data obtained solely relying on translation while ignoring the original capabilities of LLMs across languages is not always effective, which we show will limit the performance of cross-lingual knowledge transfer. In this work, we propose SDRRL, a method based on elf-istillation from esource-ich anguages that effectively improve multilingual performance by leveraging the internal capabilities of LLMs on resource-rich languages. We evaluate on different LLMs (LLaMA-2 and SeaLLM) and source languages (English and French) across various comprehension and generation tasks, experimental results demonstrate that SDRRL can significantly enhance multilingual capabilities while minimizing the impact on original performance in resource-rich languages.. }","Comparison between vanilla supervised fine-tuning (SFT), translate-then-SFT, and our proposed method. Besides using the translated question-answer pairs in the target language (e.g., Japanese), SDRRL further leverages the generated answer $A^{\star}_{\rm EN}$ by LLMs in the resource-rich language (e.g., English) and collects self-distillated data (in green box) to help enhance its multilingual capabilities.","Contemporary large language models (LLMs; ) are predominantly trained on multilingual corpora. However, the language distribution in the data is highly imbalanced. For instance, LLMs like LLaMA-2, with English as the primary language, have also been trained on Japanese text, yet the quantity of English tokens used during pre-training exceeds that of Japanese by a factor of 897. The imbalanced data distribution above has led to significant limitations in the capabilities of LLMs across most languages. To enhance the multilingual capabilities, a common approach follows the translating and then supervised fine-tuning (SFT;) paradigm, as shown in Figure~(b). Specifically, training data is translated into the target language using either the model itself or an external machine translation (MT) system before continuing the training process, thereby offering more data in the target language and improving multilingual capabilities. However, the translate-then-SFT method encounters several challenges: First, the multilingual enhancement gained from translated ``question-answer'' pairs is limited and may sometimes even degrade the capabilities in the original primary language. Second, constrained by the accuracy of machine translation (especially for the low-resource languages), the translated texts used for training can be highly noisy, containing numerous awkward sentences and incorrect content, adversely affecting the quality of the generated text and the multilingual abilities of the LLMs. Therefore, we explore a new question along this trajectory: {Besides translating the training pairs, can we enhance the abilities in other languages by leveraging the original relatively strong capabilities of LLMs in resource-rich language?} In this paper, we introduce SDRRL, a method that uses {S}elf-{D}istillation from {R}esource-{R}ich {L}anguages) to achieve the goal mentioned above. Specifically, as illustrated in Figure~(c), SDRRL comprises two parts: (1) {Self-Distillation}: Instead of the ground-truth answer, responses from LLMs in resource-rich languages are collected to construct a transfer set. These are then translated into other languages using machine translation systems and code-switching tools, forming ``question-answer'' pairs that are semantically identical but linguistically varied, and conducting sentence-level knowledge self-distillation within the same batch. (2) {Incorporating External Parallel Corpus}: We further involve a small amount of machine translation data in the distillation, aiming to align the linguistic representation spaces better and mitigate the negative impact of the noise in machine translation systems on the generative capabilities of LLMs. Our experiments, based on LLaMA-2-7B and SeaLLM-7B with English as the resource-rich language, demonstrate that even with a smaller set of English instruction data as the transfer set, SDRRL can effectively distill English capabilities into 14 other languages, showing effectiveness in both multilingual comprehension and generation tasks. Further analysis indicates that SDRRL helps preserve the original capabilities in high-resource languages and improves the quality of generated responses"
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations,2403.14112v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2403.14112v3_0.pdf,"We introduce CHARM, the first benchmark for comprehensively and in-depth evaluating the commonsense reasoning ability of large language models (LLMs) in Chinese, which covers both globally known and Chinese-specific commonsense. We evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5 representative prompt strategies for improving LLMs' reasoning ability, such as Chain-of-Thought. Our findings indicated that the LLM's language orientation and the task's domain influence the effectiveness of the prompt strategy, which enriches previous research findings. We built closely-interconnected reasoning and memorization tasks, and found that some LLMs struggle with memorizing Chinese commonsense, affecting their reasoning ability, while others show differences in reasoning despite similar memorization performance. We also evaluated the LLMs' memorization-independent reasoning abilities and analyzed the typical errors. Our study precisely identified the LLMs' strengths and weaknesses, providing the clear direction for optimization. It can also serve as a reference for studies in other fields. We will release CHARM at .",Construction of CHARM. CHARM encompasses both global and Chinese-specific commonsense. CHARM consists closely-interconnected reasoning and memorization tasks.,"Commonsense reasoning is important for the enhancement of the large language models (LLMs) towards artificial general intelligence (AGI) , therefore requires thorough evaluations. Numerous benchmarks evaluate the commonsense reasoning of LLMs, but most are English-based, limiting non-English evaluations . This paper focuses on assessing LLMs' commonsense reasoning in a Chinese context. Currently, some commonsense reasoning benchmarks in Chinese are simply English translations , which overlooks unique Chinese cultural, linguistic, regional, and historical aspects. These factors matter when Chinese users use the LLM, hence should be included in benchmarks. To effectively tackle this, we introduce CHARM, the benchmark designed to thoroughly and in-depth assess the abilities of LLMs in Chinese commonsense reasoning. It covers two domains: globally accepted commonsense (global domain) and Chinese-specific commonsense (Chinese domain). The latter includes 7 aspects: {History (H)}, {Traditional Culture and Arts (CA)}, {Daily Life and Customs (LC)}, {Entertainment (E)}, {Public Figures (F)}, {Geography (G)}, and {Chinese Language (L)}. Therefore CHARM allows a thorough evaluation of LLMs' reasoning in a Chinese context. Prompt strategies like Chain of Thought (CoT) can significantly improve LLMs' reasoning performance . Particularly, as the training corpus of LLMs is primarily in English , studies have shown that for non-English reasoning tasks, some LLMs perform better when reasoning in English than the native language. We evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5 representative prompt strategies. The result showed that prompt strategies' effectiveness depends on the LLMs' orientation and the benchmark task's domain, which enriches prior research and guides performance assessment and strategy choice for non-English LLMs. LLMs' commonsense reasoning relies on memorization. Exploring the correlation between memorization and reasoning offers insights into LLMs, aiding deeper understanding and suggesting ways to enhance these abilities. Some benchmarks aid the research of memorization-reasoning relationships by incorporate tasks for assessing knowledge memorization and application (like reasoning). However, they used the existing and disparate datasets for different tasks, resulting in a lack of intrinsic connections between these tasks. For instance, the question $Q_{rea}$ tests the LLM's reasoning with the knowledge piece $K$. However, in memorization tasks, there probably is not any matching questions to determine if the LLM has effectively memorized $K$. Hence, if the LLM fails on $Q_{rea}$, it's unclear whether due to poor reasoning or forgetfulness of $K$. This results in the disjointed evaluation of memorization and reasoning, failing to uncover their intrinsic links. To address this limitation, we selected suitable reasoning tasks from CHARM's Chinese domain, and built related memorization questions for each reasoning question (see Figure ). This design produces the closely-interconnected reasoning and memorization tasks, therefore allows for not only the concurrent evaluation of the two abilities, but also the assessment of memorization-independent reasoning, providing the clear guidance for the LLMs' enhancement. The contributions of this paper are as follows: {itemize} We present CHARM, the first benchmark for comprehensively evaluating the LLMs' commonsense reasoning ability in Chinese, by encompassing not only the global but also the Chinese-specific commonsense. {itemize} {itemize} We evaluated the representative prompt strategies on CHARM. Results showed that LLMs' orientation and the task's domain affect prompt strategy performance, which enriches previous research findings. {itemize} {itemize} In CHARM, we built closely-interconnected reasoning and memorization tasks in Chinese commonsense domain, allowing for in-depth understanding the correlation between these abilities and precisely identifying the LLMs' strengths and weaknesses. The design approach could serve as the reference for other fields. {itemize"
Model Composition for Multimodal Large Language Models,2402.12750v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.12750v2_0.pdf,"Recent developments in Multimodal Large Language Models (MLLMs) have shown rapid progress, moving towards the goal of creating versatile MLLMs that understand inputs from various modalities. However, existing methods typically rely on joint training with paired multimodal instruction data, which is resource-intensive and challenging to extend to new modalities. In this paper, we propose a new paradigm through the model composition of existing MLLMs to create a new model that retains the modal understanding capabilities of each original model. Our basic implementation, , demonstrates the effectiveness of this paradigm by reusing modality encoders and merging LLM parameters. Furthermore, we introduce to address parameter interference and mismatch issues during the merging process, thereby enhancing the model performance. To facilitate research in this area, we propose , a benchmark for assessing ability of MLLMs to understand inputs from diverse modalities. Experiments on this benchmark and four other multimodal understanding tasks show significant improvements over baselines, proving that model composition can create a versatile model capable of processing inputs from multiple modalities.}",Illustration of various approaches for multimodal large language models: (a) aligning LLM with a multimodal encoder and (b) joint training with multiple modal encoders and (c) our proposed model composition method that creates a versatile model from existing MLLMs through a training-free and extensible process.,"Recent advancements in Multimodal Large Language Models~(MLLMs) have established them as the forefront of multimodal learning paradigms. The prevalent approach involves aligning modality encoders with large language models~(LLMs) through extensive modality-text paired data and then fine-tuning with modality-specific instruction data. This paradigm has been successfully applied to a wide range of modalities such as image, audio, video, and point cloud, resulting in the emergence of a diverse array of MLLMs with unique modal capabilities. There are also some efforts to enable a single MLLM to handle multiple modalities. One way to achieve this is to align the LLM with a multimodal encoder such as ImageBind with only image-text data~(Figure (a)). This approach leverages the inherent alignment of different modalities within the multimodal encoder, allowing the MLLM to comprehend various modalities to a certain degree. However, the absence of modality-specific instruction data often results in suboptimal performance. Another approach entails the concurrent training of the MLLM with multiple modality encoders~(Figure (b)). For example, ChatBridge connects image, video and audio encoders with the LLM through a joint training process with multimodal instruction data~(i.e., video-audio chats). This kind of methods show potential but faces two major challenges. First, it is resource-heavy to collect paired data across multiple modalities. Second, adapting these models to new modalities requires additional training, adding to the complexity and resource demands of the development process. Given the limitations of current approaches, we propose and study a more practical setting: {model composition} for MLLMs~(Figure (c)). Our primary research question is simple yet fundamental: {Can we create a new MLLM by combining existing MLLMs to inherit their understanding of different modalities without training?} Model composition for MLLMs is advantageous for two key reasons: (1) it eliminates the need for the resource-heavy process of training and gathering multimodal data, and (2) it promises enhanced adaptability, facilitating seamless incorporation of new modalities. Some recent studies, such as X-InstructBLIP , serve as pioneering efforts in model composition for MLLMs. These works primarily train projectors to align different encoders with a single LLM and demonstrate the ability to process multiple modalities concurrently. However, a critical limitation is their applicability only to MLLMs with {frozen} language model weights. This constraint restricts the range of models that can be utilized, and impairs the overall performance of the MLLMs. In this paper, we first propose a framework for model composition for MLLMs. Our implementation, named {}, is elegantly simple: for the MLLMs to be composed, we directly reuse their modality-specific encoders and merge their LLM parameters. We demonstrate that MLLMs, as long as initialized from the same LLM, can achieve zero-shot multi-modality expansion through this model composition framework, regardless of whether the parameters of the LLM have been fine-tuned. Furthermore, to mitigate parameter interference in the composition process and optimize the performance of the composite model, we propose {}, an advanced framework with parameter {D}ecoupling and {A}djustment for {M}odel {C}omposition. By separating modality-specific parameters from language model parameters during initial MLLM training, {} allows for the selective merging of textual parameters, reducing cross-modal interference. Moreover, {} introduces an adaptive parameter adjustment mechanism to ensure optimal compatibility and effectiveness of the composite model, achieving a balanced and efficient multi-modality expansion. To assess the efficacy of our proposed frameworks, we conduct comprehensive experiments on tasks that require an integrated understanding of inputs from diverse combinations of four prevalent modalities: image, audio, video, and point cloud. To facilitate the research in model composition for MLLMs, we also build {}, a benchmark specifically designed for evaluating the capability to concurrently comprehend multiple modalities by identifying commonalities across inputs from various modalities. Experimental results indicate that our frameworks enable the composition of existing MLLMs from different modalities without requiring further training, yielding a versatile and high-performing multimodal model adept at handling any combination of these modalities. Our contributions are three-fold: {itemize} {}{0pt} {}{0pt} {}{0pt} We propose the concept of model composition for MLLMs, realized through the {} framework, which allows for seamless integration of different MLLMs without additional training, enabling zero-shot multi-modality expansion. We introduce {}, an advanced model composition framework that employs parameter decoupling and adaptive adjustment to mitigate parameter interference and optimize composite model performance across multiple modalities. We create {}, a benchmark designed to evaluate the unified understanding of diverse modalities, and demonstrate the efficacy of our model composition frameworks via extensive experiments on various multimodal understanding tasks and {}. {itemize"
Draft & Verify: Lossless Large Language Model Acceleration via Self-Speculative Decoding,2309.08168v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2309.08168v2_0.pdf,"We present a novel inference scheme, self-speculative decoding, for accelerating Large Language Models (LLMs) without the need for an auxiliary model. This approach is characterized by a two-stage process: drafting and verification. The drafting stage generates draft tokens at a slightly lower quality but more quickly, which is achieved by selectively skipping certain intermediate layers during drafting. Subsequently, the verification stage employs the original LLM to validate those draft output tokens in one forward pass. This process ensures the final output remains {identical} to that produced by the unaltered LLM. Moreover, the proposed method requires no additional neural network training and no extra memory footprint, making it a plug-and-play and cost-effective solution for inference acceleration. Benchmarks with LLaMA-2 and its variants demonstrated a speedup up to 1.99$$., and will be released with the Apache-2.0 License.}","Visualization of the self-speculative decoding process. The verification stage evaluates all drafted tokens in a single forward pass, with accepted tokens marked in green and rejected tokens highlighted in red. Each verification step also predicts one more token, which is denoted in blue.","Transformer-based Large Language Models (LLMs), such as GPT-3/4, PaLM, and LLaMA, have been widely adopted in various real-world applications . However, their inference costs have raised significant concerns, especially for latency-sensitive scenarios . The main efficiency bottleneck is the {autoregressive decoding} process. This process decodes each output token sequentially, leading to a high number of Transformer calls; furthermore, each Transformer call is typically memory bandwidth-bound, resulting in low computation utility and thus longer wall-clock time . For instance, decoding 128 tokens autoregressively using LLaMA-2-13B on an A100 GPU can take up to 100$$ longer than a sequence-level forward pass on the same number of tokens, highlighting the substantial inefficiency inherent in the current decoding process. Established model compression techniques such as quantization , pruning , and distillation have been employed to alleviate these costs. While these solutions have proven extremely effective, they usually require changing the model architecture, changing the training procedure, re-training or fine-tuning the models, and do not maintain identical outputs. In parallel to model compression, {speculative execution} is being explored to accelerate the autoregressive decoding process . These methods train an auxiliary {draft model} that can quickly generate some draft output tokens. Subsequently, the original LLM, referred to as the {verify model}, then checks the acceptability of these draft tokens with one single forward pass. This verification step ensures that the outputs are derived from the original LLM's probability distribution. However, an essential issue of existing speculative execution methods is the need to identify or train a suitable draft model that can generate outputs consistent with the verify model. It becomes more tricky when the LLM is already a fine-tuned model, e.g.~LLaMA-2-Chat , CodeLLaMA . How to find or train a draft model that can effectively mimic the outputs of such a tailored model is a formidable task, with no straightforward or guaranteed solutions. Furthermore, the introduction of an additional draft model escalates the GPU memory overhead, increasing deployment challenges particularly on devices with restricted memory capacity. In this paper, we present {self-speculative decoding}, a novel approach to accelerate the inference of LLMs. This method builds on the principles of speculative execution, but with a unique twist: it utilizes one LLM for both drafting and verification stages. The key insight driving our approach is the observation that skipping certain layers in LLMs does not significantly compromise the generation quality . As such, by selectively bypassing some intermediate layers, we can use the LLM itself to generate draft tokens. These tokens are then verified by the original LLM in a single forward pass. {fig:intro} illustrates this two-stage decoding process. The blue arrow indicates the inference path of the original model, while the green arrow depicts the inference path during the drafting stage. Notably, both inference paths share the same model so we do not need a standalone draft model with extra memory overhead. Implementing self-speculative decoding poses two main challenges: (a) determining which layers and the number of layers to skip during drafting, and (b) deciding the timing to stop generating draft tokens. To tackle the first challenge, we formulate it as an optimization problem, which accepts the combinations of layers to bypass as input and aims to minimize the average inference time per token. We employ Bayesian optimization to solve this problem. The optimization is performed offline at the model level, and the searched layer combinations are fixed. The second challenge pertains to determining the optimal number of draft tokens ($K$) to generate. As shown in {fig:analyse}, the choice of $K$ significantly influences the end-to-end speedup: for an acceptance rate below 80\ This observation underscores that a static $K$ is not universally applicable. To tackle this variability, we introduce an adaptive draft-exiting mechanism, which stops generating draft tokens once its confidence level drops below a threshold. This intervention prevents unnecessary computation and potential discard of additional draft tokens, thereby enhancing efficiency. To summarize, our main contributions are: (1) {Inference scheme}: we propose self-speculative decoding, a practical, plug-and-play solution for inference acceleration that does not require further neural network training and avoids additional memory overhead; (2) {Optimization strategies}: we adopt Bayesian optimization to select which layers to skip during drafting and propose a simple yet effective method to adaptively determine the number of draft tokens; (3) {Evaluation}: we evaluate our method on text summarization and code generation tasks, and the experimental results indicate that our method can achieve up to 1.99$$ in end-to-end speedup"
Measuring Meaning Composition in the Human Brain with Composition Scores from Large Language Models,2403.04325v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2403.04325v3_0.png,"The process of meaning composition, wherein smaller units like morphemes or words combine to form the meaning of phrases and sentences, is essential for human sentence comprehension. Despite extensive neurolinguistic research into the brain regions involved in meaning composition, a computational metric to quantify the extent of composition is still lacking. Drawing on the key-value memory interpretation of transformer feed-forward network blocks, we introduce the Composition Score, a novel model-based metric designed to quantify the degree of meaning composition during sentence comprehension. Experimental findings show that this metric correlates with brain clusters associated with word frequency, structural processing, and general sensitivity to words, suggesting the multifaceted nature of meaning composition during human sentence comprehension. {GitHub}.}",Comparing Composition Scores with fMRI data during naturalistic listening comprehension.,"When encountering words such as ""milk"" and ""pudding"", the human mind effortlessly combines them to form a complex concept, such as a milk-flavored pudding. This combinatory process is a fundamental aspect of human language comprehension and production, enabling us to generate an infinite array of meanings from a finite set of words. Despite extensive neurolinguistic research into the localization of meaning composition in the human brain , understanding the detailed mechanism of how a complex meaning is constructed from its components and how it is processed by the human brain has become a challenging problem. One of the primary difficulties lies in the absence of a suitable computational metric to quantify the extent of meaning composition. This absence significantly complicates quantitative analyses of meaning composition in the human brain. Recent advancements in Large Language Models (LLMs) offer promising insights into this problem. By training on large-scale natural language corpora and aligning with human preferences, these computational models achieve unprecedented levels of proficiency in understanding and generating natural languages . In addition to their high performance, studies have shown that their internal states correlate with human behavioral and neural data , suggesting shared principles between their algorithms and the human brain. Given this background, it is natural to inquire whether we can develop a computational metric to quantify meaning composition from the internal states of LLMs. Motivated by this inquiry, our study introduces a novel model-based metric, the Composition Score, to evaluate meaning composition in the human brain. Leveraging the key-value memory interpretation of the Feed-Forward Network (FFN) modules in the transformer model , this metric computes the composition of memory-induced vocabulary distributions within the FFN blocks given an input prefix, thereby reflecting the degree of meaning composition of each word. To assess its validity, we examine the patterns of Composition Scores using the novel ""The Little Prince"" in English and compare them with other control variables such as word frequency and syntactic node count based on top-down, bottom-up, and left-corner parsing. Additionally, we correlate Composition Scores with an openly available fMRI dataset where participants listened to ""The Little Prince"" in the scanner . Our findings reveal that: {itemize} The Composition Score exhibits partial correlation with word frequency and syntactic node counts but reveals more intricate patterns; The Composition Score is associated with a broader brain cluster and exhibits a higher regression score with the fMRI data compared to the control variables; Brain regions associated with the Composition Score encompass those underlying word frequency, structural processing, and general sensitivity to words, indicating the multifaceted nature of meaning composition. {itemize"
Complex Reasoning over Logical Queries on Commonsense Knowledge Graphs,2403.07398v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2403.07398v2_0.pdf,"Event commonsense reasoning requires the ability to reason about the relationship between events, as well as infer implicit context underlying that relationship. %, which state-of-the-art language models still struggle to perform. However, data scarcity makes it challenging for language models to learn to generate commonsense inferences for contexts and questions involving interactions between complex events. To address this demand, we present ({COM}plex {COM}monsense), a new dataset created by sampling multi-hop logical queries (e.g., {the joint effect or cause of both event A and B, or the effect of the effect of event C}) from an existing commonsense knowledge graph (CSKG), and verbalizing them using handcrafted rules and large language models into multiple-choice and text generation questions. Our experiments show that language models trained on exhibit significant improvements in complex reasoning ability, resulting in enhanced zero-shot performance in both in-domain and out-of-domain tasks for question answering and generative commonsense reasoning, without expensive human annotations.}",An example of conjunctive logical queries and their verbalization as complex commonsense inferences.,"Large language models struggle to effectively perform reasoning when presented with complex tasks, such as reasoning about multiple events and their relationships. This shortcoming is due to both the inherent difficulty of reasoning over multiple pieces of information, as well as a lack of adequate-scale, supervised training datasets for learning. Unfortunately, complex and multi-hop commonsense reasoning benchmarks are both technically challenging and financially expensive to curate. Consequently, previous efforts either constructed datasets (a) with simpler reasoning structures, such as single-hop chains, (b) using distant supervision based on one-hop inference, or (c) with human-annotations, but at a relatively small scale. To alleviate this training data bottleneck, recent works have explored extracting and formulating questions from existing CommonSense Knowledge Graphs (CSKGs; ), which store commonsense triples. However, using CSKGs to produce high-quality reasoning datasets poses several challenges. First, while the shared entities in commonsense triples encode a complex, interconnected graph structure, the sparsity of this structure limits the number of potential questions that encode more than one reasoning hop . Second, triples in CSKGs are represented in a context-free manner, such as the event ``PersonX gets tired of it'' in {fig:Introduction_demo}, yielding ambiguous (and sometimes incorrect) human annotations in the CSKG, e.g., ATOMIC has an error rate of over 10\ In this paper, we construct {} ({COM}plex {COM}monsense), a novel commonsense reasoning dataset using multi-hop queries in commonsense knowledge graphs to construct question answer pairs requiring complex narrative reasoning. To build this dataset, we use {conjunctive logical queries}, a subset of First-Order Logical queries that use existential quantifiers and conjunction. The multi-hop projection operation involves inferring hidden contexts, while the intersection operation enables reasoning among multiple events, encompassing common cause or effect, and abduction. For example, in {fig:Introduction_demo}, an intersection of two triples can be verbalized to a short narrative, and the process of inferring the common tail can be seen as an {abduction} of the hidden cause between the two heads. To address the challenges above, we propose to first {densify} the CSKG to merge nodes with high semantic similarity, increasing the connectivity of the graph. Then, we use an off-the-shelf plausibility scorer to filter out low quality triples, avoiding error propagation as we construct more complicated queries. Finally, we verbalize the queries to a natural language context with handcrafted rules and Large Language Models to derive coherent and informative narrative contexts for our questions. Our final {} dataset comprises 790K question-answer pairs (both with multiple-choice and generative answer settings), including 1.3K examples that we manually verify for evaluation. Our results demonstrate the challenges faced by even powerful LLMs and supervised question answering models on the {} dataset, underscoring the difficulty of performing complex multi-hop reasoning. Moreover, fine-tuning question answering models and generative commonsense inference models on {} leads to substantial improvements across eight commonsense reasoning datasets, showing the efficacy of our framework for boosting commonsense reasoning ability. To conclude, our contributions are three-fold. First, we present a pipeline for sampling and verbalizing complex logical queries from CSKGs, to form a complex commonsense reasoning benchmark, {}, with minimal human effort. Second, we benchmark the complex reasoning ability of various state-of-the-art language models and question answering models on {}. Finally, we validate the benefit of fine-tuning on {} on eight zero-shot commonsense reasoning datasets"
Learning to Plan and Generate Text with Citations,2404.03381v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2404.03381v3_0.pdf,"The increasing demand for the deployment of LLMs in information-seeking scenarios has spurred efforts in creating verifiable systems, which generate responses to queries along with supporting evidence. In this paper, we explore the {attribution} capabilities of plan-based models which have been recently shown to improve the faithfulness, grounding, and controllability of generated text. We conceptualize plans as a sequence of questions which serve as {blueprints} of the generated content and its organization. We propose two attribution models that utilize different variants of blueprints, an {abstractive} model where questions are generated from scratch, and an {extractive} model where questions are copied from the input. Experiments on long-form question-answering show that planning consistently improves attribution quality. Moreover, the citations generated by blueprint models are more accurate compared to those obtained from LLM-based pipelines lacking a planning component.","{figure:example} Query (top), followed by most relevant (abridged) passages, and summaries (bottom) with in-line citations. Summary~(a) is the output of a vanilla sequence-to-sequence model trained to generate long answers with citations. Summaries~(b) and~(c) are the output of models with abstractive and extractive plans, respectively. Citations for plan-based models can have different formats (e.g., references to the question plan; see Section~\ref{sec:results:analysis}).","Large language models (LLMs) have demonstrated remarkable abilities to engage in creative conversations, summarize information from contextual cues , and deliver zero-shot performance on a wide range of previously unseen predictive and generative tasks . They are also becoming increasingly useful in information-seeking scenarios, ranging from answering simple questions to generating responses to search-like queries . The increasing demand for the deployment of LLMs in information-seeking scenarios has further spurred efforts in creating verifiable systems, which generate responses to queries along with supporting evidence. The evidence can take the form of a URL pointing to a short segment of text which supports an answer , an attribution report with evidence snippets , quotes cited verbatim from pages retrieved from a search engine , and references to passages extracted while browsing . In fact, this last type of evidence has been recently adopted in the form of in-line citations by commercial search engines such as BingChat{{https://bing.com/new}} and perplexity.ai{{https://perplexity.ai}}. Regardless of how the evidence is presented, recent approaches tend to rely on a retrieval system (e.g., a commercial search engine) to obtain passages relevant to a query, while an LLM conditions on them to generate a response . Other work generates an answer to the input query first and subsequently retrieves relevant evidence in a post-processing step . Alternatively, the retrieved evidence can be used to further revise the generated response rendering it more consistent with the evidence . Despite recent efforts, it remains an open question how to best develop models with a {built-in} mechanism for attribution to external evidence. A related question is whether said mechanism contributes to generating more factually faithful output. Large-scale evaluation studies paint a worrying picture. find that long-form responses from existing search engines frequently contain unsupported statements or inaccurate citations, while show that model performance on attribution varies greatly (between 46\ architectures for the simpler question answering task. In this paper, we explore the attribution capabilities of plan-based models which have been shown to be less prone to hallucinations and more controllable {moryossef-etal-2019-step, puduppully2019data, narayan-etal-2021-planning, narayan2022conditional,huot-etal-2023-text,huot2023muplan}. We focus on long-form question answering which aims to generate summaries from a set of passages that answer a specific query. We simulate how a search engine might synthesize passages of high relevance to a user query by assuming access to a retriever, and some way of verifying the output, i.e.,~by citing sources (see Figure~). Our models operate on retrieved passages and learn to plan and generate summaries with attribution. On account of being more expressive, plan-based models allow us to formalize different forms of attribution, e.g.,~plans can be verified via citations to passages, while summaries can be verified through citations to the plan, passages, or both. Our models conceptualize text plans as a sequence of questions operating as {blueprints} for generation, determining what to say and in which order . Questions as a planning mechanism are ideally suited to attribution, since they provide a natural link between retrieved passages and their summaries. We define two models which differ on whether the question-based plan is {generated} (see Figure~(b)) or {copied} from input passages (Figure~(c)) and explore whether explicit planning has any bearing on citation quality. Our contributions can be summarized as follows: {itemize} 0em We develop automatic methods to annotate training data with plans {and} citations, and fine-tune several Transformer models to generate attributed text.{Our data and code are available from {xxx.yyy.zzz}.} Experimental results on the AQuAMuSe dataset demonstrate that plans consistently improve attribution quality. Furthermore, summary quality improves with an extractive blueprint model. Out-of-domain experiments on the ALCE benchmark show that, once acquired, attribution is a robust skill (across information-seeking tasks). In terms of attribution quality, our models are competitive with (and sometimes better than) pipelines that heavily rely on large language models. {itemize"
Split and Rephrase with Large Language Models,2312.11075v4,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2312.11075v4_0.png;/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2312.11075v4_1.png,"The Split and Rephrase (SPRP) task, which consists in splitting complex sentences into a sequence of shorter grammatical sentences, while preserving the original meaning, can facilitate the processing of complex texts for humans and machines alike. It is also a valuable testbed to evaluate natural language processing models, as it requires modelling complex grammatical aspects. In this work, we evaluate large language models on the task, showing that they can provide large improvements over the state of the art on the main metrics, although still lagging in terms of splitting compliance. Results from two human evaluations further support the conclusions drawn from automated metric results. We provide a comprehensive study that includes prompting variants, domain shift, fine-tuned pretrained language models of varying parameter size and training data volumes, contrasted with both zero-shot and few-shot approaches on instruction-tuned language models. Although the latter were markedly outperformed by fine-tuned models, they may constitute a reasonable off-the-shelf alternative. Our results provide a fine-grained analysis of the potential and limitations of large language models for SPRP, with significant improvements achievable using relatively small amounts of training data and model parameters overall, and remaining limitations for all models on the task.","Impact of parameter size on the DeSSE (left) and BiSECT (right) test sets, with Pythia model variants fine-tuned over data from the DeSSE (-DE) or BiSECT (-BI) training data.","Transforming complex sentences into a sequence of shorter sentences, while preserving the original meaning, can facilitate the processing of complex texts for humans and machines alike. It is, for instance, an important operation in text simplification , where simpler split sentences can provide content that is easier to read and comprehend. This task, commonly referred to as Split and Rephrase (SPRP) , also provides a relevant framework to evaluate natural language processing models, as complex linguistic properties need to be tackled to provide optimal splits and appropriate rephrasing. The SPRP task is typically performed via dedicated transformation models, e.g. sequence-to-sequence modelling with a copy mechanism or graph-based neural segmentation . The main limitation for this type of approach is the dependency on training datasets of paired complex and split sentences, which are scarce across languages and domains. As an alternative, exploit one-to-many alignments in parallel corpora and pivot machine translation (MT) to create an aligned corpus of complex and split sentences, which can in turn be exploited by sequence-to-sequence models. For this type of approach, the main bottleneck is the dependency on parallel corpora with sufficient many-to-one alignments and on quality pivot machine translation models. Large language models (LLMs), such as the GPT models based on the Transformer architecture , have demonstrated their strong potential on a large number of downstream tasks . In this work, we measure their performance on the SPRP task, prompting the models to generate split and rephrase hypotheses from complex sentences. We evaluate different variants of the approach on datasets of varying complexity, contrasting zero shot, few shot in-context learning, and fine-tuning scenarios. We notably measure the impact of parameter and training data size, domain shift, and prompt optimisation. Overall, our results demonstrate that LLMs outperform the previous state of the art by large margins across multiple metrics, with remaining relative deficiencies, notably in terms of split generation. Results from two human evaluations, a comparative 3-way ranking task and a qualitative evaluation, further support our conclusions. Our main contributions can be summarised as follows: {itemize} New state of the art on the SPRP task, established on publicly available datasets of varying complexity, over multiple metrics and two human evaluations. A comprehensive evaluation of LLMs for SPRP, covering LLM variants with and without instruction tuning, along with the impact of different prompting strategies, parameter size, training data volumes, and domain shift. Empirical results on LLM strengths and limitations for SPRP, in particular over splitting compliance and rephrasing variability. {itemize"
SAPT: A Shared Attention Framework for Parameter-Efficient Continual Learning of Large Language Models,2401.08295v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2401.08295v3_0.pdf,"The continual learning (CL) ability is vital for deploying large language models (LLMs) in the dynamic world. Existing methods devise the learning module to acquire task-specific knowledge with parameter-efficient tuning (PET) block and the selection module to pick out the corresponding one for the testing input, aiming at handling the challenges of catastrophic forgetting and knowledge transfer in CL. However, these methods tend to address only one of the challenges, ignoring the potential of aligning the two modules to effectively address catastrophic forgetting and knowledge transfer simultaneously. To this end, we propose a novel Shared Attention Framework (SAPT), to align the PET learning and selection via the Shared Attentive Learning \& Selection module. Extensive experiments on two CL benchmarks demonstrate the superiority of SAPT. Moreover, SAPT consistently demonstrates its superiority when we scale it to different model sizes (from 770M to 13B), different model architectures (T5 and LLaMA-2) and unseen tasks..}",The conceptual framework for the learning and the selection module to achieve the continual learning of large language models based on \petlWithEmoji \ when the new Dialogue Generation task arrives. Dashed lines represent the working process of existing works while solid lines are for that of our SAPT in this work.,"Endowing the continual learning (CL) ability for large language models (LLMs) to learn different tasks sequentially is crucial for their deployment in the real-world, which allows them to dynamically adapt to novel tasks and acquire additional knowledge . However, this scenario presents two significant challenges: (1) Catastrophic Forgetting (CF), referring to the loss of previously acquired knowledge when learning new tasks , and (2) Knowledge Transfer (KT), involving the efficient utilization of knowledge from past tasks to facilitate the learning of new ones . Due to the heavy burden on computation resources, recent attempts study the CL of LLMs based on parameter-efficient tuning (PET) methods . Inspired by the parameter isolation CL methods , existing methods can be conceptualized as two pivotal components working in the pipeline fashion. As shown in Figure~ (dashed lines), when a new Dialogue Generation task arrives, a private PET block is allocated by the {learning module} to acquire task-specific knowledge and then saved to the PET pool for the following {selection module} to pick it out when a test sample is coming. However, the designs of each module in current works exhibit certain limitations in effectively dealing with KT and CF challenges. {On one hand}, the design of {learning module} is supposed to function to facilitate KT among different tasks. Unfortunately, for existing works, the learning of PET block is either performed seperately within each single task , or kept orthogonal to each other to minimize interference . Such isolation cuts off the potential transfer of acquired knowledge stored in the previous PET blocks and hinders them to assist the current acquisition of new knowledge. {On the other hand}, the {selection module} plays the pivotal roles in mitigating CF because only when it is capable of automatically selecting the PET block to which the current input belongs can the LLM backbone successfully accomplish the current task. However, it would make LLMs vulnerable to CF by simply implementing such selection process via the summation or concatenation of all existing PET blocks or selecting them from a fixed PET pool . {More importantly}, they ignore the opportunity of aligning the two modules to address challenges of CF and KT simultaneously. The intuition is that (illustrated by solid lines in Figure ), in order to facilitate KT in the learning of the new task, the learning module should rely on task correlations to leverage the most relevant knowledge in previous PET blocks. And such attentive process, expressed as {shared attention} in our study, could be naturally repeated by the selection module to resist CF through the combination of the corresponding PET blocks belonging to each testing input. As a result, the end-to-end alignment of these two modules is established via such shared attention. To this end, we propose a novel {{S}}hared {{A}}ttention Framework for {{P}}arameter-efficient con{{T}}inual learning ({SAPT}) of large language models. In SAPT, the Shared Attentive Learning \& Selection Module (SALS) is devised, where each training sample is navigated to utilize the optimal combinations of existing PET blocks for completing the current task. This is achieved through an attention weight obtained via instance-level shared attention operation. Then inputs in the testing time are capable of following the same shared attention operation to reach the attention weight and pick out the appropriate PET blocks accordingly. However, continually updating the SALS leads to the optimal attentive combination only for the newest task, resulting in the forgetting for that of previous ones. Thus, we introduce Attentive Reflection Module (ARM) to help SALS recall what the shared attention operation of inputs from previous tasks should be originally performed with pseudo samples. And the success of ARM offers a new perspective for the utilization of generated pseudo samples instead of just blindly mixing them with samples of new tasks for multi-task training. We conduct extensive experiments to evaluate SAPT on SuperNI and Long Sequence benchmarks. State-of-the-art performance is achieved by SAPT compared with recent PET-based CL methods. Moreover, SAPT also exhibits superior performance when we scale it to different model sizes (from 770M to 13B), different model architectures, including T5 (encoder-decoder) and LLaMA-2 (decoder-only) and previously unseen tasks. The main contributions of this work are summarized as follows: {itemize} We propose a novel framework SAPT, including SALS and ARM, to align the PET learning and selection process to effectively handle the CF and KT challenges simultaneously. A novel perspective for the utilization of pseudo generated samples is offered in ARM, exhibiting both improved effectiveness and efficiency than naive (generative) replay. Results of extensive experiments on the benchmark datasets demonstrate the effectiveness of SAPT to mitigate CF and facilitate KT. {itemize"
CharacterEval: A Chinese Benchmark for Role-Playing Conversational Agent Evaluation,2401.01275v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2401.01275v2_0.pdf,"Recently, the advent of large language models (LLMs) has revolutionized generative agents. Among them, Role-Playing Conversational Agents (RPCAs) attract considerable attention due to their ability to emotionally engage users. However, the absence of a comprehensive benchmark impedes progress in this field. To bridge this gap, we introduce {CharacterEval}, a Chinese benchmark for comprehensive RPCA assessment, complemented by a tailored high-quality dataset. The dataset comprises 1,785 multi-turn role-playing dialogues, encompassing 11,376 examples and featuring 77 characters derived from Chinese novels and scripts. It was carefully constructed, beginning with initial dialogue extraction via GPT-4, followed by rigorous human-led quality control, and enhanced with in-depth character profiles sourced from Baidu Baike. {CharacterEval} employs a multifaceted evaluation approach, encompassing thirteen targeted metrics on four dimensions. To facilitate the convenient evaluation for these subjective metrics in {CharacterEval}, we further developed CharacterRM, a role-playing reward model based on human annotations, which has a higher correlation with human judgment compared to GPT-4. Comprehensive experiments on {CharacterEval} demonstrate that Chinese LLMs exhibit more promising capabilities than GPT-4 in Chinese role-playing conversation. Source code, data source and reward model will be publicly accessible at~.","An example of the {CharacterEval}, including the dialogue, scene and character's profile.","The development of large language models (LLMs) has marked the beginning of a new era in conversational AI, and opened up a wide range of application possibilities, particularly in agent-based interactions. The automated agents, equipped with the emerging capabilities of LLMs such as planning, reasoning, and in-context learning, can perform complex tasks for humans without any supervision. Among the diverse agents, the Role-Playing Conversational Agent (RPCA), designed to offer emotional value instead of the productivity, attracts amount of interest. RPCA represents a unique category within the realm of conversational agents, distinguished by their capability for immersive interaction. Different from traditional dialogue systems, which typically focus on chit-chat, knowledge-based, personalized and empathetic dialogue, RPCAs engage users in dynamic scenarios, where LLM agents are assumed as specific characters or roles, often derived from existing composition such as novels, films, cartoons, and games. The development of connections between fictional characters and humans has the potential to not only deepen the impact of cultural works but also improve human engagement. Furthermore, RPCAs hold significant application value in their ability to offer emotional value to users, positioning fictional characters as virtual friends. The multifaceted nature of RPCAs has sparked considerable attention, leading to a surge in both research and application development (e.g., Character AI{https://beta.character.ai}, Tongyi Xingchen{https://xingchen.aliyun.com/xingchen} and Glow{https://www.glowapp.tech/}). However, these implementations of RPCAs vary significantly in both approach and objectives, presenting a challenge in systematically assessing and comparing their capabilities. Therefore, we propose the {CharacterEval}, a Chinese role-playing conversation benchmark for advancing RPCA development. To develop a benchmark, the primary problem is the construction of a dataset. While there are existing datasets, their quality are concerning, which are either generated by LLMs or suffering from significant noise due to the extractive methods. These limitations render the evaluation results unreliable for the RPCA's actual capabilities. To address it, we constructed a Chinese role-playing conversation dataset comprising 1,785 multi-turn role-playing dialogues, encompassing 11,376 examples and 77 leading characters, drawn from diverse Chinese novels and scripts. Our process began with the collection of well-known sources across various genres. After that, GPT-4 was employed to extract dialogue scenes, utterances, and behaviors of the leading roles of these sources. Following basic preprocessing and the removal of dialogues with fewer turns, we invited annotators to assess the quality of the dialogues. Their task was to identify and retain high-quality dialogues, while discarding those of lower quality. Additionally, we crawled detailed character profiles from Baidu Baike{https://baike.baidu.com/}, composing a comprehensive dataset for RPCA evaluation. The example from the dataset is as Figure~ shows. Otherwise, role-playing conversation is a complicated task that requires not only mimicking a character's behavior and utterance but also maintaining the character's knowledge, as well as the excellent multi-turn ability. Considering this, we proposed a multifaceted evaluation approach including thirteen specific metrics on four dimensions for a fair and thorough assessment of RPCAs, Our evaluation approach considered the conversational ability, character consistency, role-playing attractiveness, and utilized a personality back-testing method to evaluate the personality accuracy of a RPCA. To assess conversational ability, we measured the conversational fluency, coherence, and consistency at both the sentence and conversation levels. Character consistency is the most crucial in role-playing conversation. Hence, we evaluated knowledge and persona consistency to measure how vividly an RPCA can simulate a character. This involves assessing knowledge exposure, accuracy, and hallucination for knowledge consistency, and evaluating behavior and utterance consistency for persona consistency. Considering that RPCAs are entertainment-oriented, role-playing attractiveness is also a important elements. We assessed this through human-likeness, communication skill, expression diversity, and empathy. Finally, we introduced personality back-testing. With the collected Myers-Briggs Type Indicator(MBTI) personality types as a reference, we let RPCAs do the MBTI assessment and calculate the MBTI accuracy (personality back-test) as implemented in. For convenient re-implementation, we invited 12 annotators to score responses generated by different models for the subjective metrics in our evaluation system. Based the human judgments, we developed a role-playing reward model—CharacterRM, whose correlation with human could surpass state-of-the-art LLM GPT-4. On {CharacterEval}, We conducted comprehensive evaluations for existing LLMs, encompassing both open- and closed-source models. Experimental results shows the broad prospect of existing Chinese LLM while GPT-series models do not take the predominance in Chinese role-playing conversation. In summary, our contributions of are as follows: {itemize} We create a large-scale, high-quality dataset for RPCA evaluation, consisting of 1,785 multi-turn role-playing dialogues, 11,376 example, featuring 77 leading characters from diverse Chinese novels and scripts. We propose {CharacterEval}, a new benchmark for RPCAs, which contain comprehensive set of evaluation principles, encompassing thirteen specific metrics on four dimensions. We develop CharacterRM, a role-playing reward model for evaluating RPCAs in several subjective metrics, achieving the better performance than GPT-4 on correlation with human. We conducted thorough evaluations of existing LLMs on {CharacterEval}, including open- and closed-source, and derived valuable findings from the results. {itemize"
Generative Cross-Modal Retrieval: Memorizing Images in Multimodal Language Models for Retrieval and Beyond,2402.10805v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.10805v1_0.pdf,"The recent advancements in generative language models have demonstrated their ability to memorize knowledge from documents and recall knowledge to respond to user queries effectively. Building upon this capability, we propose to enable multimodal large language models (MLLMs) to memorize and recall images within their parameters. Given a user query for visual content, the MLLM is anticipated to ``recall'' the relevant image from its parameters as the response. Achieving this target presents notable challenges, including inbuilt visual memory and visual recall schemes within MLLMs. To address these challenges, we introduce a generative cross-modal retrieval framework, which assigns unique identifier strings to represent images and involves two training steps: learning to memorize and learning to retrieve. The first step focuses on training the MLLM to memorize the association between images and their respective identifiers. The latter step teaches the MLLM to generate the corresponding identifier of the target image, given the textual query input. By memorizing images in MLLMs, we introduce a new paradigm to cross-modal retrieval, distinct from previous discriminative approaches. The experiments demonstrate that the generative paradigm performs effectively and efficiently even with large-scale image candidate sets.",Real cases from GPT4 illustrate the necessity of visual outputs for LLMs.,"Recently, we have witnessed the explosive development of generative large language models (LLMs), such as GPT series and LLaMA. Undergone extensive pretraining on document corpora and instruction tuning, these language models have demonstrated an impressive ability to memorize a lot of knowledge in their parameters and effectively recall them to answer users' instructions and queries. As shown in Figure~, GPT4{{https://openai.com/gpt-4}.} could directly respond to the user's question, ``Who is Sheldon Cooper?'', without any external document or database. Building upon the advancements of LLMs, multimodal LLMs (MLLMs) have been developed to expand the capabilities beyond text and allow users to express their needs using visual input. Despite the impressive capabilities of LLMs and MLLMs, their responses are limited to textual outputs. For instance, a user might ask, ``What does Sheldon Cooper look like?'' as shown in Figure~. While the MLLM tries to describe the person's appearance, it is often said that ``an image is worth a thousand words.'' It would greatly enhance the response capabilities of MLLMs if they could give visual outputs, like a photograph in this case. Despite the impressive capabilities of LLMs and MLLMs, their responses are limited to textual outputs. This limitation becomes apparent in scenarios where visual representation is more impactful. For instance, a user might ask, ``What does Sheldon Cooper look like?'' as shown in Figure~ (b). While the MLLM tries to describe the person's appearance, it is often said that ``an image is worth a thousand words.'' The response with a single photograph of ``Sheldon Cooper'' speaks more than many words. Therefore, it would greatly enhance the response capabilities of MLLMs if they could give visual outputs in addition to textual responses. A straightforward solution is to enhance MLLMs with external image synthesis tools, like diffusion models and Generative Adversarial Networks, for visual output capabilities. However, a significant challenge with these modules is their propensity to produce unrealistic or hallucinatory images, which cannot accurately describe real-world images, such as a photograph of ``Sheldon Cooper''. The integration of an image retrieval module seems a more viable solution. Nonetheless, such a combination often encounters a transition gap between two independent modules. Considering the massive benefits of LLMs in memorizing textual knowledge, a bold and innovative idea emerges: Is it possible to equip MLLMs with the ability to memorize visual information within their parameters for retrieval and beyond? In this light, we formulate a generative cross-modal retrieval task: given a user query for visual content, MLLMs are expected to recall desired images from their parameters directly as the response. Accomplishing this task poses a significant challenge, necessitating the presence of two essential abilities of MLLMs: 1) Visual memory. As the prerequisite requirement, the MLLM model must possess the capability to memorize visual information within its parameters. This goes beyond simply encoding images into dense vectors within a vector database. It necessitates a distinct, differentiable, and integrated visual memory scheme within MLLMs' parameters. 2) Visual recall. Given a textual query, the MLLM should be able to recall the relevant visual information from the complicated visual memory bank. Above this, for user comprehension, the activated visual information must be grounded to the complete and original images rather than mere patches or fragmented visuals. In this work, we propose a novel GeneRAtive Cross-modal rEtrieval framework, GRACE, to overcome the above issues. GRACE assigns images unique identifiers, where each identifier is a distinct string representing an image. Based on the identifiers, GRACE comprises two training steps, as illustrated in Figure~. 1) Learning to memorize. Given an image, the MLLM is trained to generate the corresponding identifier string via the standard text generation loss. The goal of this phase is for the MLLM to effectively learn and memorize the associations between the visual content of images and their respective identifiers. 2) Learning to retrieve. The MLLM is trained to generate the identifier string of the relevant image while given a textual query. In this way, the MLLM learns to associate user queries with visual memory. After the two training steps above, GRACE enables generative cross-modal retrieval: given a textual query, the MLLM generates an identifier string corresponding to a real image. We delve into GRACE from various perspectives, including different identifier types, effectiveness, and efficiency of the generative paradigm. We evaluate GRACE on text-image matching datasets to verify the feasibility of generative cross-modal retrieval. Without any image's visual information during inference, GRACE performs comparably to the advance one-tower approaches (e.g., CLIP) and demonstrates higher efficiency with large-scale image sizes. It is acknowledged that as a new retrieval paradigm, GRACE still lags behind one-tower approaches. One-tower approaches are only applicable to ranking stage due to their low efficiency, while GRACE and CILP are specifically designed for the retrieval stage. By comprehensive analysis, we hope to comprehensively understand its capabilities and limitations. We believe exploring generative cross-modal retrieval holds great significance. {itemize} Benefiting from inbuilt visual memory within MLLMs, GRACE introduces a new paradigm to cross-modal retrieval. GRACE transforms the original matching problem into a generation problem, eliminating the need for negative samples during training and retrieval index during inference. No matter the size of the image set, the retrieval efficiency remains constant. This new cross-modal retrieval paradigm leaves much room for investigation. Inbuilt visual memory serves for retrieval, yet its utility extends beyond mere retrieval. In Section~, we demonstrate that the MLLM could describe the memorized image and even answer questions about the memorized images, just like humans do. This opens up the possibility of injecting personalized visual experiences of humans into MLLMs for them to memorize and understand an individual's journey, and accomplish more visual tasks. {itemize"
Self-Training with Pseudo-Label Scorer for Aspect Sentiment Quad Prediction,2406.18078v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.18078v1_0.pdf,"Aspect Sentiment Quad Prediction (ASQP) aims to predict all quads (aspect term, aspect category, opinion term, sentiment polarity) for a given review, which is the most representative and challenging task in aspect-based sentiment analysis. A key challenge in the ASQP task is the scarcity of labeled data, which limits the performance of existing methods. To tackle this issue, we propose a self-training framework with a pseudo-label scorer, wherein a scorer assesses the match between reviews and their pseudo-labels, aiming to filter out mismatches and thereby enhance the effectiveness of self-training. We highlight two critical aspects to ensure the scorer's effectiveness and reliability: the quality of the training dataset and its model architecture. To this end, we create a human-annotated comparison dataset and train a generative model on it using ranking-based objectives. Extensive experiments on public ASQP datasets reveal that using our scorer can greatly and consistently improve the effectiveness of self-training. Moreover, we explore the possibility of replacing humans with large language models for comparison dataset annotation, and experiments demonstrate its feasibility..}",Illustration of our pseudo-label scorer.,"Aspect-Based Sentiment Analysis (ABSA) aims to recognize aspect-level opinions and sentiments from user-generated content . This problem has consistently attracted interest owing to its proficiency in distilling and summarizing fine-grained opinions from vast data . The most representative and challenging task in ABSA is Aspect Sentiment Quad Prediction (ASQP) . This task formulates aspect-level opinions and sentiments as quadruples, each consisting of an aspect term, aspect category, opinion term, and sentiment polarity. For example, given a review ``{the food is great and reasonably priced},'' the output of ASQP would be \{({food}, food\_quality, {great}, positive), ({food}, food \_prices, {reasonably priced}, positive)\}. As a fine-grained problem, ABSA faces the challenge of insufficient labeled data, which is particularly severe in the ASQP task. This issue limits the performance of existing models. Many efforts explore data augmentation methods to alleviate this issue. They synthesize new samples by modifying existing ones , applying self-training techniques , or utilizing generative methods . However, a significant limitation of these methods is that the synthetic samples often inevitably contain mismatches between sentences and labels, which can adversely affect model learning. To reduce such mismatches, this paper introduces a pseudo-label scorer for data augmentation. As illustrated in Figure , the scorer assesses the degree of match between the review and its pseudo-label. If we have a sufficiently robust scorer, we can filter out all mismatched samples, thereby significantly enhancing the effectiveness of data augmentation. We propose that the effectiveness and reliability of this scorer hinge on two critical aspects: (1) the quality of the training dataset and (2) its architecture along with the training objective. We discuss these two aspects below. For the first aspect, previous works typically produce negative labels by modifying real labels using heuristic rules . However, such negative labels are usually simplistic and patterned, limiting the scorer's learning. To overcome this limitation, we create a human-annotated comparison dataset. Specifically, we train an ASQP model with existing labeled data, use it to infer several pseudo-labels for unlabeled data, and then have human annotators choose the most appropriate pseudo-labels. The labels chosen by annotators are designated as positive labels, while the rest as negative labels. Our dataset, in contrast to the rule-based datasets, is more challenging and better aligned with human judgment. For the second aspect, previous works formalize label-scoring as a question-answering problem or embed the discriminative matching token into the label . However, our findings suggest that these methods underperform in complex tasks like ASQP, due to their limited capacity to model the interactions between reviews and pseudo-labels. Recent works in preference optimization reveal that the language model itself can serve as a scorer . This motivates us to use the conditional likelihoods that a generative model assigns to a pseudo-label as the measure of its quality. Compared with the previous methods, this approach enables the scorer to examine the plausibility of a pseudo-label in a token-by-token fashion, thus offering a more comprehensive and effective scoring. We then fine-tune this scorer on our comparison dataset using ranking-based objectives. Upon developing this pseudo-label scorer, we apply it in a data augmentation framework, specifically opting for the self-training framework due to its simplicity. We conduct extensive experiments on public ASQP datasets to examine its effectiveness and further investigate the following questions: (1) how does the pseudo-label scorer perform using our comparison data and model architecture?; (2) is it feasible to replace humans with large language models to annotate the comparison data?; and (3) how to utilize the scorer to filter out low-quality samples? Furthermore, inspired by , we extend the application of this scorer, employing it as a reranker for multiple candidate labels, and assess its impact and effectiveness. Our contributions are summarized as follows: {enumerate} [(1)] To the best of our knowledge, we are the first to apply a pseudo-label scorer to data augmentation in the ASQP task. [(2)] We investigate how to enhance the scorer's effectiveness and reliability from both dataset and model architecture perspectives. [(3)] We empirically demonstrate that the proposed pseudo-label scorer can significantly and consistently enhance the performance of existing models. {enumerate"
Self-Training with Direct Preference Optimization Improves Chain-of-Thought Reasoning,2407.18248v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2407.18248v1_0.pdf,"Effective training of language models (LMs) for mathematical reasoning tasks demands high-quality supervised fine-tuning data. Besides obtaining annotations from human experts, a common alternative is sampling from larger and more powerful LMs. However, this knowledge distillation approach can be costly and unstable, particularly when relying on closed-source, proprietary LMs like GPT-4~, whose behaviors are often unpredictable. In this work, we demonstrate that the reasoning abilities of small-scale LMs can be enhanced through self-training, a process where models learn from their own outputs. We also show that the conventional self-training can be further augmented by a preference learning algorithm called Direct Preference Optimization (DPO)~. By integrating DPO into self-training, we leverage preference data to guide LMs towards more accurate and diverse chain-of-thought reasoning. We evaluate our method across various mathematical reasoning tasks using different base models. Our experiments show that this approach not only improves LMs' reasoning performance but also offers a more cost-effective and scalable solution compared to relying on large proprietary LMs.","Our approach demonstrates superior performance on the GSM8K benchmark while minimizing the required compute cost, including both training and inference. Compute cost calculations are based on the methodology outlined by~\citet{yuan2023scaling}.\protect\footnotemark","Making language models (LMs) perform mathematical reasoning is a valuable, yet challenging research objective. Recent efforts have focused on enhancing large-scale LMs' reasoning abilities through various methods, including chain-of-thought prompting, continual pretraining, and adding external verifiersq. However, the research question of how to enhance the reasoning capabilities of smaller-sized LMs remains relatively under-explored. Recent studies demonstrate that the reasoning capabilities of smaller LMs can be significantly enhanced through learning from the outputs of larger and more advanced LMs, such as Codex, PaLM, and GPT-4. While this method is straightforward to implement, the associated costs can be substantial. The computational demand, measured in { floating-point operations} (FLOPs), increases considerably when using large LMs. Additionally, the reliance on proprietary large LMs for data annotation not only incurs high economic costs but also raises concerns regarding the sustainability and scalability of such practices. For instance, highlighted that while employing large LMs as annotators can largely enhance the performance of smaller LMs, it introduces a clear trade-off between economic costs and performance gains. { All methods presented here are integrated with an external calculator except for the Codex distillation by. } Another line of research focuses on exploring enhancements through self-improvement methods. These methods diverge from using outputs from larger models, instead encouraging LMs to learn from their own generated data. The effectiveness of these techniques is evident, yet their success largely depends upon the inherent capabilities of the base models. For example, initiated self-improvement by few-shot prompting GPT-J, a relatively large LM which has 6 billion parameters, to generate rationales -- an emergent ability typically reserved for large models. However, the extent to which small-scale LMs can gain from self-improvement remains uncertain. In this work, we introduce a novel enhancement to the conventional self-training framework by incorporating Direct Preference Optimization (DPO). This integration specifically targets performance objectives within chain-of-thought reasoning, with a particular focus on mathematical reasoning. The clear-cut nature of mathematical solutions enables straightforward validation of a model's outputs, facilitating the creation of a preference dataset for DPO. Our empirical results indicate that this method notably enhances the reasoning capabilities of LMs while also reducing computational overhead. We visualize the relationship between the GSM8K performance and computational cost across various specialized models in Figure~. It can be observed that our method not only achieves strong performance, but also reduces computational demands by effectively utilizing self-generated data for learning. Overall, the main contribution of this work can be summarized as follows: {itemize}[topsep=5pt, partopsep=0pt, leftmargin=15pt, parsep=0pt, itemsep=10pt] We propose a novel extension to the classic self-training framework by integrating Direct Preference Optimization, demonstrating its effectiveness across various math reasoning tasks. Our method significantly enhances the reasoning abilities of language models while requiring minimal computational resources, optimizing both performance and efficiency. We present an efficient method for integrating LMs with external tools, which significantly boosts downstream task performance without notably compromising inference speed. {itemize"
Document-level Claim Extraction and Decontextualisation for Fact-Checking,2406.03239v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.03239v2_0.pdf,"Selecting which claims to check is a time-consuming task for human fact-checkers, especially from documents consisting of multiple sentences and containing multiple claims. However, existing claim extraction approaches focus more on identifying and extracting claims from individual sentences, identifying whether a sentence contains a claim or the exact boundaries of the claim within a sentence. In this paper, we propose a method for {document-level} claim extraction for fact-checking, which aims to extract check-worthy claims from documents and decontextualise them so that they can be understood out of context. Specifically, we first recast claim extraction as extractive summarization in order to identify central sentences from documents, then rewrite them to include necessary context from the originating document through sentence decontextualisation. Evaluation with both automatic metrics and a fact-checking professional shows that our method is able to extract check-worthy claims from documents more accurately than previous work, while also improving evidence retrieval.","An example of document-level claim extraction. Document\protect\footnotemark[1] is a piece of news from CNBC. Gold Claim\protect\footnotemark[2] is annotated by the fact-checking organization, Misbar. Sentences in orange denote check-worthy claims extracted by sentence-level CE (Claimbuster). Sentences in blue denote salient claims extracted by our document-level CE. The claim in green is a decontextualised claim derived from the 4th sentence obtained by our document-level CE.","Human fact-checkers typically select a claim in the beginning of their day to work on for the rest of it. Claim extraction (CE) is an important part of their work, as the overwhelming volume of claims in circulation means the choice of {what} to fact-check greatly affects the fact-checkers' impact. Automated approaches to this task have been proposed to assist them in selecting check-worthy claims, claims that the public has an interest in knowing the truth. Existing CE methods mainly focus on detecting whether a sentence contains a claim or the boundaries of the claim within a sentence. In real-world scenarios though, claims often need to be extracted from documents consisting of multiple sentences and containing multiple claims, not all of which are relevant to the central idea of the document, and verifying all claims manually or even automatically would be inefficient. Moving from sentence-level CE to document-level CE is challenging; we illustrate this with the example in Figure~. Sentences in orange are claims selected by a popular sentence-level CE method, Claimbuster, that are worth checking in principle but do not always relate to the central idea of the document, and multiple sentences with similar claims are selected, which would not all need to be fact-checked ( sentences 1 and 6). Claims extracted for fact-checking are expected to be unambiguous, which means that they cannot be misinterpreted or misunderstood when they are considered outside the context of the document they were extracted from, consequently allowing them to be fact-checked more easily. Figure~ shows an example of claim decontextualisation, where the claim ``{Bird is scrapping thousands of e-scooters in the Middle East ...... }'' requires coreference resolution to be understood out of context, ``{Bird}'' refers to ``{California scooter sharing start-up Bird}''. However, existing CE methods primarily focus on extracting sentence-level claims ( extracting sentences that contain a claim) from the original document and ignore their decontextualisation, resulting in claims that are not unambiguously understood and verified. To address these issues, we propose a novel method for {document-level} claim extraction and decontexualisation for fact-checking, aiming to extract salient check-worthy claims from documents that can be understood outside the context of the document. Specifically, assuming that salient claims are derived from central sentences, $i)$ we recast the document-level CE task into the extractive summarization task to extract central sentences and reduce redundancy; $ii)$ we decontextualise central sentences to be understandable out of context by enriching them with the necessary context; $iii)$ we introduce a QA-based framework to obtain the necessary context by resolving ambiguous information units in the extracted sentence. [1]{{https://www.cnbc.com/2020/06/03/bird-circ-scooters-middle-east.html}} [2]{{https://misbar.com/en/factcheck/2020/06/18/are-bird-e-scooters-leaving-the-middle-east}} [3]{{https://github.com/Tswings/AVeriTeC-DCE}} To evaluate our method we derive a CE dataset[3] containing decontextualised claims from AVeriTeC , a recently proposed benchmark for real-world claim extraction and verification. Our method achieves a Precision@1 score of 47.8 on identifying central sentences, a 10$\ outperforming all baselines. When evaluated for evidence retrieval potential, the decontextualised claims obtained by enriching original sentences with the necessary context, are better than the original claim sentences, with an average 1.08 improvement in precision"
LLMs Learn Task Heuristics from Demonstrations: A Heuristic-Driven Prompting Strategy for Document-Level Event Argument Extraction,2311.06555v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2311.06555v3_0.png,"In this study, we explore in-context learning (ICL) in document-level event argument extraction (EAE) to alleviate the dependency on large-scale labeled data for this task. We introduce the Heuristic-Driven Link-of-Analogy (HD-LoA) prompting tailored for the EAE task. Specifically, we hypothesize and validate that LLMs learn task-specific heuristics from demonstrations in ICL. Building upon this hypothesis, we introduce an explicit heuristic-driven demonstration construction approach, which transforms the haphazard example selection process into a systematic method that emphasizes task heuristics. Additionally, inspired by the analogical reasoning of human, we propose the link-of-analogy prompting, which enables LLMs to process new situations by drawing analogies to known situations, enhancing their performance on unseen classes beyond limited ICL examples. Experiments show that our method outperforms existing prompting methods and few-shot supervised learning methods on document-level EAE datasets. Additionally, the HD-LoA prompting shows effectiveness in other tasks like sentiment analysis and natural language inference, demonstrating its broad adaptability {https://github.com/hzzhou01/HD-LoA-Prompting}}.",CoT's step-by-step reasoning degrades to a single step for non-reasoning tasks. Reasoning steps of reasoning tasks (in orange) and non-reasoning tasks (in blue) are compared. Different colors indicate distinct reasoning steps. Prompts are from \citep{shum-etal-2023-automatic}.,"Document-level Event Argument Extraction (EAE) aims to transform unstructured event information from documents into structured formats encapsulating event arguments, facilitating their interpretation and application in various domains . The prevalent approach for this task relies on the collection of labeled data and the subsequent model training via supervised learning . While effective, this approach comes with the significant drawback: it necessitates a substantial amount of training data, which is particularly burdensome and costly given the complexity inherent to document-level EAE. In this context, in-context learning (ICL) , an emergent ability of large language models (LLMs), offers a promising alternative to supervised learning. ICL alleviates the need for large-scale data as it only uses a few examples as input-output pairs of the prompt to guide LLMs in performing the task on an unseen example. \\ However, applying ICL to document-level EAE presents numerous challenges. The ICL performance is highly sensitive to the design of in-context demonstrations, such as the selection of examples and the formatting of reasoning steps . Consequently, several crucial challenges emerge concerning the prompting strategy:\\ {1) Example Selection Challenge.} Selecting optimal in-context examples for ICL is pivotal, yet the understanding of what LLMs learn from these examples remains largely under-explored . This gap leads to a lack of systematic guidelines, resulting in a disorganized and inefficient example selection process.\\ {2) Context Length Limit}. In document-level EAE, selecting multiple documents as ICL examples could significantly extend the context length, potentially surpassing the token limit of LLMs.\\ {3) Abundance of Event Types.} The EAE task can involve more than a hundred distinct event types and argument roles. Yet, ICL examples can only capture a narrow subset, leaving the majority of argument roles unseen. Handling unseen classes beyond limited ICL examples is a common problem in classification tasks with diverse class types.\\ {4) Prompting Strategy for Non-reasoning Task.} While the chain-of-thought (CoT) prompting is extensively used across a variety of tasks, its effectiveness is compromised in non-reasoning scenarios. As shown in Figure , applying CoT to non-reasoning tasks will degrade its step-by-step reasoning into an potentially inadequate single-step. Consequently, there is a need for prompting strategy tailored for non-reasoning tasks. In this work, we put forward a novel hypothesis that LLMs learn task-specific heuristics from examples and validate it through experiments. Building upon this hypothesis, we propose heuristic-driven link-of-analogy prompting to address the aforementioned challenges. To elaborate:\\ {We propose and empirically validate the hypothesis that LLMs learn task specific heuristics from examples in ICL}. Heuristics, defined as {'a high-level rule or strategy for inferring answers to a specific task'}, play a crucial role in human cognition. Humans use heuristics as efficient cognitive pathways, which often lead to more accurate inferences than complex methods . Similarly, in supervised machine learning (ML) systems, models also learn task-specific patterns through training . Drawing a parallel, we hypothesize that LLMs learn task-specific heuristics from explanations of in-context examples to aid inference. We qualitatively illustrate how heuristics are {implicitly} embedded in explanations of in-context examples in Figure , and quantitatively validates our hypothesis with experiments detailed in Section . \\ Notably, while drawing parallels to supervised ML, ICL is fundamentally different from supervised ML in mechanism: supervised ML learns and updates model parameters during training, whereas LLMs do ICL with all parameters frozen. Therefore, understandings of supervised ML systems (e.g. pattern learning) are not applicable for ICL , which necessitates distinct explorations on the mechanism of ICL. \\ {We propose a heuristic-driven demonstration construction method.} Based on our hypothesis, task heuristics are crucial for the ICL performance of LLMs, yet they are often {implicitly} conveyed through examples. This implicitness complicates the examination of whether demonstrations contain diverse heuristics and leads to uncertainty about whether LLMs have recognized these heuristics. Furthermore, the selection of in-context examples remains an underexplored challenge for ICL. To address these issues, in parallel with human's exploitation of explicit heuristics, our method {explicitly} incorporates task heuristics into demonstrations, transforming the haphazard example selection process into a systematic method that emphasizes task heuristics. \\ {We propose the link-of-analogy prompting method that is suitable for non-reasoning tasks.} To address the aforementioned challenges of abundance of event types in EAE and the limitations of CoT prompting on non-reasoning tasks, we present the link-of-analogy prompting. Inspired by the analogical reasoning––a core mechanism of human cognition, this approach enables LLMs process new situations (new classes) through drawing an analogy to known situations (known classes). Empirical results demonstrate its effectiveness in enhancing the ICL performance for classes not seen in ICL examples.\\ Our contributions are as follows: {itemize}[leftmargin=10pt] We introduce a pioneering work to prompting strategies for the document-level EAE, showcasing significant accuracy improvements on two document-level EAE datasets compared to prompting methods and few-shot supervised learning methods. We investigate what LLMs learn from ICL, and unveil a new insight that LLMs learn task-specific heuristics from ICL examples. We propose an heuristic-driven demonstration construction approach, tackling the example selection issue with a fresh perspective on task heuristics, facilitating explicit heuristic learning in ICL. Furthermore, we propose the link-of-analogy prompting, which allows LLMs to process new situations by drawing analogies to known situations. To further evaluate the adaptability of our method, we validate it on the sentiment analysis and natural language inference tasks, achieving notable accuracy enhancements. {itemize"
mCoT: Multilingual Instruction Tuning for Reasoning Consistency in Language Models,2406.02301v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.02301v2_0.pdf,"Large language models (LLMs) with Chain-of-thought (CoT) have recently emerged as a powerful technique for eliciting reasoning to improve various downstream tasks. As most research mainly focuses on English, with few explorations in a multilingual context, the question of how reliable this reasoning capability is in different languages is still open. To address it directly, we study multilingual reasoning consistency across multiple languages, using popular open-source LLMs. First, we compile the first large-scale multilingual math reasoning dataset, , covering eleven diverse languages. Then, we introduce multilingual CoT instruction tuning to boost reasoning capability across languages, thereby improving model consistency. While existing LLMs show substantial variation across the languages we consider, and especially low performance for lesser resourced languages, our 7B parameter model achieves impressive consistency across languages, and superior or comparable performance to close- and open-source models even of much larger sizes.","Overview of multilingual reasoning; LLMs are expected to have consistent reasoning capabilities across different languages when given the same problem which has the same answer. Shown in picture are three example languages: English (EN), Swahili (SW), and Chinese (ZH). For EN, we show the problem formulation, and the Chain-of-Thought (CoT) reasoning.","Recent progress on language models shows that they can achieve surprising performance on complex reasoning tasks in natural language processing (NLP), such as symbolic reasoning, math word problem, and commonsense reasoning. Most of the research focuses on prompting large language models (LLMs), where the LLMs are conditioned on a few examples or instructions describing the target task. While most previous works focus on reasoning with LLMs in English, recently have extended it to a multilingual setting leveraging a few-shot prompting approach. However, performance for lesser resourced languages still lags behind, in a similar way as generalization of factual knowledge has been shown to vary widely across languages, mainly due to the fact that most languages are not well represented in LLMs. The work we present here has the twofold aim of (i) better understanding and evaluating the general reasoning capabilities of LLMs beyond just English, and (ii) providing lesser resourced languages with capable but manageable models which can be used for reasoning tasks. To this end, we propose to measure reasoning consistency across multiple languages. As shown in Figure~, LLMs are expected to produce logically similar reasoning solutions and consistent final results for inputs which are semantically equivalent but expressed in different languages. Based on our findings, we aim to enhance multilingual reasoning abilities through instruction tuning, yielding a model that can solve reasoning tasks in various languages, with similar reasoning capabilities across those languages. Specifically, we focus on math word problems and empirically investigate the reasoning consistency of current open-source state-of-the-art LLMs across multiple languages. The model's reasoning consistency is evaluated toward the final answer (the final results should be the same across languages). On the basis of preliminary results showing substantial reasoning gaps between different languages, we propose a multilingual Chain-of-Thought reasoning (mCoT) framework using instruction tuning, which aims to boost reasoning capability across languages, thereby improving consistency. So, first we construct the multilingual instruction training dataset (mCoT-MATH) by automatically translating English data into multiple languages, and then we use it for LLM finetuning. In summary, our contributions are:{Data, code, and model are available at {https://github.com/laihuiyuan/mcot}.} {itemize}[itemsep=1.2pt, topsep=1.2pt, parsep=1.2pt] We propose to study reasoning consistency of LLMs across different languages, providing insights into (the evaluation of) this ability of LLMs. We compile and distribute mCoT-MATH, the first large-scale multilingual math CoT reasoning dataset containing around 6.3 million samples for 11 diverse languages. Based on mCoT-MATH, we train and make available a 7B parameter model mCoT for multilingual math reasoning, which achieves impressive consistency across languages, and superior or comparable performance to close- and open-source models. {itemize"
Beyond Traditional Benchmarks: Analyzing Behaviors of Open LLMs on Data-to-Text Generation,2401.10186v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2401.10186v3_0.pdf,"We analyze the behaviors of open large language models (LLMs) on the task of data-to-text (D2T) generation, i.e., generating coherent and relevant text from structured data. To avoid the issue of LLM training data contamination with standard benchmarks, we design -- a tool for collecting novel structured data records from public APIs. We find that open LLMs (Llama 2, Mistral, and Zephyr) can generate fluent and coherent texts in zero-shot settings from data in common formats collected with . However, we show that the semantic accuracy of the outputs is a major issue: both according to human annotators and our reference-free metric based on GPT-4, more than 80\% of the outputs of open LLMs contain at least one semantic error. We publicly release the code, data, and model outputs.}","To benchmark LLMs, we download unlabeled structured data from public APIs and prompt LLMs to generate texts based on the data. We annotate semantic errors in the outputs using reference-free metrics.","Large language models (LLMs; ) have already left a mark in many areas of natural language processing (NLP). Surprisingly, their applicability to the task of data-to-text (D2T) generation remains underexplored, with limited evaluation on a handful of well-established benchmarks only . Generating text from structured data is arguably challenging for LLMs, given the specifics of D2T generation, such as long inputs, complex non-linear structure, and strict requirements on semantic accuracy. However, a more significant issue is the lack of testing grounds. The current D2T generation benchmarks are not only getting saturated , but also promote optimization towards traditional reference-based evaluation metrics, which were shown to correlate poorly with human judgment . When it comes to the models, using closed LLMs is increasingly considered a bad research practice due to its non-reproducibility . On top of that, contamination of LLM training data with standard benchmarks further restricts the space for experiments . In this paper, we propose an approach that allows us to analyze model behavior in D2T generation on novel, real-world structured data records with reference-free evaluation metrics. We begin by realizing that {unlabeled data are plentiful}. To leverage the data for our experiments, we introduce {{Q}uintet of {U}nlabeled {I}nputs for {N}atural {T}asks in {D}ata-to-text, pronounced as ``quintet''} -- a tool for collecting structured data from five domains in standard formats: JSON, CSV, and Markdown. We choose the domains so that the data can be directly used as input for five distinct D2T generation tasks. Our tasks include generating weather forecasts, sports reports, product descriptions, chart captions, and entity descriptions (see {tab:data}). Next, we collect a set of 1,000 inputs with and use the inputs as an ad-hoc benchmark (called ) for testing the abilities of LLMs for D2T generation. We assume that the data formats in are common in the LLMs' pretraining corpora, so we specify the task using instructions instead of standard finetuning with human-written outputs, capitalizing on the zero-shot abilities of instruction-tuned LLMs (§). We push towards better reproducibility by {focusing on open LLMs}, which -- apart from being more accessible -- also achieve increasingly better results across tasks . For our experiments, we use three open LLMs with 7B parameters: Llama 2 , Mistral , and Zephyr . We also use GPT-3.5 as a closed model baseline for the final experiments. Given the behavioral nature of the experiments with LLMs , we put emphasis on reporting model behavior throughout the process (§). Another piece of the puzzle is {reference-free evaluation}: using the input data as a ground for comparison instead of reference outputs (§). We focus on identifying semantic errors in the model outputs, i.e., the information that is not supported by the input data. We use two separate evaluation methods: manual annotations from human crowdworkers and a custom automatic metric based on GPT-4 . We annotate the errors on the level of individual words, getting fine-grained annotations of error spans in several categories . Based on our results, we provide general recommendations for D2T generation with open LLMs across tasks and formats (§). Our main findings are as follows: {itemize} {Open LLMs can generate fluent outputs from structured data} in common formats under zero-shot settings. {Semantic accuracy is a major obstacle}: both human annotators and GPT-4-based metric report that over 80\ {Long data inputs cause practical issues}, including the need for long-context models, increased GPU memory requirements, and unavailability of few-shot approaches. {Outputs can be empirically improved by following several rules-of-thumb} for preprocessing the model input, such as including units, removing unnecessary fields, or prefixing the model answer. {itemize"
One Prompt To Rule Them All: LLMs for Opinion Summary Evaluation,2402.11683v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.11683v2_0.png,"Evaluation of opinion summaries using conventional reference-based metrics often fails to provide a comprehensive assessment and exhibits limited correlation with human judgments. While Large Language Models (LLMs) have shown promise as reference-free metrics for NLG evaluation, their potential remains unexplored for opinion summary evaluation. Furthermore, the absence of sufficient opinion summary evaluation datasets hinders progress in this area. In response, we introduce the dataset, encompassing $7$ dimensions crucial to the evaluation of opinion summaries: , , , , , , and . We propose , a dimension-independent prompt, along with , a dimension-dependent set of prompts for opinion summary evaluation. Our experiments demonstrate that emerges as a good alternative for evaluating opinion summaries, achieving an average Spearman correlation of $$ with human judgments, surpassing prior methodologies. Remarkably, we are the first to explore the efficacy of LLMs as evaluators, both on closed-source and open-source models, in the opinion summary evaluation domain.","{\textsc{G-Eval} vs. \textsc{Op-I-Prompt}}. On closed-source model (\chatgpt{3.5}) our \textsc{Op-I-Prompt} shows comparable performance whereas on open-source model (\mistral{7}) our approach outperforms \textsc{G-Eval} on $7$ dimensions: \fl \:(FA), \coh \:(CO), \rel \:(RE), \fa \:(FA), \asp \:(AC), \sent \:(SC), and \spec \:(SP). Check Figure \ref{fig:generation_runs} for more details.","Opinion summarization systems predominantly use traditional metrics such as {Rouge} and {BertScore} for automatic evaluation, however, they have been shown to have poor correlations with human judgments . Moreover, these metrics fall short of comprehensively evaluating opinion summaries. Additionally, obtaining reference-based datasets at a large scale is an expensive process. Recently, Large Language Models (LLMs) have been utilized as reference-free evaluators for Natural Language Generation (NLG) outputs . The idea is to prompt a powerful LLM such as {3.5}/{4} to evaluate an output on certain criteria. However, their suitability has not been explored at all for evaluating opinion summaries. Moreover, these approaches have been tested only on closed-source models ({3.5}/{4}) primarily because of the limitations of the open-source models in following instructions and producing the desired output . To this end, we first create {SummEval-Op}, a reference-free opinion summarization dataset covering $7$ dimensions, for the e-commerce domain. Next, we present {Op-I-Prompt} and {Op-Prompts} tailored for opinion summary evaluation. We investigate their suitability to both closed-source and open-source models. Experiments reveal that {Op-I-Prompt} emerges as a good alternative for evaluating opinion summaries across all $7$ dimensions. Our contributions are: {enumerate} {{SummEval-Op}}{{https://github.com/tjsiledar/SummEval-OP}}, an opinion summary evaluation benchmark dataset, consisting of a total of $2,912$ summary annotations, assessing $13$ opinion summaries for $32$ products from the Amazon test set. The evaluation covers ${7}$ {dimensions}- { fluency}, { coherence}, { relevance}, { faithfulness}, { aspect coverage}, { sentiment consistency}, and { specificity} related to the evaluation of opinion summaries (Section ). {{Op-I-Prompt}}, a dimension-independent prompt and {{Op-Prompts}}, a dimension-dependent set of prompts, enabling opinion summary evaluation for all the $7$ dimensions. Experiments indicate that the {Op-I-Prompt} generally outperforms existing approaches on both closed-source and open-source models by ${9\ Benchmarking of ${9}$ recent LLMs (closed and open-source) on the aforementioned $7$ dimensions for the task of opinion summarization, which to the {best of our knowledge} is first of its kind (Table , Section ). Detailed analysis, comparing an open-source LLM against a closed-source LLM acting as evaluators for automatic evaluation of opinion summaries on $7$ dimensions. Analysis indicates that {Op-I-Prompt} emerges as a good alternative for evaluating opinion summaries showing a high correlation (spearman correlation of ${0.70}$ on average) with humans when compared with alternatives (Section ). {enumerate"
AutoDSL: Automated domain-specific language design for structural representation of procedures with constraints,2406.12324v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.12324v1_0.pdf,"Accurate representation of procedures in restricted scenarios, such as non-standardized scientific experiments, requires precise depiction of constraints. Unfortunately, , as an effective tool to express constraints structurally, often requires case-by-case hand-crafting, necessitating customized, labor-intensive efforts. To overcome this challenge, we introduce the framework to automate -based constraint design across various domains. Utilizing domain specified experimental protocol corpora, optimizes syntactic constraints and abstracts semantic constraints. Quantitative and qualitative analyses of the designed by across five distinct domains highlight its potential as an auxiliary module for language models, aiming to improve procedural planning and execution.","{Representative constraints in protocols.} {(A)} Parameter omission: This refers to the absence of essential parameter values within a predefined set, \eg, the lack of temperature specification during the denaturation step in Protein Gel Electrophoresis. {(B)} Parameter under-specification: This occurs when a quantitative parameter is described using qualitative terms, leading to ambiguity, \eg, unclear mixture configurations in DNA Extraction. {(C)} Action undefinition: This involves the description of procedural steps at a high level without grounding to the specific, executable actions required, \eg, the vague \texttt{change} operation in Cell Preparation. {(D)} Iterative control logic: Loops that operate iteratively to satisfy a final condition rather than straightforwardly, as seen in PCR Optimization. {(E)} Memory management: Drawing a parallel with computer memory mechanisms, laboratory procedures also face constraints on the availability of storage for intermediates, necessitating explicit reallocation of containers and devices, such as managing buffers in Protein Digestion. {(F)} Concurrent management: The synchronization of actions without dependencies to maximize time efficiency and resource utilization, \eg, reagent splitting in RNA Extraction.","Comprehending and executing procedures articulated in natural language to achieve a specified goal represents a fundamental challenge for {ai} systems. With the boost of {llm}, {ai} systems possess the capability of reasoning over and planning for procedural tasks intended for both human and robotic execution across a broad spectrum of everyday scenarios{Visit {www.wikiHow.com} for demonstrations.}, such as cooking according to a recipe, obviating the necessity for external representation of procedures beyond text. However, contexts significantly more restricted than everyday scenarios, such as conducting non-standardized experiments in scientific laboratories, need to follow specific {protocols}{Visit {www.nature.com/nprot/} for examples.}. A protocol delineates every aspect of an experiment's procedure to facilitate its reproduction, emphasizing the necessity for precision in every step, to ensure accurate execution by an experimenter. The complexity of procedures, limitation in resources, and susceptibility to error in such scenarios render any deviation from the established protocols inadmissible. Unfortunately, natural language inherently possesses ambiguities. Within protocols, description of actions can be semantically under-specified, and the logic of procedure may be non-linear, as demonstrated in {fig:Fig1_example}. Given these unique distinctions against daily procedures, accurate interpretation of protocols requires explicit depiction of constraints (see {fig:Fig1.5_program_examples} for example). Intuitively, structural representation imposes constraints on the processing of protocols. This is achieved through purely symbolic approaches that depict procedures as flow-graphs, and neuro-symbolic hybrid methods that superimpose procedural structures onto neural networks. Both strategies impose constraints on the interpretation of procedures, thereby reducing the incidence of superfluous operations. Symbolic constraints range from elementary grammars to general-purposed programming languages, with the capability of a constraint system being pivotal in refining a coarse interpretation space into a more precise one. In light of the context, what level of capability should we expect the constraint to possess? This discourse introduces {dsl}, a category of symbolic systems endowed with the most potent constraints. {dsl} are programming languages tailored to specific problem domains, encapsulating both {syntactic constraints} and {semantic constraints} inherent to those domains. For instance, BioCoder, developed by Microsoft, is a {dsl} explicitly designed to constrain experimental protocols. On the syntactic level, the variable management mechanism inherited from C/C++ enables {dsl} to monitor the lifecycle of each intermediate product, ensuring no omissions or duplications. On the semantic level, the precise definitions of actions, combinations of reagents' names and volumes, with subprocedures abstracted from domain-specific concepts, guaranteeing procedural execution consistency. Drawing inspiration from {dsl}, can we design constraints for protocols in {dsl} fashion? Hardly, due to the deterministic and substantial cost. Structural constraints necessitate custom design for particular domains, which is prohibitively expensive, given that these domains are highly specialized and often diverge significantly from the conventional purview of computer scientists. The development of a {dsl} necessitates a comprehensive integration of in-depth domain knowledge. Furthermore, the designed {dsl} must align with theoretical aspects of formal language design while also meeting the distinct requirements of the specialized domain. This necessitates a bidirectional alignment between computer scientists and domain experts, a process that is intrinsically case-by-case, implying that a {dsl} developed for one domain is unlikely to be applicable or easily adaptable to another, thus limiting the scalability of {dsl}-based constraints across various domains. In this study, our objective is to offer an initial proof-of-concept aimed at reducing the design cost of {dsl}-based constraints for protocols. We propose a scalable framework, termed \!\!, that facilitates the automated creation of {dsl}. The framework approaches the task as a bidirectional optimization problem, where the design of a {dsl} is abstracted from domain-specific corpora through a bottom-up process and concurrently derived by general programming language design principles in a top-down manner. This approach emulates the iterative dialogue between computer scientists and domain experts, progressively bridging the conceptual gap between their respective fields of expertise. The syntactic constraint should adequately define consecutive actions and their repetitions, interruption, concurrence, subcomponents, and reactants. Constructs of the semantics constraint need to accurately reflect the domain's concepts and the relations between them, without redundancy or incompleteness. We utilize protocols from various domains within the experimental sciences --- namely {Genetics}, {Medical and Clinical Research}, {Ecology and Environmental Research}, {Bioengineering}, and {Computational Biology} --- as the primary testing ground for our methodology, due to their inherent complexity, resource constraints, and susceptibility to errors. These domains exhibit significant disparities both in syntactic and semantic language features. Comprehensive experiments demonstrate that is capable of generalizing {dsl}-based constraints tailored to these diverse domains, upholding the integrity from both programming language design and domain expertise perspectives. We further demonstrate that syntactic and semantic constraints effectively work as an auxiliary module of {llm} in the processing of unseen protocols, thereby suggesting a promising future for constraining protocols through a synergistic blend of programs and natural language. The contributions of this work are threefold: (i) We introduce the framework for automated design of {dsl}-based constraints, which includes a bidirectional syntax optimization module and a non-parametric learning module for semantic reduction. (ii) We establish a systematic and end-to-end evaluation platform to assess the quality of the designed {dsl}-based constraints, employing both quantitative and qualitative metrics. (iii) We showcase the efficacy of {dsl}-based constraints in processing new coming protocols with syntactic complexity and semantic errors"
VIEScore: Towards Explainable Metrics for Conditional Image Synthesis Evaluation,2312.14867v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2312.14867v2_0.pdf,"In the rapidly advancing field of conditional image generation research, challenges such as limited explainability lie in effectively evaluating the performance and capabilities of various models. This paper introduces , a Visual Instruction-guided Explainable metric for evaluating any conditional image generation tasks. leverages general knowledge from Multimodal Large Language Models (MLLMs) as the backbone and does not require training or fine-tuning. We evaluate on seven prominent tasks in conditional image tasks and found: (1) (GPT4-o) achieves a high Spearman correlation of 0.4 with human evaluations, while the human-to-human correlation is 0.45. (2) (with open-source MLLM) is significantly weaker than GPT-4o and GPT-4v in evaluating synthetic images. (3) achieves a correlation on par with human ratings in the generation tasks but struggles in editing tasks. With these results, we believe shows its great potential to replace human judges in evaluating image synthesis tasks.",We study the correlation between MLLMs and human perspectives on rating images.,"Diffusion models have become a focal point in AI research for image synthesis. Over the past year, several new models have been introduced to enhance control over image generation. However, comprehensively evaluating AI-synthesized images remains a challenging and unresolved issue. While metrics like LPIPS , CLIP-Score , and DreamSim were proposed, they have certain limitations: (1) these metrics are agnostic the end task, which can fail to measure the desired aspects of the generated images, (2) the score is opaque with limited explainability. These limitations heavily restrict their effectiveness in assessing conditional image generation. Some research work relied on human-driven evaluation methods. While humans excel at understanding and interpreting visual content, such methods in the context are facing challenges such as scalability limits and preference subjectivity issues. This reliance on human judgment highlights the need for more uniform evaluation methods in the field. To solve the mentioned issues, we formulate the problem definition with our desired properties, as presented in equation . The function $f$ takes an instruction $I$, a synthesized image $O$, and $C^*$ which is a set of conditions (e.g. style, subject, background, canny-edge, etc). The score function should produce the intermediate rationale in the form of natural language before generating the final score according to the prompt instruction $I$: {equation} f_{VIE}(I, O, C^*) = ({rationale}, {score}) {equation} The function $f$ can be any Multimodal Large Language Model (MLLM) such as GPT-4 and LLaVA , which can take input images to generate human-like text responses. Unlike the automatic metrics, MLLM can receive human instructions and produce rationale. With such motivation, we introduce (Visual Instruction-guided Explainable Score), a framework to assess synthetic images in different conditional image generation tasks. has multiple advantages compared to auto-metrics and human evaluation. It includes: {Task Awareness.} Existing metrics were often designed to measure a certain aspect of generated images. For example, LPIPS measures the perceptual similarity of a pair of images, while CLIP-Score measures the text alignment of one single image. As a consequence, these metrics cannot be adapted to evaluate other tasks. acts as a silver bullet to tackle all conditional image generation evaluation processes due to its instruction-guiding property. It can be carefully adjusted with different instruction requirements. {Explainability.} The existing metrics normally output a single float-point score, which cannot offer detailed insights into the 'rationale' behind its evaluations. Such a score makes it difficult to interpret the decisions from the metric output. Instead, can offer the rationale in the form of natural languages to help humans understand the reasoning process. As depicted in Figure , the rationale can significantly improve the trustworthiness of . While the ultimate goal is to derive an MLLM that can rate images like humans, in this paper we also explore how well MLLMs can assess synthetic images compared to human evaluation and present insights and challenges on state-of-the-art MLLMs towards human evaluators, as shown in Figure"
Tree-of-Traversals: A Zero-Shot Reasoning Algorithm for Augmenting Black-box Language Models with Knowledge Graphs,2407.21358v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2407.21358v1_0.pdf;/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2407.21358v1_1.pdf;/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2407.21358v1_2.pdf;/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2407.21358v1_3.pdf;/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2407.21358v1_4.pdf;/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2407.21358v1_5.pdf,"Knowledge graphs (KGs) complement Large Language Models (LLMs) by providing reliable, structured, domain-specific, and up-to-date external knowledge. However, KGs and LLMs are often developed separately and must be integrated after training. We introduce Tree-of-Traversals, a novel zero-shot reasoning algorithm that enables augmentation of black-box LLMs with one or more KGs. The algorithm equips a LLM with actions for interfacing a KG and enables the LLM to perform tree search over possible thoughts and actions to find high confidence reasoning paths. We evaluate on two popular benchmark datasets. Our results show that significantly improves performance on question answering and KG question answering tasks. Code is available at","An example of how \tot uses a KG interface for the query, ``What actor played in both Inception and Interstellar?''.","Large language models (LLMs) are used for a range of knowledge-intensive tasks such as information retrieval , summarization , and question answering . Trained on large amounts of textual data, these models learn a wide breadth of information. However, LLMs suffer from several limitations -- they produce hallucinated information, lack deep domain-specific knowledge , and have a static knowledge cutoff when training ends. Knowledge graphs (KGs) naturally complement LLM weaknesses. KGs contain up-to-date information covering general and/or domain-specific topics in highly structured and interpretable format. Augmenting an LLM's ability to reason and respond in natural language with an external KG's up-to-date knowledge presents a path toward a reliable and factual LLM. The rise of powerful LLMs with new capabilities has renewed interest in combining LLMs with KGS. Numerous survey and position papers have recently emphasized their combined potential . Existing works augmented LLMs with KGs in multiple ways, such as integration into pretraining, fine-tuning, or later adaptation with subsequently trained components. All of these carry some limitations. In particular, training or fine-tuning a large scale LLMs is computationally expensive. In some cases, model weights are unavailable publicly. Finally, the largest KGs require their own servers and cannot be integrated in-memory with LLMs. Additionally, previous works do not consider the case of augmenting with multiple KGs. An algorithm that allows the augmentation of a powerful black-box LLM with any number of internal or external KGs without training from scratch or fine-tuning the model is valuable. Such a zero-shot algorithm would enable several innovative use cases such as (1) customers using a black-box LLM API in conjunction with an internal domain-specific KG, (2) integration of personalized KGs into an LLM without the risks associated with training a model with such personal data, (3) integration of deep domain knowledge via an array of API accessible KGs (e.g., IMDb{{https://aws.amazon.com/marketplace/seller-profile?id=0af153a3-339f-48c2-8b42-3b9fa26d3367}{IMDb API}}, MusicBrainz{{https://musicbrainz.org/doc/MusicBrainz_API}{MusicBrainz API}}). We introduce {Tree-of-Traversals}, a novel algorithm that addresses the above issues by allowing the augmentation of any powerful LLM with arbitrary number of KGs in a zero-shot fashion. It requires no training, functions with black-box access to the LLM, and works with any API accessible KG. Our contributions are: {enumerate} Tree of traversals: A novel zero-shot algorithm for augmenting any powerful LLM with arbitrary number of KGs and enabling advanced KG reasoning using tree search. Evaluation of on two question answering tasks: 2WikiMultiHop and QALD-10 and comparison with baselines. Development of a new dataset to test combined reasoning over a general and a domain-specific KG, and evaluation of on this dataset. {enumerate} We conduct detailed experiments on three models of varied sizes hosted on Amazon Bedrock and present detailed ablation studies"
Investigating Cultural Alignment of Large Language Models,2402.13231v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.13231v2_0.pdf,"The intricate relationship between language and culture has long been a subject of exploration within the realm of linguistic anthropology. Large Language Models (LLMs), promoted as repositories of collective human knowledge, raise a pivotal question: do these models genuinely encapsulate the diverse knowledge adopted by different cultures? Our study reveals that these models demonstrate greater cultural alignment along two dimensions—firstly, when prompted with the dominant language of a specific culture, and secondly, when pretrained with a refined mixture of languages employed by that culture. We quantify cultural alignment by simulating sociological surveys, comparing model responses to those of actual survey participants as references. Specifically, we replicate a survey conducted in various regions of Egypt and the United States through prompting LLMs with different pretraining data mixtures in both Arabic and English with the personas of the real respondents and the survey questions. Further analysis reveals that misalignment becomes more pronounced for underrepresented personas and for culturally sensitive topics, such as those probing social values. Finally, we introduce Anthropological Prompting, a novel method leveraging anthropological reasoning to enhance cultural alignment. Our study emphasizes the necessity for a more balanced multilingual pretraining dataset to better represent the diversity of human experience and the plurality of different cultures with many implications on the topic of cross-lingual transfer.}",Our framework for measuring the cultural alignment of LLM knowledge/output and ground-truth cultural data collected through survey responses.,"Large Language Models (LLMs) such as ChatGPT have garnered widespread utilization globally, engaging millions of users. Users interacting with these models across multiple languages have observed a noteworthy phenomenon: Prompting with different languages may elicit different responses to similar queries . From our observations, one reason for the difference between the responses is that they tend to reflect the culturally specific views commonly expressed by the people which use the same language as the prompt. Here, we hypothesize that the root cause of this phenomenon lies in the training data, which encodes different and at times conflicting ``knowledge'' across different languages.{In this work, we advocate for the term ``{Cultural Trends}'' instead of ``Biases.'' This choice is deliberate as the term ``bias'' outside mathematical context often carries a negative connotation---a problematic default position. The use of {Cultural Trends} emphasizes that a model reflecting a particular cultural inclination does not inherently imply danger or stereotyping. Instead, it signifies alignment with the views of a specific population, highlighting cultural significance.} Culture is a complicated term and defining it stands at the core of anthropological inquiry. Hundreds of definitions exist in literature which cover different aspects of interest . In this paper, we consider culture as as a multi-faceted inquiry that demonstrates substantial diversity among human communities, encompassing worldviews and belief systems. Through this lens, we aim to measure the {cultural alignment} of Large Language Models (LLMs) by simulating existing surveys that have been carried out by sociologists in specific populations. We utilize the responses from actual survey participants as our reference or gold standard. Then we measure the similarity between the model's answer when prompted with the participant's ``persona"" and the actual survey answer. The term ``persona'' in this context refers to an explicit description of a survey participant, encompassing various traits of interest such as social class, education level, and age (see Section for a detailed description). This is done for various LLMs trained and prompted under different configurations. We use this similarity as a proxy for the degree of a model's knowledge of a particular culture. This enables us to assess the LLMs' capacity to capture the diversity not only of a specific country but also among individuals within that country. We focus on a survey conducted in two countries: Egypt (EG) and the United States of America (US). It covers a diverse demographic set within each country with questions spanning various themes that include topics of social, cultural, material, governmental, ethical, and economic significance. This work primarily explores the impact of the language used for prompting and the language composition of pretraining data on a model's cultural alignment as defined above. We consider two languages for prompting: English and Arabic as they are the primary languages used in the surveys. Specifically, we consider four pretrained LLMs: {GPT-3.5}{GPT-3.5 is {gpt-3.5-turbo-1106} throughout this work.} also known as ChatGPT, and three $13$B parameter instruction-tuned models. The multilingual {mT0-XXL} is trained on a variety of languages, {LLaMA-2-13B-Chat} which is trained primarily on English data, and {AceGPT-13B-Chat} , a model finetuned from {LLaMA-2-13B-Chat} focusing on Arabic. Our contributions include highlighting the significant role of language in the perceived, functional cultural alignment in model responses, which is affected by both (1) the language in the pretraining data and (2) that of the prompt. Further analysis shows that (3) models capture the variance of certain demographics more than others, with the gap increasing for underrepresented groups. Finally, (4) we propose Anthropological Prompting as a method to enhance cultural alignment in LLMs"
RAID: A Shared Benchmark for Robust Evaluation of Machine-Generated Text Detectors,2405.07940v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2405.07940v2_0.pdf,"Many commercial and open-source models claim to detect machine-generated text with extremely high accuracy (99\% or more). However, very few of these detectors are evaluated on shared benchmark datasets and even when they are, the datasets used for evaluation are insufficiently challenging—lacking variations in sampling strategy, adversarial attacks, and open-source generative models. In this work we present RAID: the largest and most challenging benchmark dataset for machine-generated text detection. RAID includes over 6 million generations spanning 11 models, 8 domains, 11 adversarial attacks and 4 decoding strategies. Using RAID, we evaluate the out-of-domain and adversarial robustness of 8 open- and 4 closed-source detectors and find that current detectors are easily fooled by adversarial attacks, variations in sampling strategies, repetition penalties, and unseen generative models. We release our data} along with a leaderboard} to encourage future research.",Detectors for machine-generated text are often highly performant on default model settings but fail to detect more unusual settings such as using random sampling with a repetition penalty.,"Large Language Models (LLMs) have been able to fool humans into thinking their outputs are human-written for roughly four years . In that short span of time we have seen LLM-generated text be used for targeted phishing attacks , mass spam and harassment , disinformation campaigns , and spurious scientific publication . In order to document and eventually mitigate such harms, we must develop robust automatic detectors of machine-generated text. Many exciting and inventive methods have been proposed in recent years for detecting generated text . However, when evaluating these methods, authors typically generate their own evaluation datasets and fail to test their models on shared resources---making it difficult to verify claims of accuracy and robustness. This has led to an erosion of trust in the efficacy of automatic detection methods and a generally fatalistic sentiment towards detection among researchers and practitioners . To combat this trend, in this work, we introduce the Robust AI Detection (RAID) benchmark. RAID is the largest and most challenging benchmark of generated text ever released, consisting of 6M+ generations spanning 11 generators, 8 domains, 11 adversarial attacks, and 4 decoding strategies. Using RAID, we benchmark 12 detectors (8 open- and 4 closed-source). We find that detectors have difficulty generalizing to unseen models and domains and that simple changes such as changing the sampling strategy, adding a repetition penalty, and adversarially modifying text lead to marked decreases in performance"
"Silent Signals, Loud Impact: LLMs for Word-Sense Disambiguation of Coded Dog Whistles",2406.06840v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.06840v2_0.pdf,"*{-1pt} {{Warning: This paper contains content that may be upsetting or offensive to some readers.}} A dog whistle is a form of coded communication that carries a secondary meaning to specific audiences and is often weaponized for racial and socioeconomic discrimination. Dog whistling historically originated from United States politics, but in recent years has taken root in social media as a means of evading hate speech detection systems and maintaining plausible deniability. In this paper, we present an approach for word-sense disambiguation of dog whistles from standard speech using Large Language Models (LLMs), and leverage this technique to create a dataset of high-confidence coded examples of dog whistles used in formal and informal communication. {Silent Signals}} is the largest dataset of disambiguated dog whistle usage, created for applications in hate speech detection, neology, and political science.",This figure demonstrates the nuances of dog whistle detection as a word can be used in a {coded} or {non-coded} sense. {All illustrations were created using Adobe Firefly.},"quote} { ``{Ronald Reagan liked to tell stories of Cadillac-driving 'welfare queens' and 'strapping young bucks' buying T-bone steaks with food stamps. In flogging these tales about the perils of welfare run amok, Reagan always denied any racism and emphasized he never mentioned race.}'' \\{abc}--- {Ian Haney-Lopez} () } {quote} Dog whistles are coded language which, though seemingly innocuous to the general public, can communicate a covert harmful meaning to a specific in-group . Though this coded language appears in all kinds of speech, the idea of the `dog whistle' historically originates in politics . In the United States, political dog whistles gained popularity in the Civil Rights Era following the landmark Brown vs. Board of Education Supreme Court decision, as overt racism became less acceptable and politicians turned to coded language to maintain racial animus in political discussions while maintaining plausible deniability . Dog whistle use has fluctuated in the last six decades, but their use remains a consistent signal of a speaker's underlying prejudices, whether in the domain of American politics or otherwise. Their use has been shown to successfully provoke the underlying prejudice of target audiences, and wield racial anxiety to steer public opinion and policy . Improved understanding of dog whistles has applications in content moderation, computational social science, and political science. detecting and explaining coded discriminatory speech is a challenging task for NLP systems, as dog whistles famously evade toxicity and hate speech detection . This is because many dog whistle terms have standard vernacular meanings. Consider the example in Figure~ on the word ``soy,"" which in most contexts refers to a soybean product, but can also serve as a dog whistle to denigrate liberal or establishment Republican men for perceived feminine attributes, as in ``{That guy has soy face}.'' To study this language, prior work has focused on taxonomically organizing and archiving dog whistles with representative examples . However, dog whistles can also evolve over time in order to remain covert, a process which has only become more rapid in the age of the internet . This work presents a large dataset to track examples of dog whistles in their various forms, and help train language models to do the same. This resource can be used to (1) study how dog whistles emerge and evolve , (2) uncover ways to predict new dog whistle terms from knowledge of old ones, (3) study the prevalence of dog whistles in natural settings, and (4) improve hate speech and toxicity detection systems. As a preliminary step, this work employs LLMs for dog whistle word-sense disambiguation---a new task. We then apply these architectures to create {Silent Signals}, which is the largest dataset of coded dog whistle examples. It contains {formal} dog whistles from 1900-2023 Congressional records, and {informal} dog whistles from Reddit between 2008-2023. Silent Signals also contains vital contextual information for reliably decoding their hidden meanings and enabling future study of how Dog Whistles are used in discourse. Our contributions include: {itemize}0em The {Silent Signals} dataset of {} dog whistle examples. A novel task and verified method for dog whistle word-sense disambiguation. Experiments with GPT-3.5, GPT-4, Mixtral and Gemini on dog whistle detection. The {Potential Dog Whistle Instance} dataset with over 7 million records from informal and formal communication that contain dog whistle key words, and can be used for further scaling Silent Signals. {itemize"
Analyzing LLM Behavior in Dialogue Summarization: Unveiling Circumstantial Hallucination Trends,2406.03487v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.03487v1_0.pdf,"Recent advancements in large language models (LLMs) have considerably advanced the capabilities of summarization systems. However, they continue to face concerns about {hallucination}. While prior work has evaluated LLMs extensively in news domains, most evaluation of dialogue summarization has focused on BART-based models, leaving a gap in our understanding of their faithfulness. Our work benchmarks the faithfulness of LLMs for dialogue summarization, using human annotations and focusing on identifying and categorizing span-level inconsistencies. Specifically, we focus on two prominent LLMs: GPT-4 and Alpaca-13B. Our evaluation reveals subtleties as to what constitutes a hallucination: LLMs often generate plausible inferences, supported by circumstantial evidence in the conversation, that lack direct evidence, a pattern that is less prevalent in older models. We propose a refined taxonomy of errors, coining the category of ""Circumstantial Inference"" to bucket these LLM behaviors. Using our taxonomy, we compare the behavioral differences between LLMs and older fine-tuned models. Additionally, we systematically assess the efficacy of automatic error detection methods on LLM summaries and find that they struggle to detect these nuanced errors. To address this, we introduce two prompt-based approaches for fine-grained error detection that outperform existing metrics, particularly for identifying ""Circumstantial Inference."" }","In the example provided, GPT-4 infers that the speakers are discussing ""their son."" Although this inference seems plausible given the circumstantial evidence in the conversation, it lacks direct evidence.","Considerable progress has been made in summarization using large language models (LLMs) . However, the challenge of so-called ``hallucinations'', characterized in this context as statements in summaries that do not have direct evidence in the source material persists. As a result, evaluation of these summaries is an active area of research. In prior research, news articles have been the main testbed for LLM-generated summary evaluation . Dialogue summarization remain less explored, with prior works mostly focused on smaller fine-tuned models . In this work, we close the evaluation gap, focusing our analysis on LLM summaries of chit-chat style dialogues. We obtain fine-grained inconsistency annotations for summaries generated (zero-shot) by two prominent LLMs (GPT-4 and Alpaca-13B ) and across two summarization datasets (SAMSum and DialogSum ). In the domain of dialogues, a further gap exists in understanding the differences between summaries generated by LLMs and those generated by smaller fine-tuned models. In the news domain, prior work has found that LLM-generated summaries have fewer inconsistencies . Work done by , also in the news domain, notes varying error distributions across different model categories. In our work in the dialogue domain, we compare differences in error rates and analyze the categories of errors for summaries of dialogues with fine-tuned models versus summaries with LLMs. As in the news domain, we find that LLM-generated summaries have fewer inconsistencies. Surprisingly, our analysis reveals that over 30\ of LLM-generated summaries contain inconsistencies, contrasting sharply with the inconsistency rate of less than 5\ To further elucidate the differences between LLMs and fine-tuned models, we annotate spans with error categories. Previous work has primarily relied on part-of-speech-based tags for error classification . However, complexities inherent in LLM-generated summaries, often lengthier and more intricate, do not neatly align with error categories based solely on part of speech, warranting alternative strategies for a more meaningful categorization. Hence, our work proposes a refined taxonomy integrating existing error types. We further introduce a {new} error category specific to LLM behavior: {""Circumstantial Inference.""} This category stems from the observation that LLMs frequently produce statements that appear plausible based on circumstantial (but not direct) evidence in the dialogues, an aspect hitherto unexplored. In particular, LLMs tend to produce statements that may be {circumstantially} implied based on contextual cues in the conversation but not explicitly stated as seen in Figure . Although these inferences are not directly stated and can be inherently unsupported, they can still be useful in some instances, especially when summarizing ambiguous dialogues. However, the appropriateness of such inferred details varies depending on context and domain, highlighting the need for further investigation. In addition, there is limited understanding regarding the automatic detection of the mentioned error types. Therefore, we systematically evaluate the performance of state-of-the-art error detectors on LLM-generated dialogue summaries. We also introduce two prompt-based methods for fine-grained error detection, which notably outperform all prior state-of-the-art error detectors, particularly in identifying the newly introduced error type, ""Circumstantial Inference.""\\ In summary, our primary contributions are as follows: {enumerate} We bridge a gap in understanding LLM effectiveness for dialogue summarization by collecting fine-grained human annotations that highlight inconsistencies and make the benchmark publicly available. We propose a refined taxonomy for error categorization of LLM-generated summaries, including a new error category called ""Circumstantial Inference"" that captures the tendency of LLMs to produce plausible hallucinations based on conversation context. We examine differences in behavior in dialogue summarization between LLMs and fine-tuned models by comparing error rates and types. We introduce two prompt-based methods for fine-grained error detection, which notably outperform existing metrics. These methods excel even in detecting the recently identified error type ""Circumstantial Inference."" Additionally, we evaluate state-of-the-art error detectors on model-generated summaries across model categories and error types unveiling their effectiveness and limitations. {enumerate"
LLM in a flash: Efficient Large Language Model Inference with Limited Memory,2312.11514v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2312.11514v3_0.pdf,"Large language models (LLMs) are central to modern natural language processing, delivering exceptional performance in various tasks. However, their substantial computational and memory requirements present challenges, especially for devices with limited DRAM capacity. This paper tackles the challenge of efficiently running LLMs that exceed the available DRAM capacity by storing the model parameters in flash memory, but bringing them on demand to DRAM. Our method involves constructing an inference cost model that takes into account the characteristics of flash memory, guiding us to optimize in two critical areas: reducing the volume of data transferred from flash and reading data in larger, more contiguous chunks. Within this hardware-informed framework, we introduce two principal techniques. First, ``windowing'' strategically reduces data transfer by reusing previously activated neurons, and second, ``row-column bundling'', tailored to the sequential data access strengths of flash memory, increases the size of data chunks read from flash memory. These methods collectively enable running models up to twice the size of the available DRAM, with up to 4x and 20x increase in inference speed compared to naive loading approaches in CPU and GPU, respectively. Our integration of sparsity awareness, context-adaptive loading, and a hardware-oriented design paves the way for effective inference of LLMs on devices with limited memory.","Average inference latency for a single token when only half of the model's memory is available: Our method selectively loads parameters on demand for each token generation step. The latency represents the time required to repeatedly load parameters from flash memory, combined with the time needed for computations.","In recent years, large language models (LLMs) have demonstrated strong performance across a wide range of natural language tasks. However, the unprecedented capabilities of these models come with substantial computational and memory requirements for inference. LLMs can contain hundreds of billions or even trillions of parameters, which makes them challenging to load and run efficiently, especially on personal devices. Currently, the standard approach is to load the entire model into DRAM (Dynamic Random Access Memory) for inference . However, this severely limits the maximum model size that can be run. For example, a 7 billion parameter model requires over 14GB of memory just to load the parameters in half-precision floating point format, exceeding the capabilities of most personal devices such as smartphones. While it is possible to employ techniques such as quantization to reduce the model size, still, this cannot address the main limitation of loading the entire model into DRAM. To address this limitation, we propose to store the model parameters in flash memory, which is at least an order of magnitude larger than DRAM. Then, during inference, we directly load the required subset of parameters from the flash memory, avoiding the need to fit the entire model in DRAM. To this end, our work makes several contributions: {itemize}[leftmargin=*, noitemsep, topsep=1pt, parsep=0.5pt, partopsep=0pt] First, we study the hardware characteristics of storage systems (e.g., flash, DRAM). We show that hardware constraints such as capacity and bandwidth limitations can have significant considerations when designing efficient algorithms for serving LLMs from flash ( ). Motivated by our findings, we propose several techniques that can help with (i) reducing the required data transfer, (ii) increasing the transfer throughput, and (iii) managing loaded parameters efficiently in DRAM ( ). Finally, as partially demonstrated in {fig:latency-intro}, we show that our proposed techniques for optimizing the cost model and selectively loading parameters on demand allows us to run models 2x larger than the device's DRAM capacity and speed up inference up to 4x, 7x, and 20x compared to naive implementation in CPU, Metal and NVIDIA GPU backends, respectively ( ). {itemize"
Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models,2306.05424v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2306.05424v2_0.png,"Conversation agents fueled by Large Language Models (LLMs) are providing a new way to interact with visual data. While there have been initial attempts for image-based conversation models, this work addresses the under-explored field of {video-based conversation} by introducing Video-ChatGPT. It is a multimodal model that merges a video-adapted visual encoder with an LLM. The resulting model is capable of understanding and generating detailed conversations about videos. We introduce a new dataset of 100,000 video-instruction pairs used to train Video-ChatGPT acquired via manual and semi-automated pipeline that is easily scalable and robust to label noise. We also develop a quantitative evaluation framework for video-based dialogue models to objectively analyze the strengths and weaknesses of video-based dialogue models. Code: . [1]{Equal contribution.}","{Architecture of Video-ChatGPT.} Video-ChatGPT leverages the CLIP-L/14 visual encoder to extract both spatial and temporal video features. This is accomplished by averaging frame-level features across temporal and spatial dimensions respectively. The computed spatiotemporal features are then fed into a learnable linear layer, which projects them into the LLMs input space. In our approach, we utilize the Vicuna-v1.1 model, comprised of 7B parameters, and initialize it with weights from LLaVA~\cite{liu2023llava}.","The surge of deep learning applications for video understanding has lead to major advancements in video-related tasks. However, the current video understanding models are still unable to hold an open-ended conversation about the video content in a coherent manner. A video-based dialogue model can revolutionize video search, surveillance operations and help summarize key events and abnormal event detection. Above all, it can provide a unified human-understandable interface to video-related tasks such as action recognition, localization, detection, segmentation, retrieval, and tracking. Further, such a capability is of great interest as it will demonstrate the model's ability to encode temporal and spatial cues, contextual relationships and long-term dependencies. Recent advancements in multimodal understanding are largely based on the combination of pretrained {image} models with Large Language Models (LLMs) but generally do not consider video inputs. It is therefore interesting to leverage the vast capabilities of LLMs for video understanding tasks in a way that would not only maintain the temporal and spatial characteristics but also be adept at generating human-like conversations about videos. In this paper, we introduce Video-ChatGPT, a novel multimodal model that merges the representational abilities of a pretrained visual encoder and the generative powers of an LLM, capable of understanding and conversing about videos. Video-ChatGPT leverages an adapted LLM that integrates the visual encoder of CLIP with Vicuna as a language decoder, fine-tuned on generated instructional image-text pairs. Our approach further adapts the design for spatiotemporal video modeling and fine-tunes the model on video-instruction data to capture temporal dynamics and frame-to-frame consistency relationships available in video data. In contrast to other concurrent works for video-based conversation, Video-ChatGPT excels at temporal understanding, spatial consistency and contextual comprehension as demonstrated by our extensive evaluations. A fundamental contribution of this work is the creation of a dataset of 100,000 video-instruction pairs using a combination of human-assisted and semi-automatic annotation methods. Each pair consists of a video and its associated instruction in the form of a question-answer. This provides Video-ChatGPT with a large and diverse dataset to learn from, increasing its video-specific understanding, attention to temporal relationships and conversation capabilities. Moreover, we introduce the first quantitative video conversation evaluation framework for benchmarking, allowing for a more accurate evaluation of the performance of video conversation models. This framework evaluates models on a variety of capabilities, such as correctness of information, detail orientation, contextual understanding, temporal understanding, and consistency. The contributions of this work are as follows,{-0.5em} {itemize}{}{0mm} We propose Video-ChatGPT, a video conversation model capable of generating meaningful conversations about videos. It combines the capabilities of LLMs with a pretrained visual encoder adapted for spatiotemporal video representations. We introduce 100,000 high-quality video instruction pairs together with a novel annotation framework that is scalable and generates a diverse range of video-specific instruction sets. We develop the first quantitative video conversation evaluation framework for benchmarking video conversation models. We demonstrate Video-ChatGPT to perform well compared to concurrent conversational engines for videos such as Video Chat . {itemize"
A Community-Centric Perspective for Characterizing and Detecting Anti-Asian Violence-Provoking Speech,2407.15227v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2407.15227v1_0.pdf,"Violence-provoking speech – speech that implicitly or explicitly promotes violence against the members of the targeted community, contributed to a massive surge in anti-Asian crimes during the COVID-19 pandemic. While previous works have characterized and built tools for detecting other forms of harmful speech, like fear speech and hate speech, our work takes a community-centric approach to studying anti-Asian violence-provoking speech. Using data from $420k$ Twitter posts spanning a 3-year duration (January 1, 2020 to February 1, 2023), we develop a codebook to characterize anti-Asian violence-provoking speech and collect a community-crowdsourced dataset to facilitate its large-scale detection using state-of-the-art classifiers. We contrast the capabilities of natural language processing classifiers, ranging from BERT-based to LLM-based classifiers, in detecting violence-provoking speech with their capabilities to detect anti-Asian hateful speech. In contrast to prior work that has demonstrated the effectiveness of such classifiers in detecting hateful speech ($F_1 = 0.89$), our work shows that accurate and reliable detection of violence-provoking speech is a challenging task ($F_1 = 0.69$). We discuss the implications of our findings, particularly the need for proactive interventions to support Asian communities during public health crises.\\ {{Warning: this paper contains content that may be offensive or upsetting.}}","{{Study Overview.} Our study comprises 4 key parts: {(i)} collecting anti-Asian COVID-19 data from Twitter, {(ii)} developing and validating anti-Asian violence-provoking speech codebook, {(iii)} obtaining community-centric annotations, and {(iv)} training and evaluating detection classifiers on community-crowdsourced data.}","Online platforms struggle with various forms of information pollution, including but not limited to harmful speech and misinformation. These malicious phenomena often intertwine in complex ways, exacerbating real-world issues. A glaring example of this is the dramatic increase in hate crimes against Asian communities during the COVID-19 pandemic, which included physical assaults and verbal harassment. Rumors about the virus's origins coalesced with pre-existing prejudices, resulting in narratives that portrayed Asians as ``uncivilized'', blamed them for the virus, and labeled them as ``spies''. This intricate interplay between different forms of malicious content led to a 339\ In light of the dramatic uptick in anti-Asian violence and ensuing community fear, the necessity to accurately identify {violence-provoking speech} --- i.e., {speech that could promote real-world violence against members of targeted communities} , becomes paramount. This differs from {hateful speech}, which is {a more subjective form of expression that may not directly incite violence}. While both these phenomena share commonalities --- being rooted in prejudice and derogation --- violence-provoking speech constitutes a specific {subset} of hateful speech that explicitly or implicitly encourages acts of aggression. The higher severity of harm associated with violence-provoking speech calls for targeted approaches for its detection, beyond treating hate as a monolithic entity. Recognizing that the perception of {what} qualifies as violence-provoking is not universal but varies among the targeted communities, we adopt a {community-centric} approach. We leverage the ``insider perspectives'' and the subjective lived experiences of community members to capture the nuances, slurs, and coded language that may be overlooked by outsiders. Our focus is particularly on Asian communities in the context of the COVID-19 pandemic, who were disproportionately impacted by violence-provoking speech leading to real-world harm. We address two key research questions: {0.01in} {RQ1}: {What are the characteristics of violence-provoking speech that targets Asian communities? How is anti-Asian violence-provoking speech different from anti-Asian hateful speech?}\\ {RQ2}: {Can state-of-the-art natural language processing (NLP) approaches accurately detect violence-provoking speech? How do the detection abilities compare to that of hate speech detection?} {0.01in} {We address these research questions by developing and validating a codebook for identifying anti-Asian violence-provoking speech, while working with Anti-Defamation League, a leading non-governmental organization that specializes in tackling real-world hate and extremism ({RQ1}).} We then use the codebook to obtain crowd-sourced annotations for violence-provoking and hateful content from individuals who self-identify as Asian community members. Our dataset demonstrates high inter-rater agreement (Fleiss' $$ = 0.66 for violence-provoking speech labels).{Project webpage: {https://claws-lab.github.io/violence-provoking-speech/}} We then use the annotated data to develop binary classifiers that are trained to distinguish {(i)} violence-provoking content from not-violence-provoking content and {(ii)} hateful content from non-hateful content. We find that while the NLP approaches effectively detect hateful speech ($F_1$ score = $0.89$), it is relatively more challenging to detect violence-provoking speech ($F_1$ score = $0.69$) ({RQ2}), perhaps due to its nuanced and subjective nature that relies on victims' own perceptions. We discuss possible reasons why detecting violence-provoking speech is challenging and the implications of lacking capabilities of the detectors. Additionally, we discuss how our developed approaches could aid in the development of moderation algorithms that employ tiered penalties for content that violate norms of varying severities. Finally, we highlight the need to develop trauma-informed approaches to proactively support targeted communities"
Generating Coherent Sequences of Visual Illustrations for Real-World Manual Tasks,2405.10122v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2405.10122v1_0.png,"Multistep instructions, such as recipes and how-to guides, greatly benefit from visual aids, such as a series of images that accompany the instruction steps. While Large Language Models (LLMs) have become adept at generating coherent textual steps, Large Vision/Language Models (LVLMs) are less capable of generating accompanying image sequences. The most challenging aspect is that each generated image needs to adhere to the relevant textual step instruction, as well as be visually consistent with earlier images in the sequence. To address this problem, we propose an approach for generating consistent image sequences, which integrates a Latent Diffusion Model (LDM) with an LLM to transform the sequence into a caption to maintain the semantic coherence of the sequence. In addition, to maintain the visual coherence of the image sequence, we introduce a copy mechanism to initialise reverse diffusion processes with a latent vector iteration from a previously generated image from a relevant step. Both strategies will condition the reverse diffusion process on the sequence of instruction steps and tie the contents of the current image to previous instruction steps and corresponding images. Experiments show that the proposed approach is preferred by humans in 46.6\% of the cases against 26.6\% for the second best method. In addition, automatic metrics showed that the proposed method maintains semantic coherence and visual consistency across the sequence of visual illustrations.",The properties of the elements in illustrations should remain coherent throughout the whole sequence.,"When humans undertake a task with numerous intricate steps, merely reading a step description is limiting, leaving the user to imagine and infer some of the more nuanced details. Complementing the textual step instructions with images enhances the user experience by better communicating and representing the text semantics and ideas. Although prompt-based image generation has advanced significantly , state-of-the-art (SOTA) models such as Latent Diffusion Models (LDMs) still struggle when generating image sequences to accompany textual instruction steps. The challenge lies in effectively combining two key aspects: (a) accurately portraying the actions outlined in the step instructions, and (b) ensuring coherence between successive images to avoid confusing the user. Existing storytelling approaches operate mostly on linear storytelling and use synthetic cartoon datasets with explicit sequence information, i.e., the textual prompts describe the images appropriately and have no implicit co-references. These aspects limit the applicability of existing methods to real-world scenarios (Figure~), where there is a lack of informative prompts accompanying images, and dependencies between prompts are not necessarily linear. In this paper, we explore the generation of image sequences within two domains: recipe instructions, and Do It Yourself (DIY) guides, both showing increasing online consumption. In these domains, accuracy and coherence are of utmost importance to ensure that the result of all manual actions is correct, and that the user is correctly guided to the target output, Figure~. These domains contain (i) complex sequential manual tasks of detailed actions, (ii) coherence requirements for the images accompanying the sequence step descriptions, and (iii) a non-linear sequential structure, where steps may be related to earlier steps--not necessarily the previous step. To tackle these challenges, we propose to extend Latent Diffusion Models, with an LLM decoder to semantically condition the reverse diffusion process in the sequence of steps and a copy mechanism to select the best LDM initialisation. The image generation process is conditioned on the current step and the previous steps, to increase semantic coherence. In addition, our method initializes the reverse diffusion process with a latent vector iteration copied from a previous generation process to ensure the visual coherence of the generated image. Through this dual attendance to past textual and visual items in the sequence, we aim to achieve {semantic coherence}, which pertains to the presence and persistence of objects in consecutive images, and {visual coherence}, which aims to ensure the consistency of backgrounds and visual object properties across successive images. Extensive automatic and manual evaluations confirmed that our model outperforms strong baselines in terms of the overall quality of the generated sequence of illustrations in the cooking and DIY domains"
LoRA-Flow: Dynamic LoRA Fusion for Large Language Models in Generative Tasks,2402.11455v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.11455v1_0.pdf,"LoRA employs lightweight modules to customize large language models (LLMs) for each downstream task or domain, where different learned additional modules represent diverse skills. Combining existing LoRAs to address new tasks can enhance the reusability of learned LoRAs, particularly beneficial for tasks with limited annotated data. Most prior works on LoRA combination primarily rely on task-level weights for each involved LoRA, making different examples and tokens share the same LoRA weights. However, in generative tasks, different tokens may necessitate diverse skills to manage. Taking the Chinese math task as an example, understanding the problem description may depend more on the Chinese LoRA, while the calculation part may rely more on the math LoRA. To this end, we propose LoRA-Flow, which utilizes dynamic weights to adjust the impact of different LoRAs. The weights at each step are determined by a fusion gate with extremely few parameters, which can be learned with only 200 training examples. Experiments across six generative tasks demonstrate that our method consistently outperforms baselines with task-level fusion weights. This underscores the necessity of introducing dynamic fusion weights for LoRA combination..}","Illustration of the proposed LoRA-Flow method. For the token $y_t$ at the $t$-th step, we use a gate that conditions on the prefix $\mathbf{y}_{<t}$ to determine the fusion weights. The dynamic fusion weights are intended to control the influence of different LoRA modules, to better cope with various types of tokens in generative tasks. Red and blue rectangles represent the weights assigned to the two involved LoRAs.","Large language models (LLMs) have demonstrated superior performance over previous smaller models across a wide range of tasks, thereby extending the applicability of AI systems to numerous real-world scenarios. Because of their substantial model size, training all parameters of LLMs can often be prohibitively expensive. Therefore, several researchers have proposed a set of parameter-efficient fine-tuning (PEFT) approaches. Among these, LoRA (low-rank adaptation; ) stands out as one of the most popular due to its efficiency and simplicity. The basic idea of LoRA is to learn an additional module for each downstream domain or task. Rather than solely employing a single LoRA to address learned tasks, several recent studies have delved into the potential of combining existing LoRAs to tackle unseen tasks. This direction holds the potential to substantially enhance the reusability of learned parameters, facilitating the integration of diverse model capabilities. Most existing LoRA fusion approaches employ a task-level weight distribution when combining different LoRAs. This implies that all test examples and tokens share the same fusion ratio. However, for some complex generative tasks (e.g., solving mathematical problems or generating code according to provided instructions), the LLM may need to dynamically employ various types of capabilities to address the entire problem effectively. Figure~ illustrates an example, where we have trained a Chinese chat LoRA (i.e., Zh Chat) and an English math LoRA (i.e., En Math), and our objective is to address a Chinese math problem. Intuitively, comprehending the Chinese problem description may rely more on the Chinese chat LoRA, whereas performing the calculation might depend more on the English math LoRA. In this work, we propose LoRA-Flow, which can dynamically determine the token-level weights of different LoRAs for generative tasks. At each time step, the fusion weights are generated by a gate module that conditions the current prefix. The fusion gate comprises an extremely small number of parameters, accounting for only approximately 0.2\ In summary, the contributions of this work can be outlined as follows: {itemize} We propose LoRA-Flow, a method that combines existing LoRAs with dynamic fusion weights to effectively control the influence of each LoRA across various generation steps. We verify the effectiveness of LoRA-Flow on six different generation tasks, and the results show that LoRA-Flow can consistently outperform the baselines that use task-level fusion weights (e.g., LoRA-Hub). By carefully designed analyses from various aspects, we provide deeper insights into the integration of LoRAs. We consider this journey to be fruitful in constructing a flexible plug-and-play community for LLMs, enabling developers to leverage plugins created by others to build up their own LLM applications. {itemize"
Generalizability of Mixture of Domain-Specific Adapters from the Lens of Signed Weight Directions and its Application to Effective Model Pruning,2402.10639v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.10639v2_0.pdf,"Several parameter-efficient fine-tuning methods based on adapters have been proposed as a streamlined approach to incorporate not only a single specialized knowledge into existing Pre-Trained Language Models (PLMs) but also multiple of them at once. Recent works such as AdapterSoup propose to mix not all but only a selective sub-set of domain-specific adapters during inference via model weight averaging to optimize performance on novel, unseen domains with excellent computational efficiency. However, the essential generalizability of this emerging weight-space adapter mixing mechanism on {unseen, in-domain examples} remains unexplored. Thus, in this study, we conduct a comprehensive analysis to elucidate the generalizability of domain-specific adapter mixtures in in-domain evaluation. We also provide investigations into the inner workings of the mixture of domain-specific adapters by analyzing their weight signs, yielding critical analysis on the negative correlation between their fraction of weight sign difference and their mixtures' generalizability. The code is available at {Github}.","Mixing the adapter weights across various tasks may result in the importance weights of individual tasks nullifying each other, thereby yielding a merged mixture losing important information.","Recently, several {parameter-efficient fine-tuning methods that are based on adapters} have been introduced as a streamlined approach for fine-tuning Pre-trained Language Models (PLMs) to equip them with new, specialized knowledge or domain. Several algorithms have been proposed to train a distinct adapter for each new domain. To further improve a model's generalizability, existing works mostly focus on training multiple adapters for multiple tasks and continuously adding more adapters for incoming new tasks. This can be inefficient for the new domain tasks that have only a few examples, making the learning among the tasks unequal. Thus, more recent works such as opt for weight-space averaging of model and/or adapters trained on different domains, resulting in so-called {Mixture of Expert Adapters}. One recent notable work in this space is AdapterSoup, which proposes to merge the weights of a fixed-size, selective sub-set of different domain-specific adapters via an averaging function to accommodate unseen tasks or domains during inference. Such weight-space merging mechanism on adapters is efficient in practice as one can efficiently train a small, additional adapter and plug it into existing PLMs to incorporate new knowledge. Although the work reported favorable evaluation results on unseen, novel domains, it is unclear to what extent such weight-space merging mechanism on domain-specific adapters can generalize in an in-domain evaluation setting--i.e., how well it makes predictions on unseen examples of domains already seen during training. Moreover, to the best of our knowledge, no existing works comprehensively study the generalization of the mixture of adapters in the in-domain setting. This literature gap seems counter-intuitive because in-domain evaluation is fundamental and should precede out-of-domain evaluation. Moreover, in real-world applications, model owners have incentives to utilize as much as possible available information to improve their models over time. With the availability of parameter-efficient finetuning methods that are fairly easy to adopt with minimal space and runtime cost, the model owners are then incentivized to quickly fine-tune their models on a few examples collected from a new domain on an additional adapter to optimize the performance (rather than totally relying on out-of-domain prediction capability). As a result, although in-domain evaluation seems trivial, it is fundamental as one must ensure that the mixture of adapters works well on the tasks they have already been trained on. Furthermore, several key questions regarding the resulting mixtures of domain-specific adapters remain unanswered, especially those regarding their generalizability and their adversarial robustness when mixing adapters trained from very different tasks. Therefore, borrowing the pop-culture saying that {``mixed drinks and cocktails aren't actually the same thing''}, in contrast from existing works, we hypothesize that {not all} mixture of expert adapters are created equal and all have superior performance. Then, through an array of comprehensive experiments, we attempt to give answers to questions about {when and what to mix} when it comes to domain-specific adapters. We found that the weight-space merging mechanism suffers from performance degradation in terms of both generalizability and adversarial robustness even with inputs from domains it already trains on. Moreover, we also attempt to explain such performance degradation by revealing a critical negative correlation between {signed directions of adapter weights during mixing} and domain-specific predictive performance (Fig.~). Although simple, this intuitive and novel observation also allows us to select {``when and what adapters to mix?''} and design a more effective model pruning as a by-product application. Overall, our study does {not} focus on proposing a new mechanism, algorithm, or method. Instead, we focus on analyzing and bringing understanding of an existing and emerging paradigm of mixing multiple domain-specific adapters that was previously introduced . Specifically, we focus on in-domain prediction when mixing adapters from different domains as an emerging and potential paradigm for the deployment of PLMs in practice. Our contributions are summarized as follows. {enumerate}[leftmargin=-0.2,noitemsep,topsep=0pt] This is the first and most comprehensive analysis of in-domain generalizability of a mixture of domain-specific adapters with 3 different adapter methods on 13 diverse classification datasets, We provide insights and analysis on when and what adapters to mix to minimize performance degradation via the lens of signed directions of adapters' parameter weights, We demonstrate the utility of such insights to train mixtures of adapters with 90\ {enumerate"
CritiqueLLM: Towards an Informative Critique Generation Model for Evaluation of Large Language Model Generation,2311.18702v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2311.18702v2_0.pdf,"Since the natural language processing (NLP) community started to make large language models (LLMs) act as a critic to evaluate the quality of generated texts, most of the existing works train a critique generation model on the evaluation data labeled by GPT-4's direct prompting. We observe that these models lack the ability to generate informative critiques in both pointwise grading and pairwise comparison especially without references. As a result, their generated critiques cannot provide fine-grained distinguishability on generated texts, causing unsatisfactory evaluation performance. In this paper, we propose a simple yet effective method called {Eval-Instruct}, which can first acquire pointwise grading critiques with pseudo references and then revise these critiques via multi-path prompting to obtain informative evaluation data in different tasks and settings, including pointwise grading and pairwise comparison with / without references. After fine-tuning on these data, the resulting model is empirically shown to outperform ChatGPT and all the open-source baselines and even achieve comparable evaluation performance to GPT-4 in system-level correlations of pointwise grading. We also demonstrate that our generated critiques can act as scalable feedback to further improve the generation quality of strong LLMs like ChatGPT.}.","Overview of Eval-Instruct. Starting from referenced pointwise grading data, our proposed multi-path prompting method can apply pointwise-to-pairwise and referenced-to-reference-free prompting strategies to acquire evaluation data in other tasks and settings via two different paths. Cross validation is adopted to filter out the contradictory data from these two paths and further improve the data quality.","Recently, large language models (LLMs) have been improved rapidly and approached human-level performance on various natural language processing (NLP) tasks, such as question answering, text summarization, dialogue generation, and code generation . How to automatically measure the performance of LLMs has now become an essential research problem and attracted extensive attention . Strong evaluation methods are expected to provide high-quality critiques (including not only rating scores but also explanations) that act as scalable feedback and guide LLMs to improve persistently . Traditional evaluation metrics, usually based on n-gram overlap between generated texts and reference texts (such as BLEU and ROUGE ), have limited effectiveness. Recent works mostly resort to model-based evaluation metrics, especially LLM-based ones . Since most of the best-performing LLMs such as ChatGPT and GPT-4 can only be accessed via OpenAI APIs, researchers start to automatically collect evaluation data by directly prompting GPT-4 and train their own evaluation models, aiming to avoid potential risks of commerical APIs, such as high cost, unstable usage, and data leakage . However, we argue that these evaluation models are still struggling to generate informative critiques in different evaluation tasks including pointwise grading and pairwise comparison. Especially in the challenging reference-free setting, these models tend to generate general critiques without fine-grained distinguishability on generated texts, causing unsatisfactory evaluation performance . In this work, we propose a simple yet effective method called {Eval-Instruct}, which can automatically construct informative instruction-tuning data for different evaluation tasks and settings, including pointwise grading and pairwise comparison with / without references. Our main idea is to fully utilize referenced pointwise grading critiques, which are shown to possess rich information with the assistance of references and elaborate prompt design , to construct evaluation data for other tasks and settings. Specifically, after acquiring pointwise grading critiques with pseudo references via GPT-4, we devise a multi-path prompting method including two strategies: 1) {Pointwise-to-Pairwise} Prompting aims to inject pointwise grading critiques into pairwise critiques and enrich them with more information about the respective quality of text pairs. 2) {Referenced-to-Reference-Free} Prompting is targeted at removing direct comparison with references in referenced critiques, while keeping other details to improve the specificity of reference-free critiques. The evaluation data in different tasks and settings can be acquired via different paths consisting of these two strategies. And we also design a cross validation mechanism to improve the data quality of reference-free pairwise comparison because both of the two paths reach this task. After fine-tuning on the data of all the tasks and settings, the resulting model is empirically shown to outperform all the open-source baselines and even achieve comparable performance with GPT-4 in system-level correlations of pointwise grading. We also show the potential of to act as effective feedback to enhance the performance of LLMs like ChatGPT. Our main contributions are as follows: {itemize} We propose an evaluation data construction method called Eval-Instruct to automatically acquire informative evaluation data in both pointwise grading and pairwise comparison with / without references. We conduct extensive experiments on , which is fine-tuned on the data constructed by Eval-Instruct. Experimental results on three instruction following benchmark datasets show that our model can outperform all the open-source baselines and even perform comparably with GPT-4 in system-level correlations of pointwise grading. We reveal the potential of to guide LLMs to improve persistently by showing the positive impact of our generated critiques as scalable feedback on the generation quality of LLMs. {itemize"
Effects of diversity incentives on sample diversity and downstream model performance in LLM-based text augmentation,2401.06643v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2401.06643v3_0.png,"The latest generative large language models (LLMs) have found their application in data augmentation tasks, where small numbers of text samples are LLM-paraphrased and then used to fine-tune downstream models. However, more research is needed to assess how different prompts, seed data selection strategies, filtering methods, or model settings affect the quality of paraphrased data (and downstream models). In this study, we investigate three text diversity incentive methods well established in crowdsourcing: {taboo} words, {hints} by previous outlier solutions, and {chaining} on previous outlier solutions. Using these incentive methods as part of instructions to LLMs augmenting text datasets, we measure their effects on generated texts' lexical diversity and downstream model performance. We compare the effects over 5 different LLMs, 6 datasets and 2 downstream models. We show that diversity is most increased by {taboo} words, but downstream model performance is highest with {hints}.","Overview of our methodology. For each dataset, we randomly sample 6 samples per label that are used as seed sentences for LLM data augmentation. There, we collect data in in 2 rounds - 1st only using the {prompt} method and then in parallel for {prompt} method and 3 different diversity incentive methods. These are added together to form the datasets. BERT-large or Mistral classifier is fine-tuned 5 or 3 times respectively on each of the collected data and then evaluated. We repeat the entire process 5 times.","The emergence of large language models (LLMs) such as GPT-4, LLaMA, etc., has sparked interest in using them to augment textual datasets. In these scenarios, the number of samples is expanded by paraphrasing existing ones through LLM prompting. The created paraphrases are then added to the original dataset and used for downstream model training. Such methods have been explored for various domains such as sentiment classification, news classification and health symptoms classifications. However, investigation of the effect of various prompts, specific instructions, and selection of seed data inspired by crowd in the text augmentation process when using LLMs is lacking. Crowdsourcing is an established practice for collecting training or validation examples for a variety of NLP tasks. Scenarios of data collection using human workers can be similar to those of data augmentation: workers create paraphrases on existing sentences chosen from a dataset. The aim of such data collection is to increase the {data diversity} and subsequent performance of classifiers trained on the data. To increase the diversity, various methods are used in crowdsourcing to guide workers. These include {taboo} words - where most significant words from the collected data are identified and listed in the worker instructions to be avoided during paraphrasing, {chaining} - where outliers in the previous paraphrases are identified and used as seed sentences in the next round of data collection, and {hints} where previous outlier paraphrases are used as examples in the instructions. The {hints} method itself is similar to LLM in-context learning, where examples are included in the instructions for the model to achieve better performance. All of these {diversity incentive methods} report increased diversity of paraphrases and some also report increased performance of the classifiers trained on the so-collected data. This work is inspired by the parallels between crowdsourcing and LLM prompting and by the performance of {diversity incentive methods} on the diversity of paraphrases and the performance of models trained on them. We investigate the effects of the three {diversity incentive} methods (originating in crowdsourcing) on data augmentation using LLMs. The baseline, taken from a previous study, is a simple prompting for paraphrases. Measuring paraphrase diversity and downstream performance of classification models, we assess whether the diversity incentives (added to the base prompt) improve LLM outputs similarly as in crowdsourcing scenarios. To our knowledge, this is the first work to investigate the effects of {diversity incentive methods} on LLMs. In this paper, we answer the following research questions: {description}[labelwidth = 24pt, leftmargin = !] [RQ1:] {Does the usage of diversity incentive methods on LLMs yield more diverse paraphrases? (compared to base prompting)} [RQ2:] {Do classifiers achieve better performance if trained on data augmented using diversity incentive methods on LLMs? (compared to base prompting)} {description} To answer these questions~{Data and code at: {https://github.com/kinit-sk/LLM-div-incts}}, we have conducted a data augmentation experiment using 5 different LLMs on 6 different datasets in the tasks of sentiment (movie and app reviews), news, and intent (flight and voice assistant commands) classification. In this experiment, we repeatedly collect LLM paraphrases using different diversity incentive methods. Then, we compare the {lexical diversity} of the collected data and the {performance of downstream classifiers}. Additionally, we also conduct an {ablation study}, where we modify the diversity incentive methods with random data to validate, that the inputs used by these methods (e.g., most influential taboo words, outlier paraphrases) contribute to the method's performance and a combination of the best performing methods for lexical diversity and model performance. In total, we collected 253,500 paraphrases. The most prominent findings are the following: 1) We do not observe statistically significant improvements in lexical diversity of the generated datasets, but only minor improvements using the {taboo} method, 2) The {hints} method increases the performance of classification models trained on such data compared to the baseline, while also reducing standard deviation and thus increasing the stability of results, 3) The {chaining} method and {taboo} method both do not significantly affect the performance of classification models trained on such data compared to the baseline"
Context versus Prior Knowledge in Language Models,2404.04633v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2404.04633v3_0.pdf,"To answer a question, language models often need to integrate prior knowledge learned during pretraining and new information presented in context. We hypothesize that models perform this integration in a predictable way across different questions and contexts: models will rely more on prior knowledge for questions about entities (e.g., persons, places, etc.) that they are more familiar with due to higher exposure in the training corpus, and be more easily persuaded by some contexts than others. To formalize this problem, we propose two mutual information-based metrics to measure a model's dependency on a context and on its prior about an entity: first, the of a given context represents how much a model depends on the context in its decision, and second, the of a given entity represents how much the model can be swayed away from its original answer distribution about an entity. We empirically test our metrics for their validity and reliability. Finally, we explore and find a relationship between the scores and the model's expected familiarity with an entity, and provide two use cases to illustrate their benefits. [width=1.25em,height=1.15em]{figures/github.png} {}","In answering a given query, a model may be more {\entitytext{susceptible}} to context for some \entitytext{entities} than others, while some \contexttext{contexts} may be more {\contexttext{persuasive}} than others (as indicated in this figure by color darkness in the rightmost column). We introduce mutual information-based metrics to evaluate how much impact the context has relative to the prior knowledge of a model.","Language models have displayed remarkable abilities to answer factual queries about entities, suggesting that they encode knowledge about these entities learned during pretraining . For prompts that extend a question with additional information or context, the model can draw on both its prior knowledge and the additional context to answer the query . While previous research has investigated how often a model will rely on prior knowledge over conflicting contextual information in answering questions , we hypothesize that models will not behave identically for all contexts and entities. For example, if a language model is prompted with {Harry hugged Voldemort.} {How friendly are {Harry Potter} and {Lord Voldemort}?}, we might expect the prior knowledge learned from training data describing the rivalry between these two characters to significantly influence the model's answer. However, if the model lacks a strong prior on, say, {Susie} and {Alia}, then we might expect its answer to be primarily context-driven when prompted with {Susie hugged Alia.} {How friendly are {Susie} and {Alia}?}. We formalize this problem through the lens of evaluating the change in a model's answer distribution for different contexts and entities. We present two mutual information-based metrics that allow us to explore differences in the effect of specific contexts on model behavior for different entities. The {persuasion score} of a given context measures how much a model's answer distribution is affected by the context when prompted with a particular query about a given entity. The {susceptibility score} of a given entity measures how much the model's answer distribution can be swayed for a particular query about that entity, marginalized over all contexts. Given their basis in mutual information, our metrics are natural operationalizations of persuasion and susceptibility. Furthermore, we offer empirical evidence of the validity and reliability of these measures by comparing them against similar metrics and showing their robustness to paraphrases and different samples. To study how language models behave for different contexts and entities, we create a synthetic dataset of queries covering topic areas extracted from the YAGO knowledge graph , entities extracted from YAGO and generated with GPT-4 , and contexts constructed with different qualities, e.g., relevancy and assertiveness. We apply our new metrics to Pythia models ranging from 70m to 12b parameters to find evidence that relevant contexts are consistently more persuasive than irrelevant ones, and assertive contexts are more persuasive than less assertive ones for yes--no questions. In a deep dive into one model, we find evidence that entities with high expected familiarity, as measured by both training data frequency and entity degree statistics in the YAGO knowledge graph, have lower susceptibility scores. We further conduct case studies to show how these metrics could be useful in applied settings. In a study on friend--enemy stance measurement, we find evidence that enemy duos are less susceptible than friend duos. Applying our metrics to gender bias, we find evidence for a difference in susceptibility between stereotypically masculine and feminine names for gender-biased contexts. Through this, we show how our proposed metrics can be used to better analyze the effects of context and prior knowledge, with the potential for application toward greater control over model behavior"
Word Matters: What Influences Domain Adaptation in Summarization?,2406.14828v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.14828v1_0.png;/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.14828v1_1.png;/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.14828v1_2.png,"Domain adaptation aims to enable Large Language Models (LLMs) to generalize domain datasets unseen effectively during the training phase. However, factors such as the size of the model parameters and the scale of training data are general influencers and do not reflect the nuances of domain adaptation performance. This paper investigates the fine-grained factors affecting domain adaptation performance, analyzing the specific impact of `words' in training data on summarization tasks. We propose quantifying dataset learning difficulty as the learning difficulty of generative summarization, which is determined by two indicators: word-based compression rate and abstraction level. Our experiments conclude that, when considering dataset learning difficulty, the cross-domain overlap and the performance gain in summarization tasks exhibit an approximate linear relationship, which is not directly related to the number of words. Based on this finding, predicting a model's performance on unknown domain datasets is possible without undergoing training. Source code and scripts are available at .",Single-domain adaptation. The dotted line reflects a changing trend fitting the scatter data. The light-colored area represents a standard deviation of plus or minus.,"With the continuous development of Large Language Models (LLMs), remarkable capabilities have been demonstrated in knowledge comprehension, logical reasoning, problem-solving, and other aspects. As a result, LLMs have been widely applied to various summarization tasks on different domains to improve productivity like law, medicine, finance and so on, including both natural and social science study. However, when LLMs are applied to specific domains, it often necessitates the selection of corresponding domain-specific knowledge bases for training. This results in a limitation where a model trained in one domain struggles to be effectively applied in others, leading to a waste of resources. This constraint arises from the disparity in the distribution between the training data and the target domain data. In light of this limitation, effective methods must be taken to fix the gap and then enhance the model's adaptability and efficiency in summarization tasks. Domain adaptation aims to train a model from multiple source domains, enabling it to generalize well to unseen domains. Consequently, enhancing domain adaptation performance is a key objective for large-scale models in improving downstream tasks. It is worthwhile to explore which factors can affect the domain adaptation performance. proposes that metrics based on nonlinear or non-contiguous tokens are crucial to a model demonstrating emergent abilities and that ROUGE-L-Sum shows sharper variations. This has inspired us to consider the performance changes of models in domain adaptation from the perspective of more granular units. Tokens typically do not possess complete semantics, whereas words are the basic language units with specific meanings or functions. Therefore, we consider exploring the impact on model performance in domain adaptation from the perspective of words. Summarization tasks involve generating concise texts that encapsulate the main components of longer documents, considering factors such as coherence, information diversity, and coverage scope. This differs from other downstream tasks like machine translation and classification. note that reducing summary extractors' size or compression ratio can lead to losing vital content, features, concepts, and other significant information. Therefore, we explore how the degree of information extraction between input documents and target summaries impacts domain adaptation performance in summarization tasks. This paper investigates how words impact the domain adaptation of summary tasks. We first introduce two indicators, compression rate, and abstraction level, to quantify the learning difficulty of datasets, thereby more accurately reflecting the performance gain of models. Then, we identify two key aspects affecting domain adaptation: cross-domain overlap and word count, hypothesizing a linear relationship between them and model performance. Experiments are conducted with models of various sizes on summarization datasets from four domains. The results indicate that the cross-domain overlap exhibits an approximately linear relationship with performance gain when considering dataset learning difficulty. In contrast, word count shows no significant correlation. Based on this linear relationship, it is possible to predict model performance without undergoing training by using the cross-domain overlap calculated from the dataset. Our contributions can be summarized as follows: {itemize} We propose two factors affecting the domain adaptation of summarization tasks: (1) {Learning difficulty coefficient} of the dataset more accurately reflects the performance gain; (2) {Cross-domain overlap} directly represents the closeness between the source and target domains. Our experiments show that cross-domain overlap has an approximately linear relationship with performance gains based on the learning difficulty coefficient, revealing the connection between datasets and domain adaptation from the perspective of words. We demonstrate that without undergoing training, it is possible to predict a model's performance on unknown domain datasets solely based on the learning difficulty coefficient and cross-domain overlap. This provides a resource-efficient and rapid validation method for models regarding domain adaptation. {itemize"
Faithful Logical Reasoning via Symbolic Chain-of-Thought,2405.18357v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2405.18357v2_0.pdf,"While the recent Chain-of-Thought (CoT) technique enhances the reasoning ability of large language models (LLMs) with the theory of mind, it might still struggle in handling logical reasoning that relies much on symbolic expressions and rigid deducing rules. To strengthen the logical reasoning capability of LLMs, we propose a novel Symbolic Chain-of-Thought, namely {SymbCoT}, a fully LLM-based framework that integrates symbolic expressions and logic rules with CoT prompting. Technically, building upon an LLM, SymbCoT 1) first translates the natural language context into the symbolic format, and then 2) derives a step-by-step plan to solve the problem with symbolic logical rules, 3) followed by a verifier to check the translation and reasoning chain. Via thorough evaluations on 5 standard datasets with both First-Order Logic and Constraint Optimization symbolic expressions, SymbCoT shows striking improvements over the CoT method consistently, meanwhile refreshing the current state-of-the-art performances. We further demonstrate that our system advances in more faithful, flexible, and explainable logical reasoning. To our knowledge, this is the first to combine symbolic expressions and rules into CoT for logical reasoning with LLMs. Code is open at .",An illustrative example of logical reasoning via Chain-of-Thought and our proposed Symbolic CoT (SymbCoT).,"Achieving human-like logical reasoning capabilities is crucial for realizing AGI, which plays a pivotal role in enabling intelligent systems to engage in problem-solving, decision-making, and critical thinking. Recently, LLMs have demonstrated unprecedented capabilities in semantic understanding, casting a beacon of hope toward achieving AGI. Further enhancing LLMs to achieve human-level reasoning abilities, particularly in logical reasoning, is of paramount importance. Logical reasoning stands out as a quintessential form of reasoning that, unlike other types, is crucial and challenging. It epitomizes a cognitive process characterized by rigorous evidence evaluation, argument construction, and logical deduction . The latest trend is integrating LLMs with symbolic solvers to enhance their performance . Unfortunately, these efforts have been limited to using LLMs merely as text-to-symbolic translators, with the core reasoning still reliant on traditional external reasoners . Such an approach, first, does not intrinsically strengthen LLMs' capability in logical reasoning. Besides, over-reliance on external symbolic solvers often results in inflexibility, information omission, and unexplainability. On another note, the concept of CoT has been introduced to mimic human thinking processes by encouraging LLMs to explicitly consider intermediate steps during problem-solving and to provide rationales for decisions, thereby enhancing the reliability of the reasoning process. CoT has been successfully integrated into a wide array of tasks , significantly improving LLMs' reasoning capabilities, sometimes even matching human performance in certain scenarios . There is growing interest in applying CoT for logical reasoning , and developing advanced strategies such as self-consistency and Tree-of-Thought for enhancement. However, applying basic CoT directly to logical reasoning is inherently limited, due to the abstractive nature of language expression. Logical reasoning demands rigorous logical calculations, heavily relying on both symbolic expressions and rigid deducing rules to represent the internal structure of problems. Plain texts often fall short of supporting such precise logic, especially in scenarios that demand strict logical representation. For instance, as shown in Fig. , when tackling a logical reasoning problem, utilizing symbolic representations like First-Order Logic (FOL) is more representative and precise than fully natural language rationales in CoT, enabling strict logical reasoning through clear inference rules. To address these challenges, we introduce a novel Symbolic CoT (namely {SymbCoT}) for logical reasoning. Unlike existing state-of-the-art (SoTA) LLM-based symbolic reasoning systems , SymbCoT is entirely facilitated by LLMs without relying on any external reasoners/tools, i.e., encompassing both the initial translation and subsequent reasoning phases. Fig. provides a high-level illustration of the overall system workflow. Technically, SymbCoT comprises four main modules: {Translator}, {Planner}, {Solver}, and {Verifier}. Notably, SymbCoT is characterized by the following three core aspects: {-2.5mm} {1.5em}{2.2em}{1.87em}{1.7em}{1em}{1em} {compactitem} [1)] SymbCoT integrates symbolic expressions into CoT to describe intermediate reasoning processes, facilitating more precise logical calculations. However, relying solely on symbolic representation still has its limitations, as it often fails to capture certain content, such as implicit intentions or crucial contextual information embedded within questions. Yet LLMs excel at interpreting such nuanced information and contexts. Thus, we consider a combination of symbolic and natural language expressions to leverage the mutual strengths of both: freely expressed implicit intents and contextual information in natural language and rigorous expression in symbolic forms. [2)] Unlike the straightforward prompting of ``{thinking step by step}'' in vanilla CoT, SymbCoT considers a {plan-then-solve} architecture. This involves decomposing the original complex problem into a series of smaller, more manageable sub-problems, which are then addressed one by one. This way, the entire reasoning process becomes more trackable, enabling a clearer and more structured approach to problem-solving. [3)] Furthermore, we devise a retrospective verification mechanism. At both the translation and subsequent problem-solving stages, we retrospectively validate the correctness of each step's outcome, by tracing back to the original given condition. This verification process ensures the accuracy and reliability of the operations performed during the reasoning process. {compactitem} In experiments, we test SymbCoT with symbolic expressions of FOL and Constraint Optimization (CO) on five logical reasoning datasets using both GPT-3.5 and GPT-4. Results demonstrate that SymbCoT significantly enhances the reasoning capabilities of vanilla CoT, outperforming current SoTA solutions clearly. We further demonstrate that the more complex the logical reasoning task, the more pronounced the improvement of SymbCoT over vanilla CoT, further with the verification mechanism ensuring the faithfulness of the reasoning process. Our in-depth analysis reveals that fully LLM-based logical reasoning can offer better symbolic syntax robustness, human-readable explanations, and fuller utilization of information. In summary, our technical contributions are: {compactitem} proposing a fully LLM-based logical reasoning framework based on CoT, demonstrating that LLMs can achieve robust logical reasoning capabilities without external reasoning tools. Compared to existing SoTA solutions relying on external resolvers, SymbCoT offers better robustness against translation errors and more human-understandable explanations. innovatively integrating the strengths of symbolic forms and natural language expressions, enabling precise reasoning calculations while fully interpreting implicit information and capturing rich contexts. introducing a plan-then-solve architecture for CoT reasoning, along with a retrospective verification mechanism, enhancing the faithfulness of the reasoning process. {compactitem"
ESCoT: Towards Interpretable Emotional Support Dialogue Systems,2406.10960v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.10960v1_0.pdf,"Understanding the reason for emotional support response is crucial for establishing connections between users and emotional support dialogue systems. Previous works mostly focus on generating better responses but ignore interpretability, which is extremely important for constructing reliable dialogue systems. To empower the system with better interpretability, we propose an emotional support response generation scheme, named {E}motion-Focused and {S}trategy-Driven {C}hain-{o}f-{T}hought ({ESCoT}), mimicking the process of {identifying}, {understanding}, and {regulating} emotions. Specially, we construct a new dataset with ESCoT in two steps: (1) {Dialogue Generation} where we first generate diverse conversation situations, then enhance dialogue generation using richer emotional support strategies based on these situations; (2) {Chain Supplement} where we focus on supplementing selected dialogues with elements such as emotion, stimulus, appraisal, and strategy reason, forming the manually verified chains. Additionally, we further develop a model to generate dialogue responses with better interpretability. We also conduct extensive experiments and human evaluations to validate the effectiveness of the proposed ESCoT and generated dialogue responses. Our data and code are available at {https://github.com/TeigenZhang/ESCoT}.","Illustration of the ESCoT scheme. The supporter first {identifies emotion}, then {understands emotion} from perspectives of emotional stimulus and individual appraisal, and finally chooses the appropriate strategy and responds to the seeker to {regulate emotion}.","Emotional support is conceptualized as expressing care, concern, affection, and interests, especially for the individuals feeling stressed or upset. Incorporating emotional support can yield positive effects in many scenarios, such as therapeutic sessions, customer service counters, and palliative cares. Realizing reliable emotional support dialogue systems capable of automating these interactions is expected to expand the scope and efficacy of such services. Moreover, a reliable emotional support dialogue system should not work like a black box, providing conversational responses but unable to explain how those responses were generated. As shown in Figure~, let's imagine how a helpful supporter would work by considering the feelings of a seeker who asks for help: the supporter would first {identify} the situation and the emotion of the seeker, then {understand} and acknowledge the emotion, and finally choose appropriate strategies to respond in order to {regulate} the emotion. Therefore, it is extremely desired to build a reliable and trustworthy emotional support dialogue system that can not only generate emotional support responses but also provide the reasoning or chain-of-thought (CoT) behind the generated responses. Some previous endeavors have attempted to improve the interpretability of emotional support dialogue systems, such as controlling the response by emotion or strategy, or using commonsense to augment the emotional support response. However, to the best of our knowledge, there is currently no such emotional support dialogue system that can provide comprehensive reasoning explanations. Therefore, in this work, we aim to build an interpretable emotional support dialogue system. Due to the high expertise requirements for supporter roles in emotional support conversations, building a human-annotated emotional support dialogue dataset is very costly. Recently, the powerful language generation and reasoning capabilities of large language models (LLMs) have demonstrated a viable pathway to generate high-quality data. Efforts such as {AugESC} and {SmileChat} have attempted to expand emotional support dialogue datasets via LLMs. However, the reasoning or chain-of-thought behind the dialogue responses has been overlooked. In this paper, we propose an emotional support response generation scheme, named {E}motion-focused and {S}trategy-driven {C}hain-{o}f-{T}hought ({ESCoT}), to generate dialogue data, inspired by the human emotional support generation process of {identifying}, {understanding}, and {regulating} emotions. Specifically, to emphasize the critical role of conversation strategies and dialogue situations, we first create diverse dialogue situations, and then enhance dialogue generation using richer emotional support strategies based on these situations. Furthermore, we complement selected dialogues with the chain-of-thought (CoT), which is represented as a quintuple \((EM, ES, IA, SR, RE)\), reflecting the process as illustrated in Figure~. After careful manual checking, we build the first dataset for {E}motional {S}upport {D}ialogue with {CoT} ({ESD-CoT}), containing 1.7k+ dialogues. Moreover, we build our emotional support dialogue system with better interpretability via supervised fine-tuning a pre-trained language model on ESD-CoT, providing a strong baseline for future investigation. Our main contributions in this work include: (1) We develop an effective emotion-focused strategy-driven chain-of-thought automatic data generation scheme called ESCoT to increase the interpretability of emotional support response generation. (2) We build the first chain-of-thought emotional support dataset ESD-CoT, containing 1.7k+ dialogues through automatic generation and manual correction. (3) We conduct human evaluations to validate from different aspects the effectiveness of our data generation scheme and the quality of our constructed dialogue dataset. (4) We build an interpretable emotional support response generation model on ESD-CoT and conduct a comprehensive assessment of the performance, providing a strong baseline for future research"
PathReasoner: Modeling Reasoning Path with Equivalent Extension for Logical Question Answering,2405.19109v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2405.19109v1_0.jpg,"Logical reasoning task has attracted great interest since it was proposed. Faced with such a task, current competitive models, even large language models (e.g., ChatGPT and PaLM 2), still perform badly. Previous promising LMs struggle in logical consistency modeling and logical structure perception. To this end, we model the logical reasoning task by transforming each logical sample into reasoning paths and propose an architecture {PathReasoner}. It addresses the task from the views of both data and model. To expand the diversity of the logical samples, we propose an atom extension strategy supported by equivalent logical formulas, to form new reasoning paths. From the model perspective, we design a stack of transformer-style blocks. In particular, we propose a path-attention module to joint model in-atom and cross-atom relations with the high-order diffusion strategy. Experiments show that PathReasoner achieves competitive performances on two logical reasoning benchmarks and great generalization abilities.","Probing tests on representative LMs (e.g., RoBERTa). (a) is about model prediction consistency. (b) is related to the perception of logical connectives. Detailed pilot experiments are shown in the Appendix.","With the emergence of pre-trained language models (PLMs) , recent years have witnessed remarkable progress in the task of machine reading comprehension (MRC) . To tackle more complex scenarios in reality, the challenging logical reasoning task has been proposed to exploit the model reasoning capability over text{Logical reasoning is a broad concept covering various tasks, but we mainly address the task in the form of MRC.}. Similar to the traditional MRC task, it also takes the context, question and options as inputs and requires the model to predict the final answer. Due to the diverse logical characteristics implied in the text, logical reasoning task brings huge challenges to current LMs. Especially, faced with such tasks, large language models (LLMs), e.g., ChatGPT{https://chat.openai.com} and PaLM 2{https://ai.google/discover/palm2/}, also struggle a lot which is proved by previous evaluation works . Under such circumstances, this paper will focus more on addressing logical reasoning tasks with small LMs, which are light-weighted and more flexible for future applications{The focus of this paper is mainly on small LMs, since they are more efficient and effective compared with LLMs on the logical reasoning tasks. But we still report LLM performances for comparison in the experiment section.}. Previous competitive LMs expose two limitations in the performance. Firstly, it lacks consistent model predictions on samples with equal logical semantics. For example in Figure (a), we make changes to the expression of the original context while maintaining the semantic unchanged, where {not...unless} is equally transformed into the expression of {only if}. However, the LMs give inconsistent predictions between the original sample and the modified one. We blame the problem on the lack of training samples in logical reasoning. Compared with some classic MRC datasets like SQuAD , CoQA with over 100,000 training samples, logical reasoning datasets like ReClor and LogiQA are much more sparse with only several thousand samples. Thus, such sparsity limits the learning of logic semantics. Previous work leverages general corpus to conduct continual pretraining, but it does not address the sparsity of logical text in essential. Secondly, it remains a challenge to enhance the model perception for logical structures. For example in Figure (b), we randomly replace the explicit logical relation words or inverse the negations for the context, which destroys the original semantics. But the LMs fail to change the prediction accordingly. It demonstrates that current LMs are insensitive to the logical connectives, instead they focus more on facts within the text. Considering that current LMs are pre-trained with general objectives on the fact corpus (e.g., Wikipedia), they are naturally weak in capturing the logical structures usually existing in logical-specific scenarios. Some studies like DAGN , Logiformer , and AdaLoGN have attempted to model the explicit logical relations from various perspectives, such as causal and co-occurrence. All of them build text graphs to conduct the reasoning, which limits the scalability to larger text and more complex scenarios. In view of the above challenges, we propose an architecture {PathReasoner}, which considers a new paradigm for logical reasoning tasks via reasoning path modeling. Based on the predefined logical rule forms, we represent each natural sentence as an atom and transform each sample into reasoning paths with confidence scores. Under such a paradigm, PathReasoner addresses the task from two views. From the view of expanding the data diversity, we first obtain equivalent atom combinations through external logical formulas, generating new reasoning paths and textualizing them as new samples. From the model view, we propose a reasoning path modeling network. It encodes both function symbols and variables in atoms and forms an atom embedding sequence as the input. In a path-attention module, we model high-order relations from both in-atom and cross-atom perspectives. Through the fusion of token, atom, and path embedding, the prediction can be derived. Our technical contributions are as follows, and additional key values are in Appendix~: (1) We unify the text inputs into atoms and reasoning paths. Based on it, an architecture PathReasoner is proposed to improve both the diversity of samples and logic perception capability. (2) In light of the sparsity of training data, we propose an atom extension strategy to form new training samples. To better capture logical structures, we introduce a path-attention module with high-order relation modeling, enabling joint updates of information within atoms and across atoms. (3) Extensive experiments show superior performances on two logical reasoning benchmarks. Significant generalization capabilities are also verified"
Advancing Parameter Efficiency in Fine-tuning via Representation Editing,2402.15179v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.15179v3_0.png,"Parameter Efficient Fine-Tuning (PEFT) techniques have drawn significant attention due to their ability to yield competitive results while updating only a small portion of the adjustable parameters. However, existing PEFT methods pose challenges in hyperparameter selection, such as choosing the rank for LoRA or Adapter, or specifying the length of soft prompts. To address these challenges, we propose a novel fine-tuning approach for neural models, named Representation EDiting (RED), which modifies the representations generated at some layers through the application of scaling and biasing operations. While existing PEFT methods still demonstrate over-parameterization that could potentially undermine the generalization ability acquired from pre-training, RED can substantially reduce the number of trainable parameters by a factor of $25,700$ compared to full parameter fine-tuning and by a factor of $32$ relative to LoRA. Remarkably, RED achieves results comparable or superior to both full parameter fine-tuning and other PEFT methods. Extensive experiments across various model architectures and scales, including RoBERTa, GPT-2, T5, and LLaMA-2, have demonstrated the effectiveness and efficiency of RED.}, thereby positioning it as a promising PEFT strategy for large-scale neural models.","{fig:figure_01} Comparison of previous representative PEFT methods with the proposed RED. (a) LoRA incorporates learnable bottleneck-shaped modules (highlighted in \textcolor{orange}{orange}) by integrating additional connections parallel to the $\mathbf{W}_q$ and $\mathbf{W}_v$ matrices of attention blocks, along with modifying the weights of these matrices in a low-rank fashion. Adapter, on the other hand, introduces learnable modules within similar structures (also highlighted in \textcolor{orange}{orange}) by incorporating additional connections following both the attention and feed-forward sub-layers. (b) RED introduces two learnable vectors, $l_\text{scaling}$ and $l_\text{bias}$, to directly edit the representations (marked in \textcolor{teal}{green}) generated by feed-forward sub-layers, which significantly reduces the number of parameters required for fine-tuning. \vspace{-5mm}","Pre-training on large-scale unlabeled datasets followed by fine-tuning on task-specific dataset has demonstrated remarkable efficacy across various natural language processing (NLP) tasks, establishing itself as the prevailing training paradigm . However, conducting full parameter fine-tuning for each task can be exceedingly costly and increasingly daunting as model scales continue to grow . For instance, BERT comprises up to $220$ million parameters, T5 scales up to $11$ billion parameters, and GPT-3 boasts an astounding $175$ billion parameters. Consequently, the efficient and effective adaptation of large models to specific downstream tasks presents an intriguing research challenge . In response to this challenge, researchers have put forward three main lines of Parameter Efficient Fine-Tuning (PEFT) techniques . Firstly, addition-based methods involve the introduction of additional trainable neural modules or parameters that were not present in the original model . Specification-based methods, on the other hand, identify certain parameters in the original model to be trainable, while the rest are kept frozen . Lastly, reparameterization-based methods reconfigure trainable parameters into a more parameter-efficient form through certain transformations . Among these PEFT methods, Low-Rank Adaptation (LoRA) stands out as one of the most efficient techniques with its effectiveness empirically validated across various models of diverse scales. Despite its impressive performance, LoRA still demands a significant number of trainable parameters. Recent studies by and indicate that the upper bound for intrinsic dimensions is substantially smaller than what is typically used in such methods. For example, the $d_{90}$ value (the minimum number of trainable parameters required to reach $90\ In addition to the issue of requiring too many adjustable parameters, existing PEFT methods primarily focus on the design of lightweight modules and their integration (or placement) within base models. Nonetheless, the implementation of these PEFT techniques introduces additional complexities in hyperparameter selection, such as choosing the rank of LoRA and Adapter, or deciding on the length of Soft Prompt and Prefix. Inspired by the concept of representation engineering , we shift our focus away from the weights of models and turn our attention to their representations. In the neural architecture, network weights govern neural activities (or representations), which in turn determine the networks' output, and the networks' output ultimately shapes the networks' behavior. Rather than concentrating on neurons and their interconnections (or weights), we explored to achieve control over network behavior by manipulating its internal representations. Specifically, we fine-tune neural network models by directly editing the representation generated at each layer while maintaining the model parameters frozen, as illustrated in Figure (b). It is worth noting that the number of parameters required to edit representations is substantially fewer than that of weights within neural networks. Taking LLaMA-2 ($7$B) as an example, the proposed representation editing (RED) method achieves competitive performance by adjusting only $0.26$M parameters. This is approximately $25,700$ times less than what is required for full-parameter fine-tuning, rendering the method both storage and computation efficient. The contributions of this study are summarized as follows: {itemize} {}{0pt} {}{0pt} {}{0pt} We propose a novel perspective on fine-tuning by directly editing model representations, diverging from exiting PEFT methods that focused on adjusting the model's weights. Our proposed PEFT technique, termed RED, embodies this new perspective. Extensive experiments are conducted across models of varying structures and scales, including RoBERTa, GPT-2, T5, and LLaMA-2. The effectiveness of RED is validated across a range of natural language understanding and generation tasks. Notably, RED demonstrates both efficacy and efficiency while requiring only a minimal number of trainable parameters and maintaining ease of implementation. A comprehensive ablation study is conducted to dissect the individual components of RED and understand their impacts on performance. {itemize"
"Chunk, Align, Select: A Simple Long-sequence Processing Method for Transformers",2308.13191v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2308.13191v2_0.pdf,"Although dominant in natural language processing, transformer-based models still struggle with long-sequence processing, due to the computational costs of their self-attention operations, which increase exponentially as the length of the input sequence grows. To address this challenge, we propose a {Sim}ple framework to enhance the long-content processing of off-the-shelf pre-trained transformers via three steps: {C}hunk, {A}lign, and {S}elect (SimCAS). More specifically, we first divide each long-sequence input into a batch of chunks, then align the inter-chunk information during the encoding steps, and finally, select the most representative hidden states from the encoder for the decoding process. With our SimCAS, the computation and memory costs can be reduced to linear complexity. In experiments, we demonstrate the effectiveness of the proposed method on various real-world long-text summarization and reading comprehension tasks, in which SimCAS significantly outperforms prior long-sequence processing baselines. The code is at . %{https://github.com/xjw-nlp/SimCAS}.","The SimCAS framework: The long inputs are first divided into a batch of chunks, each of which is filled with start token \texttt{[S]}, padding token \texttt{[P]} and end token \texttt{[E]}. Then the inter-chunk information can be transferred via the alignment of \texttt{[S]} and \texttt{[E]} representations after each encoder layer. Next, hidden states are selected for decoding steps. The decoder output logits and attention scores are utilized as rewards for updating the token selector.","Transformers have become a fundamental model architecture for sequential data modeling , especially in Natural Language Processing (NLP) , where texts are regarded as sequences of tokens. Built with transformer blocks, pre-trained language models (PLMs) have recently shown astonishing empirical performance in various NLP tasks such as question answering , controllable generation , summarization and logic reasoning. However, one fatal weakness, that has hindered transformer-based models from being applied in broader application scenarios, is the quadratically raised computational consumption of self-attention operations when the input length increases. Hence, vanilla transformers have continuously been challenged by long-context tasks, such as machine reading comprehension and long-text summarization. To enhance transformers with more efficient long-sequence processing, prior works focus on two perspectives, efficient attention operations and sub-sequence processing. Efficient attention targets on reducing the memory and calculation cost of self-attention operations while preserving transformers' empirical performance on downstream tasks. Unfortunately, most efficient attention methods require customized self-attention implementations, which demand from-scratch training instead of being directly plugged into existing pre-trained models. Moreover, some empirical studies have demonstrated that efficient-attention methods inevitably sacrifice the short-sequence processing performance compared with full-attention models. The alternative solution to long-sequence processing is to decompose the long-sequence input into multiple sub-sequences and then feed-forward each separately, known as sub-sequence processing. Although full-attention blocks are sufficiently utilized in each sub-sequence, these methods have been challenged to capture the semantic information across different sub-contexts. To solve this, some works assign the same fragment to different chunks, which however significantly increases the computational cost of each chunk and runs counter to the original efficiency goal. To gather the advantages of the methods above, we introduce a {Sim}ple yet effective learning framework with three typical operations: {C}hunk, {A}lign, and {S}elect ({SimCAS}). In detail, SimCAS first chunks the input sequence into a batch of sub-sequences then feeds each subsequence forward the elaborately designed encoding blocks with an inter-chunk alignment mechanism. Finally, the most semantically representative hidden representations are selected via a tailored selection policy to compress the overall sequence length. To align the semantic information across sub-sequence chunks, we introduce a sequential batch alignment operation to calibrate the start and end token embeddings of each sub-sequence in the encoder layers. To learn the selector policy, inspired by the recent success of reinforcement learning in NLP, we adopt the Proximal Policy Optimization (PPO) algorithm with the decoder treated as the environment. Moreover, we use the attention scores and output likelihood to calculate effective rewards for the selector's policy optimization. To verify the effectiveness of SimCA, we conducted comprehensive experiments on seven long-context datasets from the domains of document-level summarization, multi-document summarization, and reading comprehension. The empirical results show that SimCAS outperforms other long-sequence processing baselines with high-level scalability. The main contributions of this paper are: {itemize}[leftmargin=0.4cm] We propose a simple yet effective long-sequence processing framework, which noticeably extends the application range of existing full-attention PLMs. Unlike prior works compromising the performance on short sequences, our method maintains satisfying performances processing either short or long contexts. We discover that transformers can be conceptualized as simulation environments for policy learning. We leverage transformers' attention scores and output likelihoods to optimize the selector policy to compress the long-sequence information. The optimized selector policy meanwhile facilitates the transformer to concentrate more on hidden states with high semantic importance. We conduct comprehensive experiments illustrating that SimCAS consistently surpasses previous baselines. Furthermore, we provide model scaling, efficiency comparisons, and ablation studies to substantiate the superior performance of our proposed method. {itemize"
Combining Supervised Learning and Reinforcement Learning for Multi-Label Classification Tasks with Partial Labels,2406.16293v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.16293v1_0.pdf,"Traditional supervised learning heavily relies on human-annotated datasets, especially in data-hungry neural approaches. However, various tasks, especially multi-label tasks like document-level relation extraction, pose challenges in fully manual annotation due to the specific domain knowledge and large class sets. Therefore, we address the multi-label positive-unlabelled learning (MLPUL) problem, where only a subset of positive classes is annotated. We propose Mixture Learner for Partially Annotated Classification (MLPAC), an RL-based framework combining the exploration ability of reinforcement learning and the exploitation ability of supervised learning. Experimental results across various tasks, including document-level relation extraction, multi-label image classification, and binary PU learning, demonstrate the generalization and effectiveness of our framework.","{A.} A partially annotated data sample in DocRE task. {B.} Severe imbalanced distribution of annotated positive (red scatters) and negative labels (blue and orange scatters) corresponding to the DocRED \cite{yao-etal-2019-docred} dataset. Orange scatters are actually-positive labels reannotated by Re-DocRED dataset \cite{wu2022revisiting} {C.} Results of training on incomplete DocRED and testing on reannotated Re-DocRED and DocGNRE \cite{li2023semi}. SSR-PU is sensitive to prior estimation, while ours is prior agnostic. {D.} Performance comparison in DocRE task.","Multi-Label Classification (MLC) task treats a problem that allows instances to take multiple labels, and traditional Supervised Learning (SL) methods on MLC heavily rely on human-annotated data sets, especially neural approaches that are data-hungry and susceptible to over-fitting when lacking training data. However, in many MLC tasks that generally have dozens or hundreds of sizes of class sets, incompleteness in the acquired annotations frequently arises owing to the limited availability of expert annotators or the subjective nature inherent in human annotation processes. . Therefore, we focus on the fundamentally important problem, typically termed Multi-Label Positive-Unlabelled Learning (MLPUL) , which involves learning from a multi-label dataset in which only {a subset of positive classes} is definitely annotated, while all the remaining classes are unknown (which could be positives or negatives). For instance, as shown in {fig:intro}A\&B, human annotators find it hard to completely annotate all the relations due to the confusion of understanding relation definitions and long-context semantics in document-level relation extraction (DocRE) task . Positive and unlabelled (PU) classification has received extensive attention in binary settings, with several recent MLPUL approaches adapting traditional binary PU loss functions to address multi-label classification tasks . These methods typically operate under the assumption that the prior distribution of positive labels can be inferred from fully labeled training samples or closely unbiased estimations. In a specific instance, supposed that the actual positive classes are three times the number of observed labels in the DocRE task, and their model's performance is heavily influenced by the prior ({fig:intro}C). However, estimating the prior distribution of labels in real-world scenarios poses significant challenges, as it is rarely feasible to ensure a comprehensive data set encompassing all label types . Additionally, noted that many long-tail label types tend to be omitted from training annotations. Consequently, we focus on addressing MLPUL without prior knowledge of class distribution. Moreover, MLC generally faces the challenge of imbalanced positive and negative labels, which is severely exacerbated by missing positive class annotations under MLPUL, as shown in {fig:intro}B. Previous works typically adopted the re-balance factor to re-weight the loss functions, containing positive up-weight and negative under-weight . We simply attempt these approaches and find they partly improve the model performance but still perform unsatisfactorily when only a very small set ($10\ Previous works have demonstrated the powerful exploration ability of Reinforcement Learning (RL). Furthermore, RL has shown great success on distant or partial annotations recently { feng2018reinforcement, luo2021pulns, chen2023reinforcement}. Inspired by these successful RL attempts, we believe that the exploratory nature of RL has the potential ability to discover additional positive classes while mitigating the overfitting issues typically encountered in supervised learning, especially when the observed label distribution is severely biased, which holds promise in addressing MLPUL. Besides, recent works have shown that supervised learning can be remarkably effective for the RL process . Based on this intuition, we introduce a novel framework termed Mixture Learner for Partially Annotated Classification (MLPAC), which combines the exploratory capacity of RL in tandem with the exploitation capabilities of supervised learning. Specifically, we design a policy network (as a multi-label classifier) and a critic network, along with two types of reward functions: global rewards calculated by a recall function, which evaluates the all-classes prediction performance for each instance, and local rewards provided by the critic network, which assesses the prediction quality of each individual class for a given instance. The local rewards are expected to narrow the exploration space of traditional RL and offer a preliminary yet instructive signal to guide the learning process, while the global rewards encourage the policy network to explore a broader spectrum of positive classes, consequently mitigating distribution bias stemming from imbalanced labels and incomplete annotations. In addition, inspired by the traditional actor-critic RL algorithm , we iteratively train the policy network and the critic network, which achieves dynamic reward estimation in our setting. The absence of fully annotated samples in both training and validation sets precludes the direct attainment of perfectly accurate rewards. Hence, we introduce label enhancement through collaborative policy and critic network efforts during iterative training, boosting label confidence and enhancing the critic network's reward estimation accuracy. Moreover, our RL framework is concise and flexible, guaranteeing its generalization and adaptation to many tasks. Beyond the experiments on document-level relation extraction task () in Natural Language Process (NLP) field, we also conduct sufficient experiments in multi-label image classification task~() in Computer Vision (CV) field and general PU learning setting in binary case () to verify the generalization and effectiveness of our framework. All experimental results demonstrate the advantage and significant improvement of our framework"
HiRoPE: Length Extrapolation for Code Models Using Hierarchical Position,2403.19115v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2403.19115v2_0.pdf,"Addressing the limitation of context length in large language models for code-related tasks is the primary focus of this paper. Existing LLMs are constrained by their pre-trained context lengths, leading to performance issues in handling long complex code sequences. Inspired by how human programmers navigate code, we introduce Hierarchical Rotary Position Embedding (HiRoPE), a novel approach that enhances the traditional rotary position embedding into a hierarchical format based on the hierarchical structure of source code. HiRoPE offers easy integration into existing LLMs without extra training costs. Our method is extensively evaluated with various LLMs, demonstrating stable performance in tasks such as language modeling and long code completion. We also introduce a new long code understanding task with real-world code projects, in hopes of promoting further development in this code-related field. Theoretically and experimentally, we find that HiRoPE also addresses the out-of-distribution issue in position encoding. Our HiRoPE significantly expands the context length capabilities of LLMs, enabling inference at lengths exponentially greater than the training length.","Illustration of the hierarchical position in source code, such as function-level and token-level positions. We also show a simplified abstract syntax tree of the code in the bottom left corner.","Large language models (LLMs) such as LLaMA-2 , and CodeLLaMA have achieved significant performances in code-related tasks. These Transformer-based models excel in code comprehension and generation but face a notable challenge: the limitation of maximum context length. LLMs are typically pre-trained with a context length ranging from 2k to 16k tokens, which often proves insufficient for complex, extended source code. Exceeding this length limitation during inference may lead to performance degradation for these code models, particularly in tasks like project-level code completion or long code generation. Various methods have been developed to extend the context window of LLMs. Some approaches involve fine-tuning on extensive texts , which can be resource-intensive and potentially lead to overfitting and loss of performance on shorter sequences. There are also some training-free methods . However, these methods usually use window attention rely on local information, and ignore the long dependency in code. It is essential to incorporate {certain structural characteristics} of the code into position encoding to efficiently model these long-distance code dependencies. Our work diverges from these methods by focusing on the hierarchical information of source code in position encoding, inspired by how human programmers navigate code. Traditional positional encoding uses token counts for positioning, and treats code as plain text. However, human programmers often use hierarchical information in the code, representing positions in the code efficiently through multi-level hierarchical positions. We propose a hierarchical position approach that identifies token positions within specific levels, such as functions or statements. Figure shows the comparison of the traditional position and our hierarchical position. It is clear that the hierarchical positional encoding, benefiting from the full utilization of structural information in the code, can more conveniently locate positional information within long code sequences. This method could more effectively model long dependencies in source code. Following such inspirations, we introduce a novel approach, Hierarchical Rotary Position Embedding (HiRoPE), which enhances the popular rotary position embedding (RoPE) into a hierarchical format. HiRoPE differentiates itself by extracting hierarchical information from the source code and splitting the RoPE dimension to represent different hierarchical levels. It simultaneously models token-level relative location and higher-level relative location information. We also add a window mechanism to ensure stability with short texts, aligning with traditional positional encoding. HiRoPE is a plug-and-play solution, easily integrated into existing LLMs without additional training costs. Our extensive experiments with popular LLMs on tasks like language modeling and token completion in long code contexts demonstrate its effectiveness. We compare HiRoPE with existing length extrapolation methods using long code benchmarks such as LCC and RepoBench . We also introduce a new long code understanding task named code symbol understanding with real-world code libraries. Theoretically and experimentally, we find that HiRoPE effectively addresses the out-of-distribution issue in position encoding. Our HiRoPE significantly expands the context length capabilities of LLMs, enabling inference at lengths exponentially greater than the training length. We believe our work with HiRoPE not only addresses a critical length limitation in LLM applications but also opens new avenues for long-structured data modeling research. In summary, we make the following main contributions: {itemize} We propose Hierarchical RoPE (HiRoPE), enhancing the traditional rotary position embedding into a hierarchical format based on the hierarchical structure of source code, providing improved extrapolation capabilities. We conducted comprehensive experiments with LLMs on various long code tasks involving language modeling and code completion. We also introduce a new long code understanding task with real-world code projects, in hopes of promoting further development in this code-related field. We demonstrate that HiRoPE effectively addresses the out-of-distribution issue in position encoding, enabling inference at lengths exponentially greater than the training length. {itemize"
Never Lost in the Middle: Mastering Long-Context Question Answering with Position-Agnostic Decompositional Training,2311.09198v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2311.09198v2_0.png,"While large language models (LLMs) are equipped with longer text input capabilities than before, they struggle to seek correct information in long contexts. The ""lost in the middle"" problem challenges most LLMs, referring to the dramatic decline in accuracy when correct information is located in the middle. To overcome this crucial issue, this paper proposes to enhance the information searching and reflection ability of LLMs in long contexts via specially designed tasks called }osition-}gnostic }ulti-step QA (PAM QA). Trained with this task, our model excels in focusing more precisely on the desired information. Experimental results show substantial improvement in Multi-doc QA and other benchmarks, surpassing state-of-the-art models by a 13.7\% absolute gain in shuffled settings and by 21.5\% in the passage retrieval task. We release our model and code to promote related research in the community. and } %and }","The workflow of PAM QA. The blue dashed lines indicate information flows. The desired output of a sample is composed of three parts, corresponding to three steps: Question repetition, index prediction, and answer summarization. [i] refers to the index of the $i$-th document. An input sample is displayed on the top.","Large Language Models (LLMs), renowned for their exceptional generative and zero-shot learning abilities across diverse natural language processing (NLP) fields, have found extensive downstream applications . However, LLMs suffer from severe hallucinations, significantly compromising their performance in knowledge-oriented QA, dialogue, and writing . Retrieval Augmented Generation (RAG) is an effective solution to hallucinations, and remarkable improvements have been achieved by incorporating supporting knowledge into the input of LLMs . The most fundamental challenge to address in RAG is long context and Multi-document question answering (Multi-doc QA). Some research works around the problem with a complicated pipeline or system , but we aim to improve foundation models as they are a core component of those methods. Thorough research has been conducted to deal with long context inputs, categorized into three mainstreams: The first is to expand the context window using a sliding window . Other researchers proposed to enhance the extrapolation ability by improving the Relative Positional Encoding in Transformers, the backbone of most LLMs . These two kinds of modifications both show substantial improvement in language modelling (LM). The third category of studies focuses on the recurrent compression of memory for long-range sequence learning . This methodology effectively learns the comprehensive representation of context, demonstrating notable proficiency in rapid computation and cost-effectiveness during inference. Though the methods above show strong performance in specific tasks and support LLMs with extra-long context windows, i.e. GPT3.5-Turbo-16K, Claude-v1.3-100K and Longchat , LLMs fail to produce correct answers if related documents are located in the middle of the context, called {{""lost in the middle""}} . It is fatal for Multi-doc QA. However, whether a similar deterioration exists in Chinese LLMs has been unexplored and solutions to this problem have rarely been researched. We hypothesise that the scale of attention scores of the beginning context grows large after pre-training and instruction tuning while that of the middle context, whose position is less trained, remains small for a long distance to the current token. This limits the contribution of related information to the answer and results in lower QA accuracy. To overcome the pitfall, we proposed position-agnostic decompositional training to even up the attention scores over input context. Concretely, we designed a tailored Multi-doc QA task in which positive documents are located at arbitrary positions in contexts among noisy documents. The task presents a significant challenge, compelling the models to extract and summarize information despite the interference of useless ones . As human beings routinely solve complex tasks by decomposition to obtain higher quality outcomes , we modified the Multi-doc QA task as a multi-step reasoning task, called {P}osition-{A}gnostic {M}ulti-step QA ({PAM} QA), combining the Chain-of-Thought (COT, ) and position-agnostic Multi-doc QA. Trained with explicit extraction of the question and the index of supporting documents before generating answers, models learn to distinguish correct information from noisy ones and attend to them. It also forces attention to the question and supporting indexes stronger although the attention scale decays with increasing distance . Empirical results on Multi-doc QA and other benchmarks show that, with only 1/2 or 1/4 context window size, our model improves upon state-of-the-art (SOTA) models by 7.0\ The contribution of this paper is threefold: {itemize} This paper proposed a novel task named PAM QA to tackle the ""lost in the middle"" issue, which is fatal for knowledge-intensive scenarios. To our knowledge, it is the first attempt to solve the problem by training models on special tasks. We investigate the model's behaviour in-depth, revealing that failing to focus on target information may be the cause of ""lost in the middle"". Comprehensive experiments have shown that the proposed PAM QA is effective in solving the ""lost in the middle"" problem. Our model surpasses SOTA in Multi-doc QA and other related tasks on renowned Chinese benchmarks. It is non-trivial that the general QA ability of the model is also strong and satisfying. The model is open-sourced to boost future research in the community. {itemize"
Meta-Tuning LLMs to Leverage Lexical Knowledge for Generalizable Language Style Understanding,2305.14592v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2305.14592v2_0.pdf,"Language style is often used by writers to convey their intentions, identities, and mastery of language. In this paper, we show that current large language models struggle to capture some language styles without fine-tuning. To address this challenge, we investigate whether LLMs can be meta-trained based on representative lexicons to recognize new styles they have not been fine-tuned on. Experiments on 13 established style classification tasks, as well as 63 novel tasks generated using LLMs, demonstrate that meta-training with style lexicons consistently improves zero-shot transfer across styles. We release the code and data at .","Overview of using lexicon-based instructions for cross-style zero-shot classification. It consists of two steps: (1) instruction tuning the model on training styles; (2) evaluating the learned model on unseen target styles zero-shot. A lexicon-based instruction is composed of \textcolor{blue}{instruction}, \textcolor{randClassForMethod}{class names}, \textcolor{subsetLexForMethod}{lexicons} and \textcolor{inputSentForMethod}{an input}.","The style of a text refers to unique ways authors select words and grammar to express their message , providing insights into social interactions and implicit communication. The open-ended and ever-evolving nature of style motivates the need for zero-shot classification, as it is costly to annotate data for every possible style in every language. {black}{Recent large language models (LLMs) and their instruction-tuned variants have achieved notable success in zero-shot learning for diverse tasks using prompting strategies . Yet, their efficacy in style classification has not been thoroughly investigated. As we will show in this paper ( ), style classification remains a challenge for standard LLM prompting.} On the other hand, before the paradigm in NLP shifted to pre-trained language models, lexicons of words that are stylistically expressive were commonly used as important lexical knowledge in rule-based , feature-based , and deep learning models for style identification. Many lexicons have been developed for varied styles, such as politeness , happiness , emotions , etc. This leads to a natural question: {can we leverage lexicons during instruction fine-tuning of LLMs to improve their understanding of language style?} In this paper, we examine the effectiveness of fine-tuning LLMs to interpret lexicons that are provided as inputs to elicit latent knowledge of language styles that were acquired during pre-training. We first compile a benchmark of 13 diverse writing styles with both annotated test sets and style-representative lexicons. Using this benchmark, we show that {meta-tuning with lexicons} enables different pre-trained LLMs to generalize better to new styles that have no labeled data. For example, meta-tuning LLaMA-2-7B on seven styles can improve the average F1 score on a separate set of six held-out styles by 12\ To further verify the capability of LLMs to generalize to novel styles using lexicons as the only source of supervision, we generated a diverse set of 63 unique writing styles with examples ( ) using an approach inspired by self-instruction . We demonstrate that using a small lexicon of as few as five words can effectively improve generalization to new styles. We found it helpful to replace class names with random identifiers when meta-training models with lexicons, which prevents models from ignoring the lexicons and simply memorizing source styles' class names. In addition, we show that when combined with {meta in-context learning} , incorporating lexicons can significantly reduce variance. {black}{In summary, our contributions are: (1) we introduce lexicon-based instructions (), a simple yet effective method for zero-shot style classification leveraging lexical knowledge in LLMs; (2) we show class randomization () can improve generalization capability of lexicon-instructed models significantly (); (3) we provide a benchmark for zero-shot style classification, featuring 13 established tasks () and a synthetic dataset of 63 new tasks (), complete with representative lexicons"
Navigating the Dual Facets: A Comprehensive Evaluation of Sequential Memory Editing in Large Language Models,2402.11122v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.11122v1_0.pdf,"Memory Editing (ME) has emerged as an efficient method to modify erroneous facts or inject new facts into Large Language Models (LLMs). Two mainstream ME methods exist: parameter-modifying ME and parameter-preserving ME (integrating extra modules while preserving original parameters). Regrettably, previous studies on ME evaluation have two critical limitations: (i) {evaluating LLMs with single edit only}, %{single-edit evaluations}, neglecting the need for continuous editing, and (ii) {evaluations focusing solely on basic factual triples}, overlooking broader LLM capabilities like logical reasoning and reading understanding. This study addresses these limitations with contributions threefold: (i) We explore how ME affects a wide range of fundamental capabilities of LLMs under sequential editing. Experimental results reveal an intriguing phenomenon: Most parameter-modifying ME consistently degrade performance across all tasks after a few sequential edits. In contrast, parameter-preserving ME effectively maintains LLMs' fundamental capabilities but struggles to accurately recall edited knowledge presented in a different format. (ii) We extend our evaluation to different editing settings, such as layers to edit, model size, instruction tuning, etc. Experimental findings indicate several strategies that can potentially mitigate the adverse effects of ME. (iii) We further explain why parameter-modifying ME damages LLMs from three dimensions: parameter changes after editing, language modeling capability, and the in-context learning capability. Our in-depth study advocates more careful use of ME in real-world scenarios.","A comparison of two main limitations in previous memory editing evaluations. (a) shows the conventional method, assessing models after each edit, focused solely on the modified knowledge triples. (b) presents our approach, evaluating LLMs after a series of edits to assess their overall impact on various capabilities of LLMs, for a deeper insight into the enduring effects of memory editing.","Memory Editing (ME) was introduced as an effective method to correct erroneous facts or inject new knowledge into Large Language Models (LLMs). Previous ME methods can be roughly divided into two categories: (1) parameter-modifying ME methods, for example, MEND , ROME , and MEMIT , which directly modify a small number of parameters within the model, (2) parameter-preserving ME methods, such as GRACE and MELO , which integrate additional modules into the LLMs architecture without altering the original model parameters. Although ME has shown much promise, previous studies evaluating and analyzing ME methods have two critical limitations, as depicted in Figure . First, they only consider the performance of LLMs after every single editing. However, in practice, LLMs usually need to be edited sequentially, i.e., sequential memory editing, which edits the same model multiple times to incorporate new knowledge continuously. Sequential memory editing is more important in real-world scenarios because new knowledge always appears over time. Second, prior research has predominantly concentrated on assessing ME's impact on factual knowledge. However, it is crucial to evaluate ME's influence on the broader capabilities of LLMs, such as logical reasoning, multilingual proficiency, code generation, and so on. Unfortunately, previous studies on evaluating and analyzing ME tend to overlook these broader aspects, hindering the popularity of ME methods in practical applications. To address these limitations, our study comprehensively evaluates the general capabilities of memory-edited LLMs in sequential editing scenarios. This evaluation involves four distinct ME methods, including three parameter-modifying ME methods - MEND , ROME and MEMIT , and one parameter-preserving ME method - GRACE . We leverage three different checkpoints of {LLaMA-2} , consisting of {LLaMA-2-7B}, {LLaMA-2-7B-Chat} and {LLaMA-2-13B} as base LLMs. The evaluation framework spans six core capabilities of LLMs: Professional Knowledge, Common Sense Knowledge, Logical Reasoning, Reading Understanding, Multilingual Proficiency, and Code Generation, based on eight downstream evaluation benchmarks. The experimental findings reveal varied impacts of the parameter-modifying versus parameter-preserving ME methods on LLMs in sequential editing scenarios. Specifically, all parameter-modifying ME methods systematically damage all fundamental capabilities of LLMs after a few sequential edits. On the contrary, the parameter-preserving ME method, GRACE , effectively maintains the core capabilities of the model even after 100 sequential edits, without any noticeable degradation in the performance across various downstream tasks. However, models edited using GRACE exhibit limited {generalization}, suggesting that the edited model struggles to recall the newly incorporated knowledge when it is presented in a different format. For example, if the edited knowledge is ``who is the CEO of Apple? Tim Cook'', the post-edited model cannot correctly answer the same question described differently - ``Who leads Apple as CEO?'' We then extend our analysis of parameter-modifying ME methods - the ROME and MEMIT, to more editing settings, including increasing the model size, instruction tuning, editing different layers, and the batch size of memory editing. Interestingly, experimental results indicate that larger models show more robustness on multilingual and code-generation tasks, while instruction tuning can alleviate the decline in knowledge QA tasks. Besides, editing deeper layers and increasing the batch size are also beneficial to maintain the general capabilities of LLMs. However, these strategies can not entirely overcome the observed performance decline. Our findings underscore the inherent complexity and challenges of applying ME in the sequential editing setting. To explain how parameter-modifying ME methods damage the general capabilities of LLMs, we further analyze the post-edited models from three aspects: the changes in the model parameters, the language modeling capability, and the in-context learning capability. Experimental findings reveal that with each sequential edit, there is an increasing deviation in the model's parameters from those of the original model. This divergence is identified as the primary cause of noted performance damage. As a result of these parameter shifts, the language modeling capability of post-edited LLMs suffers a noticeable degradation after sequential edits. Interestingly, the post-edited LLMs can maintain the in-context learning capability when editing shallow and deep layers instead of middle layers. Our analysis provides insights into the understanding of parameter-modifying ME methods and sheds light on proposing new strategies to alleviate the damage or new ME methods in the future. In summary, our study makes several pivotal contributions to the field: {itemize} {-2mm} We pioneer a comprehensive evaluation of post-edited LLMs to assess their general capabilities in sequential memory editing scenarios. Our study uniquely covers both types of ME methods and examines their impacts across six core capabilities of LLMs, revealing distinct drawbacks. {-2mm} Our comprehensive experiments suggest that instruction tuning, editing deeper layers, increasing model size, and increasing the batch size of memory editing are beneficial to mitigate the damage caused by the parameter-modifying ME methods, but cannot entirely overcome the adverse effect. {-2mm} We analyze the damage of ME to LLMs in three dimensions: (1) parameter changes, (2) language modeling capability, and (3) in-context learning capability, which partially explains how memory editing influences LLMs, providing insights for the development of new ME methods and mitigation strategies. {itemize"
"LLM-Rubric: A Multidimensional, Calibrated Approach to Automated Evaluation of Natural Language Texts",2501.00274v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2501.00274v1_0.pdf,"This paper introduces a framework for the automated evaluation of natural language texts. A manually constructed rubric describes how to assess multiple dimensions of interest. To evaluate a text, a large language model (LLM) is prompted with each rubric question and produces a distribution over potential responses. The LLM predictions often fail to agree well with human judges---indeed, the humans do not fully agree with one another. However, the multiple LLM distributions can be {combined} to {predict} each human judge's annotations on all questions, including a summary question that assesses overall quality or relevance. accomplishes this by training a small feed-forward neural network that includes both judge-specific and judge-independent parameters. When evaluating dialogue systems in a human-AI information-seeking task, we find that with $$ questions (assessing dimensions such as naturalness, conciseness, and citation quality) predicts human judges' assessment of overall user satisfaction, on a scale of 1--4, with RMS error $< $, a $2$ improvement over the uncalibrated baseline. =-1","An overview of the \llmeval framework. The LLM and its prompts are fixed across texts and judges, but the calibration network weights are trained to predict the responses of various human judges.","Many fields that must assess large numbers of short documents have turned to NLP-assisted workflows. For example, lawyers conducting legal discovery must identify all relevant documents ---a task also faced by journalists and historians. Social scientists and market researchers must code survey responses (; {https://www.enumerate.ai/}{enumerate.ai}; {https://atlasti.com/}{ATLAS.ti}). Teachers or examiners must evaluate student writing and provide feedback . Doctors, social workers, or public health agencies or researchers may assess an individual's mental health or safety from their social media posts or from clinical interviews and assessments . The above settings evaluate human-authored texts. In addition, NLP developers must assess the quality of their machine-generated texts---texts that are consumed by end users, but also hidden intermediate steps in agentic workflows (such as chains of thought, tool calls, and revisions). With the recent commercialization of conversational AI, for example, it is crucial to evaluate dialogue systems during development and monitor them after deployment. Special care is needed in high-stakes settings like medical dialogue . Manual evaluation has long been the gold standard for assessing text, including generated text . Humans are often asked to consider multiple criteria and then provide a final assessment . Humans may also be asked to produce reference answers to which other humans can compare the target text. Yet manual evaluation is expensive, time-consuming, and not without its own quality and reliability issues . Because of these challenges, and the increasing abilities of large language models (LLMs) , experimenters have recently been eliciting ratings directly from an LLM (; {https://chainforge.ai/docs/evaluation/#llm-scorers}{ChainForge}; and others). {But can LLM evaluation be trusted?} It solves the time, scaling, and possibly cost issues, but leaves open the problem of aligning these LLM ratings with human judgments. We present a general approach to this alignment problem. We demonstrate its value for the evaluation and comparison of LLM-powered dialogue systems, in an information-seeking dialogue task similar to . Evaluation in this setting is complex owing to competing factors that might affect a human judge's assessment of the dialogue. These may include correctness of responses, accuracy and helpfulness of citations, length and complexity of responses, and more . Our approach begins with a manually authored evaluation rubric. The rubric's multiple-choice questions cover various evaluation dimensions, and it may also include a question that assesses {overall} quality or relevance. Evaluating a text, such as a dialogue, then consists of two main steps: (1)~for each rubric question we elicit the LLM's probability distribution over possible responses, by prompting it with the text and the rubric question, and (2)~we aggregate and calibrate these distributions with a small feed-forward network that has been trained to match the individual preferences of human judges. A high-level overview of is shown in {fig:llm_rubric}. For research in generative NLP, once the rubric and LLM are fixed, can be used like other metrics ({Bleu}, {Rouge}, etc.) to drive system development, monitor quality, demonstrate the value of a new technique, and conduct competitions. In our dialogue evaluation experiments, each user--AI dialogue is evaluated by 3 trained annotators (randomly drawn from a larger pool) who each answered the same 9 rubric questions. Our method uses these data to train an automatic LLM-based evaluator, without treating the 24 human annotators as interchangeable. Overall, we find{See {tab:main_results}, right side, rows 3, 4, and 6.} that {itemize}[noitemsep] Personalized calibration of an LLM evaluator of overall satisfaction on $<$ {750} synthetic dialogues significantly improves its prediction of human judgments and correlations with human judgments, but still works poorly. Incorporating LLM evaluations of {8} additional criteria () improves these metrics by over $2$ over the uncalibrated LLM. {itemize} Accurate automated text assessment could replace human assessment in many other settings, such as those reviewed at the start of this paper. It could also be used in new settings where human assessment was never feasible. In AI-powered user interfaces, instantaneous scoring of user-written text can feed into downstream decisions such as providing writing feedback or deciding how to proceed with a dialogue. An AI reasoning engine may internally apply a rubric to assess the validity of a proposed natural-language reasoning step . When processing a large document collection, an LLM can be used to assess the compatibility of two text passages , potentially in a more nuanced way than vector similarity; this problem arises in workflows for matching, routing, clustering, and fact-checking (; and the papers just mentioned). Finally, automated assessments could provide signals for {training} text generation . To allow to support all of these use cases, we release general code along with the datasets we created for this paper (see URL on page 1). We discuss limitations at the end of the paper"
LIEDER: Linguistically-Informed Evaluation for Discourse Entity Recognition,2403.06301v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2403.06301v2_0.pdf,"Discourse Entity (DE) recognition is the task of identifying novel and known entities introduced within a text. While previous work has found that large language models have basic, if imperfect, DE recognition abilities , it remains largely unassessed which of the fundamental semantic properties that govern the introduction and subsequent reference to DEs they have knowledge of. We propose the Linguistically-Informed Evaluation for Discourse Entity Recognition (LIEDER) dataset that allows for a detailed examination of language models' knowledge of four crucial semantic properties: , , , and . We find evidence that state-of-the-art large language models exhibit sensitivity to all of these properties except , which demonstrates that they have yet to reach human-level language understanding abilities.",Results for singular continuations by model and comparison type. The dotted lines indicate chance performance and the error bars indicate bootstrapped 95\% confidence intervals.,"One central component of language understanding is the ability to recognize entities in text. A large body of research in Natural Language Processing focuses on the task of Named Entity Recognition, where a system must identify whether a noun phrase (NP), typically a proper name, refers to a known individual of a certain semantic class . The recognition of discourse entities (DEs), in contrast, involves identifying not only the occurrence of known entities but also novel ones that are introduced within a text. The recognition of DEs takes place at two sites: introduction sites and reference sites. Introduction refers to the first time where an entity appears in a discourse. Reference sites are subsequent mentions of an entity that has been previously introduced. As humans, not only are we able to recognize DEs at both of these sites, but we also have knowledge of {how} to coordinate the introduction and subsequent reference to entities using appropriate linguistic means. For example, we know that the introduction of DEs is typically done using indefinite NPs such as {a man} in `{A man walked into the room}.' We also know that subsequent mentions often involve definite NPs like {the man} in `{The man sat down}.' DE recognition is an important component of more complex semantic understanding tasks such as coreference resolution. Coreference relationships cannot be established between entities that have not been introduced into the discourse. [aboveexskip=3pt, belowexskip=3pt]<crex> John owns a dog. The dog is cute. John doesn't own a dog. \#The dog is cute. For example, in ({crex}a), the NP {the dog} in the second sentence and {a dog} in the first sentence refer to the same entity. This is not the case in ({crex}b) because no entities have been introduced in the first sentence, which makes the continuation in ({crex}b) infelicitous. Therefore, before establishing coreference relationships, language models first need to perform DE recognition, present an evaluation suite for DE recognition that focuses on the question of whether language models are sensitive to the linguistic context in which DEs are introduced. They find that transformer-based language models do not always demonstrate a clear preference for referring to entities that have been properly introduced into the discourse. While this work provides important insight into LM abilities with discourse reference, it does not engage directly with the underlying linguistic properties responsible for DE introduction and reference. As a result, it does not provide a means of assessing more precisely what LMs know about the linguistic encoding of discourse reference. Semantics research has established properties of definite and indefinite NPs from which their use in introducing and referring to entities follows, four of which are particularly relevant here: {existence}, {uniqueness}, {plurality}, and {novelty}. We will define and discuss these properties in detail in Section . A good language model (LM) should reflect knowledge of all of these properties. In this paper, we provide a novel dataset, which builds on Schuster and Linzen's work, that provides a method of testing these properties directly.{All code, data, and results are available at {https://github.com/xiaomeng-zhu/LIEDER}.} Our results, across a number of state-of-the-art (SOTA) large language models (LLMs), provide evidence for knowledge of {existence}, {uniqueness}, and {plurality} (all conditions on the use of definite NPs to refer to DEs), but difficulty with {novelty} (a condition on the introduction of DEs by indefinite NPs) unless information about distinctiveness is made explicit. In addition, we find that transformer LMs, unlike humans, show strong sensitivity to linear distance in establishing DE reference. Taken together, these results suggest that SOTA LLMs do not reach human-level language understanding abilities"
NEO-BENCH: Evaluating Robustness of Large Language Models with Neologisms,2402.12261v4,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.12261v4_0.pdf,"The performance of Large Language Models (LLMs) degrades from the temporal drift between data used for model training and newer text seen during inference. One understudied avenue of language change causing data drift is the emergence of neologisms -- new word forms -- over time. We create a diverse resource of recent English neologisms by using several popular collection methods. We analyze temporal drift using neologisms by comparing sentences containing new words with near-identical sentences that replace neologisms with existing substitute words. Model performance is nearly halved in machine translation when a single neologism is introduced in a sentence. Motivated by these results, we construct a benchmark to evaluate LLMs' ability to generalize to neologisms with various natural language understanding tasks and model perplexity. Models with later knowledge cutoff dates yield lower perplexities and perform better in downstream tasks. LLMs are also affected differently based on the linguistic origins of words, indicating that neologisms are complex for static LLMs to address. We release our benchmark at: {https://github.com/JonathanQZheng/NEO-BENCH}.",{\sc Neo-Bench} collects neologisms from 2020-2023 for LLM evaluation. ``Pig Butchering'' originated as a Mandarin expression \begin{CJK*}{UTF8}{gbsn}(杀猪盘)\end{CJK*}.,"Neologisms -- recent word forms representing a new meaning, sense, or connotation -- consistently surface as language changes. Neologisms emerge to describe the ever-changing state of the world, such as new terms created during the COVID-19 pandemic. While humans easily adapt to language change, large language models (LLMs) struggle with the misalignment of training data and new test data distributions . Prior work on temporal language change observed model degradation when finetuning on older text and evaluating on newer data and named entities . However, as far as we are aware there has not been prior work that analyzes the robustness of LLMs on handling neologisms. We show that adding a neologism to text decreases machine translation quality by an average of 43\ In this paper, we present { Neo-Bench}, a new benchmark designed to test the ability of LLMs to understand and process neologisms. We combine multiple methods and online text corpora to collect a diverse set of 2,505 neologisms based on the linguistic taxonomy devised by : (i) {lexical neologisms} -- words representing new concepts, e.g., {``long covid''}; (ii) {morphological neologisms} -- blends of existing subwords, e.g., {``doomscrolling''}; and (iii) {semantic neologisms} -- existing words that convey a new meaning or sense, e.g., {``ice''} (a term that refers to petrol- or diesel-powered cars taking electric car charging spots). We estimate word prevalence over time with Google Trends to obtain trending neologisms. We also create 4 benchmark tasks to evaluate the impact of neologisms on LLMs with Perplexity, Cloze Question Answering, Definition Generation, and Machine Translation. We show that lower neologism perplexities correlate with higher downstream task performance. Older LLMs -- BART, T5, GPT-J, and Flan-T5 -- perform much worse with an average of 32.20\ { Neo-Bench} evaluates a diverse set of LLM capabilities on handling neologisms in various tasks. Models must also understand compositionality for morphological neologisms, differentiate between word senses for semantic neologisms, and handle different contexts for lexical neologisms. {comment} In summary, our contributions include: {itemize}[noitemsep, topsep=0pt] {}{0em} We motivate the study of neologisms by demonstrating significant performance degradation in machine translation (). A new corpus of diverse neologisms and benchmark to evaluate LLM robustness to neologisms () is presented. Analysis is presented, based on our benchmark, shedding light on the ability of LLMs to understand and process neologisms in (), in addition to analysis based on a linguistic taxonomy of neologisms. {itemize} {comment"
What is the Best Way for ChatGPT to Translate Poetry?,2406.03450v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.03450v1_0.pdf,"Machine translation (MT) has historically faced significant challenges when applied to literary works, particularly in the domain of poetry translation. The advent of Large Language Models such as ChatGPT holds potential for innovation in this field. This study examines ChatGPT's capabilities in English-Chinese poetry translation tasks, utilizing targeted prompts and small sample scenarios to ascertain optimal performance. Despite promising outcomes, our analysis reveals persistent issues in the translations generated by ChatGPT that warrant attention. To address these shortcomings, we propose an Explanation-Assisted Poetry Machine Translation (EAPMT) method, which leverages monolingual poetry explanation as a guiding information for the translation process. Furthermore, we refine existing evaluation criteria to better suit the nuances of modern poetry translation. We engaged a panel of professional poets for assessments, complemented evaluations by using GPT-4. The results from both human and machine evaluations demonstrate that our EAPMT method outperforms traditional translation methods of ChatGPT and the existing online systems. This paper validates the efficacy of our method and contributes a novel perspective to machine-assisted literary translation.",Comparison between the framework of the traditional translation method and the proposed Explanation-Assisted Poetry Machine Translation (EAPMT).,"Foreign poems translated into Chinese still have to be like poems.} {flushright}—— {flushright} Poetry translation is widely regarded as one of the most challenging tasks in the field of translation. When translating a foreign poem into Chinese, the resulting text should still be recognizable as a poem. In discussing poetry, we often refer to specific genres or styles . Unlike classical poetry, the term ``modern'' in modern poetry refers to the poetic styles of the 20th and 21st centuries, which represent a significant departure from traditional forms. The primary characteristic of modern poetry is its embrace of freedom and lack of restrictions . Modern poetry is specifically characterized by open forms, diverse genres, a break from conventional narratives, and innovative language combinations . Unlike classical poetry, rhythm is no longer an essential feature of modern poetry. Consequently, when translating modern poetry, it is not necessary to adhere to the original poem's rhythm . However, the poeticity must not be overlooked; the poetic essence of the source poem must be preserved throughout the translation process . For Chinese poetry, there are significant differences between various genres. Classical Chinese poetry is characterized by strict constraints on format, meter, sentence length, and rhyme. In contrast, modern poetry is free from these constraints and breaks away from the rigid structures of classical poetry . Previous work has successfully applied machine translation to poetry, but these poems typically have clear format or rhyme restrictions , which differ significantly from modern Chinese poetry. Recent research has taken an innovative approach by first obtaining an initial translation of the input prose using traditional Neural Machine Translation (NMT) methods. This initial translation is then mapped to a set of masked sequences via a designed heuristic method. Finally, these sequences are used to generate poetry translations through a pre-trained Masked Language Modeling (MLM) technique . Another study compared the differences between machine translation and human translation of Arabic poetry into English . The authors concluded that machine translation is not suitable for translating Arabic poetry into English as it fails to comprehend the socio-cultural background of poetry creation and the contextual nuances, particularly the genre-specific elements. Promisingly, the artificial intelligence chatbot ChatGPT, released by OpenAI, has demonstrated excellent performance across various tasks and domains, including translation tasks . Although previous work has studied the performance of ChatGPT on translation tasks , and recent studies have explored the application of ChatGPT to poetry-related tasks, these investigations primarily focused on poetry generation. For instance, recent research examined the effectiveness of ChatGPT-4 in generating Arabic poetry and found the results to be unsatisfactory . The study highlighted several issues with the text generated by ChatGPT-4, including poor language quality, superficial content, lack of emotion, inconsistent speech, inappropriate word choices, and an ease of recognition by human evaluators. Unlike previous work, this paper focuses on the capabilities of ChatGPT in translating English poetry into modern Chinese poetry. We explored optimal strategies for utilizing ChatGPT to translate poetry and evaluated its maximum performance in this specific task. Inspired by and , we investigated ChatGPT's performance on modern poetry translation tasks by designing appropriate prompts and providing example shots for the model. Experimental results demonstrate the effectiveness of our designed prompts. Despite these promising outcomes, our analysis reveals persistent issues in the translations generated by ChatGPT that warrant further attention. Consequently, we propose a new poetry translation method called Explanation-Assisted Poetry Machine Translation (EAPMT). Our method leverages the explanation of monolingual poetry as guidance information to achieve high-quality translations from English poetry to modern Chinese poetry. Furthermore, existing evaluation criteria are typically designed for ordinary texts or poems with specific restrictions and are not fully applicable to modern poetry. Therefore, we refined these criteria to better capture the nuances of contemporary poetry translation. We engaged a panel of professional poets for assessments and complemented their evaluations with those conducted using GPT-4. The results from both human and machine evaluations demonstrate that our EAPMT method outperforms traditional translation techniques of ChatGPT and existing online systems. The contributions of our work are as follows: {itemize}[itemsep=0pt] We are the first to examine ChatGPT's capabilities in English-Chinese modern poetry translation tasks. We construct and release a high-quality bilingual poetry dataset. We identify the optimal prompts and examples (shots) for ChatGPT to effectively translate poetry. We propose a novel method for poetry translation that uses monolingual poetry explanations as guiding information. This method significantly enhances ChatGPT's performance in translating modern poetry and can be extended to other language pairs and models of language understanding and generation. We design a new framework for human evaluation criteria specifically applicable to modern poetry translation and engage several professional poets to evaluate the translation results. {itemize"
Representation Learning with Conditional Information Flow Maximization,2406.05510v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.05510v2_0.png,"This paper proposes an information-theoretic representation learning framework, named conditional information flow maximization, to extract noise-invariant sufficient representations for the input data and target task. It promotes the learned representations have good feature uniformity and sufficient predictive ability, which can enhance the generalization of pre-trained language models (PLMs) for the target task. Firstly, an information flow maximization principle is proposed to learn more sufficient representations for the input and target by simultaneously maximizing both input-representation and representation-label mutual information. Unlike the information bottleneck, we handle the input-representation information in an opposite way to avoid the over-compression issue of latent representations. Besides, to mitigate the negative effect of potential redundant features from the input, we design a conditional information minimization principle to eliminate negative redundant features while preserve noise-invariant features. Experiments on 13 language understanding benchmarks demonstrate that our method effectively improves the performance of PLMs for classification and regression. Extensive experiments show that the learned representations are more sufficient, robust and transferable.",Venn information diagram comparison of our CIFM with existing principles. The learned representations by each principle is circled by the red dashed line.,"The goal of deep representation learning is to transform the raw observational data into low-dimensional representations that are essential for various downstream tasks. In recent years, information-theoretic representation learning has been widely studied, aiming to discover useful representations in a principled manner. The InfoMax principle has extensive applications in the field of self-supervised representation learning . In supervised scenarios, minimizing the standard cross-entropy is actually equivalent to maximizing the mutual information between the representations and the target task . But InfoMax tends to preserve potential redundant features that are irrelevant to the given target, leading to biased representations. Another noteworthy line of information-theoretic research is built upon the information bottleneck (IB) principle , which aims to discover compact and informative representations that can reduce redundant features from the inputs . IB seeks to find a maximally compressed representation of the input that preserves as much information as possible about the target, striking a balance between compression and prediction. However, in the information flow of neural networks, directly reducing the mutual information between the input $X$ and representations $Z$ would violate the sufficiency constraint, and may lose the necessary information for the target task $Y$. Under the Markov chain constraint $Y X Z$, it's hard to determine beforehand how close we are to optimal compression, and this can easily lead to the over-compression issue of latent representations . As a result, current IB-based methods would yield insufficient representations for the target task, and hamper prediction ability of neural networks. To ensure sufficiency for the target task and mitigate the negative effect of redundant features, we propose a principled representation learning framework, named conditional information flow maximization (CIFM), to extract noise-invariant sufficient representations for the input and target. It promotes the learned representations have good feature uniformity and sufficient predictive ability, which can enhance the generalization of pre-trained language models (PLMs) for the target task. Firstly, we propose an information flow maximization (IFM) principle to learn more sufficient representations for the input and target. It simultaneously maximizes both input-representation and representation-label mutual information. Maximizing input-representation information $I(X;Z)$ ensures sufficiency of the representations for the input $X$ and preserves information relevant to the target $Y$, and maximizing representation-label information $I(Y;Z)$ captures necessary information relevant to the target $Y$. In this way, the learned representations can be of good uniformity and sufficient predictive ability. Unlike IB that minimizes input-representation information, we handle the information in an opposite way to avoid the over-compression issue of latent representations. Besides, we design a conditional information minimization (CIM) principle to mitigate the negative effect of potential redundant features from the input. In the information flow of $X Z$, InfoMax may introduces excessive and noisy information . For IFM, the task-irrelevant redundant nuisances features obtained by maximizing $I(X;Z)$ interfere with the optimization of maximizing $I(Y;Z)$. There are spurious correlations among these redundant features, forcing the model to learn a biased representation $Z$. As a conditional regularization term for IFM, the CIM principle eliminates negative redundant features while preserves noise-invariant features from inputs. Under the IFM principle with the conditional regularization, CIFM can extract noise-invariant sufficient representations for the input and target. We conduct experiments on 13 natural language understanding benchmarks. The results demonstrate that CIFM can significantly improve the performance of PLMs for classification and regression. Our CIFM framework consistently achieves the best average performance compared to other methods, including 4 universal models and 7 representative deep representation learning technologies under different backbone models. For instance, with the RoBERTa backbone, CIFM improves average performance by {+3.8\ Extended experiments prove that CIFM can enhance the model's generalization including out-of-distribution and data-constrained scenarios, robustness to random and adversarial noise, and transferability to new tasks. And the results also indicates that the learned representations by CIFM are more sufficient, robust and transferable. The contributions are as follows: 1) we propose an information flow maximization principle to learn more sufficient representations for the input and target by simultaneously maximizing both input-representation and representation-label information. 2) We design a conditional information minimization principle to eliminate negative redundant features while preserve noise-invariant features from the input. 3) We present an information-theoretic CIFM framework to learn noise-invariant sufficient representations for the input and target. It can enhance the generalization of PLMs for better language understanding. 4) Experiments on 13 language understanding benchmarks demonstrate that CIFM achieves better performance under different backbone models. Extensive experiments show that the learned representations are more sufficient, robust and transferable.{The source code is available at {https://github.com/zerohd4869/CIFM"
GPT is Not an Annotator: The Necessity of Human Annotation in Fairness Benchmark Construction,2405.15760v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2405.15760v1_0.png,"Social biases in LLMs are usually measured via bias benchmark datasets. Current benchmarks have limitations in scope, grounding, quality, and human effort required. Previous work has shown success with a community-sourced, rather than crowd-sourced, approach to benchmark development. However, this work still required considerable effort from annotators with relevant lived experience. This paper explores whether an LLM (specifically, GPT-3.5-Turbo) can assist with the task of developing a bias benchmark dataset from responses to an open-ended community survey. We also extend the previous work to a new community and set of biases: the Jewish community and antisemitism. Our analysis shows that GPT-3.5-Turbo has poor performance on this annotation task and produces unacceptable quality issues in its output. Thus, we conclude that GPT-3.5-Turbo is not an appropriate substitute for human annotation in sensitive tasks related to social biases, and that its use actually negates many of the benefits of community-sourcing bias benchmarks.","Comparison of human post-evaluation results for GPT-WinoQueer (blue, left) and GPT-WinoSemitism (orange, right) datasets. GPT-WQ evaluation results are generally worse, with a lower proportion of correct responses and higher proportions of grammatically incorrect, opposite, and hallucinated responses.","Though seemingly ubiquitous, large language models (LLMs) still treat users unequally and exhibit harmful social biases .{Clear and explicit definitions of the terms {bias} and {harm} are essential for productive discussion of AI fairness . For purposes of this paper, we define {bias} as ``substantially differing treatment of a marginalized group relative to a dominant group that replicates existing social stereotypes about the marginalized group'' and {harm} as ``physical, psychological, financial, or professional events that affect a person in perceived negative way.'' } Quantitative LLM bias measurement is a necessary first step to understanding and mitigating bias-related harms of AI systems. Measurement is essential because it allows model creators to understand potential fairness issues with their models, downstream users to compare models and choose those that are relatively fair in their use context, and fairness researchers to determine whether debiasing methods are effective. The current standard for bias measurement in LLMs is paired sentence bias benchmarks, which consist of pairs of similar sentences and generally rely on comparing the model's probability of predicting the stereotypical sentence to the probability of predicting a contrasting sentence. There are significant quality and grounding issues with most current benchmarks, especially those developed via crowd-sourcing. Current methods for community-sourced benchmark development, which mitigate some of the problems with crowd-sourcing, require significant human effort for annotation of survey responses. This work is time-consuming; in an unfunded, community-led benchmark development effort, this is either cost-prohibitive or requires asking annotators to work for free. This annotation also places a significant psychological burden on annotators. In order to maintain the usefulness and participatory nature of community-sourced bias benchmarks while reducing the financial and psychological costs, we tested {model-assisted harm extraction.} The main contributions of our work are as follows:{Our code and data are available at {https://github.com/katyfelkner/winosemitism}} {itemize} We introduce WinoSemitism, a community-sourced benchmark for antisemitism, generalizing method of . We create GPT-WinoQueer and GPT-WinoSemitism, which are versions of the WQ and WS datasets annotated by GPT-3.5-Turbo instead of human experts. We provide a thorough quantitative and qualitative comparison of human-annotated and model-annotated datasets, finding significant quality issues with model annotations. {itemize"
Multi-modal Preference Alignment Remedies Degradation of Visual Instruction Tuning on Language Models,2402.10884v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.10884v2_0.png,"Multi-modal large language models (MLLMs) are expected to support multi-turn queries of interchanging image and text modalities in production. However, the current MLLMs trained with visual-question-answering (VQA) datasets could suffer from degradation, as VQA datasets lack the diversity and complexity of the original text instruction datasets with which the underlying language model was trained. To address this degradation, we first collect a lightweight, 5k-sample VQA preference dataset where answers were annotated by Gemini for five quality metrics in a granular fashion and investigate standard Supervised Fine-tuning, rejection sampling, Direct Preference Optimization (DPO) and SteerLM algorithms. Our findings indicate that with DPO, we can surpass the instruction-following capabilities of the language model, achieving a 6.73 score on MT-Bench, compared to Vicuna's 6.57 and LLaVA's 5.99. This enhancement in textual instruction-following capability correlates with boosted visual instruction performance (+4.9\% on MM-Vet, +6\% on LLaVA-Bench), with minimal alignment tax on visual knowledge benchmarks compared to the previous RLHF approach. In conclusion, we propose a distillation-based multi-modal alignment model with fine-grained annotations on a small dataset that restores and boosts MLLM's language capability after visual instruction tuning.","From a visual-instruction-tuned pre-trained model, we generate 4 completions for a given image-question prompt. These answers are then presented to Gemini to obtain granular annotations given a labeling guide. We construct a preference dataset of (image-text prompt, preferred completion) and (image-text prompt, rejected completion). We benchmarked DPO, Rejection sampling, and SteerLM alignment methods, in addition to a pure SFT baseline using Gemini-provided answers directly.","Recent advancements in artificial intelligence have led to the rise of multi-modal large language models (MLLMs), which combine textual and visual interpretation capabilities in a single model . However, effectively blending multi-modality in one system has proven non-trivial. Integrating diverse data forms often creates internal representation conflicts, giving rise to the issue known as ""catastrophic forgetting"" . The diversity constraint in visual question answering (VQA) datasets could be attributed as a source of the issue. VQA tasks typically focus on descriptive queries about image contents, whereas textual datasets encompass a broader range of complex cognitive tasks, including reasoning, writing, summarization, and coding. This discrepancy in dataset complexity is a key factor contributing to the observed performance degradation in MLLMs. Our evaluation of models such as BLIP-2, InstructBLIP, and LLaVA against language instruction-following benchmarks like MT-Bench and AlpacaEval revealed diminished language capabilities in comparison to their linguistic backbones. For instance, LLaVA, built on the Vicuna-13b LLM, demonstrated a decline in MT-Bench performance from 6.57 to 5.92, even underperforming the Vicuna-7B model. Driven by the limitations observed in distillation-based instruction tuning, particularly its constrained generalizability and the narrow performance improvements on tasks outside the training distribution, this study investigates the efficacy of distillation-based preference alignment in addressing modality conflict in MLLMs. The decision to explore this avenue is predicated on the hypothesis that integrating AI-generated preference data can provide a more granular and nuanced alignment with human expectations, potentially mitigating the adverse effects of modality conflict. This study rigorously evaluates three baseline methodologies—Direct Preference Optimization (DPO), SteerLM, and Rejection Sampling—as potential solutions to utilize the distilled preference data to enhance the instruction-following capabilities and address the modality conflict inherent in MLLMs. Each of these methods offers a unique approach to model alignment, from the direct optimization of preferences in DPO to the conditional supervision in SteerLM and the selective acceptance in Rejection Sampling. Our empirical analysis reveals that DPO, in particular, demonstrates a pronounced efficacy in reconciling the performance disparities observed between textual and visual modalities. By leveraging a refined preference dataset, fine-tuned with the DPO objective and supplemented with comprehensive annotations from advanced AI models, DPO not only addresses the modality conflict but also significantly enhances the model's performance across a spectrum of benchmarks. The results indicate that, through the application of DPO, MLLMs can achieve a more robust alignment with human-like preferences, thereby mitigating the adverse effects of catastrophic forgetting and modality conflict and elevating the models' capabilities to a level that surpasses traditional instruction tuning methods. Our main contributions are: {Exploration of Modality Degradation:} This work is the first to identify and address modality degradation in MLLMs, a phenomenon where visual instruction tuning detrimentally impacts language instruction capabilities. Our systematic investigation into this issue contributes novel insights to the field, laying the groundwork for further research in mitigating such degradation. {Efficient and scalable preference alignment pipeline as remedy} Our data collection strategy employs a granular quality metric annotation format, leveraging cost-effective commercial APIs. This scalable approach enables the efficient production of high-quality datasets. We are able to surpass LLaVA and Vicuna's language instruction-following capability with DPO on a 6k dataset"
Multistage Collaborative Knowledge Distillation from a Large Language Model for Semi-Supervised Sequence Generation,2311.08640v4,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2311.08640v4_0.jpg,"We study semi-supervised sequence generation tasks, where the few labeled examples are too scarce to finetune a model, and meanwhile, few-shot prompted large language models (LLMs) exhibit room for improvement. In this paper, we present the discovery that a student model distilled from a few-shot prompted LLM can commonly generalize better than its teacher to unseen examples on such tasks. We find that the student is able to learn a general pattern from the high-quality pseudolabels produced by the teacher during knowledge distillation (KD), and favorably not a general pattern from the low-quality pseudolabels. Leveraging this discovery, we propose a new method, ~(), for these tasks. first few-shot prompts an LLM to produce pseudolabels for unlabeled data. Then at each stage of an iterative KD process, a new pair of students is trained on disjoint partitions of the pseudolabeled data, and produces new and improved pseudolabels for their unseen partitions. We conduct extensive experiments on four syntactic and semantic parsing datasets and show the effectiveness of \ for low-resource semi-supervised sequence generation. On CRAFT biomedical parsing, for example, 3-stage with 50 labeled examples outperforms an LLM teacher and vanilla KD by 7.5\% and 3.7\% parsing F1, respectively, and matches the performance of supervised finetuning with 500 labeled examples.~{github.com/andotalao24/Multistage-Collaborative-Knowledge-Distillation}.}%under an Apache-2.0 license.","Overview of \ours. {(1)} We use demonstrations from labeled data $\mathcal{D}^\text{labeled}$ to few-shot prompt an LLM teacher $t$ to produce pseudolabels for unlabeled data $\mathcal{D}^\text{unlabeled}$. We partition $\mathcal{D}^\text{unlabeled}$ into $\mathcal{D}^\text{unlabeled}_A$ and $\mathcal{D}^\text{unlabeled}_B$, and let $\mathcal{D}^{(x, \hat{y}_0)}_A$ and $\mathcal{D}^{(x, \hat{y}_0)}_B$ denote the same partitions but with teacher-generated pseudolabels. {(2)} At the $i$-th intermediate KD stage, students $s_A^i$ and $s_B^i$ are trained on previously pseudolabeled data $\mathcal{D}_A^{(x,\hat{y}_{i-1})}$ and $\mathcal{D}_B^{(x,\hat{y}_{i-1})}$, respectively, and leveraged to label the other partitions $\mathcal{D}_B^\text{unlabeled}$ and $\mathcal{D}_A^\text{unlabeled}$ and produce $\mathcal{D}_B^{(x,\hat{y}_i)}$ and $\mathcal{D}_A^{(x,\hat{y}_i)}$, which will be used to train the next-stage student(s). {(3)} In the final KD stage, a single final student $s^n$ is trained on both latest pseudolabeled partitions $\mathcal{D}^{(x, \hat{y}_{n-1})}_A$ and $\mathcal{D}^{(x, \hat{y}_{n-1})}_B$.","Low-resource tasks are common in real life, including within specialized domains, since data annotation often requires expert knowledge and incurs significant costs. Semi-supervised learning has been proposed as a solution when abundant unlabeled data are available. In a typical application, a model is trained on limited labeled data and produces pseudolabels for unlabeled data. The pseudolabeled data are then filtered according to confidence thresholds and used to train a new model. In more extreme few-shot cases, labeled data are too scarce to finetune a model to begin with. Large language models (LLMs) offer a useful mechanism for synthesizing pseudolabels, thanks to their remarkable ability to learn {in context} from only a handful of demonstrations . Smaller and faster models can then be trained for these tasks using knowledge distillation (KD) from LLMs. =-1 In this paper, we study a challenging semi-supervised sequence generation setting where labeled data are too few to finetune a model and few-shot prompted LLMs exhibit room for improvement. These will happen when the task is both expensive to annotate and under-represented in the pretraining of off-the-shelf LLMs. For example, it took 80 annotators around 2.5 years to parse 20k sentences of biomedical text in the CRAFT corpus. Meanwhile, pretrained LLMs do not always excel at tasks in specialized domains and tasks that involve specialized structured outputs, {}, semantic or syntactic parsing. The overarching research question of this paper is whether LLMs can still be leveraged in such scenarios to develop strong prediction models. To this end, we examine knowledge distillation (KD) from a few-shot prompted LLM to a much smaller model. We discover that the student can commonly outperform its LLM teacher on {unseen evaluation data} from such tasks. Our analysis reveals that the student can learn a general pattern from high-quality pseudolabels from the teacher, while helpfully failing to capture a general pattern for the low-quality ones due to their noisy nature. This discovery is encouraging because it opens up the possibility for leveraging the KD students as teachers for further distillation. =-1 Leveraging the discovery, we propose a novel method, Multistage Collaborative KD from an LLM~(), for semi-supervised sequence generation. {} first collects pseudolabels for a large amount of unlabeled data from a few-shot prompted LLM, bootstraping a multistage KD process. At each KD stage, a new pair of students is trained on distinct partitions of the pseudolabeled data and asked to produce pseudolabels for the data that they have not been trained on. This process improves upon vanilla KD through two core mechanisms: (1) cross-partition labeling at each KD stage, which splits all pseudolabeled data into two mutually exclusive partitions to train a {collaborative} pair of students and leverages their strong generalization capabilities to relabel the data, and (2) {multistage} KD, whereby student models continue to generalize better and produce higher quality pseudolabels than their teachers over multiple KD stages. \ outperforms the LLM teacher and the relevant KD baselines in our evaluation, and is competitive with supervised finetuning with many more labeled data. On CRAFT constituency parsing, for example, {} with 50 labeled examples outperforms the LLM and vanilla KD by 7.5\ On ATIS semantic parsing, {} with 50 labeled examples outperforms the LLM and vanilla KD by 7.2\ The following is a summary of our contributions: {itemize}[leftmargin=10pt,topsep=0pt,noitemsep] We study if LLMs can be leveraged to train fast and accurate student models for semi-supervised sequence generation tasks where the LLM itself exhibits room for improvement when prompted with in-context learning examples. We find that KD students can often achieve better generalization than their LLM teachers (Section ). We propose {}, a novel KD-based solution for such tasks. Data partitioning and cross-partition labeling enable {} to gradually improve the quality of pseudolabels over multiple stages of distillation, yielding increasingly better generations of students (Section ). {} substantially outperforms prompted LLM, finetuned, and KD baselines on multiple low-resource tasks (Section -). Further analyses show that: ({a})~{} students correct many of the teacher's errors, and ({b})~{} scales well with increasing amounts of available unlabeled data (Section -). {itemize"
How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs,2401.06373v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2401.06373v2_0.pdf,"Most traditional AI safety research has approached AI models as machines and centered on algorithm-focused attacks developed by security experts. As {large language models} (LLMs) become increasingly common and competent, non-expert users can also impose risks during daily interactions. This paper introduces a new perspective on jailbreaking LLMs as human-like communicators to explore this overlooked intersection between everyday language interaction and AI safety. Specifically, we study how to persuade LLMs to jailbreak them. First, we propose a persuasion taxonomy derived from decades of social science research. Then we apply the taxonomy to automatically generate interpretable {persuasive adversarial prompts} (PAP) to jailbreak LLMs. Results show that persuasion significantly increases the jailbreak performance across all risk categories: PAP consistently achieves an attack success rate of over $92\%$ on Llama 2-7b Chat, GPT-3.5, and GPT-4 in $10$ trials, surpassing recent algorithm-focused attacks. On the defense side, we explore various mechanisms against PAP, find a significant gap in existing defenses, and advocate for more fundamental mitigation for highly interactive LLMs{Y. Zeng}, {W. Shi}, {R. Jia}}{ We have informed Meta and OpenAI of our findings. For safety concerns, we only publicly release our persuasion taxonomy at {}. Researchers can apply for the jailbreak data upon review.}.","We propose a persuasion taxonomy with persuasion techniques, and apply it to automatically paraphrase plain harmful queries into human-readable persuasive adversarial prompts (PAP). This method achieves an attack success rate of over {92\%} on Llama-2, GPT-3.5, and GPT-4 {without specialized optimization. }","Significant advancements in {large language models} (LLMs), such as Meta's Llama-2 and OpenAI's GPT series , mark a leap forward in AI. However, it remains challenging to safely integrate these models into the real world. AI safety research has largely focused on algorithmic jailbreak methods like optimization-based , side-channel-based , and distribution-based approaches . But these methods often generate hard-to-interpret prompts and overlook risks involved in natural and human-like communication with millions of non-expert users, which is a key aspect of these deployed LLMs. Persuasion is ubiquitous in everyday communication . Notably, persuasion starts early in life -- even two-year-olds can employ persuasion to some extent to influence family members . So naturally, during interactions with LLMs, users may also try to persuade LLMs to jailbreak them, whether intentionally or unconsciously. For instance, the well-known ``grandma exploit'' example shared by a Reddit user{ {https://www.reddit.com/r/ChatGPT/comments/12sn0kk/grandma_exploit}}, uses a common persuasion technique called ``emotional appeal'', and successfully elicits the LLM to provide a recipe to make a bomb. Previous safety studies, like those outlined in and explored in , have touched on such social engineering risks in LLMs. But they mainly focus on unconventional communication patterns like virtualization that explicitly creates an imaginary scene (e.g., ``The following scenario takes place in a novel...'') or role-playing that asks LLM to behave like certain related persona ({e.g., ``You are a cybersecurity expert...''}). Despite being human-readable, these methods still essentially treat LLMs as mere instruction followers rather than human-like communicators that are susceptible to nuanced interpersonal influence and persuasive communication. Therefore, they fail to cover the impact of human persuasion (e.g., emotional appeal used in grandma exploit) in jailbreak. Moreover, many virtualization-based jailbreak templates are hand-crafted{{https://www.jailbreakchat.com/}}, tend to be ad-hoc, labor-intensive, and lack systematic scientific support, making them easy to defend but hard to replicate. In contrast, our work, as shown in Figure~, introduces a taxonomy-guided approach to systematically generate human-readable {persuasive adversarial prompts} (PAP), to advance the understanding of risks associated with human-like communication. The persuasion taxonomy aims to bridge gaps between social science and AI safety research and sets a precedent for future research to better study safety risks that everyday users could invoke. In this paper, we aim to answer the question {how LLMs would react to persuasive adversarial prompts} via the following contributions: {1.}{{myblue}{{108}}} {Persuasion Taxonomy ($$):} We first introduce a persuasion technique taxonomy as the foundation for further experiments, and establish the first link between decades of social science research and AI safety. Besides AI safety, the taxonomy is also a useful resource for other domains like NLP, computational social science, and so on. {1.}{{myblue}{{108}}} {{Persuasive Paraphraser} Building ($$):} Then we discuss how to ground on the proposed taxonomy to build a {Persuasive Paraphraser}, which will paraphrase plain harmful queries to interpretable PAP automatically at scale to jailbreak LLMs. {1.}{{myblue}{{108}}} {Broad Scan ($$):} In the first jailbreak setting, we use the developed {Persuasive Paraphraser} to generate PAP and scan 14 policy-guided risk categories to assess the effect of persuasion techniques and their interplay with different risk categories. {1.}{{myblue}{{108}}} {In-depth Iterative Probe ($$):} In real-world jailbreaks, users will refine effective prompts to improve the jailbreak process. So after identifying successful PAP in the broad scan step, we mimic human users and fine-tune a more targeted {Persuasive Paraphraser} on these successful PAP, to refine the jailbreak. Then we iteratively apply different persuasion techniques to generate PAP and perform a more in-depth probe on LLMs. This approach yields an over $92\ {1.}{{myblue}{{108}}} {Defense Analysis ($$):} After the jailbreak studies, we evaluate recent post-hoc defenses against our persuasive jailbreak method and uncover a significant gap in their effectiveness against PAP, emphasizing the inadequacy of current mitigation. {1.}{{myblue}{{108}}} {Defense Exploration ($$):} Finally, we propose three adaptive defenses against PAP and find they are also effective against other attacks. The findings suggest a link between persuasion and other jailbreak methods, leading us to advocate more fundamental solutions for AI safety. In summary, this paper highlights the overlooked jailbreak risks coming from natural communication with everyday users. It also shows that a social-science-guided taxonomy can breach AI safety guardrails with minimal algorithmic design, which lays the groundwork for potential future advancements toward efficiency and efficacy. As the interaction pattern between everyday users and LLMs evolves, these risks are likely to increase, which highlights the urgency for continued research and discussion around such overlooked vulnerability rooted in human-like communication. {Responsible Disclosure.} We have disclosed our findings to Meta and OpenAI prior to publication and discuss ethical considerations in Section~"
Causal-Guided Active Learning for Debiasing Large Language Models,2408.12942v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2408.12942v2_0.pdf,"Although achieving promising performance, recent analyses show that current generative large language models (LLMs) may still capture dataset biases and utilize them for generation, leading to poor generalizability and harmfulness of LLMs. However, due to the diversity of dataset biases and the over-optimization problem, previous prior-knowledge-based debiasing methods and fine-tuning-based debiasing methods may not be suitable for current LLMs. To address this issue, we explore combining active learning with the causal mechanisms and propose a casual-guided active learning (CAL) framework, which utilizes LLMs itself to automatically and autonomously identify informative biased samples and induce the bias patterns. Then a cost-effective and efficient in-context learning based method is employed to prevent LLMs from utilizing dataset biases during generation. Experimental results show that CAL can effectively recognize typical biased instances and induce various bias patterns for debiasing LLMs.",(a) Dataset bias under causal perspective (b) Illustration of the Causal-Guided Active Learning framework.,"Large language models (LLMs) are growing to be the foundation of Natural Language Processing. Through the generative pretraining process upon a large-scale corpus, the LLMs have demonstrated impressive performance in understanding the language and conducting complex reasoning tasks , demonstrating immense potential in real-world applications. However, the generative pretraining process is a double-edged sword, as it would also inevitably incur {dataset bias} into the LLMs such as position bias and stereotype bias . This is because, the LLMs only {passively} learn to model the {correlation} between contexts in the pretraining corpus, and the pretraining corpus is biased as it reflects the inherent preference or prejudice of human beings. For example, the existence of position bias is due to the subconscious human belief that the first option is better, leading to a higher frequency of the first option in corpora, and LLMs trained to model the corpus distribution would also capture such biased correlation. Such biases would lead to {poor generalizability} and {harmfulness} of LLMs . For instance, when an LLM is asked to evaluate which option is better, the LLM may utilize position bias and tend to choose the first option. However, which option is better is completely unrelated to its position. Therefore, when the second option is generally better in some datasets, the performance of the LLM will significantly decline. While biases such as stereotyping bias would make LLMs generate harmful content such as women are less capable in STEM fields, which in turn reinforces harmful stereotypes. These problems highlight the necessity of debiasing LLMs. The key issue to debias LLMs lies in how to recognize the dataset biases and prevent it from utilizing biases during inference. To this end, prevalent methods rely on researchers' prior knowledge to artificially recognize the potential dataset biases, and then eliminate such biases through aligning or prompt-based regularization . However, due to the diversity and complexity of dataset biases , it's impractical to identify them one by one manually. A vast amount of biases remains unrecognized in different tasks and new biases are continually being discovered. Hence, there is an urgent need for methods to automatically identify biases of generative LLMs. However, previous automatic debiasing methods are mainly designed for discriminative models and are hard to adapt to generative LLMs. Moreover, these methods generally rely on a fine-tuning-based process on certain dataset(s) to regularize the model. The finetuning-based debiasing process would lead to over-optimization and undermine the generalizability of LLMs on other tasks . To address these issues, considering the powerful pattern recognition and inductive ability of LLMs, we explore combining {active} learning with the {causal} mechanisms and propose a {C}asual-guided {A}ctive {L}earning (CAL) framework, which utilizes LLMs themselves to automatically and autonomously identify biased samples and induce the bias patterns. Active learning aims at selecting the most informative instances, and then querying external information source(s) to label these data points. In the debiasing scenario, CAL identifies the biased instances by finding instances where the LLMs fail to model {causal invariant} semantic relationship among context, then selects the most informative biased instances by finding the instances on which dataset biases have the most influence on the generation of LLMs. The causal invariance can be employed to disentangle the semantic information with dataset biases, as the content of the subsequent text is decided by the semantics of the preceding text (i.e., ``{causal}''), and such relationship exists in all corpora (i.e., ``{invariant}''); on the contrary, although the subsequent text would be correlative to dataset bias, such correlation changes upon different datasets. Given the biased instances, a set of explainable bias patterns is further induced, and we devise a cost-effective and efficient in-context learning (ICL) based method to regularize LLMs using the explainable bias patterns. Based on the method of this paper, we construct a Python package to facilitate the automatic identification of dataset bias on Instruct Tuning Datasets. We attempt to discover biased instances and explainable biased patterns from several commonly used instruct-tuning datasets. The code is publicly available at https://github.com/spirit-moon-fly/CAL. Experimental results show that our approach can automatically induce various explainable bias patterns (some of them may be unreported), and improve the generalizability and safety of LLMs by using the ICL-based debiasing method based on the bias patterns and biased instances"
Towards Better Understanding of Contrastive Sentence Representation Learning: A Unified Paradigm for Gradient,2402.18281v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.18281v2_0.pdf,"Sentence Representation Learning (SRL) is a crucial task in Natural Language Processing (NLP), where contrastive Self-Supervised Learning (SSL) is currently a mainstream approach. However, the reasons behind its remarkable effectiveness remain unclear. Specifically, many studies have investigated the similarities between contrastive and non-contrastive SSL from a theoretical perspective. Such similarities can be verified in classification tasks, where the two approaches achieve comparable performance. But in ranking tasks (i.e., Semantic Textual Similarity (STS) in SRL), contrastive SSL significantly outperforms non-contrastive SSL. Therefore, two questions arise: First, {what commonalities enable various contrastive losses to achieve superior performance in STS?} Second, {how can we make non-contrastive SSL also effective in STS?} To address these questions, we start from the perspective of gradients and discover that four effective contrastive losses can be integrated into a unified paradigm, which depends on three components: the {Gradient Dissipation}, the {Weight}, and the {Ratio}. Then, we conduct an in-depth analysis of the roles these components play in optimization and experimentally demonstrate their significance for model performance. Finally, by adjusting these components, we enable non-contrastive SSL to achieve outstanding performance in STS. % SRL .}",Average Spearman's correlation on Semantic Textual Similarity tasks for ineffective optimization objectives before (``ori'') and after (``mod'') modifications under different backbones.,"Sentence Representation Learning (SRL) explores how to transform sentences into vectors (or ``embeddings''), which contain rich semantic information and are crucial to many downstream tasks in Natural Language Processing (NLP). In the era of Large Language Models (LLMs), SRL also plays an important role in providing the embedding models for Retrieval-Augmented Generation (RAG, ). The quality of sentence embeddings is usually measured through Transfer tasks (TR) and Semantic Textual Similarity tasks (STS). Contrastive Self-Supervised Learning (SSL) is now a prevalent approach in SRL, which is introduced by and . It optimizes the representation space by reducing the distance between a sentence (or ``anchor'') and semantically similar sentences (or ``positive samples''), as well as increasing the distance between the sentence and semantically dissimilar sentences (or ``negative samples''). While the mechanisms underlying contrastive SSL can be intuitively understood, its effectiveness in SRL has not been thoroughly explored. Specifically, there are still some conflicts between the existing conclusions: (1) In theoretical perspective, machine learning community has found that contrastive SSL shares many similarities with non-contrastive SSL (e.g. alignment \& uniformity, Barlow Twins, and VICReg) (2) However, in practical applications, contrastive and non-contrastive SSL show comparable performance only in classification tasks (e.g., in Visual Representation Learning (VRL) and TR in SRL). In contrast, for ranking tasks (i.e., STS in SRL), contrastive SSL seems be the only effective method, significantly outperforming non-contrastive SSL. These inconsistent conclusions suggest that to make the obtained representations suitable for STS, certain unique factors must be present in the optimization objectives, which have rarely been explored in the existing literature. In this work, we attempt to identify the key factors that enable contrastive SSL to be effective in STS. Specifically, we would like to answer two questions: (1) {What commonalities enable various contrastive losses to achieve superior performance in STS?} (2) {How can we make non-contrastive SSL, which is similar to contrastive SSL but ineffective in STS, effective?} We first analyze the commonalities among four effective losses in SRL from the perspective of gradients. From this analysis, we find that all gradients can be unified into the same paradigm, which is determined by three components: the {Gradient Dissipation}, the {Weight}, and the {Ratio}. By statistically analyzing the values of these three components under different representation space distributions, we propose three conjectures, each corresponding to the role of a component in optimizing the representation space. Subsequently, we construct a baseline model to empirically validate our conjectures and demonstrate the significance of these components to model performance by varying them in the baseline. After understanding the key factors that enable contrastive losses to be effective, we are able to analyze the reasons behind the poor performance of non-contrastive SSL in STS from the perspective of three components in the paradigm. We find that these ineffective losses do not perform as well as effective ones across these components. Therefore, by adjusting these components, we manage to make them function and achieve improved model performance in STS (refer to Figure~). Briefly, our main contributions are as follows: {itemize} We propose a unified gradient paradigm for effective losses in SRL, which is controlled by three components: the {Gradient Dissipation}, the {Weight}, and the {Ratio} (Section~); We analyze the roles in optimization for each component theoretically. Further, we propose and validate the conjectures on their effective roles in performing STS tasks (Section~); With the guidance of our analysis results, we modify four optimization objectives in non-contrastive SSL to be effective in STS by adjusting the three components (Section~). {itemize"
Emergent Word Order Universals from Cognitively-Motivated Language Models,2402.12363v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.12363v2_0.pdf,"The world's languages exhibit certain so-called typological or implicational universals; for example, Subject-Object-Verb (SOV) languages typically use postpositions. Explaining the source of such biases is a key goal of linguistics. We study word-order universals through a computational simulation with language models (LMs). Our experiments show that typologically-typical word orders tend to have lower perplexity estimated by LMs with cognitively plausible biases: syntactic biases, specific parsing strategies, and memory limitations. This suggests that the interplay of cognitive biases and predictability (perplexity) can explain many aspects of word-order universals. It also showcases the advantage of cognitively-motivated LMs, typically employed in cognitive modeling, in the simulation of language universals. [width=1.1em,height=1.1em]{logo/github.png} { }",We compare the word orders that are challenging for LMs to those that are infrequent in attested languages (\cref{sec:design}). We examine the advantage of cognitively-motivated LMs (\cref{sec:model}) in simulating the word-order universals (the world's word-order distribution) with their inductive biases (\cref{sec:experiment}).,"There are thousands of attested languages, but they exhibit certain universal tendencies in their design. For example, Subject-Object-Verb (SOV) word order often combines with postpositions, while SVO order typically employs prepositions. Researchers have argued that such implicational universals are not arbitrary but shaped by their advantage for humans. Such language universals have been recently studied through neural-based computational simulation to elucidate the mechanisms behind the universals. The languages which emerge, however, have typically not been human-like. Such mismatch arguably stems from the lack of human-like cognitive biases in neural agents, but injecting cognitive biases into systems and showing their benefits has proved challenging. In this study, expanding on a study of word-order biases in language models (LMs:), we demonstrate the advantage of {cognitively-motivated} LMs, which can simulate human cognitive load during sentence processing well, and thus predict many implicational word-order universals in terms of their inductive biases (Figure~). Specifically, we train various types of LMs in {artificial languages} with different word-order configurations ( ). Our experiments show that perplexities estimated by cognitively-motivated LMs ( ) correlate better with frequent word-order configurations in attested languages than standard LMs ( ). This confirms that such biases are a potential source of the word-order universals as well as demonstrate the plausibility of cognitively-motivated LMs as models of human language processing"
CausalGym: Benchmarking causal interpretability methods on linguistic tasks,2402.12560v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.12560v1_0.pdf,"Language models (LMs) have proven to be powerful tools for psycholinguistic research, but most prior work has focused on purely behavioural measures (e.g., surprisal comparisons). At the same time, research in model interpretability has begun to illuminate the abstract causal mechanisms shaping LM behavior. To help bring these strands of research closer together, we introduce . We adapt and expand the SyntaxGym suite of tasks to benchmark the ability of interpretability methods to causally affect model behaviour. To illustrate how can be used, we study the models (14M--6.9B) and assess the causal efficacy of a wide range of interpretability methods, including linear probing and distributed alignment search (DAS). We find that DAS outperforms the other methods, and so we use it to study the learning trajectory of two difficult linguistic phenomena in : negative polarity item licensing and filler--gap dependencies. Our analysis shows that the mechanism implementing both of these tasks is learned in discrete stages, not gradually. [width=1.25em,height=1.25em]{logos/github.png}{}","The \benchmarktitle{} pipeline: {(1)} take an input minimal pair ($\mathbf{b}, \mathbf{s}$) exhibiting a linguistic alternation that affects next-token predictions ($y_b, y_s$); {(2)} intervene on the base forward pass using a pre-defined intervention function that operates on aligned representations from both inputs; {(3)} check how this intervention affected the next-token prediction probabilities. In aggregate, such interventions assess the causal role of the intervened representation on the model's behaviour.","Language models have found increasing use as tools for psycholinguistic investigation---to model word surprisal [][{inter alia}]{smith2013effect,goodkind-bicknell-2018-predictive,wilcox-etal-2023-language,shainetal24}, graded grammaticality judgements , and, broadly, human language processing . To benchmark the linguistic competence of LMs, computational psycholinguists have created {targeted syntactic evaluation} benchmarks, which feature minimally-different pairs of sentences differing in grammaticality; success is measured by whether LMs assign higher probability to the grammatical sentence in each pair . Despite the increasing use of LMs as models of human linguistic competence and how much easier it is to experiment on them than human brains, we do not understand the mechanisms underlying model behaviour---LMs remain largely uninterpretable. The {linear representation hypothesis} claims that `concepts' form linear subspaces in the representations of neural models. An increasing body of experimental evidence from models trained on language and other tasks supports this idea . Per this hypothesis, information about high-level linguistic alternations should be localised to linear subspaces of LM activations. Methods for finding such features, and even modifying activations in feature subspaces to causally influence model behaviour, have proliferated, including probing , distributed alignment search [DAS;][]{geiger2023finding}, and difference-in-means . Psycholinguistics and interpretability have complementary needs: thus far, psycholinguists have evaluated LMs on extensive benchmarks but neglected understanding their internal mechanisms, while new interpretability methods have only been evaluated on one-off datasets and so need better benchmarking. Thus, we introduce {} ({fig:main}). We adapt linguistic tasks from SyntaxGym to benchmark interpretability methods on their ability to find linear features in LMs that, when subject to intervention, causally influence linguistic behaviours. We study the {pythia} family of models , finding that DAS is the most efficacious method. However, our investigation corroborates prior findings that DAS is powerful enough to make the model produce arbitrary input--output mappings . To address this, we adapt the notion of control tasks from the probing literature , finding that adjusting for performance on the arbitrary mapping task reduces the gap between DAS and other methods. We further investigate how LMs learn two difficult linguistic behaviours during training: filler--gap extraction and negative polarity item licensing. We find that the causal mechanisms require multi-step movement of information, and that they emerge in discrete stages (not gradually) early in training"
Speech Translation with Speech Foundation Models and Large Language Models: What is There and What is Missing?,2402.12025v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.12025v3_0.png,"The field of natural language processing (NLP) has recently witnessed a transformative shift with the emergence of foundation models, particularly Large Language Models (LLMs) that have revolutionized text-based NLP. This paradigm has extended to other modalities, including speech, where researchers actively the combination of Speech Foundation Models (SFMs) and LLMs capable of addressing multimodal tasks. Among such tasks, this paper focuses on speech-to-text translation (ST). By examining the published papers on the topic, we propose a unified view of the architectural solutions and training strategies presented so far, highlighting similarities and differences among them. Based on this examination, the lessons learned but also % how diverse settings and evaluation approaches hinder the identification of the best-performing solution for each architectural building block and training choice.} Lastly, we recommendations for future works on the topic aimed at better understanding the strengths and weaknesses of the",\mg{Architectural building blocks of ST models based on the combination of an SFM and an LLM}.,"The natural language processing (NLP) landscape has recently undergone a {paradigm shift} with the emergence of foundation models . Among them, Large Language Models (LLMs) have revolutionized text-based NLP, showcasing remarkable capabilities across a wide range of NLP tasks . This unprecedented success has spurred research into creating foundation models for other modalities, including speech processing . Building on the translation abilities of LLMs and the remarkable speech recognition and understanding capabilities achieved by Speech Foundation Models (SFMs) , researchers are now actively exploring {their combination. The resulting large multimodal models leverage, on {the} one {hand}, the SFM ability to encode speech content into rich and high-level representations and, on the other, the extensive linguistic knowledge of the LLM to} {generate fluent outputs and address a wide range of tasks} . Focusing on the speech-to-text translation (ST) task -- the scope of this paper -- the rapid pace of the advancements has led to multiple parallel endeavors, resulting in a variety of solutions. While all these efforts have the merit of demonstrating the viability and effectiveness of this {line of work}, their contemporaneity, along with methodological inconsistencies, hinders {a fair} comparison{.} {For this reason, we provide a systematic analysis of the proposed SFM+LLM solutions {for ST} with the multiple goals of identifying {their} similarities and differences, organizing the lessons learned, and suggesting future research directions, along with best practices for insightful evaluations.} {At its core, this paper addresses two key questions:} {itemize} [] {What is There?} We survey the publicly available works that propose {an SFM+LLM solution for ST,} resulting in 9 papers (henceforth referred to as {},...,{}), and analyze them ($$) focusing on two orthogonal aspects: {itemize} [] {Architectural Building Blocks} ($$): We delve into the SFM+LLM {architectures,} {identifying} a common abstraction made of 5 building blocks and underscoring similarities {and differences} in the SFM and LLM choices, {along with} the strategies adopted for combining them{;} [] {Training and Evaluation} ($$): We inspect the {training data, tasks, and strategies employed in the studies, {as well as evaluation data and supported language pairs,} gathering insights {about} promising solutions,} and {highlighting} the sparsity of the current landscape; {itemize} [] {What is Missing?} {We conclude by underscoring the {importance} {of establishing a standard training setting} based on open data {to {ease} direct comparability across works,} and {by} identifying aspects that need further investigation to better understand the potential of} {SFM+LLM combination for ST} ($$). {itemize"
Arabic Diacritics in the Wild: Exploiting Opportunities for Improved Diacritization,2406.05760v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.05760v1_0.png,"The widespread absence of diacritical marks in Arabic text poses a significant challenge for Arabic natural language processing (NLP). This paper explores instances of naturally occurring diacritics, referred to as ``diacritics in the wild,'' to unveil patterns and latent information across six diverse genres: news articles, novels, children's books, poetry, political documents, and ChatGPT outputs. We present a new annotated dataset that maps real-world partially diacritized words to their maximal full diacritization in context. Additionally, we propose extensions to the analyze-and-disambiguate approach in Arabic NLP to leverage these diacritics, resulting in notable improvements. Our contributions encompass a thorough analysis, valuable datasets, and an extended diacritization algorithm. We release our code and datasets as open source.","(a) The nine Arabic diacritics commonly used in Modern Standard Arabic, grouped by function; and four examples of diacritic clusters. (b) A visually annotated example of a diacritized phrase meaning `and the bright suns [lit. and-the-suns the-bright]'. Diacritics are marked in red; and so are the undiacritized vowel-lengthening helping letters. Silent letters appear in dotted boxes.","Arabic orthography is infamous for its high degree of ambiguity due to its infrequently used optional diacritical marks. While other Semitic languages like Hebrew and Syriac use similar systems, Arabic has a richer inflectional space with case endings and other orthographic choices that make Arabic more complex. Interestingly, diacritical marks in Arabic are common in limited contexts where correct reading is a goal: holy texts, poetry and children's books, as well as books for adult literacy and non-native learners. But in general reading contexts, for literate Arabic native speaker adults, diacritical marks are used frugally: $$1-2\ diacritized . We refer to these as Diacritics in the Wild ({}). In this paper, we follow in the footsteps of other researchers to investigate whether such precious occurrences can be exploited to help improve the quality of Arabic NLP tools . We specifically focus on the Arabic diacritization task, which is at once (a) a final target downstream application given the sparse and variable use of diacritics in Arabic; and (b) an enabling technology for other applications such as speech synthesis . While the percentage of {} is small, our guiding intuitions are that on large scales, these are objects worthy of study, and given the extra information provided in such contexts, we assume the writers who added them wanted to provide hints to support optimal reading, e.g., to avoid garden path sentences. For a well-rounded exploration, we analyze and compare the diacritization patterns across six genres: news of multiple agencies, novels, children's books, poetry, political/legal documents of the UN, and ChatGPT output (which sometimes introduces diacritics unprompted). Furthermore, we examine the diacritization patterns and choices in two commonly used datasets for evaluating Arabic diacritization: The Penn Arabic Treebank and {} . We also develop a new annotated dataset that includes instances of partially diacritized words along with their {full} diacritization, which we define carefully, acknowledging different practices. And finally, we propose an extension to the {analyze-and-disambiguate} approach to improve the quality of its choices, and we evaluate on the data we annotated. {Our contributions} are the following: {itemize} {}{0pt} {}{0pt} {}{0pt} {}{0pt} {}{0pt} We provide careful analysis and comparison of diacritization patterns in six genres of Arabic texts, shedding light on the needs and latent information in different Arabic genres. We annotate a new dataset for studying maximal diacritization from partial diacritic signals ({}), and we extend an existing dataset, {}, to address unhandled phenomena ({}). We extend a hybrid (neuro-symbolic) algorithm for Arabic diacritization to make use of the existence of {}, and demonstrate improved performance. {itemize} Our code and datasets will be open-source and publicly available.{{https://github.com/CAMeL-Lab/wild_diacritics}} The paper is organized as follows. Section~ discusses Arabic diacritics. Section~ reviews related work. Section~ covers the datasets, including our annotated datasets, and {} usage statistics. Section~ details the approach we build on and how we extend it. Section~ presents the experimental setup, evaluation, and error analysis"
Disinformation Capabilities of Large Language Models,2311.08838v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2311.08838v2_0.pdf,"Automated disinformation generation is often listed as an important risk associated with large language models (LLMs). The theoretical ability to flood the information space with disinformation content might have dramatic consequences for societies around the world. This paper presents a comprehensive study of the disinformation capabilities of the current generation of LLMs to generate false news articles in the English language. In our study, we evaluated the capabilities of 10 LLMs using 20 disinformation narratives. We evaluated several aspects of the LLMs: how good they are at generating news articles, how strongly they tend to agree or disagree with the disinformation narratives, how often they generate safety warnings, etc. We also evaluated the abilities of detection models to detect these articles as LLM-generated. We conclude that LLMs are able to generate convincing news articles that agree with dangerous disinformation narratives.","Summary of how many generated texts we consider {dangerous} or {safe}. Dangerous texts are disinformation articles that could be misused by bad actors. Safe texts contain disclaimers, provide counterarguments, argue against the user, etc. Note that GPT-4 annotations are generally slightly biased towards safety.","The threat of LLMs generating disinformation at scale is one the most commonly cited risks of their further development. The capability to generate an arbitrary amount of human-like texts can be a powerful tool for {disinformation actors} willing to influence the public by flooding the Web and social media with content during {influence operations}. The recent wave of instruction-tuned LLMs that started to appear in late 2022 only exacerbated this issue as they proved to be capable of closely following arbitrary instructions. The growing {capabilities} of LLMs, their growing {availability} (caused by capable open source models and improvements in inference libraries), and improvements in prompting techniques are all concerning in the context of disinformation generation. Despite all this, very little is known about the disinformation capabilities of the {current generation} of LLMs. While there is a body of existing work~[i.a.]{zellers2020defending, buchanan2021truth}, the experimental evaluation of certain features or capabilities is often absent or anecdotal. Our goal in this paper is to fill this gap and provide a comprehensive evaluation of instruction-tuned models prompted to generate English disinformation ""news articles"". We do this by observing how different LLMs behave when they are asked to generate texts about various harmful {disinformation narratives}, such as narratives about health-related hoaxes.{Code and data are available at: {https://github.com/kinit-sk/disinformation-capabilities}} We manually evaluated 1,200 generated texts to ascertain how much they agree or disagree with the prompted disinformation narratives, how many novel arguments they use, and how closely they follow the desired news article text style (grammar, structure, vocabulary, etc.). We observed whether there are differences in how capable different LLMs are at generating disinformation({there are}), how well their safety filters work ({with a few exceptions, not very well}), or how detectable these generated texts are ({quite detectable}). We also found out that we can, to some extent, automate such analysis by utilizing LLMs to analyze the generated texts, making the first steps toward automatic evaluation. Overall, we must conclude that existing LLMs (including open-source ones) can easily generate news articles with {real} or {hallucinated} supporting evidence about all kinds of dangerous disinformation narratives. Figure illustrates how dangerous or safe different LLMs are according to our methodology. Note that this paper provides but a snapshot of current capabilities, and understandably, we expect that newer LLMs trained on newer data might have different behavior. Our goal is to show what is the state of the field today so that we can understand how dangerous the LLMs are as a technology right now, but also to have the ability to observe how these capabilities will evolve in the future"
Learn or Recall? Revisiting Incremental Learning with Pre-trained Language Models,2312.07887v5,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2312.07887v5_0.pdf,"Incremental Learning (IL) has been a long-standing problem in both vision and Natural Language Processing (NLP) communities. In recent years, as Pre-trained Language Models (PLMs) have achieved remarkable progress in various NLP downstream tasks, utilizing PLMs as backbones has become a common practice in recent research of IL in NLP. Most assume that catastrophic forgetting is the biggest obstacle to achieving superior IL performance and propose various techniques to overcome this issue. However, we find that this assumption is problematic. Specifically, we revisit more than 20 methods on four classification tasks (Text Classification, Intent Classification, Relation Extraction, and Named Entity Recognition) under the two most popular IL settings (Class-Incremental and Task-Incremental) and reveal that most of them severely underestimate the inherent anti-forgetting ability of PLMs. Based on the observation, we propose a frustratingly easy method called SEQ* for IL with PLMs. The results show that SEQ* has competitive or superior performance compared with state-of-the-art (SOTA) IL methods yet requires considerably less trainable parameters and training time. These findings urge us to revisit the IL with PLMs and encourage future studies to have a fundamental understanding of the catastrophic forgetting in PLMs. The data, code and scripts are publicly available {https://github.com/zzz47zzz/codebase-for-incremental-learning-with-llm}}.",The comparison between the proposed SEQ* and SOTA IL methods on five class-incremental tasks. We report the average accuracy after learning the final task. The detailed results are provided in Table \ref{tab:sota_main_gen_pythia410m}.,"Learning knowledge incrementally without much forgetting is an essential ability of human beings but still an unsolved challenge for neural networks in achieving human-level intelligence . Incrementally learning a sequence of tasks can be formulated into the paradigm of Incremental Learning (IL) and has been impeded by catastrophic forgetting . Catastrophic forgetting refers to neural networks forgetting previous knowledge after learning new tasks . Recent years have witnessed significant breakthroughs in Pre-trained Language Models (PLMs) in vision and NLP tasks. Most recent studies of IL use PLMs as the backbone and design various methods for alleviating catastrophic forgetting in NLP tasks. However, is forgetting really catastrophic in PLMs? More specifically, how can we quantify forgetting and how much knowledge is forgotten in various IL scenarios when using various backbones and methods on various tasks? More recently, reveal for the first time that BERT-like models have a strong anti-forgetting ability in the task-incremental setting. Why does this happen? Does it hold for a more challenging setting, such as class-incremental learning, and for other model architectures, such as GPT-like models? To answer the above questions, we carry out extensive experiments to explore forgetting in more than 20 methods on four classification tasks (Text Classification, Intent Classification, Relation Extraction, and Named Entity Recognition) under the two most popular IL settings (Class-Incremental and Task-Incremental) with various model architecture (encoder only and decoder only) and scales (from 19M to 1.21B number of parameters). Through extensive experiments, we have several major findings: {itemize} The popular assumption that PLMs suffer from catastrophic forgetting does not hold. Even under sequential fine-tuning (SEQ), the PLMs maintain the knowledge without much forgetting (Sec. ). From the probing perspective, most existing IL methods do not learn incremental knowledge for PLMs (Sec. ). By combining SEQ with simple strategies (Sec. ), we propose SEQ* and find that SEQ* has competitive or even superior performance than SOTA IL methods (Figure , Sec. ). The inherent anti-forgetting ability of PLMs comes from both the pre-training stage as well as the architecture of Transformer (Sec. ). Randomly initialised PLMs learn incrementally when SEQ is performed on a sequence of tasks. The forgetting of SEQ is due to the deviation of the classifier from the PLM rather than the loss of old knowledge in the PLM. (Sec. ). {itemize} Our study urges the NLP community to revisit and deepen the understanding of the forgetting in PLMs"
Cendol: Open Instruction-tuned Generative Large Language Models for Indonesian Languages,2404.06138v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2404.06138v2_0.png,"Large language models (LLMs) show remarkable human-like capability in various domains and languages. However, a notable quality gap arises in low-resource languages, e.g., Indonesian indigenous languages, rendering them ineffective and inefficient in such linguistic contexts. To bridge this quality gap, we introduce Cendol, a collection of Indonesian LLMs encompassing both decoder-only and encoder-decoder architectures across a range of model sizes. We highlight Cendol's effectiveness across a diverse array of tasks, attaining $$20\% improvement, and demonstrate its capability to generalize to unseen tasks and indigenous languages of Indonesia. Furthermore, Cendol models showcase improved human favorability despite their limitations in capturing indigenous knowledge and cultural values in Indonesia. In addition, we discuss the shortcomings of parameter-efficient tunings, such as LoRA, for language adaptation. Alternatively, we propose the usage of vocabulary adaptation to enhance efficiency. Lastly, we evaluate the safety of Cendol and showcase that safety in pre-training in one language such as English is transferable to low-resource languages, such as Indonesian, even without RLHF and safety fine-tuning.",Overview of Cendol Collection and LLM adaptation into Cendol$^{inst}$ and Cendol$^{chat}$ models.,"Indonesia is the fourth most populous country in the world, with around 280 million people spread across more than 17,000 islands within a humongous area of $$2 million square kilometers. With such a large archipelago surrounding the country, digital services become immensely crucial, making Indonesia the fourth largest internet user in the world, with $$220 million users. Despite the huge demand, the technology supporting Indonesian digital businesses still lags compared to other much smaller countries. One aspect that it still left behind is the access to state-of-the-art large language model (LLM) technology, such as ChatGPT and GPT4. Although these LLMs support Indonesian and its local languages, these LLMs often have much weaker language representation for such low-resource and underrepresented languages. The weak language representation in existing LLMs hurts their ability to generate responses in Indonesian and other underrepresented languages. This also leads to inefficiency during inference due to the vocabulary mismatch, hence texts in these languages are tokenized into much longer tokens. Additionally, these LLMs are more prone to safety issues, e.g., giving unsafe responses, hallucinations, and jailbreaking. To overcome the challenge of weak language representation in Indonesian languages, we introduce Cendol{Cendol is an iced sweet dessert that contains droplets of pandan-flavored green rice flour jelly and coconut milk, served with palm sugar syrup. Cendol is popular across Southeast Asia, especially in Indonesia.}, a series of large-scale instruction-tuned LLMs specifically tailored for handling Indonesian indigenous languages. Cendol covers both decoder-only and encoder-decoder LLMs that spread across various scales from 300M up to 13B parameters. Various strategies are incorporated to enable instruction tuning across various scales. We assess the effectiveness of Cendol on a comprehensive evaluation suite, covering various general NLP tasks (e.g., sentiment analysis, topic modeling, machine translation, summarization, etc.), local knowledge, and cultural values evaluations. Our work highlights the following contributions: {itemize} We introduce Cendol, a collection of state-of-the-art Indonesian LLMs, which outperforms all existing multilingual, Southeast Asian (SEA), and Indonesian LLMs. We curate the Cendol Collection, a rigorous instruction-tuned corpus for Indonesian and local languages, covering 23 tasks and 10 languages, with a total of $$50M instructions. We highlight the generalization of Cendol through a comprehensive evaluation suite, showcasing its adaptability towards various Indonesian NLP tasks and languages. We demonstrate the ineffectiveness of parameter-efficient tuning approaches, exemplified by LoRA, in achieving high-quality regional LLMs. This prompts a consideration of the significance of parameter-efficient methods for language adaptation. We evaluate the safety of Cendol and showcase that safety in pre-training in one language such as English is transferable to low-resource languages, such as Indonesian. {itemize"
Latxa: An Open Language Model and Evaluation Suite for Basque,2403.20266v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2403.20266v2_0.png,"We introduce Latxa, a family of large language models for Basque ranging from 7 to 70 billion parameters. %, together with a complete development ecosystem. Latxa is based on Llama 2, which we continue pretraining on a new Basque corpus comprising 4.3M documents and 4.2B tokens. Addressing the scarcity of high-quality benchmarks for Basque, we further introduce 4 multiple choice evaluation datasets: EusProficiency, comprising 5,169 questions from official language proficiency exams; EusReading, comprising 352 reading comprehension questions; EusTrivia, comprising 1,715 trivia questions from 5 knowledge areas; and EusExams, comprising 16,774 questions from public examinations. In our extensive evaluation, Latxa outperforms all previous open models we compare to by a large margin. In addition, it is competitive with GPT-4 Turbo in language proficiency and understanding, despite lagging behind in reading comprehension and knowledge-intensive tasks. Both the Latxa family of models, as well as our new pretraining corpora and evaluation datasets, are publicly available under open licenses.} Our suite enables reproducible research on methods to build LLMs for low-resource languages.",Validation perplexity throughout training.,"Motivated by their increasing training cost and commercial interest, the development of Large Language Models (LLMs) has been led by close initiatives like GPT, Claude and Gemini. In recent times, a more open ecosystem has emerged following the release of various competitive models like Llama 2 and Mistral. However, despite early efforts to build open multilingual models, the most competitive ones are notoriously English-centric. As shown in Table~, all these open models perform poorly in low-resource languages like Basque, with most results marginally surpassing random chance. In this work, we present Latxa, an open family of LLMs for Basque that substantially outperforms all these previous models. Basque is an agglutinative language written in Latin script and with no known relatives, although a significant part of the vocabulary is shared with contact languages like Spanish and French. Basque is the 52th language in Common Crawl, with 0.035\ Our work builds on various open resources and models that we further expand to Basque, highlighting the importance of an open ecosystem for the development of language technology for low-resource languages. In particular, our models are based on Llama 2, which we continue training in Basque using a new corpus with 4.3M documents from 4 existing and 3 new sources. In addition, we release 4 diverse and challenging multiple-choice benchmarks comprising a total of 23,282 questions, covering language proficiency, reading comprehension, trivia questions, and public examinations. As shown in Table , Latxa performs substantially better than all existing open models, with the 70B variant outperforming the previous best open model (Yi 34B) by 18.95 points in average. In addition, it also outperforms the Llama 2 model it is based on by 25.18 points, and it is also superior to GPT-3.5 Turbo in all datasets we evaluate on. Interestingly, our best model also outperforms GPT-4 Turbo in language proficiency exams (EusProf), despite lagging behind in reading comprehension and knowledge-intensive tasks. This suggests that the capabilities that an LLM exhibits in a given language are not determined by its linguistic competence in this particular language, opening the doors to further improvements in low-resource LLMs as stronger English models become available. This paper makes the following contributions: {(1)} We release a high-quality corpus for Basque, comprising 4.3M documents and 4.2B tokens. The corpus combines the EusCrawl v1.1, Egunkaria, Booktegi, Wikipedia, CulturaX, Colossal OSCAR and HPLT v1 datasets (the first 3 being new), which we carefully deduplicate and filter. {(2)} We release the Latxa family of Basque LLMs, comprising 3 models with 7B, 13B and 70B parameters. {(3)} We release 4 new multiple-choice benchmarks for Basque: EusProficiency (official language proficiency exams), EusReading (reading comprehension), EusTrivia (trivia questions from 5 knowledge areas), and EusExams (public examinations). {(4)} We present extensive experiments comparing Latxa to previous open and closed models. {(5)} We show that it is possible to train significantly stronger LLMs for low-resource languages building on the existing ecosystem of open models and resources. In a similar spirit to other open LLMs, such as Pythia , LLM360 and OLMO , we release all the necessary data, code, weights and documentation to run and evaluate our models, facilitating similar efforts for other low-resource languages"
Why are Sensitive Functions Hard for Transformers?,2402.09963v4,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.09963v4_0.pdf,"Empirical studies have identified a range of learnability biases and limitations of transformers, such as a persistent difficulty in learning to compute simple formal languages such as PARITY, and a bias towards low-degree functions. However, theoretical understanding remains limited, with existing expressiveness theory either overpredicting or underpredicting realistic learning abilities. We prove that, under the transformer architecture, the loss landscape is constrained by the input-space sensitivity: Transformers whose output is sensitive to many parts of the input string inhabit isolated points in parameter space, leading to a low-sensitivity bias in generalization. We show theoretically and empirically that this theory unifies a broad array of empirical observations about the learning abilities and biases of transformers, such as their generalization bias towards low sensitivity and low degree, and difficulty in length generalization for PARITY. This shows that understanding transformers' inductive biases requires studying not just their in-principle expressivity, but also their loss landscape.","Training transformers on inputs of increasing length produces a steeper loss landscape for PARITY (as measured by average direction sharpness), while the loss landscape of MAJORITY does not show significant changes. Our main result (Theorem~\ref{thm:lrho-bound}) provides a rigorous explanation for this phenomenon.","Given dramatic advances in machine learning applications powered by transformer models, there has been substantial interest in understanding which functions are easier or harder to learn and represent using transformers. Empirical research on both formal languages and synthetic functions has uncovered an intriguing array of learning biases, but theoretical understanding is lacking. For instance, experimentally argued that heldout generalization is biased towards low-degree polynomials and provided empirical evidence that transformers prefer to represent functions of {low sensitivity}, that is, functions that do not strongly depend on many input bits. Perhaps the most prominent example of such learning biases is a consistent difficulty in learning the PARITY function, mapping bitstrings to their parity. This function is extremely sensitive, in the sense that flipping any bit flips the string's parity. Empirical studies have consistently found that training transformers to compute parities is difficult, and that solutions for shorter inputs do not generalize to longer inputs [e.g.][]{bhattamishra2020ability, chiang2022overcoming, deletang2022neural, ruoss2023randomized}. This stands in stark contrast to previously-popular reccurent models which easily fit PARITY with correct length generalization . While a substantial amount of theoetical work has considered both the learnability [e.g.][]{edelman2022inductive,ahn2023linear} and the expressiveness of transformers [e.g.][]{yun2019transformers,hahn2020theoretical,Yao_2021,hao2022formal,DBLP:journals/tacl/MerrillSS22,merrill2023logic, chiang2023tighter, strobl2023survey, strobl2023averagehard, angluin2023masked}, existing theoretical studies do not consistently explain such learning biases. proved that, under two formal models of self-attention, no transformer can express PARITY at all input lengths. However, various other formal results showed that slightly relaxed assumptions about the transformer architecture resolved such expressiveness limitations. Most notably, found that layer norm, by breaking the Lipschitz assumption used in 's Theorem 2, allows expressing PARITY in principle. Simultaneously, they empirically confirmed that such a solution could not be practically found via (S)GD training. Various other formal models of transformers [e.g.][]{weiss2021thinking,merrill2023logic,merrill2023parallelism,strobl2023averagehard} can also express PARITY despite its empirical difficulty. As already concluded by , these findings highlight a disconnect between expressive capacity and learnability: not all functions which transformers may express in principle are also learnt efficiently. Evidently, existing expressiveness theory for transformers is not able to consistently account for the practical learnability of problems under gradient descent. Some prior work has studied the learnability of problems for transformers. For example, bound the statistical capacity of the transformer architecture, showing that on those functions that transformers prefer to represent, they can generalize with good sample efficiency. Notably, they found that {sparse} parities could indeed be learned well by transformers. However, this result does not prove that PARITY, or other highly sensitive functions, are hard to learn, as that technique does not provide a direct characterization of which functions transformers prefer to represent. Other work has studied simplified setups such as linear attention [e.g.][]{ahn2023linear} or individual attention layers [e.g.][]{sanford2023representational}. Here, we provide results that have direct bearing on the learnability of PARITY and other sensitive functions, characterizing the loss landscape of transformers in terms of input-space sensitivity. We formally prove that, for the transformer architecture, parameter settings achieving high sensitivity in input space are necessarily brittle, so that close neighbors in parameter space will usually define different (typically much less sensitive) functions when inputs are long. As a consequence, transformers fitting high-sensitivity functions must inhabit very steep minima. We argue that this explains both difficulty in training and length generalization for PARITY (observed by ), and a low-sensitivity and low-degree bias in random initialization and generalization (observed by ). {Expressiveness theory does not account for learnability.} While unique hard attention provably cannot represent PARITY , more realistic upper bounds accounting for soft attention leave the hardness of sensitive functions unexplained. Not only does PARITY have transformers , but it can also be easily represented in formalisms that have been suggested to meaningfully upper-bound the abilities of various formal models of soft-attention:{ suggest that PARITY may not be representable in the RASP-L model, though the expressiveness of RASP-L is not well understood.} {fact}[Existing theory overpredicting abilities] Simple representations for PARITY, valid across all input lengths, exist in RASP , uniform circuits with majority gates , and FO[M] . {fact} We prove this in Appendix~. Thus, existing expressiveness bounds do not account for the difficulty that transformers encounter in learning sensitive functions, in particular given that previously-popular recurrent models do not encounter this difficulty. Another family of results consists of Lipschitzness bounds , which bound the influence that any individual input bit has on the output of a transformer. These turn out to {underpredict} the abilities of transformers: {fact}[Existing theory underpredicting abilities] By results of , the following holds: Consider a transformer without layer norm. If $x, x' \{ 1\}^n$ differ only in the $i$-th bit, then at any other position $j i$, the output of a transformer differs only by ${O}({1}{})$. {fact} This accounts for the difficulty of learning PARITY. But the bound suggests even simple sparse functions, such as FIRST (the language $1(0|1)^*$) to be difficult, but transformers learn these well . Indeed, note that the bound is overcome by layer norm or input-length-dependent scaling of attention logits, which enable modeling of sparse functions. We will show that the observed low-sensitivity bias can be understood in terms of the {loss landscape}: while transformers can express highly sensitive functions, such transformers are isolated in parameter space, and minima interpolating a sensitive function are very sharp. Indeed, we prove that tiny perturbations of a highly sensitive transformer tend to define, when inputs are sufficiently long, very different functions with much lower sensitivity"
Talk With Human-like Agents: Empathetic Dialogue Through Perceptible Acoustic Reception and Reaction,2406.12707v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.12707v1_0.pdf,"Large Language Model (LLM)-enhanced agents become increasingly prevalent in Human-AI communication, offering vast potential from entertainment to professional domains. However, current multi-modal dialogue systems overlook the acoustic information present in speech, which is crucial for understanding human communication nuances. This oversight can lead to misinterpretations of speakers' intentions, resulting in inconsistent or even contradictory responses within dialogues. To bridge this gap, in this paper, we propose PerceptiveAgent, an empathetic multi-modal dialogue system designed to discern deeper or more subtle meanings beyond the literal interpretations of words through the integration of speech modality perception. Employing LLMs as a cognitive core, PerceptiveAgent perceives acoustic information from input speech and generates empathetic responses based on speaking styles described in natural language. Experimental results indicate that PerceptiveAgent excels in contextual understanding by accurately discerning the speakers' true intentions in scenarios where the linguistic meaning is either contrary to or inconsistent with the speaker's true feelings, producing more nuanced and expressive spoken dialogues. Code is publicly available at: .",Examples illustrating the definition of empathy within dialogues.,"Artificial Intelligence (AI) agents are entities designed to replicate human-like intelligence and functionalities, serving as the essential building blocks of AI systems. An ideal agent should be capable of perceiving its environment with sensors, making informed decisions, and then taking actions in response to users or scenarios. Recently, Large Language Models (LLMs) have exhibited remarkable capabilities in diverse tasks, offering opportunities for building general AI agents that engage in human-like interactions, such as virtual assistants and intelligent robots. However, current text-only dialogue systems fall short in bridging the gap between experimental and realistic scenarios, where humans perceive and understand the world through diverse multi-modal information. Thus, the integration of acoustic information into dialogues has the potential to foster the development of more human-like agents, thereby enhancing the empathetic experience they offer. Empathetic responses involve two essential aspects: cognitive and affective empathy , which reflect an understanding of the human-talker's thoughts and feelings respectively. Specifically, cognitive empathy involves understanding the human-talker's thoughts, perspectives, and described events, enabling the agent to provide responses relevant to the dialogue topic. Conversely, affective empathy entails responding based on observed emotional expressions in the dialogue history, contributing to the naturalness of synthesized speech . While recent works leverage LLM's strong capabilities of contextual understanding and content generation to synthesize empathetic speeches, there remains a discrepancy between cognitive and affective empathy. This arises because cognitive content is preassigned before affective speech is deduced from latent representations of multi-modal dialogue history. Recently, advancements in multi-modal content perception and generation have been achieved by various methods , where audio is represented as either recognized text with an automatic speech recognition model or discrete features with a speech encoder. However, while linguistic information in speech is predominantly captured by both discrete acoustic units and textual representations, acoustic features tend to be disregarded. This oversight can lead to misinterpretations of the speaker's intentions, resulting in discrepant or even contradictory responses within the dialogue history. As illustrated in Figure , the left scenario fails to consider the perspective of the listener while the right one barely understands or empathizes with the speaker's feelings. In this paper, we propose {PerceptiveAgent}, an empathetic multi-modal dialogue system that can discern deeper or more subtle meanings beyond the literal interpretations of words, based on speaking styles described in natural language. Specifically, PerceptiveAgent first comprehends the speaker's intentions accurately by a perceptive captioner model that captures acoustic features from each speech within dialogues. Subsequently, an LLM module acts as the cognitive core, producing the relevant response content with a caption describing how to articulate the response. A Multi-Speaker and Multi-Attribute Synthesizer (MSMA-Synthesizer) is then developed to synthesize nuanced and expressive speech. Our contributions include the following: {itemize} [$$] We pioneer the construction of a speech captioner model to perceive and express acoustic information through natural language. [$$] We develop an empathetic multi-modal dialogue system capable of identifying the speaker's true intentions through audio modality perception and generating empathetic speech. [$$] Experiments demonstrate that PerceptiveAgent can accurately discern the true intentions in scenarios where the literal interpretations of words are either contrary to or inconsistent with the speaker's true feelings. {itemize"
WebCiteS: Attributed Query-Focused Summarization on Chinese Web Search Results with Citations,2403.01774v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2403.01774v2_0.pdf,"Enhancing the attribution in large language models (LLMs) is a crucial task. One feasible approach is to enable LLMs to cite external sources that support their generations. However, existing datasets and evaluation methods in this domain still exhibit notable limitations. In this work, we formulate the task of attributed query-focused summarization~(AQFS) and present WebCiteS, a Chinese dataset featuring 7k human-annotated summaries with citations. WebCiteS derives from real-world user queries and web search results, offering a valuable resource for model training and evaluation. Prior works in attribution evaluation do not differentiate between groundedness errors and citation errors. They also fall short in automatically verifying sentences that draw partial support from multiple sources. We tackle these issues by developing detailed metrics and enabling the automatic evaluator to decompose the sentences into sub-claims for fine-grained verification. Our comprehensive evaluation of both open-source and proprietary models on WebCiteS highlights the challenge LLMs face in correctly citing sources, underscoring the necessity for further improvement.}",Illustration of attributed query-focused summarization~(AQFS). Full example is shown in Table~\ref{tab:fsp_prompt}.,"In today's information-driven society, swift access to knowledge is essential. A major limitation of web search engines is the need for users to manually compile information from various sources, which can be time-consuming. Large language models~(LLMs) exhibit potential in this domain by generating straightforward and well-organized responses. However, the potential risks of hallucination and factual errors undermine their trustworthiness as knowledge sources. An emerging solution is {generative search engines} which use LLMs to synthesize web search results into responses with in-line citations. This allows users and developers to verify the generations against the cited sources. However, recent investigations on commercial products and retrieval-augmented LLMs reveal frequent occurrences of unsupported claims and incorrect citations, highlighting the challenges of attribution in LLMs. Nonetheless, the limitations of pertinent datasets and evaluation methods pose obstacles to in-depth explorations within the community. {-0.5em} {Firstly, most existing datasets are deficient in high-quality citation annotations.} For instance, the ALCE benchmark compiles three question-answering datasets without providing citations in the reference answers, limiting its utility for model training. In contrast, WebGLM prompts InstructGPT to generate training data with citations. It controls the citation quality via a sample filtering method which calculates the ROUGE score between the answers and their citations. However, this method focuses on lexical similarity rather than logical entailment, and thus could not precisely measure attribution. {Secondly, current evaluation methods are insufficient to thoroughly assess attribution.} Prior works only inspect if the generations are supported by their citations without checking all the documents provided in the input context. However, instances of unsupported generations may result from both {failing to correctly cite supporting documents} and {failing to be grounded in all input documents}. Differentiating these two types of errors is crucial for system optimization. Moreover, existing automatic evaluation solely relies on off-the-shelf natural language inference~(NLI) models which only recognize entailment~(full support) and overlook sentences with multiple sub-claims drawing {partial support} from different sources. Such complexities are common in real-world scenarios and are indicative of a strong capability of synthesizing information across various sources. {0.5em} To address the above limitations, we present {WebCiteS}, a Chinese dataset for {Attributed Query-Focused Summarization~(AQFS)}. As shown in Figure~, given a query and the retrieved documents, AQFS aims to summarize all pertinent information from the documents with in-line citations to make the summary attributable. Our dataset is built upon real-world user queries and search results from {Sogou}, a widely used Chinese web search engine.{{www.sogou.com}} We employ human efforts to ensure the quality of summaries and citations. Table~ compares WebCiteS and relevant datasets. We propose a comprehensive evaluation framework with a cost-effective automatic evaluator. Our evaluation metrics distinguish two key aspects: {groundedness}~(if the model outputs are contextually supported) and {citation quality}~(citation accuracy and comprehensiveness), enabling a more nuanced understanding of attribution errors. We also train a tailored claim-split model to extract the sub-claims of a sentence for fine-grained verification. This allows the detection of partial support and improves the alignment between our automatic evaluator and human citations. Our evaluation of both open-source and proprietary models on WebCiteS reveals the following key findings: (1) contextual grounding of generations does not guarantee the avoidance of citation errors, indicating the challenge of explicit attribution in all the tested LLMs; (2) although supervised fine-tuning improves both groundedness and citation quality, the top-performing model only reaches a citation F$_1$ score of 76.1\ (3) models perform worse when summarizing full content of web pages rather than shorter snippets, showing that LLMs are less effective at synthesizing and attributing information in the longer context; (4) making documents more fine-grained leads to poorer attribution results, highlighting the difficulty LLMs face in pinpointing the exact supporting evidence within the context"
Political Compass or Spinning Arrow? Towards More Meaningful Evaluations for Values and Opinions in Large Language Models,2402.16786v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.16786v2_0.png,"Much recent work seeks to evaluate values and opinions in large language models (LLMs) using multiple-choice surveys and questionnaires. Most of this work is motivated by concerns around real-world LLM applications. For example, politically-biased LLMs may subtly influence society when they are used by millions of people. Such real-world concerns, however, stand in stark contrast to the artificiality of current evaluations: real users do not typically ask LLMs survey questions. Motivated by this discrepancy, we challenge the prevailing {constrained} evaluation paradigm for values and opinions in LLMs and explore more realistic {unconstrained} evaluations. As a case study, we focus on the popular Political Compass Test (PCT). In a systematic review, we find that most prior work using the PCT {forces} models to comply with the PCT's multiple-choice format. We show that models give substantively different answers when not forced; that answers change depending on how models are forced; and that answers lack paraphrase robustness. Then, we demonstrate that models give different answers yet again in a more realistic open-ended answer setting. We distill these findings into recommendations and open challenges in evaluating values and opinions in LLMs.","A model is prompted with a proposition from the Political Compass Test. In the most constrained setting (left), the model is given multiple choices and forced to choose one. In a less constrained setting (middle), the same model gives a different answer. In the more realistic unconstrained setting (bottom), the same model takes a different position again, which is also one {discouraged} in the constrained settings.","What values and opinions are manifested in large language models (LLMs)? This is the question that a growing body of work seeks to answer [][{inter alia}]{hendrycks2020aligning, miotto2022whoisgpt, durmus2023globalopinionqa, hartmann2023political, santurkar2023opinionqa, scherrer2023evaluating, xu2023cvalues}. The motivation for most of this work comes from real-world LLM applications. For example, we may be concerned about how LLM opinions on controversial topics such as gun rights (mis-)align with those of real-world populations [e.g.][]{durmus2023globalopinionqa}. We may also worry about how LLMs that exhibit specific political values may influence society when they are used by millions of people [e.g.][]{hartmann2023political}. Current evaluations for LLM values and opinions, however, mostly rely on multiple-choice questions, often taken from surveys and questionnaires. , for example, take questions from Pew's Global Attitudes and the World Value Survey. primarily draw on Dutch and German voting advice applications. These may be suitable instruments for measuring the values and opinions of human respondents, but they do not reflect real-world LLM usage: while real users {do} talk to LLMs about value-laden topics and ask controversial questions, they typically {do not} use multiple-choice survey formats . This discrepancy motivates our main research question: {How, if at all, can we meaningfully evaluate values and opinions in LLMs?} To answer this question, we revisit prior work and provide new evidence that demonstrates how constrained evaluations for LLM values and opinions produce very different results than more realistic unconstrained evaluations, and that results also depend on the precise method by which models are constrained (see Figure~). As a case study, we focus on the Political Compass Test (PCT){{https://www.politicalcompass.org/test}{www.politicalcompass.org/test}}, a multiple-choice questionnaire that has been widely used to evaluate political values in LLMs [e.g.][]{feng2023pretraining,rozado2023danger,thapa2023assessing}. We make five main findings: {enumerate} 0em We systematically review Google Scholar, arXiv, and the ACL Anthology, and show that most of the 12 prior works that use the PCT to evaluate LLMs {force} models to comply with the PCT's multiple-choice format (). We show that models give different answers when {not} forced (). We show that answers also change depending on {how} models are forced (). We show that multiple-choice answers vary across minimal prompt paraphrases (). We show that model answers change yet again in a more realistic open-ended setting (). {enumerate} Overall, our findings highlight clear instabilities and a lack of generalisability across evaluations. Therefore, {we recommend the use of evaluations that match likely user behaviours {in specific applications}}, accompanied by extensive robustness tests, to make local rather than global claims about values and opinions manifested in LLMs. {We make all code and data available at {https://github.com/paul-rottger/llm-values-pct}{github.com/paul-rottger/llm-values-pct"
"Same Task, More Tokens: the Impact of Input Length on the Reasoning Performance of Large Language Models",2402.14848v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.14848v2_0.pdf,"This paper explores the impact of extending input lengths on the capabilities of Large Language Models (LLMs). Despite LLMs advancements in recent times, their performance consistency across different input lengths is not well understood. We investigate this aspect by introducing a novel QA reasoning framework, specifically designed to assess the impact of input length. We isolate the effect of input length using multiple versions of the same sample, each being extended with padding of different lengths, types and locations. Our findings show a notable degradation in LLMs' reasoning performance at much shorter input lengths than their technical maximum. We show that the degradation trend appears in every version of our dataset, although at different intensities. Additionally, our study reveals that the traditional metric of next word prediction correlates negatively with performance of LLMs' on our reasoning dataset. We analyse our results and identify failure modes that can serve as useful guides for future research, potentially informing strategies to address the limitations observed in LLMs.","Reasoning performance drops as input grows, across a variety of tasks. Inputs are composed of text containing information relevant to the task (in red), and irrelevant text (grey) which is drawn from various sources and extended incrementally. Two separate text spans are required to answer correctly, and are located randomly in the input. Each point reflects the performance across 600 samples.","Recent advancements in Large Language Models (LLMs) show impressive performance across a range of tasks , including answering correctly complex questions requiring multiple reasoning steps . These models also claim to support increasingly longer inputs. This development underscores the need to examine their performance on the longer inputs they are now technically supporting. A reasonable assumption is that support for long inputs would transfer across tasks and enable a model adept at solving a task when presented in a short input prompt, to perform the same task when it is embedded within a longer prompt. Does this assumption hold? Recent studies that benchmark models over tasks that involve longer inputs, including reasoning tasks, indicate that indeed models often struggle with reasoning over long inputs . However, these studies do not properly control their variables, and vary both the input length and the associated tasks to be performed. This makes it it hard to say if the degraded performance is due to the requirement to work with longer input, or due to the task being generally harder. In this work, we study the effect of increasing the input length on model performance, while keeping other factors as constant as possible. We employ a methodology to measure model performance trends as a function of input length, by isolating it as a variable, while keeping the underlying task intact (). To that end, we introduce {F}lexible {LEN}gth {Q}uestion {A}nswering dataset (FLenQA) {{https://github.com/alonj/Same-Task-More-Tokens}{https://github.com/alonj/Same-Task-More-Tokens}}, a QA dataset for text-based reasoning (). For each sample, composed of a True/False question over two pieces of information required to answer it (the context), we create multiple versions of different lengths by embedding the context parts within longer, irrelevant texts. To ensure that models utilize their entire input, the dataset is composed of tasks for which both pieces of information must reasoned over together in order to correctly answer the question. At the same time, we keep the tasks simple enough such that models answer most of them correctly when the information pieces are presented on their own, with no additional padding. We show that LLMs quickly degrade in their reasoning capabilities, even on input length of 3000 tokens, which is much shorter than their technical maximum (on average over all tested models, a drop in accuracy from $0.92$ to $0.68$). Additionally, we explore the effect of embedding the information pieces in various locations within the context, as well as with two kinds of contexts: similar to the information pieces, or dissimilar to them (). We find that regardless of the experimental setting, there are similar trends of degradation. We also show that next-word prediction performance of models on long inputs is uncorrelated with their performance on downstream tasks of reasoning on long inputs (). Furthermore, we find that while {Chain-of-Thought} (CoT) prompting increases performance in short inputs, in most models it does not mitigate the degradation of performance when inputs are longer: while CoT prompting increases the accuracy over non-CoT prompting, the amount of increase is roughly consistent across context lengths, and is far from closing the performance drop due to long context (). The only exception to that is GPT4{we refer to the models gpt-4-1106-preview, gpt-3.5-turbo-1106 as GPT4 and GPT3.5 accordingly.}, in which the gap between CoT and normal prompting increases as the input is longer. Finally, we analyse our results and identify several failure modes in model responses (). We find that with longer inputs models tend not to follow specific instructions in the input, either providing no answer, or - in the case of CoT prompting - presenting the final answer before outlining the reasoning steps. We also observe a bias towards answering ""false"", as well as a decline in the models' ability to incorporate relevant information in their responses, as input length increases"
Do Llamas Work in English? On the Latent Language of Multilingual Transformers,2402.10588v4,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.10588v4_0.png,"We ask whether multilingual language models trained on unbalanced, English dominated corpora use English as an internal pivot language---a question of key importance for understanding how language models function and the origins of linguistic bias. Focusing on the family of transformer models, our study uses carefully constructed non English prompts with a unique correct single token continuation. From layer to layer, transformers gradually map an input embedding of the final prompt token to an output embedding from which next token probabilities are computed. Tracking intermediate embeddings through their high dimensional space reveals three distinct phases, whereby intermediate embeddings (1)~start far away from output token embeddings; (2)~already allow for decoding a semantically correct next token in middle layers, but give higher probability to its version in English than in the input language; (3)~finally move into an input language specific region of the embedding space. We cast these results into a conceptual model where the three phases operate in ``input space'', ``concept space'', and ``output space'', respectively. Crucially, our evidence suggests that the abstract ``concept space'' lies closer to English than to other languages, which may have important consequences regarding the biases held by multilingual language models. Code and data is made available here: \url{https://github.com/epfl-dlab/llm-latent-language}.","{Illustration of logit lens,} which applies language modeling head (here, \llama{}-7B) prematurely to latent embeddings in intermediate layers, yielding one next\hyp token distribution per position ($x$-axis) and layer ($y$-axis). We show final tokens of translation prompt (\cf\ \Secref{sec:data}) ending with ``Français:\ ""fleur"" - \zh{中文}:\ ""'' (where ``\zh{中文}'' means ``Chinese''). Final layer correctly ranks ``\zh{花}'' (translation of ``fleur'') on top, whereas intermediate layers decode English ``flower''. Color indicates entropy of next\hyp token distributions from low (blue) to high (red). (Plotting tool: \citet{belrose2023eliciting}.)","Most modern large language models (LLMs) are trained on massive corpora of mostly English text . Despite this, they achieve strong performance on a broad range of downstream tasks, even in non English languages . This raises a compelling question: How are LLMs able to generalize so well from their mainly English training data to other languages? Intuitively, one way to achieve strong performance on non English data in a data efficient manner is to use English as a pivot language, by first translating input to English, processing it in English, and then translating the answer back to the input language. This method has been shown to lead to high performance when implemented {explicitly} . Our guiding inquiry in this work is whether pivoting to English also occurs {implicitly} when LLMs are prompted in non English. In the research community as well as the popular press, many seem to assume that the answer is yes, epitomized by claims such as, ``The machine, so to say, thinks in English and translates the conversation at the last moment into Estonian'' . In this work, we set out to move beyond such speculation and investigate the question empirically. The question is of major importance. On the one hand, implicitly using English as an internal pivot could bias LLMs toward Anglocentric patterns that could predispose the model to certain linguistic elements (lexicon, grammar, metaphors, ), while also shaping more profound behaviors related to, , emotional stance or temporal reasoning . On the other hand, if LLMs do not use English as a pivot, it raises questions of how else they manage to work so remarkably well even in low resource languages. Overall, the quest for an internal pivot language holds promise to advance our understanding of how LLMs function no matter if we succeed. Investigating the existence of an internal LLM language is complicated by the scale and notoriously inscrutable nature of the neural networks behind LLMs, which after the input layer do not operate on discrete tokens, but on high dimensional floating point vectors. How to understand if those vectors correspond to English, Estonian, Chinese, ---or to no language at all---is an open problem, and the question of whether LLMs use an internal pivot language has therefore, to the best of our knowledge, not been addressed empirically before. {Summary of contributions} To overcome these hurdles, we draw on, and contribute to, the nascent field of mechanistic interpretability (\ ). In a transformer, each input token's embedding vector is gradually transformed layer by layer without changing its shape. After the final layer, an ``unembedding'' operation turns the vector into a next-token distribution. Focusing on the family of models ---among today's largest open source LLMs---we find that applying the ``unembedding'' operation prematurely in intermediate, non-final layers---a technique called {logit lens} ---already decodes a contextually appropriate token early on ({fig:translation-llama2-7b-ex-single}), giving us a (limited) glimpse at the model's otherwise hard to interpret numerical internal state. Exploiting this fact, we carefully devise prompts that allow us to determine whether a logit lens decoded token is semantically correct and to what language it belongs (, a prompt asking the model to translate French ``fleur'' [``flower''] to Chinese ``{花}''; \ {fig:translation-llama2-7b-ex-single}). Tracking language probabilities across layers, we observe that no contextually appropriate tokens are decoded in the first half of layers, followed by a sudden shift of probability mass onto the English version (``flower'') of the correct next token, and finally a shift to the correct next token in the target language (``{花}''). Expanding on this first evidence of English as an internal pivot language, we analyze latent embeddings directly as high dimensional Euclidean points, rather than via the logit lens. This allows us to draw a more nuanced picture of the anatomy of 's forward pass, suggesting that, in middle layers, the transformer operates in an abstract ``concept space'' that is partially orthogonal to a language specific ``token space'', which is reached only in the final layers. In this interpretation, the latent embeddings' proximity to English tokens observed through the logit lens follows from an English bias in concept space, rather than from the model first translating to English and then ``restarting'' its forward pass from there. We conclude by discussing implications and future directions for studying latent biases and their effects---a crucial step toward trustworthy AI"
Calibrating Large Language Models Using Their Generations Only,2403.05973v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2403.05973v1_0.pdf,"As large language models (LLMs) are increasingly deployed in user-facing applications, building trust and maintaining safety by accurately quantifying a model's confidence in its prediction becomes even more important. However, finding effective ways to calibrate LLMs---especially when the only interface to the models is their generated text---remains a challenge. We propose APRICOT \ (uxiliary edction of nfidence argets): A method to set confidence targets and train an additional model that predicts an LLM's confidence based on its textual input and output alone. This approach has several advantages: It is conceptually simple, does not require access to the target model beyond its output, does not interfere with the language generation, and has a multitude of potential usages, for instance by verbalizing the predicted confidence or adjusting the given answer based on the confidence. We show how our approach performs competitively in terms of calibration error for white-box and black-box LLMs on closed-book question-answering to detect incorrect LLM answers.",Illustration of APRICOT \peach: We train an auxiliary model to predict a target LLM's confidence based on its input and the generated answer.,"When given a case description of ``{A man superglued his face to a piano and he says it's making it hard to get a full night of sleep}'', a recently released medical LLM was found to list unrelated potential causes in its diagnosis, including narcolepsy, sleep apnea and others.{{https://x.com/spiantado/status/1620459270180569090} (last accessed Nov. 7, 2023).} This, of course, ignores the seemingly obvious reason for the patient's complaints. While humorous, this example illustrates the pitfalls of practical LLM applications: Despite often looking convincing on the surface---especially to non-experts---model responses can be wrong or unreliable, leading to potentially harmful outcomes or a loss of trust in the system, foregoing its benefits. Indeed, consistent behavior (imagine e.g.\@ reliably indicating a lack of confidence for unsure responses) has been argued as one way to build trust in automated systems , while misleading predictions have been empirically shown to lead to a loss of trust that can be hard to recover from . We introduce APRICOT , a method to use an auxiliary model to infer a LLM's confidence in an open-ended question-answering setting. The auxiliary model does so based on the given input to and generated output text from the LLM alone. The model is trained by using these two parts as input and predicting calibration targets. The latter are obtained without access to the LLM's sequence likelihoods or internal states by clustering input representations produced by an additional embedding model, and thus only require black-box access. This especially relevant since an increasing number of LLM providers safeguard their model behind black-box APIs. This approach is conceptually straightforward, easy to optimize, and opens up a large number of possible applications, for instance, verbalizing uncertainty---a recently popular way to communicate uncertainty for LLMs ---or adjusting a model's response with linguistic markers of confidence. Our contributions are as follows: We propose to obtain calibration targets without requiring any additional information about LLM internals or question metadata. We show that using auxiliary models on the target LLM's input and output is sufficient to predict a useful notion of confidence. We also perform additional studies to identify which parts of the LLM's output are most useful to predict confidence. All the code is openly available.{{https://github.com/parameterlab/apricot"
Iterative Forward Tuning Boosts In-Context Learning in Language Models,2305.13016v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2305.13016v3_0.pdf,"Despite the advancements in in-context learning (ICL) for large language models (LLMs), current research centers on specific prompt engineering, such as demonstration selection, with the expectation that a single iteration of demonstrations processing can generalize effectively to a given test sample. However, this perspective overlooks the potential benefits derived from multiple iterations involving demonstrations, a practice aligning more closely with the iterative decision-making process exhibited by humans, who often learn through analogy. In this study, we introduce a novel two-stage framework to boost ICL in LLMs. Specifically, our framework delineates the ICL process into two distinct stages: and test stages. The stage incorporates a unique attention mechanism, i.e., iterative enhanced attention, which enables multiple rounds of information accumulation. This mechanism operates by manipulating the Key-Value matrices without training, fostering enhanced understanding capabilities in LLMs by ``{thinking}'' demonstrations multiple times. We evaluated across a range of benchmarks and LLMs, showing its superior performance over vanilla ICL methods and its effectiveness in challenging tasks where demonstration selection is infeasible.","The illustrations of vanilla ICL and our proposed two-stage framework through \methodname. The vanilla ICL method processes demonstrations only once, while our ``\methodname'' method enables multiple rounds of information accumulation during the reasoning process.","Large language models (LLMs), e.g. OpenAI GPTs, LLaMA and Qwen, demonstrate the mysterious in-context learning (ICL) ability, where LLMs make predictions directly by prepending demonstrations to the original input without updating model parameters. LLMs are expected to learn the patterns hidden in demonstrations and make predictions accordingly. As illustrated in Figure (a), an LLM can correctly perform inference on an unseen task by conditioning on several demonstrations. The ICL paradigm empowers LLMs to achieve impressive results in various downstream tasks with a few demonstrations, making Language-Model-as-a-Service (LMaaS) possible. Since the performance of ICL is sensitive to specific prompt settings, considerable efforts have been developed to improve the performance of ICL by refining the prompt design from different perspectives, such as demonstration selection , instruction design , and intermediate chain-of-thought (CoT) reasoning . These methods can facilitate LLMs to reduce inference variance and avoid poor worst-case accuracy to some extent by performing prompt engineering. The working mechanism of ICL also draws a lot of attention. shed light on the connections between ICL and explicit fine-tuning. Specifically, ICL computes meta-gradients via forward computation, while explicit fine-tuning obtains gradients by back-propagation. A dual form exists between attention and gradient descent-based optimization , directly connecting the test input to demonstrations. argue that label words in demonstrations act as anchors, enabling mapping from demonstrations to test input through information aggregation and label propagation. However, these studies assume that the models process demonstrations only once (i.e., perform a single forward computation), which is incoordinate with the human decision-making process by learning from analogy. Humans usually learn from analogy via an iterative thinking process, such as analyzing demonstrations, reflecting on them, and forming abstract concepts. The models learned from demonstrations in inference time by ``{thinking for longer}'' or ``{thinking multiple times}'' . These findings inspire us to ask a question: {{Can we boost the performance of ICL by learning from demonstrations through several (iterative) forward inferences?}} In this paper, we propose a two-stage framework to boost the ICL ability in LLMs. Instead of simply concatenating demonstrations and test input together for inference, we decouple the ICL process into a stage for demonstration training and a test stage, as illustrated in Figure (b). In the stage, we introduce a new attention module that manipulates the updates of Key-Value matrices within the Transformer's self-attention mechanism. This modification leverages Key-Value matrices as a bridge to change the information flow to accumulate and learn information over multiple forward iterations without any training. During the test stage, since the concepts contained in demonstrations are already stored in final Key-Value matrices, we only need to feed the test input into the model and utilize the Key-Value cache for inference. This strategy is motivated by humans' repeat logical thinking and reasoning process. LLMs are expected to extend their abilities to solve unseen, complex tasks by ``{thinking}'' demonstrations multiple times. To verify the effectiveness of the proposed , we initially conduct evaluations on conventional ICL benchmarks across language models of various sizes. The experiments show that significantly outperforms vanilla ICL in a variety of model sizes and tasks, surpassing previous state-of-the-art (SOTA) methods focused on selecting demonstrations. In addition, we introduce two more challenging benchmarks (i.e., MMLU and BBH) and conduct experiments on advanced LLMs, including LLaMA2 and Pythia. We argue that on these challenging benchmarks, demonstration selection becomes impractical due to the lack of a potential candidate pool. obtains a significant advantage over vanilla ICL"
Steering Llama 2 via Contrastive Activation Addition,2312.06681v4,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2312.06681v4_0.pdf;/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2312.06681v4_1.pdf,"We introduce Contrastive Activation Addition (CAA), a method for steering language models by modifying their activations during forward passes. CAA computes ``steering vectors'' by averaging the difference in residual stream activations between pairs of positive and negative examples of a particular behavior, such as factual versus hallucinatory responses. During inference, these steering vectors are added at all token positions after the user's prompt with either a positive or negative coefficient, allowing precise control over the degree of the targeted behavior. We evaluate CAA's effectiveness on Llama 2 Chat using multiple-choice behavioral question datasets and open-ended generation tasks. We demonstrate that CAA significantly alters model behavior, is effective over and on top of traditional methods like finetuning and system prompt design, and minimally reduces capabilities. Moreover, we gain deeper insights into CAA's mechanisms by employing various activation space interpretation methods. CAA accurately steers model outputs and sheds light on how high-level concepts are represented in Large Language Models (LLMs).","We perform forward passes on contrastive examples of answers to multiple-choice questions, extracting residual stream activations at a particular layer at the token position of the answer. We then take the mean activation difference over many contrast pairs. At inference time, this vector is added back into the residual stream with a chosen multiplier at all token positions after the instruction to control the behavior.","As the capabilities of Large Language Models (LLMs) have grown rapidly in recent years, an increasing body of research aims to ensure they are ``helpful, honest, and harmless'' to reduce risks from misaligned, unsafe behavior . Researchers have developed several techniques for aligning LLMs, such as Reinforcement Learning from Human Feedback (RLHF), instruction finetuning , and prompt engineering . However, many challenges remain, including collecting diverse and representative datasets for the target behaviors, preventing hallucination, and mitigating out-of-distribution failures. Moreover, the way in which these methods work is often opaque. The set of alignment techniques known as ``activation engineering'' or ``representation engineering'' work by making targeted perturbations to a model's activations . Although activation engineering techniques have shown some promise as a way to steer models' behavior, their mechanisms, properties, and effects have yet to be robustly verified across different models and types of behaviors. We employ Contrastive Activation Addition (CAA) to modulate high-level alignment-relevant behaviors in LLMs and study its effects and properties in various test scenarios. We apply the technique to Llama 2, a collection of pretrained and finetuned LLMs ranging in scale from 7 to 70 billion parameters , primarily focusing on Llama 2 Chat, which is optimized for dialogue use-cases and finetuned using RLHF for safety. This enables us to study the interaction between RLHF/finetuning techniques and activation engineering, building on top of the existing body of research on pretrained models and demonstrating that CAA can be used on top of finetuning techniques to improve alignment-relevant properties. describes the process of generating steering vectors, including the datasets we used to construct them. presents our main results on the effects of CAA on multiple-choice and open-ended generation evaluations. In particular, across all of the seven categories we tested, the addition/subtraction of the steering vectors increased/decreased the prevalence of the behavior, as rated by GPT-4 . We then show CAA's effects on transfer, compare it to other alignment techniques such as system-prompting and finetuning, and investigate the geometrical relationships of the steering vectors. concludes by discussing our results qualitatively and pointing towards potential future research directions"
Deciphering Oracle Bone Language with Diffusion Models,2406.00684v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.00684v3_0.pdf,"Originating from China's Shang Dynasty approximately 3,000 years ago, the Oracle Bone Script (OBS) is a cornerstone in the annals of linguistic history, predating many established writing systems. Despite the discovery of thousands of inscriptions, a vast expanse of OBS remains undeciphered, casting a veil of mystery over this ancient language. The emergence of modern AI technologies presents a novel frontier for OBS decipherment, challenging traditional NLP methods that rely heavily on large textual corpora, a luxury not afforded by historical languages. This paper introduces a novel approach by adopting image generation techniques, specifically through the development of Oracle Bone Script Decipher (). Utilizing a conditional diffusion-based strategy, generates vital clues for decipherment, charting a new course for AI-assisted analysis of ancient languages. To validate its efficacy, extensive experiments were conducted on an oracle bone script dataset, with quantitative results demonstrating the effectiveness of . {Code and decipherment results will be made available} at {https://github.com/guanhaisu/OBSD}.",Conditional diffusion model for OBS decipherment.,"Oracle Bone Script (OBS) represents an ancient language inscribed on turtle shells and animal bones, extensively utilized during China's Shang Dynasty, a feudal dynasty dating back 3,000 years. The script not only chronicled the human geography and daily activities of that period but also encapsulates invaluable historical significance, offering a unique window into the linguistic and cultural practices of early Chinese civilization. However, despite the discovery of tens of thousands of fragments of oracle bones, a significant portion of the characters remain undeciphered, leaving the rest shrouded in mystery. To date, more than 4,500 Oracle Bone Script (OBS) characters have been discovered, but only about 1,600 of these have been deciphered and linked to their modern Chinese counterparts. In modern Chinese, Unicode includes more than 90,000 Chinese characters, though only approximately 3,500 characters are commonly used in contemporary Chinese society. This challenge of understanding the remaining undeciphered OBS characters and linking them to modern Chinese has attracted significant research interest, with attempts being made to leverage modern AI technologies for the understanding of such an ancient language. However, the majority of existing methodologies primarily focus on the recognition and understanding of already deciphered OBS, with the utilization of AI to assist in the decipherment of unknown inscriptions remaining an underexplored area. This is partly because, unlike modern languages that can be digitized and stored as text due to established encoding systems, OBS lacks a standard input method or encoding scheme, resulting in its preservation predominantly in the form of images rather than digital text usually used in NLP methods. Additionally, since OBS was inscribed on turtle shells and animal bones, many of which have been damaged or fragmented upon discovery, there is essentially no complete corpus available. This absence of a comprehensive corpus severely limits the applicability of language models that require extensive datasets for training, such as BERT, RoBERTa, and GPT. To address the challenges inherent in the decipherment of OBS using conventional NLP methodologies, this paper introduces a novel approach by employing image-based generative techniques for auxiliary decipherment of OBS. Specifically, we train a conditional diffusion model that utilizes unseen categories of OBS as a conditional input to generate corresponding images of its modern counterpart. This direct provision of modern representations or potential decipherment clues leverages the model's learned evolution from ancient scripts to contemporary fonts, circumventing the corpus construction and other challenges that traditional NLP methods face with ancient languages. Notably, while our experiments focus on OBS, this training paradigm holds the potential for extension to other ancient languages, such as Cuneiform and Hieroglyphics. In summary, this paper makes three key contributions: {itemize} We introduce a novel approach to the task of ancient script decipherment by utilizing image generation techniques, offering a novel solution to challenges that conventional NLP methods struggle to address. We propose Oracle Bone Script Decipher (OBSD), a conditional diffusion model optimized for OBS decipherment. Our Localized Structural Sampling technique enhances the model's ability to discern and interpret the intricate patterns of characters. OBSD demonstrates its effectiveness in decipherment through comprehensive ablation studies and benchmark comparisons. It offers a pioneering approach for AI-assisted ancient language decipherment, potentially laying a foundation for future research. {itemize"
CHECKWHY: Causal Fact Verification via Argument Structure,2408.10918v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2408.10918v2_0.pdf,"With the growing complexity of fact verification tasks, the concern with ``thoughtful'' reasoning capabilities is increasing. However, recent fact verification benchmarks mainly focus on checking a narrow scope of semantic factoids within claims and lack an explicit logical reasoning process. In this paper, we introduce , a challenging dataset tailored to a novel causal fact verification task: checking the truthfulness of the causal relation within claims through rigorous reasoning steps. consists of over 19K ``{why}'' {claim-evidence-argument structure} triplets with {supports}, {refutes}, and {not enough info} labels. Each argument structure is composed of connected evidence, representing the reasoning process that begins with foundational evidence and progresses toward claim establishment. Through extensive experiments on state-of-the-art models, we validate the importance of incorporating the argument structure for causal fact verification. Moreover, the automated and human evaluation of argument structure generation reveals the difficulty in producing satisfying argument structure by fine-tuned models or Chain-of-Thought prompted LLMs, leaving considerable room for future improvements}.","An entry from C\textsc{heck}W\textsc{hy}: a ``{why}'' {claim} with its corresponding {cause} and {effect}, and an {argument structure} representing the reasoning process from cause to effect. Notably, the cause-effect pair is used solely during the annotation process and not included in the argument structure, implying that it is implicitly inferred from the claim, rather than being provided explicitly.","Fact verification is a crucial debunking task that entails verifying the truthfulness of claims by cross-referencing them with reliable evidence drawn from established resources, which prevents the proliferation of erroneous information online and fosters public trust. However, with the multi-step reasoning capability in fact verification models remaining uncertain, existing research reflects the deficiency in in-depth understanding of the explicit reasoning mechanisms when performing inference on multi-hop evidence. This prompts us to develop a strong benchmark that incorporates the interpretable ``thought'' process to assess the logical reasoning capabilities of models. Currently, substantial progress has been made on common fact verification benchmarks, e.g., H{o}V{er}, FEVEROUS, and S{ci}T{ab}. Nevertheless, existing resources have inherent limitations. Classic datasets primarily focus on verifying the semantic factoids of ``{who}'', ``{what}'', ``{when}'', and ``{where}'' within the claim. For example, verifying the claim ``{John Lennon was born before the astronaut who drank the first coffee in space.}'' can be decomposed into verifying factoids such as ``{who, and when the astronaut drank the first coffee in space.}'' and ``{when the man was born.}''. However, these semantic factoids can be answered individually by straightforward factoids-matching between the claim and distinct independent evidence, e.g., word overlapping or proof matching, thereby limiting its significant potential to summarize the correlational evidence with ``thought'' steps. Heretofore, noticeably absent in prior datasets are ``{why}'' claims: containing causal relations that need to be verified. These claims prompt for not simple factoid matching, but an explicit logical reasoning path for verification. More specifically, Figure~ presents a ``{why}'' claim featuring a cause-effect pair where ``{military crises}'' causes the ``{decrement of the purity of denarius silver.}''. Verifying such causal relations is quite challenging, which necessitates complex logical reasoning and context information beyond the factoids within the claim. For instance, to {support} this causal relations (i.e., {military crises} $[]{cause}$ {decrease purity of silver}), it is essential to incorporate the extra intermediate reasoning steps to bridge the connection between the cause-effect pair, thus forming a logical reasoning path: {military crises} $[]{{{1}}}$ {financial crisis} $[]{{{2}}}$ {increase amount of currency} $[]{{{3}}}$ {decrease purity of silver}. The rationale behind {1} is that {military crises} often coincide with extra factors such as {trade disruption} or {more government spending}, leading to the {financial crisis}. The reason supporting {2} is that {increase amount of currency} is a demanding response to suppress the {financial crisis}. Furthermore, {the rights to mint money that is safeguarded by Roman Law} ensures the behavior of {3} is established. The above inference reveals a coherent reasoning process, which involves aligning the construction of a ``thoughtful'' logical structure among correlational evidence with causal relation verification. In this paper, we introduce C{heck}W{hy}, a challenging dataset built around a novel causal fact verification task: assessing whether the causal relation within the claim is valid by explicit logical structure. This dataset consists of 19,596 {claim-evidence-argument structure} triplets derived from the {WikiWhy} dataset. The uniqueness of C{heck}W{hy} is that each entry contains a ``{why}'' {claim} with causal relations and an {argument structure} formed by correlated evidence: the latter is inspired by the theory literature on argument structure, which depicts how the different statements fit together as wholes to allegedly lend support to the claim. Moreover, inspired by prior research, we assume that the label of a causal relation within the claim depends on the provided argument structures, rather than the semantics itself. Thus, each claim is labeled as {supports}, {refutes}, or {not enough info} based on different argument structures. In addition, to prevent the bias in human cognition, we employ a human-model collaboration annotation approach to generate claims, evidence, and corresponding argument structures. Compared to existing datasets, {CheckWhy} covers a variety of topics and argument structures, which may prove valuable for performing causal reasoning across various scenarios. We propose a new causal fact verification task to assess whether the causal relation described in the claim is valid via logical structure and introduce the corresponding C{heck}W{hy} dataset, which consists of a total of 19,596 samples derived from the {WikiWhy} dataset. The uniqueness of C{heck}W{hy} lies in the fact that each entry contains a ``{why}'' {claim} with causal relations and an {argument structure} formed by correlated evidence. The latter is inspired by the theory literature on argument macrostructure, which depicts how different statements fit together as wholes to allegedly lend support to the claim. We adopt a complex annotation strategy in which each claim is labeled as {supports}, {refutes}, or {not enough info} based on different argument structures. We assume that the label of the causal relation within the claim depends on the provided argument structures, rather than the semantics itself. Moreover, to prevent the bias in human cognition, we employ a human-model collaboration annotation approach, as depicted in Figure~, to generate claims, evidence, and corresponding argument structure. Compared to existing datasets, {CheckWhy} covers a variety of topics and argument structures, which may prove valuable for developing the skill of causal reasoning across various scenarios.\ Based on the experiments on four tasks we propose and the human evaluation in our {CheckWhy}, our experiments reveal the significance of incorporating the argument structure for causal fact verification. Meanwhile, our experiments in argument structure generation also validate the difficulty in producing satisfying argument structures for causal claims. Our key contributions are summarized as follows: (I) We propose verifying the ``why'' claims with causal relations through reasoning on argument structure as a novel causal fact verification formulation. (II) We construct {CheckWhy} by introducing a human-model collaboration annotation approach, drawing inspiration from the theory research on argument structure. (III) We conduct thorough experiments on state-of-the-art models with four tasks, including fine-tuned models and LLMs, which investigates various settings and points out the potential for improvement"
Quality-Aware Translation Models: Efficient Generation and Quality Estimation in a Single Model,2310.06707v4,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2310.06707v4_0.pdf,"Maximum-a-posteriori~(MAP) decoding is the most widely used decoding strategy for neural machine translation~(NMT) models. The underlying assumption is that model probability correlates well with human judgment, with better translations getting assigned a higher score by the model. However, research has shown that this assumption does not always hold, and generation quality can be improved by decoding to optimize a utility function backed by a metric or quality-estimation signal, as is done by Minimum Bayes Risk~(MBR) or quality-aware decoding. The main disadvantage of these approaches is that they require an additional model to calculate the utility function during decoding, significantly increasing the computational cost. In this paper, we propose to make the NMT models themselves quality-aware by training them to estimate the quality of their own output. Using this approach for MBR decoding we can drastically reduce the size of the candidate list, resulting in a speed-up of {two-orders of magnitude}. When applying our method to MAP decoding we obtain quality gains similar or even superior to quality reranking approaches, but with the efficiency of single pass decoding.",Alignment between predicted quality scores from the QA Prediction model and actual \bleurtqe scores of translations in the en $\rightarrow$ de test dataset. The boxplots show the distribution of actual scores across all samples assigned to each bin. The median ground truth quality score increases steadily in line with the predicted bins.,"Most state-of-the-art models for natural language processing tasks are probabilistic, with the most frequent parameterization being based on neural networks. Once these models are trained, the prevailing decoding strategy for natural language generation is MAP decoding, i.e.\ select the hypothesis that maximizes the conditional probability given an input. As an exact maximization is computationally intractable, typically beam search or greedy decoding are used to approximate the search for the best hypothesis. Neural Machine translation is a prominent example of these types of models, where the system is trained to generate a sentence in a target language given a source sentence in another language. Nonetheless, have demonstrated that MAP decoding methods may be suboptimal due to the presence of misaligned probability distributions. Moreover, NMT models often assign human translations lower probabilities than their own beam search outputs due to calibration issues. applied MBR decoding for NMT models as an alternative generation approach. MBR decoding follows a self-consistency approach by sampling from the model distribution and giving preference to hypotheses that exhibit greater similarity to all other hypotheses. In contrast to MAP decoding, MBR decoding's objective is not centered on generating the translation with the highest estimated model probability, instead it selects the translation that exhibits the highest quality based on a utility metric. Subsequent research conducted by showed that MBR decoding with {neural} utility metrics leads to significant improvements over beam search decoding. However, MBR is computationally intensive, with a time complexity of $O(M^2)$ for a candidate list containing $M$ samples, ideally $M=100$ to $1\,000$ according to . Note than when using neural metrics, each ``computation step'' in the quadratic complexity is itself computationally expensive, requiring a forward pass through a large neural network. As an alternative to MBR decoding, we can use a quality-aware decoding strategy, generating a list of candidate translations and reranking them using a neural quality estimation~(QE) metric that computes a quality score for the translation conditioned only on the source sentence. This method offers the advantage of being more efficient than MBR decoding, as its inference speed scales linearly with the number of candidate translations. A study conducted by showed that employing neural metrics for QE reranking exhibits comparable advantages to those seen with MBR decoding. However, this approach still demands the use of a separate, computationally expensive QE model to evaluate the quality of each candidate. In our work, we propose a novel method that moves quality awareness inside the model itself, enabling us to guide the generation process towards higher-quality translations, and eliminating the need for an external QE model during decoding. Specifically, we investigate two key strategies: Quality-Aware Prompting, where we use quality prompts that explicitly encourage the generation of high-quality translations, and Quality-Aware Prediction, where we enable an NMT model to judge the quality of its own translations. Both strategies add special quality tokens to each NMT training example. The strategies differ only in whether the token is included in the source or the target sentence. Our main scientific contributions are the introduction of quality-aware translation models, and their application for improved, more efficient decoding strategies. We analyze two use cases: {itemize} We propose a novel reranking approach that eliminates the necessity for external QE models during decoding while maintaining the same level of translation quality. We can achieve similar or even superior results with single pass decoding, eliminating the need of a costly reranking step. We show that pre-filtering the candidate list according to the model's quality prediction can dramatically boost decoding performance of MBR, by up to two orders of magnitude, while increasing translation quality. {itemize"
American Sign Language Handshapes Reflect Pressures for Communicative Efficiency,2406.04024v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.04024v2_0.pdf,"Communicative efficiency is a key topic in linguistics and cognitive psychology, with many studies demonstrating how the pressure to communicate with minimal effort guides the form of natural language. However, this phenomenon is rarely explored in signed languages. This paper shows how handshapes in American Sign Language (ASL) reflect these efficiency pressures and provides new evidence of communicative efficiency in the visual-gestural modality. We focus on hand configurations in native ASL signs and signs borrowed from English to compare efficiency pressures from both ASL and English usage. First, we develop new methodologies to quantify the articulatory effort needed to produce handshapes and the perceptual effort required to recognize them. Then, we analyze correlations between communicative effort and usage statistics in ASL or English. Our findings reveal that frequent ASL handshapes are easier to produce and that pressures for communicative efficiency mostly come from ASL usage, rather than from English lexical borrowing.}","Examples of handshapes in ASL components. The ASL lexicon can be divided into a native component (e.g. signs native to ASL; left) and a foreign component (e.g. fingerspelling, loan signs; right). 19 out of 22 handshapes in ASL fingerspelling also appear in the native lexicon\protect\footnotemark.","There is increasing evidence suggesting that human languages adapt to the needs of their users by minimizing the effort required by the sender and receiver to achieve successful communication. It is argued that natural languages tend to find an optimal balance between the efforts of the two participants when they are at odds with each other . While the majority of existing work studying communicative efficiency has focused on spoken languages, recent results have provided evidence that signed languages are also shaped by drives for efficient communication in the visual-gestural modality. For example, found that casual signing prioritizes moving fewer, more distal joints, increasing articulatory ease. found that rare hand configurations are produced close to the signer's face, increasing perceptual ease.{Handshape artworks shared with permission from .} This paper further explores communicative pressures on visual signed languages by focusing on American Sign Language (ASL) handshapes. Handshapes refer to distinctive configurations of the hand and fingers, and are one of the five fundamental parameters characterizing signs in ASL (). A finite set of handshapes is combined with different movements, locations, palm orientations, and non-manual markers to express various ASL signs. Figure shows example handshapes, some of which are used only in native ASL signs (left), some of which are used only in ASL signs borrowed from other languages (right), and some of which are used in both contexts (center). We investigated evidence of communicative efficiency in both the {native} and {foreign} components of ASL. {Foreign} signs are borrowed from other languages in contrast to {native} signs that are inherently derived from ASL itself.{Although signs originating from lexical borrowing are technically a part of ASL, we refer to them as the ``foreign'' component for our purposes, following . The foreign component of ASL includes signs borrowed from English as well as signs borrowed from other signed languages (e.g. country signs such as JAPAN, CHINA). In this paper, we only address foreign signs that derive from English.} The foreign component of ASL includes {neutral fingerspelling} where English words are spelled out using one-handed signs that each represent a letter of the English alphabet (); {loan signs} where commonly fingerspelled words evolved into a lexicalized sign; and {initialized signs} that are produced using the handshape of the first letter of its English translation. To compare effects of ASL and English usage on handshape efficiency, we focused on fingerspelling handshapes ({FS handshapes}; Figure ) since 19 out of 22 FS handshapes appear in both the native and foreign components. We were motivated by the following research questions: {enumerate} [{RQ1}]Do handshapes reflect pressure for communicative efficiency? [{RQ2}]Is pressure for efficiency mostly or all from ASL usage, or does English usage also play a role? {enumerate} To test these ideas, we designed new methodologies to measure {articulatory effort} required by the sender to produce handshapes, as well as the {perceptual effort} needed by the receiver to recognize handshapes. We propose three predictions following from the hypothesis of communicative efficiency ({P1-3}). First, we predicted that according to the hypothesis of communicative efficiency, FS handshapes which appear frequently in native ASL signs should be easier to produce: this would help to keep overall articulatory (sender) effort low ({P1}). Our evidence indeed supports this prediction: we found a positive correlation between handshape frequency in native ASL signs and articulatory ease. Next, since foreign signs obey fewer phonological constraints observed in ASL , one might expect that handshapes in foreign signs reflect little to no pressure for communicative efficiency. If so, letters that appear frequently in English do not necessarily correspond to being easier to sign in ASL fingerspelling ({P2}). We indeed found no significant correlation between English letter frequency and fingerspelling articulatory ease, which supports {P2}. If English usage has negligible effect on communicative efficiency, foreign signs should reflect little to no pressure for efficiency in {perceptual (receiver) effort} either ({P3}). If there are perceptual pressures for efficiency, pairs of letters that appear in similar contexts in English should be easier to disambiguate from one another in ASL fingerspelling: this would help to keep receiver effort low by placing perceptually disambiguating elements in places that might otherwise be confusable. We indeed verified that there is no correlation between English letter confusability and perceptual ease in ASL fingerspelling, which supports {P3}. In summary, our analysis finds evidence of pressure for articulatory ease in ASL handshapes and suggests that pressure for communicative efficiency mostly comes from ASL usage, not from English usage"
Reasoning in Conversation: Solving Subjective Tasks through Dialogue Simulation for Large Language Models,2402.17226v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.17226v1_0.pdf,"Large Language Models (LLMs) have achieved remarkable performance in {objective} tasks such as open-domain question answering and mathematical reasoning, which can often be solved through recalling learned factual knowledge or chain-of-thought style reasoning. However, we find that the performance of LLMs in {subjective} tasks is still unsatisfactory, such as metaphor recognition, dark humor detection, etc. Compared to objective tasks, subjective tasks focus more on interpretation or emotional response rather than a universally accepted reasoning pathway. Based on the characteristics of the tasks and the strong dialogue-generation capabilities of LLMs, we propose {RiC} ({R}easoning {i}n {C}onversation), a method that focuses on solving subjective tasks through dialogue simulation. The motivation of {RiC} is to mine useful contextual information by simulating dialogues instead of supplying chain-of-thought style rationales, thereby offering potential useful knowledge behind dialogues for giving the final answers. We evaluate both API-based and open-source LLMs including GPT-4, ChatGPT, and OpenChat across twelve tasks. Experimental results show that {RiC} can yield significant improvement compared with various baselines.","Illustration of our method. (a) An example of the metaphor recognition task. (b) Incorrect responses by LLM using zero-shot-CoT~\cite{kojima2023large} prompting. (c) Our method can simulate helpful dialogues (shown in the dashed box), thereby offering useful information in the generated conversation and aiding reasoning on this subjective task.","Large language models (LLMs; ) have made rapid advancements in recent years and have achieved excellent performance on various objective tasks, including open-domain question answering, mathematical reasoning, and code generation, {etc}. Despite the success, research on LLMs in {subjective} tasks is still underexplored, as examples shown in Table~. Different from objective tasks that can often be clearly defined and solved, subjective tasks ({e.g.}, metaphor recognition and dark humor detection) involve the capability to perceive context, language nuances, and emotions, which cannot be easily quantified or objectively measured, thereby posing challenges for current LLMs. Recent methods based on chain-of-thought (CoT) style prompting have improved the reasoning abilities of LLMs, showing promising results on tasks such as commonsense and mathematical reasoning. However, compared to these objective tasks, we found that such methods are not particularly effective on subjective tasks. As an example shown in Figure~(b), the reasoning pathway does not comprehend the metaphorical expression in ``{Joseph has the heart of a lion}'' well, resulting in incorrect responses. Dialogue, alternatively, provides humans with a means to raise questions, convey emotions, and express opinions, which can be seen as another way to facilitate subjective reasoning. Considering the characteristics of subjective tasks and the strong ability of dialogue generation for LLMs, we propose {RiC} ({R}easoning {i}n {C}onversation), a method aiming to uncover the subjective expressions in simulated dialogues instead of objective and relatively unified reasoning pathways for better reasoning on subjective tasks. By employing this method, as Figure~(c) shows, the metaphorical relationship between ``{Joseph has the heart of a lion}'' and ``{Joseph is very kind}'' is correctly identified in the simulated dialogues, thus helping LLMs in giving the final answer. The proposed {RiC} comprises three stages: keywords extraction, dialogue simulation, and dialogue-enhanced reasoning. To enable better comprehension of the questions and dialogue generation, we first allow LLMs to extract task-relevant keywords according to the question, which has been shown helpful for understanding the task and generating related dialogue. Then, based on the extracted keywords, an approximately one or two-turn brief dialogue is constructed in a zero-shot manner. Finally, we enable LLMs to engage in reasoning based on both the original question and the simulated dialogue scenario. We employ both API-based and open-source LLMs including GPT-4, ChatGPT, and OpenChat, to validate the effectiveness of our method. Experimental results show that {RiC} leads to significant and consistent improvements under both zero-shot and few-shot settings, underscoring the effectiveness of leveraging the knowledge in dialogue for better solving subjective tasks"
COKE: A Cognitive Knowledge Graph for Machine Theory of Mind,2305.05390v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2305.05390v2_0.pdf,"Theory of mind (ToM) refers to humans' ability to understand and infer the desires, beliefs, and intentions of others. The acquisition of ToM plays a key role in humans' social cognition and interpersonal relations. Though indispensable for social intelligence, ToM is still lacking for modern AI and NLP systems since they cannot access the human mental state and cognitive process beneath the training corpus. To empower AI systems with the ToM ability and narrow the gap between them and humans, in this paper, we propose : the first cognitive knowledge graph for machine theory of mind. Specifically, formalizes ToM as a collection of 45k+ manually verified cognitive chains that characterize {human mental activities} and subsequent behavioral/affective responses when facing specific {social circumstances}. In addition, we further generalize using LLMs and build a powerful generation model tailored for cognitive reasoning. Experimental results in both automatic and human evaluation demonstrate the high quality of , the superior ToM ability of , and its potential to significantly enhance social applications. We release our code and data at .",\coke instantiates the Theory of Mind as cognitive chains in social situations.,"In social environments, human beings must be able not only to react to what others are doing, but also to anticipate what they will do. This ability to understand and infer human goals is typically described as Theory of Mind (ToM) . One way of accomplishing ToM is to observe what others do in various situations, and derive a set of affective and behavioral rules. When the same or highly similar things arise again, we can bring out plausible predictions accordingly . Figure presents an example that {someone will deliver a talk at the university tomorrow} (a social circumstance), and {he has substantial public speaking experience} (a trigger factor). We can plausibly anticipate that {he will deliver an impressive speech} (a mental activity), {feels joyful} (an affective response), and {has a restful sleep tonight} (a behavioral response). Here ToM is instantiated as a chained cognitive process that derives from our knowledge, experiences, and memories . ToM is indispensable to humans since it allows us to leverage our own minds to simulate others', so as to achieve efficient communication . {comment} {comment} Despite its importance for social intelligence, ToM is not well internalized by modern AI and NLP systems. illustrates a significant decline in performance and outright failure of Large Language Models (LLMs) in ToM tasks, particularly evident when confronted with adversarial samples. The main reason is that learning-based systems are usually trained on superficial text corpora, while lacking access to the underlying human mental state and cognitive process . In other words, NLP systems rely on the maximum likelihood to understand and generate texts, but do not go beneath the surface to the desires, beliefs, and intentions of humans. In this paper, we introduce : the first {{CO}gnitive {K}nowledg{E} graph} for machine theory of mind. Our goal is to formalize ToM and make it accessible and learnable for AI systems. In , we instantiate ToM as a collection of manually verified cognitive chains that characterize humans' mental activities in specific social circumstances along with their behavioral and affective responses . Each cognitive chain involves five types of nodes: {1) {situations}} denote the social circumstances; {2) {clues}} denote the trigger factors; {3) {thoughts}} denote the mental activities; {4) {actions}} denote the behavioral responses; {5) {emotions}} denote the affective responses. Moreover, as shown in Figure , individuals react differently to the same situation due to the diversified cognitive processes. Therefore, for each situation, we derive multiple cognitive chains and further label them as {{positive}} (means optimistic) or {{negative}} (means pessimistic) to mark the chain polarity. We propose to induce the raw data from LLMs, and then recruit educated workers majoring in psychology for manual selection and revision. The resulting knowledge graph constitutes 62,328 nodes and 45,369 cognitive chains. The construction of offers the basic ToM ability to understand and infer the human goals in already collected situations . But obviously, it is impossible to enumerate all situations in the real world. Thus we move one step further and build a cognitive language model to cope with unseen situations that have not appeared in the knowledge graph. Specifically, we decompose the construction of cognitive chains into four cognitive generation tasks, then finetune LLMs using the manually collected data in . By this means, we combine the commonsense knowledge embedded in LLMs and the ToM ability provided by , enabling to infer cognitive chains for unseen situations. We summarize our contributions in this work as follows. {1)} We propose the first cognitive knowledge graph for machine theory of mind. We instantiate human theory of mind as a collection of 45k$+$ manually verified cognitive chains, which provides a basic ToM ability for accessing and learning. {2)} We build a powerful cognitive language model by associating with LLaMA-2 , so as to predict cognitive chains for out-of-KG situations. {3)} We conduct extensive experiments to evaluate the ToM ability of and typical LLMs. The results show that outperforms strong baseline models such as GPT-4 in both zero-shot and few-shot settings, proved by automatic and human evaluations in all cognitive generation tasks, which in turn demonstrates the high quality of . {4)} We further substantiate the potential of in enhancing social applications, and prove its effectiveness on downstream emotional support conversation task"
MMToM-QA: Multimodal Theory of Mind Question Answering,2401.08743v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2401.08743v2_0.pdf,"Theory of Mind (ToM), the ability to understand people’s mental states, is an essential ingredient for developing machines with human-level social intelligence. Recent machine learning models, particularly large language models, seem to show some aspects of ToM understanding. However, existing ToM benchmarks use unimodal datasets – either video or text. Human ToM, on the other hand, is more than video or text understanding. People can flexibly reason about another person’s mind based on conceptual representations (e.g., goals, beliefs, plans) extracted from any available data. To address this, we introduce a multimodal Theory of Mind question answering (MMToM-QA) benchmark. MMToM-QA comprehensively evaluates machine ToM both on multimodal data and on different kinds of unimodal data about a person’s activity in a household environment. To engineer multimodal ToM capacity, we propose a novel method, BIP-ALM (Bayesian Inverse Planning Accelerated by Language Models). BIP-ALM extracts unified representations from multimodal data and utilizes language models for scalable Bayesian inverse planning. We conducted a systematic comparison of human performance, BIP-ALM, and state-of-the-art models, including GPT-4. The experiments demonstrate that large language models and large multimodal models still lack robust ToM capacity. BIP-ALM, on the other hand, shows promising results, by leveraging the power of both model-based mental inference and language models..}","{Sketch of the MMToM-QA benchmark}. Each question is associated with a video stream (representative frames highlighting key moments are shown above for illustration) and text input (illustrative text above is shortened for brevity). In the example video, Emily can see the wine glass on one of the kitchen tables (1st frame) and passes by it without picking it up (2nd frame). At the end of the clip (3rd frame), it appears that she could be walking towards the cabinets on the left side of the room; or she might want to check if a goal object is inside the microwave. The text indicates that there are no cupcakes in the cabinets, but there is a cupcake inside the microwave. To confidently choose the correct answer, a model must fuse relevant information from both the video and the text.","Theory of Mind (ToM) is the cognitive ability to ascribe hidden mental states (e.g. goals, beliefs, and desires) to other individuals based on their observed behavior. A hallmark of human social intelligence, ToM serves as the foundation for a wide range of social interactions and a pillar of commonsense reasoning . Systems designed to safely and productively interact with humans in an open-ended manner, such as assistive robots [e.g.,][]{dautenhahn2007socially, hadfield2016cooperative,patel2022proactive,puig2023nopa}, AI teachers [e.g.,][]{wang2021towards}, and autonomous vehicles [e.g.,][]{chandra2020stylepredict}, would greatly benefit from incorporating ToM reasoning capabilities. The recent advancements in machine learning, especially in the realm of Large Language Models (LLMs), have spurred increased interest in assessing these models' aptitude for ToM reasoning [e.g.,][]{rabinowitz2018machine,kosinski2023theory, sap2019socialiqa, sap2022neural, ullman2023large, shapira2023clever, shu2021agent,moghaddam2023boosting,nematzadeh2018evaluating, gandhi2021baby, gandhi2023understanding,kim2023fantom}. Many of these assessments use either text-based or video-based benchmarks inspired by classic ToM experiments in the cognitive science literature . While the recent ToM benchmarks provide well-designed, cognitively informed tools, they share several notable limitations. One such limitation is the dependence on massive training data, which raises the concern that these models work by finding data patterns in a way that deviates from human-like ToM reasoning [e.g.,][]{ullman2023large, sap2022neural, shapira2023clever}. This paper focuses on a different but related limitation: These benchmarks rely on unimodal data, either in the form of videos [e.g.,][]{gandhi2021baby}, or textual descriptions of actions and environments [e.g.,][]{kosinski2023theory, sap2022neural,gandhi2023understanding}. But ToM reasoning goes beyond merely text comprehension or video understanding. It is about forming a causal model of another person's mind, which connects mental variables to possible actions . Such a model can infer mental states from either words or vision separately, or fuse the separate information to form a single coherent mental scene. By examining multimodal ToM reasoning, we can both gain insight into the computational models that underlie human ToM and offer a stronger test for current ML models, particularly LLMs. To systematically evaluate the ability of ML models to infer mental states from multimodal data, we developed a novel Multimodal Theory of Mind Question Answering benchmark (MMToM-QA). As shown in Figure~, the benchmark includes as input both videos and text describing the activity of a person in a household environment. The benchmark also includes questions associated with different points in each of the videos. The questions refer to the mental states (goals and beliefs) of the person described by the video or text. Each question has two possible options, neither surely true nor surely false, but with one option significantly more likely to be true given the observations. Some questions can be adequately answered based on a single modality, but some questions require fusing information from both modalities (e.g. understanding the woman's goal in Figure~). We validated our benchmark through human experiments, showing that people are adept at answering the questions in the benchmark and providing a human baseline. We propose a novel multimodal ToM model, {Bayesian Inverse Planning Accelerated by Language Models} (). As illustrated in Figure~, {} first extracts symbolic representations about the physical scene and the actions of the person from both video and text inputs. Using these symbolic representations, {} then extends Bayesian inverse planning (BIP) , a cognitively grounded ToM method originally designed for visual data, to reason about the multimodal data. To accelerate the inference in real-world scenarios such as household activities in our benchmark, {} uses a language model (LM) finetuned on human activity data to evaluate the likelihood of hypotheses about the person's belief and goal. By doing so, it takes advantage of the robustness of Bayesian inverse planning, as well as the scalability and open-endedness of LMs. We compared the performance of {} and several state-of-the-art models for text QA or multimodal QA, including GPT-4(V). We found that existing models, however impressive in other QA benchmarks, make large and systematic errors in our benchmark, and fail to match human performance. In contrast, {} significantly outperforms these models. In sum, our main contributions include (1) the first benchmark for multimodal ToM, (2) a novel ToM reasoning method, , that combines Bayesian inverse planning and LMs to conduct robust yet efficient ToM inference based on multimodal data, and (3) a systematic comparison of different ML models and human ToM"
ICLEF: In-Context Learning with Expert Feedback for Explainable Style Transfer,2309.08583v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2309.08583v2_0.pdf,"While state-of-the-art large language models (LLMs) can excel at adapting text from one style to another, current work does not address the {explainability} of style transfer models. Recent work has explored generating textual explanations from larger teacher models and distilling them into smaller student models. One challenge with such approach is that LLM outputs may contain errors that require expertise to correct, but gathering and incorporating expert feedback is difficult due to cost and availability. To address this challenge, we propose , a novel human-AI collaboration approach to model distillation that incorporates {scarce} expert human feedback by combining {in-context learning} and {model self-critique}. We show that our method leads to generation of high-quality synthetic explainable style transfer datasets for formality ({}) and subjective bias ({}). Via automatic and human evaluation, we show that specialized student models fine-tuned on our datasets outperform generalist teacher models on the explainable style transfer task in one-shot settings, and perform competitively compared to few-shot teacher models, highlighting the quality of the data and the role of expert feedback. In an extrinsic task of authorship attribution, we show that explanations generated by smaller models fine-tuned on {} are more predictive of authorship than explanations generated by few-shot teacher models.","Generating \textsc{e-GYAFC}: formality style transfer dataset GYAFC \cite{rao-tetreault-2018-dear} is augmented with semi-structured natural language explanations. The LLM generates the informal attributes of the input sentence, a formal paraphrase, and the formal attributes of the resulting sentence. Expert feedback is incorporated via in-context learning and self-critique to refine the initial generations.","Attribute style transfer is the task of transforming a given text along a particular style dimension, such as changing its formality, bias, or level of offensiveness . Formality style transfer (e.g., informal$$formal) could be useful in any writing assistance system, while neutralizing text that contains subjective bias would be an important tool for Wikipedia editors or journalists . Style transfer approaches have primarily focused on the text re-writing task (e.g., informal $$formal, subjective bias $$ neutral) using various methods from supervised to unsupervised and zero-shot methods using LLMs (see also for a survey on style transfer). However, to our knowledge, no effort has focused on providing {textual explanations} for the style transfer task. For example, when transforming an informal sentence ``I would throw them out asap !'' into a formal paraphrase ``I would dispose of them promptly'' it would be useful to provide an explanation of the informal attributes in the input sentence (e.g., textese (``asap""), colloquialism (``throw out"")), and formal attributes for the paraphrase (e.g., lexical sophistication (``promptly"" and ``dispose""); lack of abbreviations (``I would"")). Similarly, for neutralizing subjective bias in ``Orbis latinus, integral site of romance language"" $$ ``Orbis latinus, comprehensive site of romance language"", it would be useful to have an explanation about which word/phrase in the input is biased and why as well as the type of bias (e.g., Framing (``integral"" implies a subjective evaluation on the site's importance)). The model's explanations could help the user better assess the correctness of the style transfer system, could be used as features in downstream tasks such as authorship attribution (Section ), or could act as a defense against spurious correlations and annotation artifacts . To enable explainability in style transfer models, we provide the following contributions: {itemize} A new task of {explainable} style transfer for which, in addition to sentence rewriting, the model needs to generate textual explanations. A novel human-AI collaboration framework, {In Context-Learning with Expert Feedback (ICLEF)} (see Figure , Figure , and ). The approach combines model distillation for explanation generation []{Ho2022LargeLM, magister2023teaching} with self-critique ability of LLMs [][{inter alia}]{madaan2023selfrefine, bai2022constitutional, saunders2022selfcritiquing, scheurer2023training}, where the critic, unlike in prior work, is instantiated with expert demonstrations. Using ICLEF, we create for the first time datasets for explainable style transfer by augmenting an existing formality style transfer dataset GYAFC and the neutralizing subjective bias dataset WNC with textual explanations ( ). We show that the datasets generated with the help of ICLEF, {{e-GYAFC}} and {{e-WNC}}, are of good quality via automatic and expert evaluation, and that ICLEF-fixed instances are preferred ( ). Experiments that show that student models outperform teacher models in one-shot setting and perform comparably even with few-shot teacher models in automatic and expert evaluation, confirming the utility and quality of the datasets ( ). Moreover, in an extrinsic evaluation, we show that explanations generated by student models fine-tuned on our data produce a better signal for the authorship attribution task than the explanations produced by few-shot teacher models ( ). {itemize} We release the data, models, and code to encourage further research on explainability, learning from scarce human feedback, and style transfer. {{https://github.com/asaakyan/explain-st}{github.com/asaakyan/explain-st"
LooGLE: Can Long-Context Language Models Understand Long Contexts?,2311.04939v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2311.04939v2_0.pdf,"Large language models (LLMs), despite their impressive performance in various language tasks, are typically limited to processing texts within context-window size. This limitation has spurred significant research efforts to enhance LLMs' long-context understanding with high-quality long-sequence benchmarks. However, prior datasets in this regard suffer from shortcomings, such as short context length compared to the context window of modern LLMs; outdated documents that have data leakage problems; and an emphasis on short dependency tasks rather than long dependency tasks. In this paper, we present , a {Lo}ng C{o}ntext {G}eneric {L}anguage {E}valuation benchmark for LLMs' long context understanding. features relatively new documents post-2022, with over 24,000 tokens per document and 6,000 newly generated questions spanning diverse domains. Human annotators meticulously crafted more than 1,100 high-quality question-answer pairs to meet the long dependency requirements. These pairs underwent thorough cross-validation, yielding the most precise assessment of LLMs' long dependency capabilities. The evaluation of eight state-of-the-art LLMs on revealed key findings: (i) commercial models outperformed open-sourced models; (ii) LLMs excelled in short dependency tasks like short question-answering and cloze tasks but struggled with more intricate long dependency tasks; (iii) in-context learning and chaining thoughts offered only marginal improvements in long context comprehension; (iv) retrieval-based techniques demonstrated substantial benefits for short question-answering, while strategies for extending context window length through optimized transformer architectures or positional encoding had limited impact on long context understanding. As such, not only provides a systematic and comprehensive evaluation schema on long-context LLMs, but also sheds light on future development of enhanced models towards ``true long-context understanding''. All evaluation codes are released at: .",The LooGLE benchmark for long context understanding.,"The pursuit of enabling large language models (LLMs), such as ChatGPT, to go beyond their limited context window size so as to process, comprehend, or even learn from long-context textual information is inevitable for next-generation of language intelligence attributed to its wide applications on real-world scenarios, such as domain-specific knowledge understanding, long-context conversational generation, long story or code generation, . Meanwhile, there is an increasing need for high-quality benchmarks with much longer text lengths and more challenging tasks to provide comprehensive evaluations. However, traditional benchmarks often fall short in text length with an average number of thousand words. Besides, existing benchmarks automatically collect possibly outdated documents from existing datasets published a few years ago, which might lead to data leakage in pre-trained LLMs and make the evaluation inaccurate. Further, the long texts are often restricted to domain-specific articles, making it hard to evaluate LLMs' ability on generic tasks and domains. Finally, it is important to note that tasks in existing benchmarks are primarily {short dependency} tasks, which only require LLMs to retrieve answers from one specific sentence or paragraph, without really testing LLMs' ability to collect pieces of information from paragraphs across the whole document and summarize them into an answer, which we call {long dependency} tasks. To mitigate the shortcomings of existing datasets, in this paper, we introduce a novel benchmark , short for {Lo}ng C{o}ntext {G}eneric {L}anguage {E}valuation, to evaluate the long context understanding abilities of LLMs illustrated in {fig:overview}. Our benchmark has the following advantages: {itemize}[leftmargin=*,topsep=0pt,noitemsep] {Extra-long realistic documents}. It contains 776 latest gathered and extremely long documents with an average of 19.3k words. There are over 6,448 test instances without distribution bias for a more generalized assessment, many of which exceed 100k words. On one hand, they can better evaluate LLMs' capability on memorizing and understanding longer text that is far beyond their context window size. On the other hand, the excessive length is well suited to the common usage of long text scenarios. {Manually designed both short and long dependency tasks.} It is composed of 7 major tasks to evaluate LLMs' ability to understand both short and long dependency content. We refer ``long dependency"" tasks as those that require the understanding of the inter-dependency across multiple evidence widely spanning over the entire long text. We delicately design 5 types of long dependency tasks and recruited a group of human annotators to manually create 1101 long dependency Question-Answer (QA) instances, despite the high costs and huge effort involved in this process. {Relatively new documents.} Our benchmark comprises texts all published after 2022 which ensures that most modern LLMs (at the date of submission) have not been pre-trained on these documents, {forcing them to rely on their in-context learning ability} rather than memorization. In contrast, existing benchmarks are usually a combination of content from traditional NLP dataset, whose world knowledge may have already been learned by LLMs and thus are less convincing for assessment. Furthermore, our data collection process is fully open-sourced, making it easy for the community to reconstruct/update the benchmark with newer documents, possibly on a yearly basis. {Cross-domain generic data.} Our benchmark is derived from popular open-source documents, including arXiv papers, Wikipedia articles, and movie and TV scripts, spanning diverse domains and multiple categories such as academia, history, sports, politics, arts, events, and entertainment. {itemize} We conduct a comprehensive evaluation of 8 representative LLMs on . We specifically select LLMs which have made great effort in addressing the challenge of understanding long contexts as the baselines. The results indicate that better base models with a larger context window size generally achieve better performance. However, all models experience a significant performance decline in long dependency tasks, indicating there is a desperate need to improve the true long dependency understanding capabilities of LLMs. Our dataset serves as an up-to-date benchmark for cutting-edge assessment and research on the long context understanding and modeling of LLMs"