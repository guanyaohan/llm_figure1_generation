paper_title,arxiv_id,fig1_file_path,abstract,fig1_caption
Quantized Side Tuning: Fast and Memory-Efficient Tuning of Quantized Large Language Models,2401.07159v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2401.07159v1_0.pdf;/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2401.07159v1_1.pdf,"Finetuning large language models (LLMs) has been empirically effective on a variety of downstream tasks. Existing approaches to finetuning an LLM either focus on parameter-efficient finetuning, which only updates a small number of trainable parameters, or attempt to reduce the memory footprint during the training phase of the finetuning. Typically, the memory footprint during finetuning stems from three contributors: model weights, optimizer states, and intermediate activations. However, existing works still require considerable memory and none can simultaneously mitigate memory footprint for all three sources. In this paper, we present Quantized Side Tuing (QST), which enables memory-efficient and fast finetuning of LLMs by operating through a dual-stage process. First, QST quantizes an LLM's model weights into 4-bit to reduce the memory footprint of the LLM's original weights; QST also introduces a side network separated from the LLM, which utilizes the hidden states of the LLM to make task-specific predictions. Using a separate side network avoids performing backpropagation through the LLM, thus reducing the memory requirement of the intermediate activations. Furthermore, QST leverages several low-rank adaptors and gradient-free downsample modules to significantly reduce the trainable parameters, so as to save the memory footprint of the optimizer states. Experiments show that QST can reduce the total memory footprint by up to 2.3 $$ and speed up the finetuning process by up to 3 $$ while achieving competent performance compared with the state-of-the-art. When it comes to full finetuning, QST can reduce the total memory footprint up to 7 $$.","Figure \ref{fig:mem_comp_70b} shows the memory footprint of different methods of fintuning LLaMA-2-70b. Figure \ref{fig:acc_comp_7ob} shows the MMLU 5-shot accuracy of different methods when tuning LLaMA-2-7B, LLaMA-2-13B, and LLaMA-2-70B. Note that we set the batch size to 16 and the sequence length to 384. Larger markers represent larger models."
Unsupervised Multimodal Clustering for Semantics Discovery in Multimodal Utterances,2405.12775v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2405.12775v1_0.pdf,"Discovering the semantics of multimodal utterances is essential for understanding human language and enhancing human-machine interactions. Existing methods manifest limitations in leveraging nonverbal information for discerning complex semantics in unsupervised scenarios. This paper introduces a novel unsupervised multimodal clustering method (UMC), making a pioneering contribution to this field. UMC introduces a unique approach to constructing augmentation views for multimodal data, which are then used to perform pre-training to establish well-initialized representations for subsequent clustering. An innovative strategy is proposed to dynamically select high-quality samples as guidance for representation learning, gauged by the density of each sample's nearest neighbors. Besides, it is equipped to automatically determine the optimal value for the top-$K$ parameter in each cluster to refine sample selection. Finally, both high- and low-quality samples are used to learn representations conducive to effective clustering. We build baselines on benchmark multimodal intent and dialogue act datasets. UMC shows remarkable improvements of 2-6\% scores in clustering metrics over state-of-the-art methods, marking the first successful endeavor in this domain. The complete code and data are available at .","{example} Text-only clustering deviates from real multimodal utterance semantics, highlighting the need of multimodal information in semantics discovery."
GenTranslate: Large Language Models are Generative Multilingual Speech and Machine Translators,2402.06894v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.06894v2_0.pdf,"Recent advances in large language models (LLMs) have stepped forward the development of multilingual speech and machine translation by its reduced representation errors and incorporated external knowledge. However, both translation tasks typically utilize beam search decoding and top-1 hypothesis selection for inference. These techniques struggle to fully exploit the rich information in the diverse $N$-best hypotheses, making them less optimal for translation tasks that require a single, high-quality output sequence. In this paper, we propose a new generative paradigm for translation tasks, namely ``GenTranslate'', which builds upon LLMs to generate better results from the diverse translation versions in $N$-best list. Leveraging the rich linguistic knowledge and strong reasoning abilities of LLMs, our new paradigm can integrate the rich information in $N$-best candidates to generate a higher-quality translation result. Furthermore, to support LLM finetuning, we build and release a HypoTranslate dataset that contains over 592K hypotheses-translation pairs in 11 languages. Experiments on various speech and machine translation benchmarks ({e.g.}, FLEURS, CoVoST-2, WMT) demonstrate that our GenTranslate significantly outperforms the state-of-the-art model}.","Illustration of (a) Typical seq2seq translation with beam search decoding and top-1 hypothesis selection, (b) our ``GenTranslate'' with LLM integration."
A Unified Temporal Knowledge Graph Reasoning Model Towards Interpolation and Extrapolation,2405.18106v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2405.18106v1_0.png,"Temporal knowledge graph (TKG) reasoning has two settings: interpolation reasoning and extrapolation reasoning. Both of them draw plenty of research interest and have great significance. Methods of the former de-emphasize the temporal correlations among facts sequences, while methods of the latter require strict chronological order of knowledge and ignore inferring clues provided by missing facts of the past. These limit the practicability of TKG applications as almost all of the existing TKG reasoning methods are designed specifically to address either one setting. To this end, this paper proposes an original Temporal PAth-based Reasoning (TPAR) model for both the interpolation and extrapolation reasoning. TPAR performs a neural-driven symbolic reasoning fashion that is robust to ambiguous and noisy temporal data and with fine interpretability as well. Comprehensive experiments show that TPAR outperforms SOTA methods on the link prediction task for both the interpolation and the extrapolation settings. A novel pipeline experimental setting is designed to evaluate the performances of SOTA combinations and the proposed TPAR towards interpolation and extrapolation reasoning. More diverse experiments are conducted to show the robustness and interpretability of TPAR.",An illustration of Bellman-Ford-based recursive encoding.
Unsupervised Information Refinement Training of Large Language Models for Retrieval-Augmented Generation,2402.18150v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.18150v2_0.pdf,"Retrieval-augmented generation (RAG) enhances large language models (LLMs) by incorporating additional information from retrieval. However, studies have shown that LLMs still face challenges in effectively using the retrieved information, even ignoring it or being misled by it. The key reason is that the training of LLMs does not clearly make LLMs learn how to utilize input retrieved texts with varied quality. In this paper, we propose a novel perspective that considers the role of LLMs in RAG as ``Information Refiner'', which means that regardless of correctness, completeness, or usefulness of retrieved texts, LLMs can consistently integrate knowledge within the retrieved texts and model parameters to generate the texts that are more concise, accurate, and complete than the retrieved texts. To this end, we propose an information refinement training method named {-RAG} that optimizes LLMs for RAG in an unsupervised manner. is low-cost and general across various tasks. Extensive experiments on zero-shot prediction of 11 datasets in diverse tasks including Question Answering, Slot-Filling, Language Modeling, Dialogue, and Code Generation show that improves the performance of LLaMA2 by an average of 9.39\% relative points. also shows advantages in in-context learning and robustness of RAG.","We consider the role of LLMs in RAG as ``Information Refiner'' that can generate more concise, accurate, and complete texts than the input retrieved texts. In this way, LLM can consistently make RAG system produce positive information gain."
CSCD-NS: a Chinese Spelling Check Dataset for Native Speakers,2211.08788v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2211.08788v3_0.jpg,"In this paper, we present CSCD-NS, the first Chinese spelling check (CSC) dataset designed for native speakers, containing 40,000 samples from a Chinese social platform. Compared with existing CSC datasets aimed at Chinese learners, CSCD-NS is ten times larger in scale and exhibits a distinct error distribution, with a significantly higher proportion of word-level errors. To further enhance the data resource, we propose a novel method that simulates the input process through an input method, generating large-scale and high-quality pseudo data that closely resembles the actual error distribution and outperforms existing methods. Moreover, we investigate the performance of various models in this scenario, including large language models (LLMs), such as ChatGPT. The result indicates that generative models underperform BERT-like classification models due to strict length and pronunciation constraints. The high prevalence of word-level errors also makes CSC for native speakers challenging enough, leaving substantial room for improvement.","An error from SIGHAN: misspelling “错误” as “错勿”. Despite having the same pronunciation, it's hard to reproduce this error in the given context through a Chinese IME, no matter what input form is used."
Evaluating Dynamic Topic Models,2309.08627v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2309.08627v1_0.pdf,"There is a lack of quantitative measures to evaluate the progression of topics through time in dynamic topic models (DTMs). Filling this gap, we propose a novel evaluation measure for DTMs that analyzes the changes in the quality of each topic over time. Additionally, we propose an extension combining topic quality with the model's temporal consistency. We demonstrate the utility of the proposed measure by applying it to synthetic data and data from existing DTMs. We also conducted a human evaluation, which indicates that the proposed measure correlates well with human judgment. Our findings may help in identifying changing topics, evaluating different DTMs, and guiding future research in this area.","This figure illustrates the core concept presented in this paper. It illustrates the topic structure within DTMs. The vertical box highlights the set of topics for the first year, and the horizontal box shows the evolution of Topic~1 over time. Topic Quality (TQ) evaluates the topics for each year vertically, whereas Temporal Topic Quality (TTQ) evaluates each topic horizontally, capturing both the topic's evolution over time and the smoothness of topic progression."
How Abilities in Large Language Models are Affected by Supervised Fine-tuning Data Composition,2310.05492v4,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2310.05492v4_0.pdf,"Large language models (LLMs) with enormous pre-training tokens and parameters emerge diverse abilities, including math reasoning, code generation, and instruction following. These abilities are further enhanced by supervised fine-tuning (SFT). While the open-source community has explored ad-hoc SFT for enhancing individual capabilities, proprietary LLMs exhibit versatility across various skills. Therefore, understanding the facilitation of multiple abilities via SFT is paramount. In this study, we specificially focuses on the interplay of data composition between mathematical reasoning, code generation, and general human-aligning abilities during SFT. We propose four intriguing research questions to explore the association between model performance and various factors including data amount, composition ratio, model size and SFT strategies. Our experiments reveal that distinct capabilities scale differently and larger models generally show superior performance with same amount of data. Mathematical reasoning and code generation consistently improve with increasing data amount, whereas general abilities plateau after roughly a thousand samples. Moreover, we observe data composition appears to enhance various abilities under limited data conditions, yet can lead to performance conflicts when data is plentiful. Our findings also suggest the amount of composition data influences performance more than the composition ratio. In analysis of SFT strategies, we find that sequentially learning multiple skills risks catastrophic forgetting. Our proposed {Dual-stage Mixed Fine-tuning (DMT) strategy} offers a promising solution to learn multiple abilities with different scaling patterns.",The illustration of four different training strategies in this paper.
Inference to the Best Explanation in Large Language Models,2402.10767v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.10767v2_0.pdf,"While Large Language Models (LLMs) have found success in real-world applications, their underlying explanatory process is still poorly understood. This paper proposes {IBE-Eval}, a framework inspired by philosophical accounts on {Inference to the Best Explanation (IBE)} to advance the interpretation and evaluation of LLM explanations. {IBE-Eval} estimates the plausibility of natural language explanations through a combination of explicit logical and linguistic features including: {consistency}, {parsimony}, {coherence}, and {uncertainty}. Extensive experiments are conducted on {Causal Question Answering (CQA)}, where {IBE-Eval} is tasked to select the most plausible causal explanation amongst competing ones generated by the LLM (e.g. GPT 3.5 or LLaMA 2). The experiments reveal that {IBE-Eval} can successfully identify the best explanation with up to 77\% accuracy ($ 27\%$ above random), improving upon a GPT 3.5-as-a-judge baseline ($+17\%$) while being intrinsically more efficient and interpretable. Additional analysis suggests that, despite LLM-specific variances, generated explanations tend to conform to IBE criteria and that {IBE-Eval} is significantly correlated with human judgment, opening up opportunities for future development of automated explanation verification tools.","{IBE-Eval} qualifies LLM-generated explanations with a set of logical and linguistic selection criteria to identify the most plausible hypothesis. The corresponding explanation for each hypothesis is evaluated across the IBE criteria of logical consistency, parsimony, internal coherence, and linguistic uncertainty. A final plausibility score is computed across those features and the hypothesis with highest score is identified as the best explanation."
A Novel Cartography-Based Curriculum Learning Method Applied on RoNLI: The First Romanian Natural Language Inference Corpus,2405.11877v5,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2405.11877v5_0.pdf,"Natural language inference (NLI), the task of recognizing the entailment relationship in sentence pairs, is an actively studied topic serving as a proxy for natural language understanding. Despite the relevance of the task in building conversational agents and improving text classification, machine translation and other NLP tasks, to the best of our knowledge, there is no publicly available NLI corpus for the Romanian language. To this end, we introduce the first Romanian NLI corpus (RoNLI) comprising 58K training sentence pairs, which are obtained via distant supervision, and 6K validation and test sentence pairs, which are manually annotated with the correct labels. We conduct experiments with multiple machine learning methods based on distant learning, ranging from shallow models based on word embeddings to transformer-based neural networks, to establish a set of competitive baselines. Furthermore, we improve on the best model by employing a new curriculum learning strategy based on data cartography. Our dataset and code to reproduce the baselines are available at \url{https://github.com/Eduard6421/RONLI}.","Data cartography visualization of the RoNLI dataset based on fine-tuning the Ro-BERT model \cite{Dumitrescu-EMNLP-2020}. In the left-hand side plot, the $y$-axis corresponds to the level of confidence exhibited by the model during training, while the $x$-axis represents the variability of the confidence level. Adjacent to the primary plot, three histograms are displayed on the right-hand side, each representing a different metric: the confidence, the variability of confidence, and the correctness. The visualization offers a comprehensive overview of our dataset characteristics and the behavior of Ro-BERT during training. Best viewed in color.\vspace{-2mm}"
MinPrompt: Graph-based Minimal Prompt Data Augmentation for Few-shot Question Answering,2310.05007v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2310.05007v3_0.pdf,"Recent advances in few-shot question answering (QA) mostly rely on the power of pre-trained large language models (LLMs) and fine-tuning in specific settings. Although the pre-training stage has already equipped LLMs with powerful reasoning capabilities, LLMs still need to be fine-tuned to adapt to specific domains to achieve the best results. In this paper, we propose to select the most informative data for fine-tuning, thereby improving the efficiency of the fine-tuning process with comparative or even better accuracy on the open-domain QA task. We present , a minimal data augmentation framework for open-domain QA based on an approximate graph algorithm and unsupervised question generation. We transform the raw text into a graph structure to build connections between different factual sentences, then apply graph algorithms to identify the minimal set of sentences needed to cover the most information in the raw text. We then generate QA pairs based on the identified sentence subset and train the model on the selected sentences to obtain the final model. Empirical results on several benchmark datasets and theoretical analysis show that is able to achieve comparable or better results than baselines with a high degree of efficiency, bringing consistent improvements in F-1 scores.",Framework overview for \ours.
SportsMetrics: Blending Text and Numerical Data to Understand Information Fusion in LLMs,2402.10979v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.10979v2_0.pdf,"Large language models hold significant potential for integrating various data types, such as text documents and database records, for advanced analytics. However, blending text and numerical data presents substantial challenges. LLMs need to process and cross-reference entities and numbers, handle data inconsistencies and redundancies, and develop planning capabilities such as building a working memory for managing complex data queries. In this paper, we introduce four novel tasks centered around sports data analytics to evaluate the numerical reasoning and information fusion capabilities of LLMs. These tasks involve providing LLMs with detailed, play-by-play sports game descriptions, then challenging them with adversarial scenarios such as new game rules, longer durations, scrambled narratives, and analyzing key statistics in game summaries. We conduct extensive experiments on NBA and NFL games to assess the performance of LLMs on these tasks. Our benchmark, , introduces a new mechanism for assessing LLMs' numerical reasoning and fusion skills.","Play-by-plays of an NBA game. We include timestamps, player actions, team affiliations and a game recap. Total points for both teams are indicated in dotted circles and are withheld from LLMs."
Expedited Training of Visual Conditioned Language Generation via Redundancy Reduction,2310.03291v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2310.03291v3_0.png,"In this paper, we introduce $_{}$, a streamlined framework designed for the pre-training of visually conditioned language generation models with high computational demands, utilizing frozen pre-trained large language models (LLMs). The conventional approach in vision-language pre-training (VLP) typically involves a two-stage optimization process: an initial resource-intensive phase dedicated to general-purpose vision-language representation learning, focused on extracting and consolidating relevant visual features. This is followed by a subsequent phase that emphasizes end-to-end alignment between visual and linguistic modalities. Our novel one-stage, single-loss framework bypasses the computationally demanding first training stage by gradually merging similar visual tokens during training, while avoiding model collapse caused by single-stage training of BLIP-2 type models. The gradual merging process effectively condenses visual information while preserving semantic richness, resulting in rapid convergence without compromising performance. Our experimental findings demonstrate that our approach accelerates the training of vision-language models by a factor of 5 without a noticeable impact on overall performance. Furthermore, we illustrate that our models significantly narrow the performance gap to current vision-language models using only 1/10 of the data. Finally, we showcase how our image-text models can seamlessly adapt to video-conditioned language generation tasks through novel soft attentive temporal token contextualizing modules. Code is available at .","Overview of our $\text{EVL}_{\text{Gen}}$. $\text{EVL}_{\text{Gen}}$ employs a streamlined, single-stage training mechanism with a unified loss. Here, visual tokens (in grey) are progressively aggregated based on their inherent similarities at each layer of the TomeFormer architecture. The final set of merged tokens (in orange) serves as semantically rich but computationally efficient soft prompts, guiding the LLM to generate a corresponding caption for the input image."
Subtle Biases Need Subtler Measures: Dual Metrics for Evaluating Representative and Affinity Bias in Large Language Models,2405.14555v4,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2405.14555v4_0.png,"Research on Large Language Models (LLMs) has often neglected subtle biases that, although less apparent, can significantly influence the models' outputs toward particular social narratives. This study addresses two such biases within LLMs: {representative bias}, which denotes a tendency of LLMs to generate outputs that mirror the experiences of certain identity groups, and {affinity bias}, reflecting the models' evaluative preferences for specific narratives or viewpoints. We introduce two novel metrics to measure these biases: the Representative Bias Score (RBS) and the Affinity Bias Score (ABS), and present the Creativity-Oriented Generation Suite (CoGS), a collection of open-ended tasks such as short story writing and poetry composition, designed with customized rubrics to detect these subtle biases. Our analysis uncovers marked representative biases in prominent LLMs, with a preference for identities associated with being white, straight, and men. Furthermore, our investigation of affinity bias reveals distinctive evaluative patterns within each model, akin to `bias fingerprints'. This trend is also seen in human evaluators, highlighting a complex interplay between human and machine bias perceptions.{https://github.com/akkeshav/subtleBias}.}","Proportion of GPT-4's preferred responses for the short poem task in CoGS, categorized by identity-specific prompts, with highlighted sectors indicating a preference for outputs from those identities."
GradSafe: Detecting Jailbreak Prompts for LLMs via Safety-Critical Gradient Analysis,2402.13494v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.13494v2_0.pdf,"Large Language Models (LLMs) face threats from jailbreak prompts. Existing methods for detecting jailbreak prompts are primarily online moderation APIs or finetuned LLMs. These strategies, however, often require extensive and resource-intensive data collection and training processes. In this study, we propose , which effectively detects jailbreak prompts by scrutinizing the gradients of {safety-critical parameters} in LLMs. Our method is grounded in a pivotal observation: the gradients of an LLM’s loss for jailbreak prompts paired with compliance response exhibit similar patterns on certain safety-critical parameters. In contrast, safe prompts lead to different gradient patterns. Building on this observation, analyzes the gradients from prompts (paired with compliance responses) to accurately detect jailbreak prompts. We show that , applied to Llama-2 without further training, outperforms Llama Guard—despite its extensive finetuning with a large dataset—in detecting jailbreak prompts. This superior performance is consistent across both zero-shot and adaptation scenarios, as evidenced by our evaluations on ToxicChat and XSTest. The source code is available at . [!t] [width=1]{figures/gradsafe.pdf} .} %{`jailbreak' and `Safe' words are not aligned with the arrows well. `Safety-Critical Parameters' and `jailbreak Gradient {fig:intro}","Comparison of existing LLM-based jailbreak prompt detection and \ours: a) Zero-shot LLM detectors can be imprecise, such as overestimating safety risks; b) Finetuned LLMs demand extensive training on carefully curated datasets; c) \ours accurately detects jailbreak prompts using safety-critical gradients, without the need for LLM finetuning. Example prompt from XSTest~\cite{rottger2023xstest}."
An Information-Theoretic Approach to Analyze NLP Classification Tasks,2402.00978v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.00978v1_0.png,"Understanding the importance of the inputs on the output is useful across many tasks. This work provides an information-theoretic framework to analyse the influence of inputs for text classification tasks. Natural language processing (NLP) tasks take either a single element input or multiple element inputs to predict an output variable, where an element is a block of text. Each text element has two components: an associated semantic meaning and a linguistic realization. Multiple-choice reading comprehension (MCRC) and sentiment classification (SC) are selected to showcase the framework. For MCRC, it is found that the context influence on the output compared to the question influence reduces on more challenging datasets. In particular, more challenging contexts allow a greater variation in complexity of questions. Hence, test creators need to carefully consider the choice of the context when designing multiple-choice questions for assessment. For SC, it is found the semantic meaning of the input text dominates (above 80\% for all datasets considered) compared to its linguistic realisation when determining the sentiment. The framework is made available at: .",Data generation for multiple-choice reading comprehension for the context (blue) and question (purple) respectively.
OPEx: A Component-Wise Analysis of LLM-Centric Agents in Embodied Instruction Following,2403.03017v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2403.03017v1_0.pdf,"Embodied Instruction Following (EIF) is a crucial task in embodied learning, requiring agents to interact with their environment through egocentric observations to fulfill natural language instructions. Recent advancements have seen a surge in employing large language models (LLMs) within a framework-centric approach to enhance performance in embodied learning tasks, including EIF. Despite these efforts, there exists a lack of a unified understanding regarding the impact of various components—ranging from visual perception to action execution—on task performance. To address this gap, we introduce OPEx, a comprehensive framework that delineates the core components essential for solving embodied learning tasks: Observer, Planner, and Executor. Through extensive evaluations, we provide a deep analysis of how each component influences EIF task performance. Furthermore, we innovate within this space by deploying a multi-agent dialogue strategy on a TextWorld counterpart, further enhancing task performance. Our findings reveal that LLM-centric design markedly improves EIF outcomes, identify visual perception and low-level action execution as critical bottlenecks, and demonstrate that augmenting LLMs with a multi-agent framework further elevates performance.",Overview of our OPEx framework. We will open-source the code after acceptance.
Hyper-CL: Conditioning Sentence Representations with Hypernetworks,2403.09490v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2403.09490v2_0.pdf,"While the introduction of contrastive learning frameworks in sentence representation learning has significantly contributed to advancements in the field, it still remains unclear whether state-of-the-art sentence embeddings can capture the fine-grained semantics of sentences, particularly when conditioned on specific perspectives. In this paper, we introduce Hyper-CL, an efficient methodology that integrates hypernetworks with contrastive learning to compute conditioned sentence representations. In our proposed approach, the hypernetwork is responsible for transforming pre-computed condition embeddings into corresponding projection layers. This enables the same sentence embeddings to be projected differently according to various conditions. Evaluation of two representative conditioning benchmarks, namely conditional semantic text similarity and knowledge graph completion, demonstrates that Hyper-CL is effective in flexibly conditioning sentence representations, showcasing its computational efficiency at the same time. We also provide a comprehensive analysis of the inner workings of our approach, leading to a better interpretation of its mechanisms. Our code is available at {https://github.com/HYU-NLP/Hyper-CL}.","Illustration of our approach dubbed Hyper-CL. In the example, two sentences are provided along with two distinct conditions, $c_{high}$ and $c_{low}$. Specifically, $c_{high}$ (\textcolor{orange}{orange}) denotes a condition that results in the sentences being interpreted more similarly, whereas $c_{low}$ (\textcolor{blue}{blue}) leads to a perspective in which the two sentences are understood as being relatively more distinct. The identical pair of sentences are projected into different subspaces that reflect the provided conditions."
ABEX: Data Augmentation for Low-Resource NLU via Expanding Abstract Descriptions,2406.04286v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.04286v1_0.pdf,"We present {ABEX}, a novel and effective generative data augmentation methodology for low-resource Natural Language Understanding (NLU) tasks. ABEX is based on {AB}stract-and-{EX}pand, a novel paradigm for generating diverse forms of an input document -- we first convert a document into its concise, abstract description and then generate new documents based on expanding the resultant abstraction. To learn the task of expanding abstract descriptions, we first train BART on a large-scale synthetic dataset with abstract-document pairs. Next, to generate abstract descriptions for a document, we propose a simple, controllable, and training-free method based on editing AMR graphs. ABEX brings the best of both worlds: by expanding from abstract representations, it preserves the original semantic properties of the documents, like style and meaning, thereby maintaining alignment with the original label and data distribution. At the same time, the fundamental process of elaborating on abstract descriptions facilitates diverse generations. We demonstrate the effectiveness of ABEX on 4 NLU tasks spanning 12 datasets and 4 low-resource settings. ABEX outperforms all our baselines qualitatively with improvements of 0.04\% - 38.8\%. Qualitatively, ABEX outperforms all prior methods from literature in terms of context and length diversity~. $Equal Technical Contribution.}","Illustration of our proposed augmentation methodology. {Top: Learning to Expand Abstract Descriptions.} \textcircled{\raisebox{-0.9pt}{1}} We synthesize a large-scale synthetic dataset $\mathcal{D}_{ab}$ with abstract-document pairs by prompting LLMs with unlabeled documents from $\mathcal{D}_{ab}$. \textcircled{\raisebox{-0.9pt}{2}} We pre-train BART on this dataset with abstract as input and document as the target for learning to expand abstract descriptions. {Bottom: Data Augmentation.} \textcircled{\raisebox{-0.9pt}{1}} We convert the document into its AMR graph representation $\mathcal{G}_{i}$ using a Text-to-AMR Parser. \textcircled{\raisebox{-0.9pt}{2}} $\mathcal{G}_{i}$ then goes through multiple steps of {deletion} to obtain $\hat{\mathcal{G}}_{i}$ \textcircled{\raisebox{-0.9pt}{3}} We optionally retrieve a semantically similar document from $\mathcal{D}_{down}$, obtain its AMR graph $\mathcal{G}_{k}$, and replace subtrees in $\hat{\mathcal{G}}_{i}$ with {similar} subtrees in $\hat{\mathcal{G}}_{i}$. \textcircled{\raisebox{-0.9pt}{4}} $\hat{\mathcal{G}}_{i}$ is then converted back to text (which is now an abstract description) using an AMR-to-Text generator. \textcircled{\raisebox{-0.9pt}{5}} This abstract description is then passed to the fine-tuned BART for generating augmentations. \textcircled{\raisebox{-0.9pt}{6}} We optionally fine-tune the fine-tuned BART (from the 1st step) on abstract-document pairs from $\mathcal{D}_{down}$."
Token-wise Influential Training Data Retrieval for Large Language Models,2405.11724v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2405.11724v2_0.pdf,"{Given a Large Language Model (LLM) generation, how can we identify which training data led to this generation?} In this paper, we proposed , a scalable framework adapting to LLMs for estimating the influence of each training data. The proposed framework consists of two stages: caching and retrieval. First, we compress the gradient vectors by over 200,000x, allowing them to be cached on disk or in GPU/CPU memory. Then, given a generation, efficiently traverses the cached gradients to estimate the influence within minutes, achieving over a 6,326x speedup. Moreover, supports multi-GPU parallelization to substantially accelerate caching and retrieval. Our empirical result confirms the efficiency and effectiveness of .",Influence estimation for a given generation.
Tuning Large Multimodal Models for Videos using Reinforcement Learning from AI Feedback,2402.03746v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.03746v3_0.pdf,"Recent advancements in large language models have influenced the development of video large multimodal models (VLMMs). Previous approaches for VLMMs involve Supervised Fine-Tuning (SFT) with instruction-tuned datasets, integrating LLM with visual encoders, and additional learnable parameters. Here, aligning video with text, and vice versa, remains a challenge, primarily due to the insufficient quality and quantity of multimodal instruction-tune data compared to that of text-only. This discrepancy often results in alignments that poorly ground the video content. % that are not well-grounded in the video content. To address this, we present a novel alignment strategy that employs a multimodal AI system equipped with Reinforcement Learning from AI Feedback (RLAIF), providing self-preference feedback to refine itself and facilitating the alignment of video and text modalities. Our approach uniquely integrates detailed video descriptions as context into a multimodal AI system during preference feedback generation to enrich the understanding of video content, a process we call {context-aware reward modeling}. Empirical evaluations on various video benchmarks demonstrate that our outperforms existing approaches, including the SFT model. We commit to open-sourcing our code, models, and datasets to foster further research in this area.","{Quantitative comparison of VLMMs on various video benchmarks.} The video question answering (VQA) task is marked in \textcolor{purple}{purple}, video-based generative task in \textcolor{blue}{blue}, the text-to-video (T2V) retrieval task in \textcolor{violet}{violet} and the action recognition (AR) task in \textcolor{orange}{orange} color. \method achieves superior performances on a broad range of video benchmarks compared to previous approaches, including \methodsft. Comprehensive comparisons are provided in Tables \ref{tab:vid_bench_main_quan}, \ref{tab:main_quan} and \ref{tab:video_ret_acr_quan}."
AbsInstruct: Eliciting Abstraction Ability from LLMs through Explanation Tuning with Plausibility Estimation,2402.10646v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.10646v2_0.pdf,"Abstraction ability is crucial in human intelligence, which can also benefit various tasks in NLP study. Existing work shows that LLMs are deficient in abstract ability, and how to improve it remains unexplored. In this work, we design the framework to enhance LLMs' abstraction ability through instruction tuning. The framework builds instructions with in-depth explanations to assist LLMs in capturing the underlying rationale of abstraction. Meanwhile, we introduce a plausibility estimator to select instructions that are more consistent with the abstraction knowledge of LLMs to be aligned. Then, our framework combines abstraction instructions with general-purpose ones to build a hybrid dataset. Extensive experiments and analyses} demonstrate that our framework can considerably enhance LLMs' abstraction ability with strong generalization performance while maintaining their general instruction-following abilities.",An illustration of our \papertitle framework. We collect explanation traces for each example and design a plausibility estimator to select data that match the knowledge of an LLM to be aligned.
An Information Bottleneck Perspective for Effective Noise Filtering on Retrieval-Augmented Generation,2406.01549v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.01549v2_0.pdf,"Retrieval-augmented generation integrates the capabilities of large language models with relevant information retrieved from an extensive corpus, yet encounters challenges when confronted with real-world noisy data. One recent solution is to train a filter module to find relevant content but only achieve suboptimal noise compression. In this paper, we propose to introduce the information bottleneck theory into retrieval-augmented generation. Our approach involves the filtration of noise by simultaneously maximizing the mutual information between compression and ground output, while minimizing the mutual information between compression and retrieved passage. In addition, we derive the formula of information bottleneck to facilitate its application in novel comprehensive evaluations, the selection of supervised fine-tuning data, and the construction of reinforcement learning rewards. Experimental results demonstrate that our approach achieves significant improvements across various question answering datasets, not only in terms of the correctness of answer generation but also in the conciseness with $2.5\%$ compression rate.","In retrieval-augmented generation, passages $X$ are retrieved to enhance the generation of output $Y$. (A) Recent Noise filtering approaches obtain the compression $\tilde{X}\subseteq X$ with log likelihood objective to outputs $Y$. Our information bottleneck objective enables a precise delineation of the intersection $\tilde{X}_{\exper{IB}}=X\cap Y$. (B) Information bottleneck explicitly compresses $\tilde{X}_{\exper{IB}}=\phi$, when retrieved passages are irrelevant to outputs."
RORA: Robust Free-Text Rationale Evaluation,2402.18678v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.18678v3_0.pdf,"Free-text rationales play a pivotal role in explainable NLP, bridging the knowledge and reasoning gaps behind a model's decision-making. However, due to the diversity of potential reasoning paths and a corresponding lack of definitive ground truth, their evaluation remains a challenge. Existing evaluation metrics rely on the degree to which a rationale {supports} a target label, but we find these fall short in evaluating rationales that inadvertently {leak the labels}. To address this problem, we propose , a bust free-text tionale evaluation against label leakage.} quantifies the new information supplied by a rationale to justify the label. This is achieved by assessing the conditional $$-information with a predictive family robust against leaky features that can be exploited by a small model. consistently outperforms existing approaches in evaluating human-written, synthetic, or model-generated rationales, particularly demonstrating robustness against label leakage. We also show that aligns well with human judgment, providing a more reliable and accurate measurement across diverse free-text rationales.","\name{} framework for evaluating rationales $R_1^{\text{True}}$, $R_2^{\text{True}}$, $R_3^{\text{True}}$. Existing baselines are highly sensitive to rationales that \textcolor{orange}{simply restate the label} or \textcolor{Green}{paraphrase the given question and label}, leading to inflated scores compared to the \textcolor{blue}{human-annotated} rationale. In contrast, \name{} provides an informativeness score that better characterizes rationale quality. It is achieved by \Circled{1} detecting potential leakage tokens in the rationale (\S\ref{subsec:leakage-detection}) and \Circled{2} generate additional training data with counterfactual editing for data augmentation (\S\ref{subsec: data augmentation}), followed by \Circled{3} training an evaluation model invariant to label leakage (\S\ref{subsec: evaluation})."
ConSiDERS-The-Human Evaluation Framework: Rethinking Human Evaluation for Generative Large Language Models,2405.18638v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2405.18638v2_0.png,"In this position paper, we argue that human evaluation of generative large language models (LLMs) should be a multidisciplinary undertaking that draws upon insights from disciplines such as user experience research and human behavioral psychology to ensure that the experimental design and results are reliable. The conclusions from these evaluations, thus, must consider factors such as usability, aesthetics, and cognitive biases. We highlight how cognitive biases can conflate fluent information and truthfulness, and how cognitive uncertainty affects the reliability of rating scores such as Likert. Furthermore, the evaluation should differentiate the capabilities and weaknesses of increasingly powerful large language models -- which requires effective test sets. The scalability of human evaluation is also crucial to wider adoption. Hence, to design an effective human evaluation system in the age of generative NLP, we propose the {ConSiDERS-The-Human} evaluation framework consisting of 6 pillars -- {}sistency, {}coring Cr{}tera, {}ifferentiating, User {}xperience, {}esponsible, and {}calability.",Yearly publications of papers with keywords ``human'' and ``eval'' in title/abstract
Linguistically Conditioned Semantic Textual Similarity,2406.03673v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.03673v1_0.png,"is a fundamental NLP task that measures the semantic similarity between a pair of sentences. In order to reduce the inherent ambiguity posed from the sentences, a recent work called has been proposed to measure the sentences' similarity conditioned on a certain aspect. Despite the popularity of , we find that the current dataset suffers from various issues that could impede proper evaluation on this task. In this paper, we reannotate the validation set and observe an annotator discrepancy on 55\% of the instances resulting from the annotation errors in the original label, ill-defined conditions, and the lack of clarity in the task definition. After a thorough dataset analysis, we improve the task by leveraging the models' capability to understand the conditions under a QA task setting. With the generated answers, we present an automatic error identification pipeline that is able to identify annotation errors from the data with over 80\% F1 score. We also propose a new method that largely improves the performance over baselines on the data by training the models with the answers. Finally we discuss the conditionality annotation based on the of entity types. We show in examples that the is able to provide a linguistic foundation for constructing data with new conditions.",A problematic example from the C-STS dataset. The binarity of the condition cannot be mapped to a 5-point similarity scale. The label can be subjective depending on how much inference is made from the context. No guideline on the scenario when the information regarding the condition is missing.
DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models,2401.06066v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2401.06066v1_0.pdf,"In the era of large language models, Mixture-of-Experts (MoE) is a promising architecture for managing computational costs when scaling up model parameters. However, conventional MoE architectures like GShard, which activate the top-$K$ out of $N$ experts, face challenges in ensuring expert specialization, i.e. each expert acquires non-overlapping and focused knowledge. In response, we propose the {} architecture towards ultimate expert specialization. It involves two principal strategies: (1) finely segmenting the experts into $mN$ ones and activating $mK$ from them, allowing for a more flexible combination of activated experts; (2) isolating $K_s$ experts as shared ones, aiming at capturing common knowledge and mitigating redundancy in routed experts. Starting from a modest scale with 2B parameters, we demonstrate that 2B achieves comparable performance with GShard 2.9B, which has 1.5$$ expert parameters and computation. In addition, 2B nearly approaches the performance of its dense counterpart with the same number of total parameters, which set the upper bound of MoE models. Subsequently, we scale up to 16B parameters and show that it achieves comparable performance with LLaMA2 7B, with only about 40\% of computations. Further, our preliminary efforts to scale up to 145B parameters consistently validate its substantial advantages over the GShard architecture, and show its performance comparable with DeepSeek 67B, using only 28.5\% (maybe even 18.2\%) of computations.","Comparison between \spmoe{} 16B and open source models on the Open LLM Leaderboard. The red dashed line is linearly fitted from data points of all models except \spmoe{} 16B. \spmoe{} 16B consistently outperforms models with a similar number of activated parameters by a large margin, and achieves comparable performance with LLaMA2 7B, which has approximately 2.5 times the activated parameters."
Rewriting the Code: A Simple Method for Large Language Model Augmented Code Search,2401.04514v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2401.04514v2_0.pdf,"In code search, the Generation-Augmented Retrieval (GAR) framework, which generates exemplar code snippets to augment queries, has emerged as a promising strategy to address the principal challenge of modality misalignment between code snippets and natural language queries, particularly with the demonstrated code generation capabilities of Large Language Models (LLMs). Nevertheless, our preliminary investigations indicate that the improvements conferred by such an LLM-augmented framework are somewhat constrained. This limitation could potentially be ascribed to the fact that the generated codes, albeit functionally accurate, frequently display a pronounced stylistic deviation from the ground truth code in the codebase. In this paper, we extend the foundational GAR framework and propose a simple yet effective method that additionally writes the de (ReCo) within the codebase for style normalization. Experimental results demonstrate that ReCo significantly boosts retrieval accuracy across sparse (up to 35.7\%), zero-shot dense (up to 27.6\%), and fine-tuned dense (up to 23.6\%) retrieval settings in diverse search scenarios. To further elucidate the advantages of ReCo and stimulate research in code style normalization, we introduce Code Style Similarity, the first metric tailored to quantify stylistic similarities in code. Notably, our empirical findings reveal the inadequacy of existing metrics in capturing stylistic nuances. The source code and data are available at .","Comparison of GAR between passage retrieval and code search. In passage retrieval, the truth (yellow) is included in the generated content. In code search, despite the generated exemplar code satisfies the description of the query, it exhibits noticeable dissimilarity to the true code."
A Multidimensional Framework for Evaluating Lexical Semantic Change with Social Science Applications,2406.06052v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.06052v1_0.png,"Historical linguists have identified multiple forms of lexical semantic change. We present a three-dimensional framework for integrating these forms and a unified computational methodology for evaluating them concurrently. The dimensions represent increases or decreases in semantic 1) sentiment (valence of a target word’s collocates), 2) breadth (diversity of contexts in which the target word appears), and 3) intensity (emotional arousal of collocates or the frequency of intensifiers). These dimensions can be complemented by the evaluation of shifts in the frequency of the target words and the thematic content of its collocates. This framework enables lexical semantic change to be mapped economically and systematically and has applications in computational social science. We present an illustrative analysis of semantic shifts in {mental health} and {mental illness} in two corpora, demonstrating patterns of semantic change that illuminate contemporary concerns about pathologization, stigma, and concept creep.",Three Major Dimensions of Semantic Change.
Mitigating Catastrophic Forgetting in Large Language Models with Self-Synthesized Rehearsal,2403.01244v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2403.01244v2_0.pdf,"Large language models (LLMs) suffer from catastrophic forgetting during continual learning. Conventional rehearsal-based methods rely on previous training data to retain the model's ability, which may not be feasible in real-world applications. When conducting continual learning based on a publicly-released LLM checkpoint, the availability of the original training data may be non-existent. To address this challenge, we propose a framework called Self-Synthesized Rehearsal (SSR) that uses the LLM to generate synthetic instances for rehearsal. Concretely, we first employ the base LLM for in-context learning to generate synthetic instances. Subsequently, we utilize the latest LLM to refine the instance outputs based on the synthetic inputs, preserving its acquired ability. Finally, we select diverse high-quality synthetic instances for rehearsal in future stages. Experimental results demonstrate that SSR achieves superior or comparable performance compared to conventional rehearsal-based approaches while being more data-efficient. Besides, SSR effectively preserves the generalization capabilities of LLMs in general domains.",Comparison of standard rehearsal and our proposed Self-Synthesized Rehearsal (SSR).
Probing the Multi-turn Planning Capabilities of LLMs via 20 Question Games,2310.01468v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2310.01468v3_0.pdf,"Large language models (LLMs) are effective at answering questions that are clearly asked. However, when faced with ambiguous queries they can act unpredictably and produce incorrect outputs. This underscores the need for the development of intelligent agents capable of asking clarification questions to resolve ambiguities effectively. This capability requires complex understanding, state tracking, reasoning and planning over multiple conversational turns. However, directly measuring this can be challenging. In this paper, we offer a surrogate problem which assesses an LLMs's capability to deduce an entity unknown to itself, but revealed to a judge, by asking the judge a series of queries. This {entity-deducing game} can serve as an evaluation framework to probe the conversational reasoning and planning capabilities of language models. We systematically evaluate various LLMs and discover significant differences in their performance on this task. We find that strong LLMs like GPT-4 outperform human players by a large margin. We further employ Behavior Cloning (BC) to examine whether a weaker model is capable of imitating a stronger model and generalizing to data or domains, using only the demonstrations from a stronger model. We finally propose to use Reinforcement Learning to enhance reasoning and planning capacity of Vicuna models through episodes of game playing, which lead to significant performance improvement. We hope that this problem offers insights into how autonomous agents could be trained to behave more intelligently in ambiguous circumstances.",The entity deducing game resembles real scenarios where the agent may need to make strategic decisions regarding the clarification question to be asked based on the current conversation to elicit the actual user intent in as few turns as possible.
WaterBench: Towards Holistic Evaluation of Watermarks for Large Language Models,2311.07138v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2311.07138v2_0.pdf,"To mitigate the potential misuse of large language models (LLMs), recent research has developed watermarking algorithms, which restrict the generation process to leave an invisible trace for watermark detection. Due to the two-stage nature of the task, most studies evaluate the generation and detection separately, thereby presenting a challenge in unbiased, thorough, and applicable evaluations. In this paper, we introduce WaterBench, the first comprehensive benchmark for LLM watermarks, in which we design three crucial factors: (1) For {benchmarking procedure}, to ensure an apples-to-apples comparison, we first adjust each watermarking method's hyper-parameter to reach the same watermarking strength, then jointly evaluate their generation and detection performance. (2) For {task selection}, we diversify the input and output length to form a five-category taxonomy, covering $9$ tasks. (3) For {evaluation metric}, we adopt the GPT4-Judge for automatically evaluating the decline of instruction-following abilities after watermarking. We evaluate $4$ open-source watermarks on $2$ LLMs under $2$ watermarking strengths and observe the common struggles for current methods on maintaining the generation quality. The code and data are available at .","The generated texts without and with watermark \citep{kirchenbauer2023watermark} on a test example from AlpacaFarm~\cite{dubois2023alpacafarm}, an instruction-following benchmark. LLM equipped with watermark will be more inclined to generate tokens in the \colorbox{mygreen}{green list}, which can then be detected by a higher z-score measurement ($z>4$). We utilize TP, TN, and GM to jointly evaluate the watermarking performance."
Dependency Transformer Grammars: Integrating Dependency Structures into Transformer Language Models,2407.17406v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2407.17406v1_0.pdf,"Syntactic Transformer language models aim to achieve better generalization through simultaneously modeling syntax trees and sentences. While prior work has been focusing on adding constituency-based structures to Transformers, we introduce Dependency Transformer Grammars ( s), a new class of Transformer language model with explicit dependency-based inductive bias. simulate dependency transition systems with constrained attention patterns by modifying attention masks, incorporate the stack information through relative positional encoding, and augment dependency arc representation with a combination of token embeddings and operation embeddings. When trained on a dataset of sentences annotated with dependency trees, achieve better generalization while maintaining comparable perplexity with Transformer language model baselines. also outperform recent constituency-based models, showing that dependency can better guide Transformer language models. Our code is released at {}.",An example sentence with its dependency tree and transition sequence. Numbers in blue and red are indices of tokens and arcs respectively.
HealMe: Harnessing Cognitive Reframing in Large Language Models for Psychotherapy,2403.05574v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2403.05574v3_0.pdf,"Large Language Models (LLMs) can play a vital role in psychotherapy by adeptly handling the crucial task of cognitive reframing and overcoming challenges such as shame, distrust, therapist skill variability, and resource scarcity. Previous LLMs in cognitive reframing mainly converted negative emotions to positive ones, but these approaches have limited efficacy, often not promoting clients' self-discovery of alternative perspectives. In this paper, we unveil the Helping and Empowering through Adaptive Language in Mental Enhancement (HealMe) model. This novel cognitive reframing therapy method effectively addresses deep-rooted negative thoughts and fosters rational, balanced perspectives. Diverging from traditional LLM methods, HealMe employs empathetic dialogue based on psychotherapeutic frameworks. It systematically guides clients through distinguishing circumstances from feelings, brainstorming alternative viewpoints, and developing empathetic, actionable suggestions. Moreover, we adopt the first comprehensive and expertly crafted psychological evaluation metrics, specifically designed to rigorously assess the performance of cognitive reframing, in both AI-simulated dialogues and real-world therapeutic conversations. Experimental results show that our model outperforms others in terms of empathy, guidance, and logical coherence, demonstrating its effectiveness and potential positive impact on psychotherapy.","An example of how HealMe communicates with a client, and how we prompt both sides to generate expected conversations as training data."
Multimodal Prompt Learning with Missing Modalities for Sentiment Analysis and Emotion Recognition,2407.05374v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2407.05374v1_0.pdf,"The development of multimodal models has significantly advanced multimodal sentiment analysis and emotion recognition. However, in real-world applications, the presence of various missing modality cases often leads to a degradation in the model's performance. In this work, we propose a novel multimodal Transformer framework using prompt learning to address the issue of missing modalities. Our method introduces three types of prompts: generative prompts, missing-signal prompts, and missing-type prompts. These prompts enable the generation of missing modality features and facilitate the learning of intra- and inter-modality information. Through prompt learning, we achieve a substantial reduction in the number of trainable parameters. Our proposed method outperforms other methods significantly across all evaluation metrics. Extensive experiments and ablation studies are conducted to demonstrate the effectiveness and robustness of our method, showcasing its ability to effectively handle missing modalities. Codes are available at .",The overall architecture of our proposed method. A batch of data that contains different missing modality cases is fed to the Missing Modality Generation Module (see Section~\ref{s32}) to obtain generated features. They are then passed to the pre-trained backbone with missing-signal prompts and missing-type prompts (see Section~\ref{s33}).
Generative Pre-trained Speech Language Model with Efficient Hierarchical Transformer,2406.00976v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.00976v2_0.pdf,"While recent advancements in speech language models have achieved significant progress, they face remarkable challenges in modeling the long acoustic sequences of neural audio codecs. In this paper, we introduce {G}enerative {P}re-trained {S}peech {T}ransformer (GPST), a hierarchical transformer designed for efficient speech language modeling. GPST quantizes audio waveforms into two distinct types of discrete speech representations and integrates them within a hierarchical transformer architecture, allowing for a unified one-stage generation process and enhancing Hi-Res audio generation capabilities. By training on large corpora of speeches in an end-to-end unsupervised manner, GPST can generate syntactically consistent speech with diverse speaker identities. Given a brief 3-second prompt, GPST can produce natural and coherent personalized speech, demonstrating in-context learning abilities. Moreover, our approach can be easily extended to spoken cross-lingual speech generation by incorporating multi-lingual semantic tokens and universal acoustic tokens. Experimental results indicate that GPST significantly outperforms the existing speech language models in terms of word error rate, speech quality, and speaker similarity. The code is available at .",The comparison of frameworks for generative speech pre-training. (a) AudioLM is a three-stage model. (b) VALL-E is a two-stage model. (c) GPST is a one-stage model.
Selene: Pioneering Automated Proof in Software Verification,2401.07663v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2401.07663v2_0.png,"Ensuring correctness is a pivotal aspect of software engineering. Among various strategies available, software verification offers a definitive assurance of correctness. Nevertheless, writing verification proofs is resource-intensive and manpower-consuming, and there is a great need to automate this process. We introduce in this paper, which is the first project-level automated proof benchmark constructed based on the real-world industrial-level operating system microkernel, seL4. provides a comprehensive framework for end-to-end proof generation and a lightweight verification environment. Our experimental results with advanced large language models (LLMs), such as GPT-3.5-turbo and GPT-4, highlight the capabilities of LLMs in the domain of automated proof generation. Additionally, our further proposed augmentations indicate that the challenges presented by can be mitigated in future research endeavors.","A demonstration of the \benchmark pipeline for automated proof generation (best viewed in color). \benchmark facilitates both the construction of proofs from scratch (indicated by the gray ``generate'' path) and the refinement of existing proofs augmented by error messages (highlighted by the red ``fixing'' path). To validate the correctness of the generated proofs, they are subjected to verification by the Isabelle prover within the authentic seL4 environment."
Dissecting Human and LLM Preferences,2402.11296v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.11296v1_0.pdf,"As a relative quality comparison of model responses, human and Large Language Model (LLM) {preferences} serve as common alignment goals in model fine-tuning and criteria in evaluation. Yet, these preferences merely reflect broad tendencies, resulting in less explainable and controllable models with potential safety risks. In this work, we dissect the preferences of human and 32 different LLMs to understand their quantitative composition, using annotations from real-world user-model conversations for a fine-grained, scenario-wise analysis. We find that humans are less sensitive to errors, favor responses that support their stances, and show clear dislike when models admit their limits. On the contrary, advanced LLMs like GPT-4-Turbo emphasize correctness, clarity, and harmlessness more. Additionally, LLMs of similar sizes tend to exhibit similar preferences, regardless of their training methods, and fine-tuning for alignment does not significantly alter the preferences of pretrained-only LLMs. Finally, we show that preference-based evaluation can be intentionally manipulated. In both training-free and training-based settings, aligning a model with the preferences of judges boosts scores, while injecting the least preferred properties lowers them. This results in notable score shifts: up to 0.59 on MT-Bench (1-10 scale) and 31.94 on AlpacaEval 2.0 (0-100 scale), highlighting the significant impact of this strategic adaptation. {Interactive Demo}: {Dataset}: {Code}: [h] [width=0.95]{figs/main_vis.pdf} Scenario (including {chitchat} and {value judgment}), and we highlight the most preferred property for each in its corresponding color: {lengthy} for human, {no severe errors} for GPT-4-Turbo, and {contain rich info} for LLaMA-2-70B-Chat. The value is the probability of a response being preferred in a pair when it satisfies one property better than the other response, holding all else equal. This can be interpreted as how much human or an LLM favor a certain property. Values above and below the 50\% line indicate a preference or dislike, respectively.} {fig:decomposition-example}","The preference dissection of human, GPT-4-Turbo and LLaMA-2-70B-Chat on {Communication} Scenario (including {chitchat} and {value judgment}), and we highlight the most preferred property for each in its corresponding color: \textcolor{fig1blue}{lengthy} for human, \textcolor{fig1orange}{no severe errors} for GPT-4-Turbo, and \textcolor{fig1green}{contain rich info} for LLaMA-2-70B-Chat. The value is the probability of a response being preferred in a pair when it satisfies one property better than the other response, holding all else equal. This can be interpreted as how much human or an LLM favor a certain property. Values above and below the 50\% line indicate a preference or dislike, respectively."
UniCoder: Scaling Code Large Language Model via Universal Code,2406.16441v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.16441v1_0.pdf,"Intermediate reasoning or acting steps have successfully improved large language models (LLMs) for handling various downstream natural language processing (NLP) tasks. When applying LLMs for code generation, recent works mainly focus on directing the models to articulate intermediate natural-language reasoning steps, as in chain-of-thought (CoT) prompting, and then output code with the natural language or other structured intermediate steps. However, such output is not suitable for code translation or generation tasks since the standard CoT has different logical structures and forms of expression with the code. In this work, we introduce the universal code () as the intermediate representation. It is a description of algorithm steps using a mix of conventions of programming languages, such as assignment operator, conditional operator, and loop. Hence, we collect an instruction dataset to train our model on multi-task learning objectives. comprises natural-language questions, code solutions, and the corresponding universal code. The alignment between the intermediate universal code representation and the final code solution significantly improves the quality of the generated code. The experimental results demonstrate that with the universal code significantly outperforms the previous prompting methods by a large margin, showcasing the effectiveness of the structural clues in pseudo-code.}",An example of \ourmethod{}. The Code LLM solves the code generation question by ``translating'' the pseudocode description (Universal Code) into executable code of the target programming language.
Does DetectGPT Fully Utilize Perturbation? Bridging Selective Perturbation to Fine-tuned Contrastive Learning Detector would be Better,2402.00263v4,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.00263v4_0.pdf,"The burgeoning generative capabilities of large language models (LLMs) have raised growing concerns about abuse, demanding automatic machine-generated text detectors. DetectGPT , a zero-shot metric-based detector, first introduces perturbation and shows great performance improvement. However, in DetectGPT, the random perturbation strategy could introduce noise, and logit regression depends on the threshold, harming the generalizability and applicability of individual or small-batch inputs. Hence, we propose a novel fine-tuned detector, , bridging metric-based and fine-tuned methods by contrastive learning on selective perturbation. Selective strategy retains important tokens during perturbation and weights for multi-pair contrastive learning. The experiments show that outperforms the state-of-the-art (SOTA) by 1.20\% in accuracy on average on four public datasets. And we further analyze the effectiveness, robustness, and generalization of the method. .}","Example of the selective strategy perturbation of \modelname{}, which prevent modifying important tokens (in green). Orange tokens are the perturbed texts."
AFaCTA: Assisting the Annotation of Factual Claim Detection with Reliable LLM Annotators,2402.11073v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.11073v3_0.pdf,"With the rise of generative AI, automated fact-checking methods to combat misinformation are becoming more and more important. However, factual claim detection, the first step in a fact-checking pipeline, suffers from two key issues that limit its scalability and generalizability: (1) inconsistency in definitions of the task and what a claim is, and (2) the high cost of manual annotation. To address (1), we review the definitions in related work and propose a unifying definition of factual claims that focuses on verifiability. To address (2), we introduce {AFaCTA} ({A}utomatic {Fa}ctual {C}laim de{T}ection {A}nnotator), a novel framework that assists in the annotation of factual claims with the help of large language models (LLMs). AFaCTA calibrates its annotation confidence with consistency along three predefined reasoning paths. %to mirror the reasoning process of human experts. Extensive evaluation and experiments in the domain of political speech reveal that AFaCTA can efficiently assist experts in annotating factual claims and training high-quality classifiers, and can work with or without expert supervision. Our analyses also result in PoliClaim, a comprehensive claim detection dataset spanning diverse political topics.{ . We will open-source our code, annotations, and LLM outputs. }%Evaluation on PoliClaim$_{test}$ reveals that AFaCTA outperforms human experts on samples with consistent annotations. Comprehensive fine-tuning experiments on PoliClaim$_{train}$ (AFaCTA-annotated) demonstrate that AFaCTA-annotated data can be a strong alternative to human-annotated data in training classifiers.","AFaCTA Pipeline. All steps that need LLM prompting are annotated with the brain icon. Besides the target statement, a short context (if available) is also provided to help the model understand the statement."
Towards Faithful and Robust LLM Specialists for Evidence-Based Question-Answering,2402.08277v5,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.08277v5_0.pdf,"Advances towards more faithful and traceable answers of Large Language Models (LLMs) are crucial for various research and practical endeavors. One avenue in reaching this goal is basing the answers on reliable sources. However, this Evidence-Based QA has proven to work insufficiently with LLMs in terms of citing the correct sources (source quality) and truthfully representing the information within sources (answer attributability). In this work, we systematically investigate how to robustly fine-tune LLMs for better source quality and answer attributability. Specifically, we introduce a data generation pipeline with automated data quality filters, which can synthesize diversified high-quality training and testing data at scale. We further introduce four test sets to benchmark the robustness of fine-tuned specialist models. Extensive evaluation shows that fine-tuning on synthetic data improves performance on both in- and out-of-distribution. %Evidence-Based QA cases. Furthermore, we show that data quality, which can be drastically improved by proposed quality filters, matters more than quantity in improving Evidence-Based QA.{ All our codes, LLM generations, and human annotations are accessible through . }",Synthetic data generation pipeline and Evaluation for Evidence-Based QA.
Navigating the Metrics Maze: Reconciling Score Magnitudes and Accuracies,2401.06760v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2401.06760v2_0.pdf,"Ten years ago, a single metric, BLEU, governed progress in machine translation research. For better or worse, there is no such consensus today, and consequently it is difficult for researchers to develop and retain intuitions about metric deltas that drove earlier research and deployment decisions. This paper investigates the ``dynamic range'' of a number of modern metrics in an effort to provide a collective understanding of the meaning of differences in scores both within and among metrics; in other words, we ask {what point difference $x$ in metric $y$ is required between two systems for humans to notice?} We conduct our evaluation on a new large dataset, , using it to discover deltas at which metrics achieve system-level differences that are meaningful to humans, which we measure by pairwise system accuracy. We additionally show that this method of establishing delta-accuracy is more stable than the standard use of statistical p-values in regards to testset size. Where data size permits, we also explore the effect of metric deltas and accuracy across finer-grained features such as translation direction, domain, and system closeness.",Distribution of pairwise system deltas for each metric over all systems from WMT22. Gray rectangles show min-max range which is vastly different between metrics. Standard deviations (black lines) also differ.
Handling Ambiguity in Emotion: From Out-of-Domain Detection to Distribution Estimation,2402.12862v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.12862v1_0.pdf,"The subjective perception of emotion leads to inconsistent labels from human annotators. Typically, utterances lacking majority-agreed labels are excluded when training an emotion classifier, which cause problems when encountering ambiguous emotional expressions during testing. This paper investigates three methods to handle ambiguous emotion. First, we show that incorporating utterances without majority-agreed labels as an additional class in the classifier reduces the classification performance of the other emotion classes. Then, we propose detecting utterances with ambiguous emotions as out-of-domain samples by quantifying the uncertainty in emotion classification using evidential deep learning. This approach retains the classification accuracy while effectively detects ambiguous emotion expressions. Furthermore, to obtain fine-grained distinctions among ambiguous emotions, we propose representing emotion as a distribution instead of a single class label. The task is thus re-framed from classification to distribution estimation where every individual annotation is taken into account, not just the majority opinion. The evidential uncertainty measure is extended to quantify the uncertainty in emotion distribution estimation. Experimental results on the IEMOCAP and CREMA-D datasets demonstrate the superior capability of the proposed method in terms of majority class prediction, emotion distribution estimation, and uncertainty estimation.","The bar chart shows the number of labels assigned by annotators to the emotion class ``angry'' (Ang), ``frustrated'' (Fru), and ``neutral'' (Neu) in an example. In utterance (a), eight annotators interpret the emotion as angry while one interprets it as frustrated."
MoPS: Modular Story Premise Synthesis for Open-Ended Automatic Story Generation,2406.05690v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.05690v1_0.pdf,"A story premise succinctly defines a story's main idea, foundation, and trajectory. It serves as the initial trigger in automatic story generation. Existing sources of story premises are limited by a lack of diversity, uneven quality, and high costs that make them difficult to scale. In response, we introduce {Mo}dular Story {P}remise {S}ynthesis (MoPS) which breaks down story premises into modules like background and persona for automated design and generation. MoPS consists of three phases: (1) Pre-collect a consistent set of candidates for each module to form a nested dictionary. (2) Extract a key path from the nested dictionary as the premise design. (3) Instruct an LLM to integrate the design into a coherent premise sentence. Thorough evaluations demonstrate that our synthesized premises excel in diversity, fascination, completeness, and originality compared to those induced from large language models and captured from public story datasets. Similarly, the extended novels and scripts generated from our premises also exhibit higher quality. In supplementary materials, we provide the MoPS code suite, along with 7.6k generated premises and 1k extended stories. Code: .","Overview of MoPS. We divide the premise into four ordered modules: \theme{theme}, \background{background}, \persona{persona}, and \plot{plot}, with each module further divided into submodules. From the top down, arrows indicate the dependency relationships within and between modules."
ToolSword: Unveiling Safety Issues of Large Language Models in Tool Learning Across Three Stages,2402.10753v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.10753v2_0.pdf,"Tool learning is widely acknowledged as a foundational approach for deploying large language models (LLMs) in real-world scenarios. While current research primarily emphasizes leveraging tools to augment LLMs, it frequently neglects emerging safety considerations tied to their application. To fill this gap, we present {ToolSword}, a comprehensive framework dedicated to meticulously investigating safety issues linked to LLMs in tool learning. Specifically, ToolSword delineates six safety scenarios for LLMs in tool learning, encompassing {malicious queries} and {jailbreak attacks} in the input stage, {noisy misdirection} and {risky cues} in the execution stage, and {harmful feedback} and {error conflicts} in the output stage. Experiments conducted on 11 open-source and closed-source LLMs reveal enduring safety challenges in tool learning, such as handling harmful queries, employing risky tools, and delivering detrimental feedback, which even GPT-4 is susceptible to. Moreover, we conduct further studies with the aim of fostering research on tool learning safety. The data is released in~.","Responses of LLMs to unsafe queries between standard dialogue and tool learning Contexts. Tool learning may disrupt the safe alignment mechanism of LLMs, leading to responses to unsafe queries through tool invocation."
Enhancing Contrastive Learning with Noise-Guided Attack: Towards Continual Relation Extraction in the Wild,2305.07085v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2305.07085v1_0.png,"The principle of continual relation extraction~(CRE) involves adapting to emerging novel relations while preserving od knowledge. While current endeavors in CRE succeed in preserving old knowledge, they tend to fail when exposed to contaminated data streams. We assume this is attributed to their reliance on an artificial hypothesis that the data stream has no annotation errors, which hinders real-world applications for CRE. Considering the ubiquity of noisy labels in real-world datasets, in this paper, we formalize a more practical learning scenario, termed as {noisy-CRE}. Building upon this challenging setting, we develop a noise-resistant contrastive framework named as {N}oise-guided {a}ttack in {C}ontrative {L}earning~(NaCL) to learn incremental corrupted relations. Compared to direct noise discarding or inaccessible noise relabeling, we present modifying the feature space to match the given noisy labels via attacking can better enrich contrastive representations. Extensive empirical validations highlight that NaCL can achieve consistent performance improvements with increasing noise rates, outperforming state-of-the-art baselines.",Left Table: Noisy labels exist widely in well-annotated benchmarks. Right Plot: Performance of the state-of-the-art CRE methods drop significantly on TACRED with noise ratio ranging from 0\% to 50\%.
VariErr NLI: Separating Annotation Error from Human Label Variation,2403.01931v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2403.01931v2_0.pdf,"Human label variation arises when annotators assign different labels to the same item for valid reasons, while annotation errors occur when labels are assigned for invalid reasons. These two issues are prevalent in NLP benchmarks, yet existing research has studied them in isolation. To the best of our knowledge, there exists no prior work that focuses on teasing apart error from signal, especially in cases where signal is beyond black-and-white. To fill this gap, we introduce a systematic methodology and a new dataset, (variation versus error), focusing on the NLI task in English. We propose a 2-round annotation procedure with annotators explaining each label and subsequently judging the validity of label-explanation pairs. contains 7,732 validity judgments on 1,933 explanations for 500 re-annotated MNLI items. We assess the effectiveness of various automatic error detection (AED) methods and GPTs in uncovering errors versus human label variation. We find that state-of-the-art AED methods significantly underperform GPTs and humans. While GPT-4 is the best system, it still falls short of human performance. Our methodology is applicable beyond NLI, offering fertile ground for future research on error versus plausible variation, which in turn can yield better and more trustworthy NLP systems.","Variation or Error? We present a procedure and multi-label dataset, \name, to tease apart annotation error from plausible human label variation. We leverage {ecologically valid explanations} and {validation} as two key mechanisms (boxed: self-validations; label ``Contradiction'' is an {error}); see \S\ref{sec:dataset}-\S\ref{sec:validating-explanations-label} for details."
Benchmarking Knowledge Boundary for Large Language Models: A Different Perspective on Model Evaluation,2402.11493v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.11493v2_0.pdf,"In recent years, substantial advancements have been made in the development of large language models, achieving remarkable performance across diverse tasks. To evaluate the knowledge ability of language models, previous studies have proposed lots of benchmarks based on question-answering pairs. We argue that it is not reliable and comprehensive to evaluate language models with a fixed question or limited paraphrases as the query, since language models are sensitive to prompt. Therefore, we introduce a novel concept named knowledge boundary to encompass both prompt-agnostic and prompt-sensitive knowledge within language models. Knowledge boundary avoids prompt sensitivity in language model evaluations, rendering them more dependable and robust. To explore the knowledge boundary for a given model, we propose a projected gradient descent method with semantic constraints, a new algorithm designed to identify the optimal prompt for each piece of knowledge. Experiments demonstrate a superior performance of our algorithm in computing the knowledge boundary compared to existing methods. Furthermore, we evaluate the ability of multiple language models in several domains with knowledge boundary.","Illustration of three classes of knowledge based on the model's mastery of knowledge in different textual forms. Existing evaluation methods suffer from sensitivity to input prompt. Therefore, the knowledge ability depicted by these methods is irregularly shaped. We propose to evaluate the knowledge capacity with a knowledge boundary containing both Prompt-Agnostic Knowledge and Prompt-Sensitive Knowledge."
Exploring the Potential of Large Language Models in Computational Argumentation,2311.09022v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2311.09022v3_0.pdf,"Computational argumentation has become an essential tool in various domains, including law, public policy, and artificial intelligence. It is an emerging research field in natural language processing that attracts increasing attention. Research on computational argumentation mainly involves two types of tasks: argument mining and argument generation. As large language models (LLMs) have demonstrated impressive capabilities in understanding context and generating natural language, it is worthwhile to evaluate the performance of LLMs on diverse computational argumentation tasks. This work aims to embark on an assessment of LLMs, such as ChatGPT, Flan models, and LLaMA2 models, in both zero-shot and few-shot settings. We organize existing tasks into six main categories and standardize the format of fourteen openly available datasets. In addition, we present a new benchmark dataset on counter speech generation that aims to holistically evaluate the end-to-end performance of LLMs on argument mining and argument generation. Extensive experiments show that LLMs exhibit commendable performance across most of the datasets, demonstrating their capabilities in the field of argumentation. Our analysis offers valuable suggestions for evaluating computational argumentation and its integration with LLMs in future research endeavors. {https://github.com/DAMO-NLP-SG/LLM-argumentation}.}",Explored tasks and datasets in this work.
TaxoLLaMA: WordNet-based Model for Solving Multiple Lexical Semantic Tasks,2403.09207v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2403.09207v2_0.pdf,"In this paper, we explore the capabilities of LLMs in capturing lexical-semantic knowledge from WordNet on the example of the model and test it on multiple lexical semantic tasks. As the outcome of our experiments, we present , the ``all-in-one'' model for taxonomy-related tasks, lightweight due to 4-bit quantization and LoRA. achieves 11 SOTA results, and 4 top-2 results out of 16 tasks on the Taxonomy Enrichment, Hypernym Discovery, Taxonomy Construction, and Lexical Entailment tasks. Moreover, it demonstrates a very strong zero-shot performance on Lexical Entailment and Taxonomy Construction with no fine-tuning. We also explore its hidden multilingual and domain adaptation capabilities with a little tuning or few-shot learning. All datasets, code, and pre-trained models are available online.}",Training procedure of \text{TaxoLLaMA}: hypernym relations from the WordNet are linearized and fed into an LLM model. The model aims at generating the correct hypernym(s) as output.
CANDLE: Iterative Conceptualization and Instantiation Distillation from Large Language Models for Commonsense Reasoning,2401.07286v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2401.07286v2_0.pdf,"The sequential process of conceptualization and instantiation is essential to generalizable commonsense reasoning as it allows the application of existing knowledge to unfamiliar scenarios. However, existing works tend to undervalue the step of instantiation and heavily rely on pre-built concept taxonomies and human annotations to collect both types of knowledge, resulting in a lack of instantiated knowledge to complete reasoning, high cost, and limited scalability. To tackle these challenges, we introduce CANDLE (}onceptu}lization and I}stantiation }istillation from }arge Language Mod}ls), a distillation framework that iteratively performs contextualized conceptualization and instantiation over commonsense knowledge bases by instructing large language models to generate both types of knowledge with critic filtering. By applying CANDLE to ATOMIC~, we construct a comprehensive knowledge base comprising six million conceptualizations and instantiated commonsense knowledge triples. Both types of knowledge are firmly rooted in the original ATOMIC dataset, and intrinsic evaluations demonstrate their exceptional quality and diversity. Empirical results indicate that distilling CANDLE on student models provides benefits across three downstream tasks{https://github.com/HKUST-KnowComp/CANDLE}.}.",Examples showing several chains of {\textcolor{concept_color}{conceptualization}} and {\textcolor{instance_color}{instantiation}} over the event {PersonX enjoys exercising in the gym}. {\textcolor{new_knowledge_color}{New inferential commonsense knowledge}} can be induced when placing the {\textcolor{instance_color}{instantiation}} back into the {\textcolor{original_tail_color}{original context}}.
TransliCo: A Contrastive Learning Framework to Address the Script Barrier in Multilingual Pretrained Language Models,2401.06620v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2401.06620v2_0.pdf,"The world's more than 7000 languages are written in at least 293 scripts.} Due to various reasons, many closely related languages use different scripts, which poses a difficulty for multilingual pretrained language models (mPLMs) in learning crosslingual knowledge through lexical overlap. As a consequence, mPLMs are faced with a script barrier: representations from different scripts are located in different subspaces, which can result in crosslingual transfer involving languages of different scripts performing suboptimally. To address this problem, we propose {}, a framework that optimizes the Transliteration Contrastive Modeling (TCM) objective to fine-tune an mPLM by contrasting sentences in its training data and their transliterations in a unified script (in our case Latin{Throughout this paper we use Latin to refer to the Latin script, not the Latin language.}), which enhances uniformity in the representation space for different scripts. Using Glot500-m , an mPLM pretrained on over 500 languages, as our source model, we fine-tune it on a small portion (5\%) of its training data, and refer to the resulting model as {}. We show that not only better aligns representations from distinct scripts but also outperforms the original Glot500-m on various zero-shot crosslingual transfer tasks. Additionally, we achieve consistent improvement in a case study on the Indic group where the languages exhibit areal features but use different scripts. We make our code and models publicly available.}",An illustration of applying \frameworkname to a single batch of data during fine-tuning. The training data is used by the two training objectives in \frameworkname: Masked Language Modeling (MLM) and Transliteration Contrastive Modeling (TCM). MLM is applied to both the original sentences and their Latin transliterations. TCM is used to learn better-aligned cross-script representations by contrasting the \underline{\textcolor{red}{positive pairs}} (paired data connected with red lines) against the \underline{\textcolor{blue}{negative pairs}} (the remaining samples connected with blue lines).
Time is Encoded in the Weights of Finetuned Language Models,2312.13401v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2312.13401v2_0.png,"We present {time vectors}, a simple tool to customize language models to new time periods. Time vectors are created by finetuning a language model on data from a single time (e.g., a year or month), and then subtracting the weights of the original pretrained model. This vector specifies a direction in weight space that, as our experiments show, improves performance on text from that time period. Time vectors specialized to adjacent time periods appear to be positioned closer together in a manifold. Using this structure, we interpolate between time vectors to induce new models that perform better on intervening and future time periods, without any additional training. We demonstrate the consistency of our findings across different tasks, domains, model sizes, and time scales. Our results suggest that time is encoded in the weight space of finetuned models.","{We present {time vectors}, a simple tool to customize language models to new time periods.} Time vectors ($\tau_i$) specify a direction in weight space that improves performance on text from a time period $i$. They are computed by subtracting the pretrained weights ($\theta_{\text{pre}}$; left panel) from those finetuned to a target time period ($\theta_i$). We can customize model behavior to new time periods (e.g., intervening months or years) by interpolating between time vectors and adding the result to the pretrained model (middle panel). We can also generalize to a future time period $j$ with analogy arithmetic (right panel). This involves combining a task-specific time vector with analogous time vectors derived from finetuned language models ($\tau^{\text{LM}}_j$)."
SirLLM: Streaming Infinite Retentive LLM,2405.12528v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2405.12528v1_0.pdf,"As Large Language Models (LLMs) become increasingly prevalent in various domains, their ability to process inputs of any length and maintain a degree of memory becomes essential. However, the one-off input of overly long texts is limited, as studies have shown that when input lengths exceed the LLMs' pre-trained text length, there is a dramatic decline in text generation capabilities. Moreover, simply extending the length of pre-training texts is impractical due to the difficulty in obtaining long text data and the substantial memory consumption costs this would entail for LLMs. Recent efforts have employed streaming inputs to alleviate the pressure of excessively long text inputs, but this approach can significantly impair the model's long-term memory capabilities. Motivated by this challenge, we introduce Streaming Infinite Retentive LLM (SirLLM), which allows LLMs to maintain longer memory during infinite-length dialogues without the need for fine-tuning. SirLLM utilizes the Token Entropy metric and a memory decay mechanism to filter key phrases, endowing LLMs with both long-lasting and flexible memory. We designed three distinct tasks and constructed three datasets to measure the effectiveness of SirLLM from various angles: (1) DailyDialog; (2) Grocery Shopping; (3) Rock-Paper-Scissors. Our experimental results robustly demonstrate that SirLLM can achieve stable and significant improvements across different LLMs and tasks, compellingly proving its effectiveness. When having a coversation, ""A sir could forget himself,"" but SirLLM never does! Our code is publicly available at {https://github.com/Zoeyyao27/SirLLM}",The visualization of SirLLM versus existing attention patterns.
ItD: Large Language Models Can Teach Themselves Induction through Deduction,2403.05789v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2403.05789v1_0.pdf,"Although Large Language Models (LLMs) are showing impressive performance on a wide range of Natural Language Processing tasks, researchers have found that they still have limited ability to conduct induction. Recent works mainly adopt ``post processes'' paradigms to improve the performance of LLMs on induction (e.g., the hypothesis search \& refinement methods), but their performance is still constrained by the inherent inductive capability of the LLMs. In this paper, we propose a novel framework, nduction hrough eduction (ItD), to enable the LLMs to teach themselves induction through deduction. The ItD framework is composed of two main components: a Deductive Data Generation module to generate induction data and a Naive Bayesian Induction module to optimize the fine-tuning and decoding of LLMs. Our empirical results showcase the effectiveness of ItD on two induction benchmarks, achieving relative performance improvement of 36\% and 10\% compared with previous state-of-the-art, respectively. Our ablation study verifies the effectiveness of two key modules of ItD. We also verify the effectiveness of ItD across different LLMs and deductors. The data and code of this paper can be found at https://anonymous.4open.science/r/ItD-E844.","Task of Induction. The tested model observes a batch of input-output $(x,y)$ pairs and needs to predict the latent transformation $f$ shared by these $(x,y)$ pairs."
Enhancing In-Context Learning via Implicit Demonstration Augmentation,2407.00100v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2407.00100v1_0.pdf,"The emergence of in-context learning (ICL) enables large pre-trained language models (PLMs) to make predictions for unseen inputs without updating parameters. Despite its potential, ICL's effectiveness heavily relies on the quality, quantity, and permutation of demonstrations, commonly leading to suboptimal and unstable performance. In this paper, we tackle this challenge for the first time from the perspective of demonstration augmentation. Specifically, we start with enriching representations of demonstrations by leveraging their deep feature distribution. We then theoretically reveal that when the number of augmented copies approaches infinity, the augmentation is approximately equal to a novel logit calibration mechanism integrated with specific statistical properties. This insight results in a simple yet highly efficient method that significantly improves the average and worst-case accuracy across diverse PLMs and tasks. Moreover, our method effectively reduces performance variance among varying demonstrations, permutations, and templates, and displays the capability to address imbalanced class distributions.",Illustration for demonstration augmentation using semantic directions (vectors) sampled from the deep feature distribution of demonstration examples.
PRoLoRA: Partial Rotation Empowers More Parameter-Efficient LoRA,2402.16902v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.16902v2_0.pdf;/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.16902v2_1.pdf;/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.16902v2_2.pdf;/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.16902v2_3.pdf,"With the rapid scaling of large language models (LLMs), serving numerous low-rank adaptations (LoRAs) concurrently has become increasingly impractical, leading to unaffordable costs and necessitating more parameter-efficient finetuning methods. In this work, we introduce {P}artially {Ro}tation-enhanced {Lo}w-{R}ank {A}daptation (PRoLoRA), an intra-layer sharing mechanism comprising four essential components: broadcast reduction, rotation enhancement, partially-sharing refinement, and rectified initialization strategy. As a superset of LoRA, PRoLoRA retains its advantages, and effectively circumvent the drawbacks of peer parameter-sharing methods with superior model capacity, practical feasibility, and broad applicability. Empirical experiments demonstrate the remarkably higher parameter efficiency of PRoLoRA in both specific parameter budget and performance target scenarios, and its scalability to larger LLMs. Notably, with one time less trainable parameters, PRoLoRA still outperforms LoRA on multiple instruction tuning datasets. Subsequently, an ablation study is conducted to validate the necessity of individual components % explore the features of critical hyperparameters, and highlight the superiority of PRoLoRA over three potential variants. Hopefully, the conspicuously higher parameter efficiency can establish PRoLoRA as a resource-friendly alternative to LoRA.","Illustration of the original LoRA, our proposed PRoLoRA, and their intermediate states (i.e., CLoRA and RoLoRA). Here we set the rank $r$, unshared rank $u$, sharing rates $m$ and $n$ of the $\mathbf{A}$ and $\mathbf{B}$ matrices to be 4, 1, 2 and 3, respectively. Different shades of color in matrices $\mathbf{A}$ and $\mathbf{B}$ denote distinct ranks. The rotation arrows and center numbers indicate rotation directions and base strides, while dotted lines and higher transparency denote replicated or rotated weights, emphasizing that these weights do not contribute to the trainable parameters. Additionally, the center numbers % numerical values at the center of each matrix block represent the relative displacement of the $\mathbf{A}_i$ and $\mathbf{B}_i$ chunks compared to those of top-left block (i.e., $\mathbf{A}_0$ and $\mathbf{B}_0$)."
Interpreting Conversational Dense Retrieval by Rewriting-Enhanced Inversion of Session Embedding,2402.12774v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.12774v2_0.pdf,"Conversational dense retrieval has shown to be effective in conversational search. However, a major limitation of conversational dense retrieval is their lack of interpretability, hindering intuitive understanding of model behaviors for targeted improvements. This paper presents , a simple yet effective approach to shed light on interpretable conversational dense retrieval models. transforms opaque conversational session embeddings into explicitly interpretable text while faithfully maintaining their original retrieval performance as much as possible. Such transformation is achieved by training a recently proposed Vec2Text model~ {vec2text} based on the ad-hoc query encoder, leveraging the fact that the session and query embeddings share the same space in existing conversational dense retrieval. To further enhance interpretability, we propose to incorporate external interpretable query rewrites into the transformation process. Extensive evaluations on three conversational search benchmarks demonstrate that can yield more interpretable text and faithfully preserve original retrieval performance than baselines. Our work connects opaque session embeddings with transparent query rewriting, paving the way toward trustworthy conversational search. Our code is available at {this repository}.","The blue section on the left signifies the conversational dense retrieval, and the green section on the right provides an overview of \textsc{ConvInv}."
Hypergraph based Understanding for Document Semantic Entity Recognition,2407.06904v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2407.06904v1_0.pdf,"Semantic entity recognition is an important task in the field of visually-rich document understanding. It distinguishes the semantic types of text by analyzing the position relationship between text nodes and the relation between text content. The existing document understanding models mainly focus on entity categories while ignoring the extraction of entity boundaries. We build a novel hypergraph attention document semantic entity recognition framework, HGA, which uses hypergraph attention to focus on entity boundaries and entity categories at the same time. It can conduct a more detailed analysis of the document text representation analyzed by the upstream model and achieves a better performance of semantic information. We apply this method on the basis of GraphLayoutLM to construct a new semantic entity recognition model HGALayoutLM. Our experiment results on FUNSD, CORD, XFUND and SROIE show that our method can effectively improve the performance of semantic entity recognition tasks based on the original model. The results of HGALayoutLM on FUNSD and XFUND reach the new state-of-the-art results.",Difference in Document Task.
An Iterative Associative Memory Model for Empathetic Response Generation,2402.17959v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.17959v2_0.pdf,"Empathetic response generation aims to comprehend the cognitive and emotional states in dialogue utterances and generate proper responses. Psychological theories posit that comprehending emotional and cognitive states necessitates iteratively capturing and understanding associated words across dialogue utterances. However, existing approaches regard dialogue utterances as either a long sequence or independent utterances for comprehension, which are prone to overlook the associated words between them. To address this issue, we propose an Iterative Associative Memory Model (IAMM)} for empathetic response generation. Specifically, we employ a novel second-order interaction attention mechanism to iteratively capture vital associated words between dialogue utterances and situations, dialogue history, and a memory module (for storing associated words), thereby accurately and nuancedly comprehending the utterances. We conduct experiments on the Empathetic-Dialogue dataset. Both automatic and human evaluations validate the efficacy of the model. Variant experiments on LLMs also demonstrate that attending to associated words improves empathetic comprehension and expression.",{fig 1} An example of iterative association. Words with the same color are associated. The memory stores the associated words.
Dr.Academy: A Benchmark for Evaluating Questioning Capability in Education for Large Language Models,2408.10947v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2408.10947v1_0.pdf,"Teachers are important to imparting knowledge and guiding learners, and the role of large language models (LLMs) as potential educators is emerging as an important area of study. Recognizing LLMs' capability to generate educational content can lead to advances in automated and personalized learning. While LLMs have been tested for their comprehension and problem-solving skills, their capability in teaching remains largely unexplored. In teaching, questioning is a key skill that guides students to analyze, evaluate, and synthesize core concepts and principles. Therefore, our research introduces a benchmark to evaluate the questioning capability in education as a teacher of LLMs through evaluating their generated educational questions, utilizing Anderson and Krathwohl's taxonomy across general, monodisciplinary, and interdisciplinary domains. We shift the focus from LLMs as learners to LLMs as educators, assessing their teaching capability through guiding them to generate questions. We apply four metrics, including relevance, coverage, representativeness, and consistency, to evaluate the educational quality of LLMs' outputs. Our results indicate that GPT-4 demonstrates significant potential in teaching general, humanities, and science courses; Claude2 appears more apt as an interdisciplinary teacher. Furthermore, the automatic scores align with human perspectives.",Comparison between general and educational questions.
UniBridge: A Unified Approach to Cross-Lingual Transfer Learning for Low-Resource Languages,2406.09717v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.09717v3_0.png,"In this paper, we introduce UniBridge (Cross-Lingual Transfer Learning with Optimized Embeddings and Vocabulary), a comprehensive approach developed to improve the effectiveness of Cross-Lingual Transfer Learning, particularly in languages with limited resources. Our approach tackles two essential elements of a language model: the initialization of embeddings and the optimal vocabulary size. Specifically, we propose a novel embedding initialization method that leverages both lexical and semantic alignment for a language. In addition, we present a method for systematically searching for the optimal vocabulary size, ensuring a balance between model complexity and linguistic coverage. Our experiments across multilingual datasets show that our approach greatly improves the F1-Score in several languages. UniBridge is a robust and adaptable solution for cross-lingual systems in various languages, highlighting the significance of initializing embeddings and choosing the right vocabulary size in cross-lingual environments.","Some languages/scripts are not covered in the pre-trained corpora. Hence, the pre-trained tokenizer will eventually produce many unknown tokens which corrupts the sentence's meaning and results in poor performance."
VISTA: Visualized Text Embedding For Universal Multi-Modal Retrieval,2406.04292v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.04292v1_0.pdf,"Multi-modal retrieval becomes increasingly popular in practice. However, the existing retrievers are mostly text-oriented, which lack the capability to process visual information. Despite the presence of vision-language models like CLIP, the current methods are severely limited in representing the text-only and image-only data. In this work, we present a new embedding model {VISTA} for universal multi-modal retrieval. Our work brings forth threefold technical contributions. Firstly, we introduce a flexible architecture which extends a powerful text encoder with the image understanding capability by introducing visual token embeddings. Secondly, we develop two data generation strategies, which bring high-quality composed image-text to facilitate the training of the embedding model. Thirdly, we introduce a multi-stage training algorithm, which first aligns the visual token embedding with the text encoder using massive weakly labeled data, and then develops multi-modal representation capability using the generated composed image-text data. In our experiments, VISTA achieves superior performances across a variety of multi-modal retrieval tasks in both zero-shot and supervised settings. Our model, data, and source code are available at {https://github.com/FlagOpen/FlagEmbedding}.","The model architecture of our VISTA model. We use the pre-trained language model as the foundation, making the ViT encoder transfer the Image to recognized tokens of the text encoder."
Open Ko-LLM Leaderboard: Evaluating Large Language Models in Korean with Ko-H5 Benchmark,2405.20574v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2405.20574v2_0.pdf,"This paper introduces the Open Ko-LLM Leaderboard} and the Ko-H5 Benchmark as vital tools for evaluating Large Language Models (LLMs) in Korean. Incorporating private test sets while mirroring the English Open LLM Leaderboard, we establish a robust evaluation framework that has been well integrated in the Korean LLM community. We perform data leakage analysis that shows the benefit of private test sets along with a correlation study within the Ko-H5 benchmark and temporal analyses of the Ko-H5 score. Moreover, we present empirical support for the need to expand beyond set benchmarks. We hope the Open Ko-LLM Leaderboard sets precedent for expanding LLM evaluation to foster more linguistic diversity.","Data curation process for the Ko-H5 benchmark. We perform thorough human review of the machine translation results by culturally aligning the reviewers with the Korean language. Additionally, we perform filtering for data that require specific domain knowledge and re-translate them with translators that are trained with the required domain knowledge."
T2S-GPT: Dynamic Vector Quantization for Autoregressive Sign Language Production from Text,2406.07119v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.07119v1_0.pdf,"In this work, we propose a two-stage sign language production (SLP) paradigm that first encodes sign language sequences into discrete codes and then autoregressively generates sign language from text based on the learned codebook. However, existing vector quantization (VQ) methods are fixed-length encodings, overlooking the uneven information density in sign language, which leads to under-encoding of important regions and over-encoding of unimportant regions. To address this issue, we propose a novel dynamic vector quantization (DVA-VAE) model that can dynamically adjust the encoding length based on the information density in sign language to achieve accurate and compact encoding. Then, a GPT-like model learns to generate code sequences and their corresponding durations from spoken language text. Extensive experiments conducted on the PHOENIX14T dataset demonstrate the effectiveness of our proposed method. To promote sign language research, we propose a new large German sign language dataset, PHOENIX-News, which contains 486 hours of sign language videos, audio, and transcription texts. Experimental analysis on PHOENIX-News shows that the performance of our model can be further improved by increasing the size of the training data. Our project homepage is .",Comparison of fixed-length encoding and variable-length encoding.
OceanGPT: A Large Language Model for Ocean Science Tasks,2310.02031v8,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2310.02031v8_0.pdf,"Ocean science, which delves into the oceans that are reservoirs of life and biodiversity, is of great significance given that oceans cover over 70\% of our planet's surface. Recently, advances in Large Language Models (LLMs) have transformed the paradigm in science. Despite the success in other domains, current LLMs often fall short in catering to the needs of domain experts like oceanographers, and the potential of LLMs for ocean science is under-explored. The intrinsic reasons are the immense and intricate nature of ocean data as well as the necessity for higher granularity and richness in knowledge. To alleviate these issues, we introduce {}, the first-ever large language model in the ocean domain, which is expert in various ocean science tasks. We also propose {}, a novel framework to automatically obtain a large volume of ocean domain instruction data, which generates instructions based on multi-agent collaboration. Additionally, we construct the first oceanography benchmark, {}, to evaluate the capabilities of LLMs in the ocean domain. Though comprehensive experiments, {} not only shows a higher level of knowledge expertise for oceans science tasks but also gains preliminary embodied intelligence capabilities in ocean technology.",Capabilities of {\ours}. Our proposed model not only shows a higher level of knowledge expertise for oceans science tasks but also gains preliminary embodied intelligence capabilities in ocean technology.
BIPED: Pedagogically Informed Tutoring System for ESL Education,2406.03486v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.03486v1_0.png,"Large Language Models (LLMs) have a great potential to serve as readily available and cost-efficient Conversational Intelligent Tutoring Systems (CITS) for teaching L2 learners of English. Existing CITS, however, are designed to teach only simple concepts or lack the pedagogical depth necessary to address diverse learning strategies. To develop a more pedagogically informed CITS capable of teaching complex concepts, we construct a BIlingual PEDagogically-informed Tutoring Dataset ({BIPED}) of one-on-one, human-to-human English tutoring interactions. Through post-hoc analysis of the tutoring interactions, we come up with a lexicon of dialogue acts (34 tutor acts and 9 student acts), which we use to further annotate the collected dataset. Based on a two-step framework of first predicting the appropriate tutor act then generating the corresponding response, we implemented two CITS models using GPT-4 and SOLAR-KO, respectively. We experimentally demonstrate that the implemented models not only replicate the style of human teachers but also employ diverse and contextually appropriate pedagogical strategies.","Example of our dataset, BIPED. It includes a series of dialogues between a tutor and a student, annotated with dialogue acts, content information, and the correctness of student responses."
Progressively Modality Freezing for Multi-Modal Entity Alignment,2407.16168v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2407.16168v1_0.png,"Multi-Modal Entity Alignment aims to discover identical entities across heterogeneous knowledge graphs. While recent studies have delved into fusion paradigms to represent entities holistically, the elimination of features irrelevant to alignment and modal inconsistencies is overlooked, which are caused by inherent differences in multi-modal features. To address these challenges, we propose a novel strategy of progressive modality freezing, called PMF, that focuses on alignment-relevant features and enhances multi-modal feature fusion. Notably, our approach introduces a pioneering cross-modal association loss to foster modal consistency. Empirical evaluations across nine datasets confirm PMF's superiority, demonstrating state-of-the-art performance and the rationale for freezing modalities. Our code is available at~{https://github.com/ninibymilk/PMF-MMEA}.",Illustration of irrelevant vs. relevant features in multi-modal knowledge graphs.
What Does the Bot Say? Opportunities and Risks of Large Language Models in Social Media Bot Detection,2402.00371v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.00371v2_0.pdf,"Social media bot detection has always been an arms race between advancements in machine learning bot detectors and adversarial bot strategies to evade detection. In this work, we bring the arms race to the next level by investigating the opportunities and risks of state-of-the-art large language models (LLMs) in social bot detection. To investigate the opportunities, we design novel LLM-based bot detectors by proposing a mixture-of-heterogeneous-experts framework to divide and conquer diverse user information modalities. To illuminate the risks, we explore the possibility of LLM-guided manipulation of user textual and structured information to evade detection. Extensive experiments with three LLMs on two datasets demonstrate that instruction tuning on merely 1,000 annotated examples produces specialized LLMs that outperform state-of-the-art bot detection baselines by up to 9.1\% on both datasets. On the other hand, LLM-guided manipulation strategies could significantly bring down the performance of existing bot detectors by up to 29.6\% and harm the calibration and reliability of bot detection systems. Ultimately, this works identifies LLMs as the new frontier of social bot detection research.{https://github.com/BunsenFeng/botsay}.}",Overview of the opportunities of LLM-based bot detectors and risks of LLM-based evasive bots.
Self-Contrast: Better Reflection Through Inconsistent Solving Perspectives,2401.02009v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2401.02009v3_0.pdf,"The reflection capacity of Large Language Model (LLM) has garnered extensive attention. A post-hoc prompting strategy, e.g., reflexion and self-refine, refines LLM's response based on self-evaluated or external feedback. However, recent research indicates without external feedback, LLM's intrinsic reflection is unstable. Our investigation unveils that the key bottleneck is the quality of the self-evaluated feedback. We find LLMs often exhibit overconfidence or high randomness when self-evaluate, offering stubborn or inconsistent feedback, which causes poor reflection. To remedy this, we advocate {Self-Contrast}: It adaptively {explores} diverse solving perspectives tailored to the request, {contrasts} the differences, and {summarizes} these discrepancies into a checklist which could be used to re-examine and eliminate discrepancies. Our method provides LLM with diverse perspectives to alleviate stubborn biases. Moreover, their discrepancies indicate potential errors or inherent uncertainties that LLM often overlooks. Reflecting upon these can prompt more accurate and stable reflection. Experiments conducted on a series of reasoning and translation tasks with different LLMs serve to underscore the effectiveness and generality of our strategy.","LLMs evaluate the initial response and provide feedback for revision. However, most erroneous responses remain uncorrected after reflection as the feedback is either overconfident (46.7\%) or inconsistent (45.7\%). Bottom: Self-Contrast \color{black}{explores} \color{black}multiple solving perspectives, and \color{black}{contrast} \color{black} their differences, and \color{black}{summarize} \color{black}them into insightful checklist for self-correction."
When Good and Reproducible Results are a Giant with Feet of Clay: The Importance of Software Quality in NLP,2303.16166v5,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2303.16166v5_0.png,"Despite its crucial role in research experiments, code correctness is often presumed solely based on the perceived quality of results. This assumption, however, comes with the risk of erroneous outcomes and, in turn, potentially misleading findings. To mitigate this risk, we posit that the current focus on reproducibility should go hand in hand with the emphasis on software quality. We support our arguments a case study in which we identify and fix three bugs in widely used implementations of the state-of-the-art Conformer architecture. As countermeasures, we release , a library dedicated to testing neural models, and propose a Code-quality Checklist, with the goal of promoting coding best practices and improving software quality within the NLP community.",Convolution module in the Conformer encoder layer. Convolutional blocks are 1D convolutions.
StreamAtt: Direct Streaming Speech-to-Text Translation with Attention-based Audio History Selection,2406.06097v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.06097v1_0.png,"Streaming speech-to-text translation (StreamST) is the task of automatically translating speech while incrementally receiving an audio stream. Unlike simultaneous ST (SimulST), which deals with pre-segmented speech, StreamST faces the challenges of handling continuous and unbounded audio streams. This requires additional decisions about what to retain of the previous history, which is impractical to keep entirely due to latency and computational constraints. Despite the real-world demand for real-time ST, research on streaming translation remains limited, with existing works solely focusing on SimulST. To fill this gap, we introduce StreamAtt, the first StreamST policy, and propose StreamLAAL, the first StreamST latency metric designed to be comparable with existing metrics for SimulST. Extensive experiments across all 8 languages of MuST-C v1.0 show the effectiveness of StreamAtt compared to a naive streaming baseline and the related state-of-the-art SimulST policy, providing a first step in StreamST research.","Decision steps of the StreamST policy. The order followed by our StreamAtt policy (step \stepone{}, step \steptwo\texthist, and step \steptwo\audio{}) is indicated from 1 (first) to 3 (last)."
FLEUR: An Explainable Reference-Free Evaluation Metric for Image Captioning Using a Large Multimodal Model,2406.06004v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.06004v1_0.pdf,"Most existing image captioning evaluation metrics focus on assigning a single numerical score to a caption by comparing it with reference captions. However, these methods do not provide an explanation for the assigned score. Moreover, reference captions are expensive to acquire. In this paper, we propose FLEUR, an explainable reference-free metric to introduce explainability into image captioning evaluation metrics. By leveraging a large multimodal model, FLEUR can evaluate the caption against the image without the need for reference captions, and provide the explanation for the assigned score. We introduce {score smoothing} to align as closely as possible with human judgment and to be robust to user-defined {grading criteria}. FLEUR achieves high correlations with human judgment across various image captioning evaluation benchmarks and reaches state-of-the-art results on Flickr8k-CF, COMPOSITE, and Pascal-50S within the domain of reference-free evaluation metrics. Our source code and results are publicly available at: .","{Top}: Comparison between other non-explainable metrics and our explainable metric, FLEUR. FLEUR provides the explanation for the assigned score as well. {Bottom}: Existing explainable metric cannot consider the image. The information highlighted in red in the candidate caption is not present in the reference caption set, causing confusion for that metric."
Understanding and Addressing the Under-Translation Problem from the Perspective of Decoding Objective,2405.18922v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2405.18922v1_0.pdf;/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2405.18922v1_1.pdf,"Neural Machine Translation (NMT) has made remarkable progress over the past years. However, under-translation and over-translation remain two challenging problems in state-of-the-art NMT systems. In this work, we conduct an in-depth analysis on the underlying cause of under-translation in NMT, providing an explanation from the perspective of decoding objective. To optimize the beam search objective, the model tends to overlook words it is less confident about, leading to the under-translation phenomenon. Correspondingly, the model's confidence in predicting the End Of Sentence (EOS) diminishes when under-translation occurs, serving as a mild penalty for under-translated candidates. Building upon this analysis, we propose employing the confidence of predicting EOS as a detector for under-translation, and strengthening the confidence-based penalty to penalize candidates with a high risk of under-translation. Experiments on both synthetic and real-world data show that our method can accurately detect and rectify under-translated outputs, with minor impact on other correct translations.",Comparison of EOS log-probability distribution: under-translated sentences vs. all sentences on sentence-level (left) and document-level (right) synthetic test sets.
Identifying while Learning for Document Event Causality Identification,2405.20608v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2405.20608v1_0.pdf,"Event Causality Identification (ECI) aims to detect whether there exists a causal relation between two events in a document. Existing studies adopt a kind of {identifying after learning} paradigm, where events' representations are first learned and then used for the identification. Furthermore, they mainly focus on the causality existence, but ignore causal direction. In this paper, we take care of the causal direction and propose a new {identifying while learning} mode for the ECI task. We argue that a few causal relations can be easily identified with high confidence, and the directionality and structure of these identified causalities can be utilized to update events' representations for boosting next round of causality identification. To this end, this paper designs an {iterative learning and identifying framework}: In each iteration, we construct an event causality graph, on which events' causal structure representations are updated for boosting causal identification. Experiments on two public datasets show that our approach outperforms the state-of-the-art algorithms in both evaluations for causality existence identification and direction identification.}",An example of the event causality graph and event structures in the EventStoryLine corpus.
OlympiadBench: A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems,2402.14008v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.14008v2_0.pdf,"Recent advancements have seen Large Language Models (LLMs) and Large Multimodal Models (LMMs) surpassing general human capabilities in various tasks, approaching the proficiency level of human experts across multiple domains. With traditional benchmarks becoming less challenging for these models, new rigorous challenges are essential to gauge their advanced abilities. In this work, we present OlympiadBench, an Olympiad-level bilingual multimodal scientific benchmark, featuring 8,476 problems from Olympiad-level mathematics and physics competitions, including the Chinese college entrance exam. Each problem is detailed with expert-level annotations for step-by-step reasoning. Evaluating top-tier models on OlympiadBench, we implement a comprehensive assessment methodology to accurately evaluate model responses. Notably, the best-performing model, GPT-4V, attains an average score of 17.97\% on OlympiadBench, with a mere 10.74\% in physics, highlighting the benchmark rigor and the intricacy of physical reasoning. Our analysis orienting GPT-4V points out prevalent issues with hallucinations, knowledge omissions, and logical fallacies. We hope that our challenging benchmark can serve as a valuable resource for helping future AGI research endeavors. The data and evaluation code are available at",An example of IMO in OlympiadBench. Solving this example requires AI systems to span different mathematical domains and conduct advanced reasoning.
Insert or Attach: Taxonomy Completion via Box Embedding,2305.11004v4,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2305.11004v4_0.pdf,"Taxonomy completion, enriching existing taxonomies by inserting new concepts as parents or attaching them as children, has gained significant interest. Previous approaches embed concepts as vectors in Euclidean space, which makes it difficult to model asymmetric relations in taxonomy. In addition, they introduce pseudo-leaves to convert attachment cases into insertion cases, leading to an incorrect bias in network learning dominated by numerous pseudo-leaves. Addressing these, our framework, , leverages box containment and center closeness to design two specialized geometric scorers within the box embedding space. These scorers are tailored for insertion and attachment operations and can effectively capture intrinsic relationships between concepts by optimizing on a granular box constraint loss. We employ a dynamic ranking loss mechanism to balance the scores from these scorers, allowing adaptive adjustments of insertion and attachment scores. Experiments on four real-world datasets show that significantly outperforms previous methods, yielding substantial improvements over prior methods in real-world datasets, with average performance boosts of 6.7\%, 34.9\%, and 51.4\% in MRR, Hit@1, and Prec@1, respectively.",Example of taxonomy completion with our \textsc{TaxBox} framework.
"Instruct Once, Chat Consistently in Multiple Rounds: An Efficient Tuning Framework for Dialogue",2402.06967v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.06967v2_0.pdf;/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.06967v2_1.pdf;/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.06967v2_2.pdf,"Tuning language models for dialogue generation has been a prevalent paradigm for building capable dialogue agents. Yet, traditional tuning narrowly views dialogue generation as resembling other language generation tasks, ignoring the role disparities between two speakers and the multi-round interactive process that dialogues ought to be. Such a manner often leads to unsatisfactory chat consistency for the built agent. In this work, we emphasize the interactive, communicative nature of dialogue and argue that it is more feasible to model the speaker roles of agent and user separately, enabling the agent to adhere to its role consistently. With this in mind, we propose an efficient {M}ulti-round {I}nteractive {Di}alogue Tuning (-Tuning) framework.}. It models the agent and user individually with two adapters built upon large language models. The adapters make use of respective utterances round by round in alternating order and they are tuned via a round-level memory caching mechanism. Extensive experiments demonstrate that, our framework performs superior to traditional fine-tuning and harbors the tremendous potential for improving dialogue consistency.",Comparison of different tuning manners (including data usage) for dialogue generation.
"Rule or Story, Which is a Better Commonsense Expression for Talking with Large Language Models?",2402.14355v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.14355v2_0.pdf,"Building machines with commonsense has been a longstanding challenge in NLP due to the reporting bias of commonsense rules and the exposure bias of rule-based commonsense reasoning. In contrast, humans convey and pass down commonsense implicitly through stories. This paper investigates the inherent commonsense ability of large language models (LLMs) expressed through storytelling. We systematically investigate and compare stories and rules for retrieving and leveraging commonsense in LLMs. Experimental results on 28 commonsense QA datasets show that stories outperform rules as the expression for retrieving commonsense from LLMs, exhibiting higher generation confidence and commonsense accuracy. Moreover, stories are the more effective commonsense expression for answering questions regarding daily events, while rules are more effective for scientific questions. This aligns with the reporting bias of commonsense in text corpora. We further show that the correctness and relevance of commonsense stories can be further improved via iterative self-supervised fine-tuning. These findings emphasize the importance of using appropriate language to express, retrieve, and leverage commonsense for LLMs, highlighting a promising direction for better exploiting their commonsense abilities.","Comparison between rules and a story written by ChatGPT. The rules only provide useful knowledge until the 4$^{th}$ rule and also include an incorrect answer option, ``classroom''. The story presents a detailed scenario where an adult uses glue sticks in an office."
CaMML: Context-Aware Multimodal Learner for Large Models,2401.03149v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2401.03149v3_0.pdf,"In this work, we introduce {C}ontext-{A}ware {M}ulti{M}odal {L}earner (), for tuning large multimodal models (LMMs). , a lightweight module, is crafted to seamlessly integrate multimodal contextual samples into large models, thereby empowering the model to derive knowledge from analogous, domain-specific, up-to-date information and make grounded inferences. Importantly, is highly scalable and can efficiently handle lengthy multimodal context examples owing to its hierarchical design. Based on , we have developed two multimodal models, -7B and -13B, that have shown exceptional performance across an array of benchmark datasets for multimodal tasks. Remarkably, -13B achieves the state-of-the-art performance on over ten widely recognized multimodal benchmark datasets, surpassing LLaVA-1.5 (13B) with a noticeable margin, without integration of any external resources. Moreover, we have conducted extensive ablative studies to inspect the inner workings of and performed qualitative analyses to showcase its effectiveness in handling real-world challenging cases. Code and models are available at: .","\caml achieves the state-of-the-art performance on a number of multimodal benchmarks, outperforming LLaVA-1.5 and many other large multimodal models."
NPHardEval: Dynamic Benchmark on Reasoning Ability of Large Language Models via Complexity Classes,2312.14890v4,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2312.14890v4_0.jpg,"Complex reasoning ability is one of the most important features of current Large Language Models (LLMs), which has also been leveraged to play an integral role in complex decision-making tasks. Therefore, the investigation into the reasoning capabilities of LLMs is critical: numerous benchmarks have been established to assess the reasoning abilities of LLMs. However, current benchmarks are inadequate in offering a rigorous evaluation of the full extent of reasoning abilities that LLMs are capable of achieving. They are also prone to the risk of overfitting, as these benchmarks, being publicly accessible and static, allow models to potentially tailor their responses to specific benchmark metrics, thereby inflating their performance. Addressing these limitations, our research introduces a new benchmark, named {NPHardEval}. This benchmark is designed to evaluate the reasoning abilities of LLMs across a broad spectrum of 900 algorithmic questions, extending up to the NP-Hard complexity class. These questions are meticulously chosen to represent a wide range of complexity class below the NP-hard complexity class, offering a rigorous measure of the reasoning ability of LLMs. Through this study, we shed light on the current state of reasoning in LLMs, providing an objective and rigorous perspective through the comparison of LLMs' performance across complex classes. Our findings contribute significantly to understanding the current capabilities of LLMs in reasoning tasks and lay the groundwork for future advancements in enhancing the reasoning abilities of these models. Moreover, this benchmark is designed with a {dynamic update mechanism}, where the datapoints are refreshed on a monthly basis. Such regular updates play a crucial role in mitigating the risk of LLMs overfitting to the benchmark, promoting a more accurate and reliable assessment of their reasoning capabilities. The benchmark dataset and code of {NPHardEval} are available at .","Computational complexity classes P, NP-complete, and NP-hard and corresponding tasks"
Multi-Level Feedback Generation with Large Language Models for Empowering Novice Peer Counselors,2403.15482v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2403.15482v1_0.pdf,"Realistic practice and tailored feedback are key processes for training peer counselors with clinical skills. However, existing mechanisms of providing feedback largely rely on human supervision. Peer counselors often lack mechanisms to receive detailed feedback from experienced mentors, making it difficult for them to support the large number of people with mental health issues who use peer counseling. Our work aims to leverage large language models to provide contextualized and multi-level feedback to empower peer counselors, especially novices, at scale. To achieve this, we co-design with a group of senior psychotherapy supervisors to develop a multi-level feedback taxonomy, and then construct a publicly available dataset with comprehensive feedback annotations of 400 emotional support conversations. We further design a self-improvement method on top of large language models to enhance the automatic generation of feedback. Via qualitative and quantitative evaluation with domain experts, we demonstrate that our method minimizes the risk of potentially harmful and low-quality feedback generation which is desirable in such high-stakes scenarios.","Example conversation excerpt taken from the ESConv dataset \cite{liu-etal-2021-towards} annotated using our feedback taxonomy. Feedback components ({appropriateness, goal definition and alignment, areas for improvement, alternative goal-aligned response}) are demonstrated on one utterance of peer counselor's response (in blue). Optionally, one can also provide {positive reinforcement} by highlighting areas in categories peer counselors excelled at."
Enhancing Reinforcement Learning with Label-Sensitive Reward for Natural Language Understanding,2405.19763v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2405.19763v1_0.pdf,"Recent strides in large language models (LLMs) have yielded remarkable performance, leveraging reinforcement learning from human feedback (RLHF) to significantly enhance generation and alignment capabilities. However, RLHF encounters numerous challenges, including the {objective mismatch} issue, leading to suboptimal performance in Natural Language Understanding (NLU) tasks. To address this limitation, we propose a novel Reinforcement Learning framework enhanced with Label-sensitive Reward (RLLR) to amplify the performance of LLMs in NLU tasks. By incorporating label-sensitive pairs into reinforcement learning, our method aims to adeptly capture nuanced label-sensitive semantic features during RL, thereby enhancing natural language understanding. Experiments conducted on five diverse foundation models across eight tasks showcase promising results. In comparison to Supervised Fine-tuning models (SFT), RLLR demonstrates an average performance improvement of 1.54\%. Compared with RLHF models, the improvement averages at 0.69\%. These results reveal the effectiveness of our method for LLMs in NLU tasks. Code and data available at: {https://github.com/MagiaSN/ACL2024\_RLLR}",The example of rationale-sensitive and label-sensitive pairs from sentiment classification. Highlight rationales in green and labels in yellow.
CoCA: Fusing Position Embedding with Collinear Constrained Attention in Transformers for Long Context Window Extending,2309.08646v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2309.08646v3_0.png,"Self-attention and position embedding are two key modules in transformer-based Large Language Models (LLMs). However, the potential relationship between them is far from well studied, especially for long context window extending. In fact, anomalous behaviors harming long context extrapolation exist between Rotary Position Embedding (RoPE) and vanilla self-attention unveiled by our work. To address this issue, we propose a novel attention mechanism, CoCA ({Co}llinear {C}onstrained {A}ttention). Specifically, we enforce a collinear constraint between $Q$ and $K$ to seamlessly integrate RoPE and self-attention. While only adding minimal computational and spatial complexity, this integration significantly enhances long context window extrapolation ability. We provide an optimized implementation, making it a drop-in replacement for any existing transformer-based models. Extensive experiments show that CoCA performs extraordinarily well in extending context windows. A CoCA-based GPT model, trained with a context length of 512, can seamlessly extend the context window up to 32K (60$$), without any fine-tuning. Additionally, by dropping CoCA in LLaMA-7B, we achieve extrapolation up to 32K within only 2K training length. Our code is publicly available at: {https://github.com/codefuse-ai/Collinear-Constrained-Attention}","Perplexity evaluation on 100 PG-19 documents with a sliding window strategy (Stride = 512). The perplexity of RoFormer \citep{Su2021RoFormerET} sharply exceeds 1000 beyond its training length, while CoCA maintains a low plateau even at 60 $\times$ its training length. ALibi \citep{Press2021TrainST} encounters Out of Memory (OOM) issues for input \(N_{max}>\) 8000 due to flash-attention \citep{dao2022flashattention} incompatibility, we suppose it maintains perplexity for \(N_{max}>\) 8000."
DAPR: A Benchmark on Document-Aware Passage Retrieval,2305.13915v4,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2305.13915v4_0.pdf,"The work of neural retrieval so far focuses on ranking short texts and is challenged with long documents. There are many cases where the users want to find a relevant passage within a long document from a huge corpus, e.g. Wikipedia articles, research papers, etc. We propose and name this task {Document-Aware Passage Retrieval} (DAPR). While analyzing the errors of the State-of-The-Art (SoTA) passage retrievers, we find the major errors (53.5\%) are due to missing document context. This drives us to build a benchmark for this task including multiple datasets from heterogeneous domains. In the experiments, we extend the SoTA passage retrievers with document context via (1) hybrid retrieval with BM25 and (2) contextualized passage representations, which inform the passage representation with document context. We find despite that hybrid retrieval performs the strongest on the mixture of the easy and the hard queries, it completely fails on the hard queries that require document-context understanding. On the other hand, contextualized passage representations (e.g. prepending document titles) achieve good improvement on these hard queries, but overall they also perform rather poorly. Our created benchmark enables future research on developing and comparing retrieval systems for the new task. The code and the data are available}.","An example instance from DAPR. To find the relevant passage to the query, the retriever needs to utilize the document context, which in this case means coreference resolution for the noun {the venue}. See other categories of the document context and examples in~\autoref{sec:nq_hard}."
Uncovering the Full Potential of Visual Grounding Methods in VQA,2401.07803v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2401.07803v2_0.png,"Visual Grounding (VG) methods in Visual Question Answering (VQA) attempt to improve VQA performance by strengthening a model's reliance on question-relevant visual information. The presence of such relevant information in the visual input is typically assumed in training and testing. This assumption, however, is inherently flawed when dealing with imperfect image representations common in large-scale VQA, where the information carried by visual features frequently deviates from expected ground-truth contents. As a result, training and testing of VG-methods is performed with largely inaccurate data, which obstructs proper assessment of their potential benefits. In this study, we demonstrate that current evaluation schemes for VG-methods are problematic due to the flawed assumption of availability of relevant visual information. Our experiments show that these methods can be much more effective when evaluation conditions are corrected. Code is provided on GitHub.",Example of Flawed VG that VG-methods in VQA teach based on the unverified assumption of presence of relevant visual information (left). Correct content cues are a prerequisite for teaching True VG (right).
"Small Models, Big Insights: Leveraging Slim Proxy Models To Decide When and What to Retrieve for LLMs",2402.12052v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.12052v3_0.pdf,"The integration of large language models (LLMs) and search engines represents a significant evolution in knowledge acquisition methodologies. However, determining the knowledge that an LLM already possesses and the knowledge that requires the help of a search engine remains an unresolved issue. Most existing methods solve this problem through the results of preliminary answers or reasoning done by the LLM itself, but this incurs excessively high computational costs. This paper introduces a novel collaborative approach, namely SlimPLM, that detects missing knowledge in LLMs with a slim proxy model, to enhance the LLM's knowledge acquisition process. We employ a proxy model which has far fewer parameters, and take its answers as heuristic answers. Heuristic answers are then utilized to predict the knowledge required to answer the user question, as well as the known and unknown knowledge within the LLM. We only conduct retrieval for the missing knowledge in questions that the LLM does not know. Extensive experimental results on five datasets with two LLMs demonstrate a notable improvement in the end-to-end performance of LLMs in question-answering tasks, achieving or surpassing current state-of-the-art models with lower LLM inference costs..} %Experiments also shows our method maintains system efficiency, avoiding extra load and brought by LLM inferences.","A display of the main process of SlimPLM. Solid lines with arrows represent the flow of data, while dashed lines with arrows signify control signals from the retrieval necessity judgment model. Step 1 and step 2 are mandatory in the pipeline, but step 3 involves choosing between direct generation and RAG."
Interpretability of Language Models via Task Spaces,2406.06441v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.06441v1_0.pdf,"The usual way to interpret language models (LMs) is to test their performance on different benchmarks and subsequently infer their internal processes. In this paper, we present an alternative approach, concentrating on the {quality} of LM processing, with a focus on their language abilities. To this end, we construct `linguistic task spaces' -- representations of an LM's language conceptualisation -- that shed light on the connections LMs draw between language phenomena. Task spaces are based on the interactions of the learning signals from different linguistic phenomena, which we assess via a method we call `similarity probing'. To disentangle the learning signals of linguistic phenomena, we further introduce a method called `fine-tuning via gradient differentials' (FTGD). We apply our methods to language models of three different scales and find that larger models generalise better to overarching general concepts for linguistic tasks, making better use of their shared structure. Further, the distributedness of linguistic processing increases with pre-training through increased parameter sharing between related linguistic tasks. The overall generalisation patterns are mostly stable throughout training and not marked by incisive stages, potentially explaining the lack of successful curriculum strategies for LMs.",The process of similarity probing to obtain a task space based on transfers: 1. Evaluate the untuned LM on all tasks (eval\textsubscript{1}); 2. Tune one LM for each task; 3. Re-evaluate the LMs on all tasks (eval\textsubscript{2}). Calculate all transfers (eval\textsubscript{2} - eval\textsubscript{1}) and compare the resulting transfer task space to a hypothesized set of transfers (Hypothesis space).
Factual Confidence of LLMs: on Reliability and Robustness of Current Estimators,2406.13415v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.13415v1_0.png,"Large Language Models (LLMs) tend to be unreliable in the factuality of their answers. To address this problem, NLP researchers have proposed a range of techniques to estimate LLM's confidence over facts. However, due to the lack of a systematic comparison, it is not clear how the different methods compare to one another. To fill this gap, we present a survey and empirical comparison of estimators of factual confidence. We define an experimental framework allowing for fair comparison, covering both fact-verification and question answering. Our experiments across a series of LLMs indicate that trained hidden-state probes provide the most reliable confidence estimates, albeit at the expense of requiring access to weights and training data. We also conduct a deeper assessment of factual confidence by measuring the consistency of model behavior under meaning-preserving variations in the input. We find that the confidence of LLMs is often unstable across semantically equivalent inputs, suggesting that there is much room for improvement of the stability of models' parametric knowledge. Our code is available at {https://github.com/amazon-science/factual-confidence-of-llms}.","{Overview of our factual confidence estimation framework. We work with five groups of {methods} and two formulations: $P(\text{I know})$, which applies to questions, and $P(\text{True})$, which applies to statements. All of the methods produce a continuous score, except {verbalization}, where the model generates a confidence level. }"
One-Shot Learning as Instruction Data Prospector for Large Language Models,2312.10302v4,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2312.10302v4_0.pdf,"Contemporary practices in instruction tuning often hinge on enlarging data scaling without a clear strategy for ensuring data quality, inadvertently introducing noise that may compromise model performance. To address this challenge, we introduce , a novel and efficient methodology that leverages one-shot learning to discern and select high-quality instruction data from extensive datasets. assesses the potential of individual instruction examples to act as effective one-shot learning instances, thereby identifying those that can significantly improve performance across diverse tasks. utilizes a scoring system based on the impact of candidate examples on the perplexity of a diverse anchor set, facilitating the selection of the most advantageous data for instruction tuning. Through comprehensive evaluations on two benchmarks, including MT-Bench and Alpaca-Eval, we show that instruction tuning with the top 1\% of examples curated by substantially outperforms conventional methods employing the entire dataset.","The comparison between our \textsc{Nuggets} and previous empirical methods. In contrast to empirical methods~(blue area), \textsc{Nuggets}~(orange area) can directly sample a gold subset, offering a more direct contribution to model fine-tuning."
Navigating the OverKill in Large Language Models,2401.17633v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2401.17633v1_0.pdf,"{Content warning: This paper contains examples of harmful language.}\\ Large language models are meticulously aligned to be both helpful and harmless. However, recent research points to a potential overkill which means models may refuse to answer benign queries. In this paper, we investigate the factors for overkill by exploring how models handle and determine the safety of queries. Our findings reveal the presence of shortcuts within models, leading to an over-attention of harmful words like 'kill' and prompts emphasizing safety will exacerbate overkill. Based on these insights, we introduce Self-Contrastive Decoding (Self-CD), a training-free and model-agnostic strategy, to alleviate this phenomenon. We first extract such over-attention by amplifying the difference in the model's output distributions when responding to system prompts that either include or omit an emphasis on safety. Then we determine the final next-token predictions by downplaying the over-attention from the model via contrastive decoding. Empirical results indicate that our method has achieved an average reduction of the refusal rate by 20 \% while having almost no impact on safety.","The illustration demonstrates the phenomenon of overkill. For the dangerous question on the left, the model is able to make a true refusal. However, for the safe question on the right, which contains the same harmful word 'kill', the model makes a false refusal."
A Chain-of-Thought Is as Strong as Its Weakest Link: A Benchmark for Verifiers of Reasoning Chains,2402.00559v4,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.00559v4_0.pdf,"Prompting language models to provide step-by-step answers (e.g., ``Chain-of-Thought'') is the prominent approach for complex reasoning tasks, where more accurate reasoning chains typically improve downstream task performance. Recent literature discusses automatic methods to verify reasoning steps to evaluate and improve their correctness. However, no fine-grained step-level datasets are available to enable thorough evaluation of such verification methods, hindering progress in this direction. We introduce : {Reasoning Verification Evaluation}, a new dataset to benchmark automatic verifiers of complex Chain-of-Thought reasoning in open-domain question answering settings. includes comprehensive labels for the relevance, attribution to evidence passages, and logical correctness of each reasoning step in a language model's answer, across a wide variety of datasets and state-of-the-art language models. Available at .","We collect \dataset, an evaluation benchmark for the task of verifying reasoning chains in Chain-of-Thought format, which checks whether a reasoning chain is a correct justification to the final answer (importantly, the answer can be correct even if the reasoning is incorrect, as in the example above). The figure shows four verifiers (middle) verifying the correctness of a CoT (left). We use the dataset to benchmark multiple verifiers (right)."
Re3: A Holistic Framework and Dataset for Modeling Collaborative Document Revision,2406.00197v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.00197v1_0.pdf,"Collaborative review and revision of textual documents is the core of knowledge work and a promising target for empirical analysis and NLP assistance. Yet, a holistic framework that would allow modeling complex relationships between document revisions, reviews and author responses is lacking. To address this gap, we introduce Re3, a framework for joint analysis of collaborative document revision. We instantiate this framework in the scholarly domain, and present Re3-Sci, a large corpus of aligned scientific paper revisions manually labeled according to their action and intent, and supplemented with the respective peer reviews and human-written edit summaries. We use the new data to provide first empirical insights into collaborative document revision in the academic domain, and to assess the capabilities of state-of-the-art LLMs at automating edit analysis and facilitating text-based collaboration. We make our annotation environment and protocols, the resulting data and experimental code publicly available.}","Re3 offers a holistic framework for studying the relationships between reviews (a), revisions (b-c) and responses (d) in text-based collaboration. It is instantiated in the Re3-Sci dataset that covers all edits in 314 full-length scientific publications manually labeled with edit action and intent (e) on different granularity levels, along with reviews that trigger edits and manually curated responses that summarize all edits made including self-initiated ones (f)."
DolphCoder: Echo-Locating Code Large Language Models with Diverse and Multi-Objective Instruction Tuning,2402.09136v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.09136v1_0.pdf,"Code Large Language Models (Code LLMs) have demonstrated outstanding performance in code-related tasks. Several instruction tuning approaches have been proposed to boost the code generation performance of pre-trained Code LLMs. In this paper, we introduce a diverse instruction model ({DolphCoder}) with self-evaluating for code generation. It learns diverse instruction targets and combines a code evaluation objective to enhance its code generation ability. Our model achieves superior performance on the HumanEval and MBPP benchmarks, demonstrating new insights for future code instruction tuning work. Our key findings are: (1) Augmenting more diverse responses with distinct reasoning paths increases the code capability of LLMs. (2) Improving one's ability to evaluate the correctness of code solutions also enhances their ability to create it.","The overall architecture of our proposed diverse instruction tuning with self-evaluating for code generation, DolphCoder. Stage (a) denotes Diverse Instruction Tuning (DIT) and Stage (b) denotes Multi-Objective Instruction Tuning (MOT) for self-evaluating."
Systematic Task Exploration with LLMs: A Study in Citation Text Generation,2407.04046v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2407.04046v1_0.pdf,"Large language models (LLMs) bring unprecedented flexibility in defining and executing complex, creative natural language generation (NLG) tasks. Yet, this flexibility brings new challenges, as it introduces new degrees of freedom in formulating the task inputs and instructions and in evaluating model performance. To facilitate the exploration of creative NLG tasks, we propose a three-component research framework that consists of systematic input manipulation, reference data, and output measurement. We use this framework to explore citation text generation -- a popular scholarly NLP task that lacks consensus on the task definition and evaluation metric and has not yet been tackled within the LLM paradigm. Our results highlight the importance of systematically investigating both task instruction and input configuration when prompting LLMs, and reveal non-trivial relationships between different evaluation metrics used for citation text generation. Additional human generation and human evaluation experiments provide new qualitative insights into the task to guide future research in citation text generation. We make our code{UKPLab/acl2024-citation-text-generation}} and data{TUdatalib}} publicly available.","Citation text generation with LLMs. The task~(1) is to generate a paragraph of related work from the citing paper (A) about a cited paper (B). The instruction combined with task inputs constitutes a prompt (2) that is communicated to the model. The model's response (3) is evaluated using a range of measurements, from word count to NLI-based factuality metrics (4)."
RelayAttention for Efficient Large Language Model Serving with Long System Prompts,2402.14808v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.14808v3_0.pdf,"A practical large language model (LLM) service may involve a long system prompt, which specifies the instructions, examples, and knowledge documents of the task and is reused across requests. However, the long system prompt causes throughput/latency bottlenecks as the cost of generating the next token grows the sequence length. This paper aims to improve the efficiency of LLM services that involve long system prompts. Our key observation is that handling these system prompts requires heavily redundant memory accesses in existing causal attention computation algorithms. Specifically, for batched requests, the cached hidden states (, key-value pairs) of system prompts are transferred from off-chip DRAM to on-chip SRAM multiple times, each corresponding to an individual request. To eliminate such a redundancy, we propose , an attention algorithm that allows reading these hidden states from DRAM exactly once for a batch of input tokens. is a free lunch: it maintains the generation quality while requiring no model retraining, as it is based on a mathematical reformulation of causal attention. We have observed significant performance improvements to a production-level system, vLLM, through integration with RelayAttention. The improvements are even more profound with longer system prompts.","Llama-30B attention inference latency \wrt system prompt length (A40 GPU, batch size 32). We set the length of (request-specific) contexts, which include user prompts and previously generated tokens, to 128."
Boosting Language Models Reasoning with Chain-of-Knowledge Prompting,2306.06427v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2306.06427v3_0.pdf,"Recently, Chain-of-Thought (CoT) prompting has delivered success on complex reasoning tasks, which aims at designing a simple prompt like ``{Let's think step by step}'' or multiple in-context exemplars with well-designed rationales to elicit Large Language Models (LLMs) to generate intermediate reasoning steps. However, the generated rationales often come with hallucinations, making unfactual and unfaithful reasoning chains. To mitigate this brittleness, we propose a novel Chain-of-Knowledge (CoK) prompting, where we aim at eliciting LLMs to generate explicit pieces of knowledge evidence in the form of structure triple. This is inspired by our human behaviors, i.e., we can draw a mind map or knowledge map as the reasoning evidence in the brain before answering a complex question. Benefiting from CoK, we additionally introduce a F-Verification method to estimate the reliability of the reasoning chains in terms of {factuality} and {faithfulness}. For the unreliable response, the wrong evidence can be indicated to prompt the LLM to rethink. Extensive experiments demonstrate that our method can further improve the performance of commonsense, factual, symbolic, and arithmetic reasoning tasks~}.","Comparison of three prompting methods: (a) ICL, (b) Chain-of-Thought (CoT), and (c) Chain-of-Knowledge (CoK) solving a StrategyQA question."
Estimating Agreement by Chance for Sequence Annotation,2407.11371v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2407.11371v1_0.png,"In the field of natural language processing, correction of performance assessment for chance agreement plays a crucial role in evaluating the reliability of annotations. However, there is a notable dearth of research focusing on chance correction for assessing the reliability of sequence annotation tasks, despite their widespread prevalence in the field. %This research gap can be attributed to the inherent complexity of sequence annotation tasks, which involve challenges such as varying span densities across documents and constraints related to span connectivity and overlap. To address this gap, this paper introduces a novel model for generating random annotations, which serves as the foundation for estimating chance agreement in sequence annotation tasks. %The proposed model is designed to account for the specific characteristics of text sequence labeling tasks and acknowledges the variations in annotation tendencies among annotators. By Utilizing the proposed randomization model and a related comparison approach, we successfully derive the analytical form of the distribution, enabling the computation of the probable location of each annotated text segment and subsequent chance agreement estimation. Through a combination simulation and corpus-based evaluation, we successfully assess its applicability and validate its accuracy and efficacy. The Kappa statistic is a popular chance corrected measure of agreement used as a reliability measure for annotation in the field of NLP, however its method for estimating chance agreement is not suitable for sequence annotation tasks, which are extremely prevalent in the field. The non-suitability is grounded in several complicating factors such as variation in span density across documents and constraints on span connectivity and overlap. In this paper, we propose a novel model for random annotation generation as the basis for chance agreement estimation for sequence annotation tasks. The model is jointly motivated by the specific characteristics of text sequence labeling tasks and acknowledgement of differences in annotation tendencies among annotators. Based on the proposed randomization model and related comparison approach, we successfully derive the analytical form of the distribution for computing the probable location of each annotated text segment, and subsequently chance agreement. We illustrate the approach in a simulation experiment and then apply it to several system outputs of CoNLL03 corpus annotation to evaluate its applicability, thus substantiating both the accuracy and efficacy of our method.","The probability distributions for all possible locations of each random segment in a length=100 sequence annotated with four segments. The lengths of the four segments are 1, 5, 10, 15, from left to right."
Are Emergent Abilities in Large Language Models just In-Context Learning?,2309.01809v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2309.01809v2_0.jpeg,"Large language models, comprising billions of parameters and pre-trained on extensive web-scale corpora, have been claimed to acquire certain capabilities without having been specifically trained on them. These capabilities, referred to as ``emergent abilities,'' have been a driving force in discussions regarding the potentials and risks of language models. A key challenge in evaluating emergent abilities is that they are confounded by model competencies that arise through alternative prompting techniques, including in-context learning, which is the ability of models to complete a task based on a few examples. We present a novel theory that explains emergent abilities, taking into account their potential confounding factors, and rigorously substantiate this theory through over 1000 experiments. Our findings suggest that purported emergent abilities are not truly emergent, but result from a combination of in-context learning, model memory, and linguistic knowledge. Our work is a foundational step in explaining language model performance, providing a template for their efficient use and clarifying the paradox of their ability to excel in some instances while faltering in others. Thus, we demonstrate that their capabilities should not be overestimated.~ and .}","{fig:gpt-results} Performance of non-instruction-tuned GPT models in the zero-shot setting. Grey background indicates tasks that are not previously identified as emergent. Tasks that require the output of a number or a coded string are evaluated using exact match accuracy. Note the consistent lack of ``emergence'', see text for details."
WaveCoder: Widespread And Versatile Enhancement For Code Large Language Models By Instruction Tuning,2312.14187v5,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2312.14187v5_0.pdf,"Recent work demonstrates that, after instruction tuning, Code Large Language Models (Code LLMs) can obtain impressive capabilities to address a wide range of code-related tasks. However, current instruction tuning methods for Code LLMs mainly focus on the traditional code generation task, resulting in poor performance in complex multi-task scenarios. In this paper, we concentrate on multiple code-related tasks and present {WaveCoder}, a series of Code LLMs trained with idespread nd ersatile nhanced instruction data. To enable the models to tackle complex code-related tasks, we propose a method to stably generate diverse, high-quality instruction data from open source code dataset in multi-task scenarios and obtain {CodeSeaXDataset}, a dataset comprising 19,915 instruction instances across 4 code-related tasks, which is aimed at improving the generalization ability of Code LLM. Our experiments demonstrate that WaveCoder models significantly outperform other open-source models in terms of the generalization ability across different code-related tasks. Moreover, WaveCoder-Ultra-6.7B presents the state-of-the-art generalization abilities on a wide range of code-related tasks.",The overview of the widespread and versatile enhancement for Code LLM. Part B and C indicates the LLM-based Generator and LLM-based Disciminator where the generator can leverage different examples in example database by in-context learning.
Eliciting Better Multilingual Structured Reasoning from LLMs through Code,2403.02567v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2403.02567v2_0.pdf,"The development of large language models (LLM) has shown progress on reasoning, though studies have largely considered either English or simple reasoning tasks. To address this, we introduce a multilingual structured reasoning and explanation dataset, termed xSTREET, that covers four tasks across six languages. xSTREET exposes a gap in base LLM performance between English and non-English reasoning tasks. released under CC-BY-4.0.} We then propose two methods to remedy this gap, building on the insight that LLMs trained on code are better reasoners. First, at training time, we augment a code dataset with multilingual comments using machine translation while keeping program code as-is. Second, at inference time, we bridge the gap between training and inference by employing a prompt structure that incorporates step-by-step code primitives to derive new facts and find a solution. Our methods show improved multilingual performance on xSTREET, most notably on the scientific commonsense reasoning subtask. Furthermore, the models show no regression on non-reasoning tasks, thus demonstrating our techniques maintain general-purpose abilities.","An overview of our methods to improve multilingual structured reasoning. First (top), we create the translated code comments (\tcc) dataset, and use it in a fine-tuning setup. Second (bottom), we use the resulting LLM for inference on reasoning tasks. We find the most success with a code prompt format that bridges the representations between training and inference."
Quantifying Uncertainty in Answers from any Language Model and Enhancing their Trustworthiness,2308.16175v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2308.16175v2_0.pdf;/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2308.16175v2_1.pdf,"We introduce , a method for detecting bad and speculative answers from a pretrained Large Language Model by estimating a numeric confidence score for any output it generated. Our uncertainty quantification technique works for any LLM accessible only via a black-box API, whose training data remains unknown. By expending a bit of extra computation, users of any LLM API can now get the same response as they would ordinarily, as well as a confidence estimate that cautions when not to trust this response. Experiments on both closed and open-form Question-Answer benchmarks reveal that more accurately identifies incorrect LLM responses than alternative uncertainty estimation procedures (for both GPT-3 and ChatGPT). By sampling multiple responses from the LLM and considering the one with the highest confidence score, we can additionally obtain more accurate responses from the same LLM, without any extra training steps. In applications involving automated evaluation with LLMs, accounting for our confidence scores leads to more reliable evaluation in both human-in-the-loop and fully-automated settings (across both GPT 3.5 and 4).","{fig:main_task}Pipeline of \modelnameA{}, which can be applied to any LLM API. ($T=1.0$ means temperature sampling with parameter 1.0, Sim ($\cdot$,$\cdot$) means the semantic similarities between two sentences.)"
Marathon: A Race Through the Realm of Long Context with Large Language Models,2312.09542v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2312.09542v2_0.pdf,"With the advancement of large language models (LLMs) and the expansion of their context windows, existing long-context benchmarks fall short in effectively evaluating the models' comprehension and reasoning abilities in extended texts. Moreover, conventional benchmarks relying on F1 metrics often inaccurately score responses: they may undervalue correct answers that differ from the reference responses and overvalue incorrect ones that resemble the reference texts. In response to these limitations, we introduce {Marathon}, a novel evaluation benchmark that adopts a multiple-choice question format. It is specifically designed to overcome the constraints of previous benchmarks and provide a rapid, precise, and unbiased appraisal of the long-context comprehension skills of large language models. We conducted comprehensive evaluations on the Marathon benchmark with a range of state-of-the-art LLMs and assessed the effectiveness of various optimization strategies tailored for long-context generation. We anticipate that the Marathon benchmark and its associated leaderboard will enable a more precise and equitable evaluation of LLMs' capabilities in understanding and reasoning over extended contexts. {Marathon} is available at .}","The overall accuracy of different models on Marathon. The x-axis represents the model, and the y-axis represents the average accuracy across all tasks. The different colors represent different methods of optimization."
UHGEval: Benchmarking the Hallucination of Chinese Large Language Models via Unconstrained Generation,2311.15296v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2311.15296v3_0.pdf,"Large language models (LLMs) produce hallucinated text, compromising their practical utility in professional contexts. To assess the reliability of LLMs, numerous initiatives have developed benchmark evaluations for hallucination phenomena. However, they often employ constrained generation techniques to produce the evaluation dataset due to cost and time limitations. For instance, this may involve employing directed hallucination induction or deliberately modifying authentic text to generate hallucinations. These are not congruent with the unrestricted text generation demanded by real-world applications. Furthermore, a well-established Chinese-language dataset dedicated to the evaluation of hallucinations is presently lacking. Consequently, we have developed an $$nconstrained $$allucination $$eneration uation ($$Eval) benchmark, containing hallucinations generated by LLMs with minimal restrictions.}. Concurrently, we have established a comprehensive benchmark evaluation framework to aid subsequent researchers in undertaking scalable and reproducible experiments. We have also evaluated prominent Chinese LLMs and the GPT series models to derive insights regarding hallucination.","Hallucinations from $\mathbb{UHG}$Eval. Using the IDs, you can locate the original news articles. {Note}: MOTIE denotes Ministry of Trade, Industry, and Energy. (In Chinese: Fig.~\ref{fig:halu_example_ch})"
"Triple-Encoders: Representations That Fire Together, Wire Together",2402.12332v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.12332v2_0.png,"This study introduces the innovative triple-encoders approach to conversational sequence modeling, a method that surmounts the inherent limitations of conventional models like ConveRT, particularly their weak latent space interaction and repetitive context recomputation. Rooted in Curved Contrastive Learning (CCL), triple-encoders revolutionize dialog representation by dividing the context into distinct sub-latent spaces and employing a Hebbian-inspired co-occurrence learning mechanism. This innovative strategy enables the independent encoding of utterances into a latent space and the composition of sequences for retrieval, remarkably without any additional weights. Employing straightforward yet potent operations: mean pooling, batch matrix multiplication for computing similarity, and summing across the sequential dimension. triple-encoders demonstrate a significant improvement in sequence modeling. Specifically, they exhibit a 36\% enhancement in open-dialog and a 46\% improvement in task-oriented scenarios in terms of average rank, compared to previous CCL models. Furthermore, the model's robust architecture facilitates in better planning performance and generalization to zero-shot settings, showcasing its versatility and broad applicability. Search-based dialog models typically re-encode the dialog history at every turn, incurring high cost. Curved Contrastive Learning, a representation learning method that encodes relative distances between utterances into the embedding space via a bi-encoder, has recently shown promising results for dialog modeling at far superior efficiency. While high efficiency is achieved through independently encoding utterances, this ignores the importance of contextualization. To overcome this issue, this study introduces triple-encoders, which efficiently compute distributed utterance mixtures from these independently encoded utterances through a novel hebbian inspired co-occurrence learning objective in a self-organizing manner, without using any weights, i.e., merely through local interactions. Empirically, we find that triple-encoders lead to a substantial improvement over bi-encoders, and even to better zero-shot generalization than single-vector representation models without requiring re-encoding. Our code[1] and model[2] are publicly available. [1]{{/Triple-Encoders}} [2]{{ UKPLab/Triple-Encoders-DailyDialog}}","Comparison of our Triple Encoder to \citet{henderson-etal-2020-convert} and \citet{erker-etal-2023-imagination}. Similar to CCL we only need to encode and compute similarity scores of the latest utterance. At the same time, we achieve contextualization through pairwise mean-pooling with previous encoded utterances combining the advantages of both previous works. Our analysis shows that the co-occurrence training pushes representations that occur ({fire}) together closer together, leading to stronger additive properties ({wiring}) when being superimposed (compared to \citet{erker-etal-2023-imagination}) and thus to a better next utterance selection."
Improving Hateful Meme Detection through Retrieval-Guided Contrastive Learning,2311.08110v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2311.08110v3_0.png,"Hateful memes have emerged as a significant concern on the Internet. Detecting hateful memes requires the system to jointly understand the visual and textual modalities. Our investigation reveals that the embedding space of existing CLIP-based systems lacks sensitivity to subtle differences in memes that are vital for correct hatefulness classification. We propose constructing a hatefulness-aware embedding space through retrieval-guided contrastive training. Our approach achieves state-of-the-art performance on the HatefulMemes dataset with an AUROC of 87.0, outperforming much larger fine-tuned large multimodal models. We demonstrate a retrieval-based hateful memes detection system, which is capable of identifying hatefulness based on data unseen in training. This allows developers to update the hateful memes detection system by simply adding new examples without retraining — a desirable feature for real services in the constantly evolving landscape of hateful memes on the Internet.","Illustrative (not in the dataset) examples from \citealt{KielaFBHMC2020}. Memes on the left are mean, the ones in the middle are benign image confounders, and those on the right are benign text confounders."
Agent-Pro: Learning to Evolve via Policy-Level Reflection and Optimization,2402.17574v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.17574v3_0.pdf,"Large Language Models (LLMs) exhibit robust problem-solving capabilities for diverse tasks. However, most LLM-based agents are designed as specific task solvers with sophisticated prompt engineering, rather than agents capable of learning and evolving through interactions. These task solvers necessitate manually crafted prompts to inform task rules and regulate LLM behaviors, inherently incapacitating to address complex dynamic scenarios e.g., large interactive games. In light of this, we propose {Agent-Pro}: an LLM-based {Agent} with {P}olicy-level {R}eflection and {O}ptimization that can learn a wealth of expertise from interactive experiences and progressively elevate its behavioral policy. Specifically, it involves a dynamic belief generation and reflection process for policy evolution. Rather than action-level reflection, Agent-Pro iteratively reflects on past trajectories and beliefs, ""fine-tuning"" its irrational beliefs for a better policy. Moreover, a depth-first search is employed for policy optimization, ensuring continual enhancement in policy payoffs. Agent-Pro is evaluated across two games: Blackjack and Texas Hold’em, outperforming vanilla LLM and specialized models. Our results show Agent-Pro can learn and evolve in complex and dynamic scenes, which also benefits numerous LLM-based applications }.","For interactive tasks, e.g., imperfect-information games, we propose a versatile agent framework capable of self-learning and evolving. Firstly, our agent constructs beliefs about itself and the environment. Then it autonomously updates its prompts through policy-level reflection on past trajectories and beliefs, evolving a better behavioral strategy."
Your Transformer is Secretly Linear,2405.12250v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2405.12250v1_0.png,"This paper reveals a novel linear characteristic exclusive to transformer decoders, including models such as GPT, LLaMA, OPT, BLOOM and others. We analyze embedding transformations between sequential layers, uncovering a near-perfect linear relationship (Procrustes similarity score of 0.99). However, linearity decreases when the residual component is removed due to a consistently low output norm of the transformer layer. Our experiments show that removing or linearly approximating some of the most linear blocks of transformers does not affect significantly the loss or model performance. Moreover, in our pretraining experiments on smaller models we introduce a cosine-similarity-based regularization, aimed at reducing layer linearity. This regularization improves performance metrics on benchmarks like Tiny Stories and SuperGLUE and as well successfully decreases the linearity of the models. This study challenges the existing understanding of transformer architectures, suggesting that their operation may be more linear than previously assumed.}",Linearity profiles for different open source models. Normalized depth is the layer index divided by the total depth.
Noise Correction on Subjective Datasets,2311.00619v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2311.00619v3_0.png,"Incorporating every annotator's perspective is crucial for unbiased data modeling. Annotator fatigue and changing opinions over time can distort dataset annotations. To combat this, we propose to learn a more accurate representation of diverse opinions by utilizing multitask learning in conjunction with loss-based label correction. We show that using our novel formulation, we can cleanly separate agreeing and disagreeing annotations. Furthermore, this method provides a controllable way to encourage or discourage disagreement. We demonstrate that this modification can improve prediction performance in a single or multi-annotator setting. Lastly, we show that this method remains robust to additional label noise that is applied to subjective data.",GabHateCorpus loss distribution for a single annotator.
What Do Language Models Hear? Probing for Auditory Representations in Language Models,2402.16998v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.16998v2_0.png;/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.16998v2_1.png,"This work explores whether language models encode meaningfully grounded representations of sounds of objects. We learn a linear probe that retrieves the correct text representation of an object given a snippet of audio related to that object, where the sound representation is given by a pretrained audio model. This probe is trained via a contrastive loss that pushes the language representations and sound representations of an object to be close to one another. After training, the probe is tested on its ability to generalize to objects that were not seen during training. Across different language models and audio models, we find that the probe generalization is above chance in many cases, indicating that despite being trained only on raw text, language models encode grounded knowledge of sounds for some objects.","(Top) Language (triangle) and sound (circle) representations aligned via Procrustes analysis \cite{schonemann_generalized_1966}, visualized via PCA. The language representation is from BERT \cite{devlin-etal-2019-bert} and the audio representation is from PaSST \cite{passt}. The classes are color-coded based on their parent nodes (i.e., \texttt{human voice}, \texttt{domestic sounds}, \texttt{animal}, \texttt{music}) according to the ontology from the FSD50K \cite{fonseca2021fsd50k}. (Bottom) A zoomed-in portion of the blue region of the top figure, which shows the structural similarities between the language and sound representations for the \texttt{music} category."
CodeScope: An Execution-based Multilingual Multitask Multidimensional Benchmark for Evaluating LLMs on Code Understanding and Generation,2311.08588v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2311.08588v3_0.pdf,"Large Language Models (LLMs) have demonstrated remarkable performance on assisting humans in programming and facilitating programming automation. However, existing benchmarks for evaluating the code understanding and generation capacities of LLMs suffer from severe limitations. First, most benchmarks are insufficient as they focus on a narrow range of popular programming languages and specific tasks, whereas real-world software development scenarios show a critical need to implement systems with multilingual and multitask programming environments to satisfy diverse requirements. Second, most benchmarks fail to consider the actual executability and the consistency of execution results of the generated code. To bridge these gaps between existing benchmarks and expectations from practical applications, we introduce {CodeScope}, an execution-based, multilingual, multitask, multidimensional evaluation benchmark for comprehensively measuring LLM capabilities on coding tasks. CodeScope covers {43 programming languages} and {eight coding tasks}. It evaluates the coding performance of LLMs from three dimensions (perspectives): {length}, {difficulty}, and {efficiency}. To facilitate execution-based evaluations of code generation, we develop {MultiCodeEngine}, an automated code execution engine that supports 14 programming languages. Finally, we systematically evaluate and analyze eight mainstream LLMs and demonstrate the superior breadth and challenges of CodeScope for evaluating LLMs on code understanding and generation tasks compared to other benchmarks. The CodeScope benchmark and code are publicly available at .","Diagrams illustrating four code understanding tasks, including the input and expected output for each task."
Digital Socrates: Evaluating LLMs through Explanation Critiques,2311.09613v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2311.09613v3_0.pdf,"While LLMs can provide reasoned explanations along with their answers, the nature and quality of those explanations are still poorly understood. In response, our goal is to define a detailed way of characterizing the explanation capabilities of modern models and to create a nuanced, interpretable explanation evaluation tool that can generate such characterizations automatically, without relying on expensive API calls or human annotations. Our approach is to (a) define the new task of {explanation critiquing} - identifying and categorizing any main flaw in an explanation and providing suggestions to address the flaw, (b) create a sizeable, human-verified dataset for this task, and (c) train an open-source, automatic critique model (called Digital Socrates) using this data. Through quantitative and qualitative analysis, we demonstrate how Digital Socrates is useful for revealing insights about student models by examining their reasoning chains, and how it can provide high-quality, nuanced, automatic evaluation of those model explanations for the first time. Digital Socrates thus fills an important gap in evaluation tools for understanding and improving the explanation behavior of models. { [[EARLY PLACEHOLDER ABSTRACT, WILL BE REPLACED]]\\ Current NLP models based on large language models (LLMs), such as GPT-4 and Llama-2, have the ability to provide explanations along with their answers, for instance through chain-of-thought prompting. We explore how these explanations can provide a deeper window into capabilities of the model. Taking inspiration from Socratic principles of effective tutoring, we design a critique format which gives feedback on a model's explanation. The feedback identifies and categorizes any main flaw, and provides both general and specific suggestions to fix it. We generate such critiques both by prompting a very capable model, like GPT-4, and by fine-tuning a smaller Llama2 models using gold critiques. We show that such critiques can show interesting contrasts between models (like GPT-4 vs Llama2-70B) beyond pure accuracy evaluations. We also show that the smaller critique model is more effective on certain types of critique dimensions (like sloppy reasoning) than others (like long-tail knowledge). Finally, the critiques can be used as effective feedback to a range of ""student"" models in improving their output, even when the student model is larger than the critique model.}","Given a multiple-choice question (together with the answer options and correct answer), as well as a model-generated reasoning chain and answer, our system \model{} gives a critique of the model-generated explanation. In its critiques, \model{} provides localized feedback on where and why reasoning chains are flawed (focusing on the main flaw, if any), accompanied by general and fine-grained suggestions to address the identified flaw, providing nuance and interpretability to the critiques."
Experiential Co-Learning of Software-Developing Agents,2312.17025v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2312.17025v3_0.pdf,"Recent advancements in large language models (LLMs) have brought significant changes to various domains, especially through LLM-driven autonomous agents. A representative scenario is in software development, where LLM agents demonstrate efficient collaboration, task division, and assurance of software quality, markedly reducing the need for manual involvement. However, these agents frequently perform a variety of tasks independently, without benefiting from past experiences, which leads to repeated mistakes and inefficient attempts in multi-step task execution. To this end, we introduce {Experiential Co-Learning}, a novel LLM-agent learning framework in which instructor and assistant agents gather shortcut-oriented experiences from their historical trajectories and use these past experiences for future task execution. The extensive experiments demonstrate that the framework enables agents to tackle unseen software-developing tasks more effectively. We anticipate that our insights will guide LLM agents towards enhanced autonomy and contribute to their evolutionary growth in cooperative learning. The code and data are available at .","The framework of Experiential Co-Learning. The co-tracking module promotes communicative rehearsals between the agents, fostering cooperative exploration and the creation of procedural trajectories for various training tasks. The co-memorizing module heuristically extracts ""shortcuts"" from the trajectories under external supervision, integrating these heuristic shortcuts into their collective experience pools. The co-reasoning module combines agents' collective experience pools to foster an communication of augmented instructions and solutions, improving their ability to collaboratively solve unseen tasks."
Forgetting before Learning: Utilizing Parametric Arithmetic for Knowledge Updating in Large Language Models,2311.08011v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2311.08011v2_0.png,"Recent advancements in Large Language Models (LLMs) have showcased their remarkable capabilities in text understanding and generation. However, even stronger LLMs are susceptible to acquiring erroneous or obsolete information from the training corpus. Direct secondary fine-tuning with data containing new knowledge may be ineffective in updating knowledge due to the conflict between old and new knowledge. In this paper, we propose a new paradigm for fine-tuning called {F-Learning} (orgetting before ), which employs parametric arithmetic to facilitate the forgetting of old knowledge and learning of new knowledge. Experimental results on two publicly available datasets demonstrate that our proposed F-Learning can obviously improve the knowledge updating performance of both full fine-tuning and LoRA fine-tuning, simultaneously outperforming the existing baselines in most cases. Moreover, we have also discovered that forgetting old knowledge by subtracting the parameters of LoRA can yield a similar effect to subtracting the parameters of full fine-tuning, and occasionally even surpass it significantly.",Diagram for “Forgetting before Learning”.
A Deep Dive into the Trade-Offs of Parameter-Efficient Preference Alignment Techniques,2406.04879v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.04879v1_0.pdf,"Large language models are first pre-trained on trillions of tokens and then instruction-tuned or aligned to specific preferences. While pre-training remains out of reach for most researchers due to the compute required, fine-tuning has become affordable thanks to parameter-efficient methods such as LoRA and QLoRA. Alignment is known to be sensitive to the many factors involved, including the quantity and quality of data, the alignment method, and the adapter rank. However, there has not yet been an extensive study of their effect on downstream performance. To address this gap, we conduct an in-depth investigation of the impact of popular choices for three crucial axes: (i) the alignment dataset (HH-RLHF and BeaverTails), (ii) the alignment technique (SFT and DPO), and (iii) the model (LLaMA-1, Vicuna-v1.3, Mistral-7b, and Mistral-7b-Instruct). Our extensive setup spanning over 300 experiments reveals consistent trends and unexpected findings. We observe how more informative data helps with preference alignment, cases where supervised fine-tuning outperforms preference optimization, and how aligning to a distinct preference boosts performance on downstream tasks. Through our in-depth analyses, we put forward key guidelines to help researchers perform more effective parameter-efficient LLM alignment.","{fig:hh_vs_pb} Performance comparison for helpful and harmless benchmarks when models are aligned using QLoRA over HH-RLHF (in {\colorbox{red!30}{red}}) and BeaverTails (in {\colorbox{blue!30}{blue}}). We observe better performance when using a more informative and high-quality preference alignment dataset, albeit it is often overfitting for non-instruction tuned models when aligned using DPO (Section~\ref{subsec:hh_vs_bv})."
Zero-Shot Cross-Domain Dialogue State Tracking via Dual Low-Rank Adaptation,2407.21633v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2407.21633v1_0.pdf,"Zero-shot dialogue state tracking (DST) seeks to enable dialogue systems to transition to unfamiliar domains without manual annotation or extensive retraining. Prior research has approached this objective by embedding prompts into language models (LMs). Common methodologies include integrating prompts at the input layer or introducing learnable variables at each transformer layer. Nonetheless, each strategy exhibits inherent limitations. Prompts integrated at the input layer risk underutilization, with their impact potentially diminishing across successive transformer layers. Conversely, the addition of learnable variables to each layer can complicate the training process and increase inference latency. To tackle the issues mentioned above, this paper proposes Dual Low-Rank Adaptation (DualLoRA), a plug-and-play architecture designed for zero-shot DST. DualLoRA incorporates two distinct Low-Rank Adaptation (LoRA) components, targeting both dialogue context processing and prompt optimization, to ensure the comprehensive influence of prompts throughout the transformer model layers. This is achieved without incurring additional inference latency, showcasing an efficient integration into existing architectures. Through rigorous evaluation on the MultiWOZ and SGD datasets, DualLoRA demonstrates notable improvements across multiple domains, outperforming traditional baseline methods in zero-shot settings. Our code is accessible at: .",Example of a multi-domain dialogue from MultiWOZ dataset.
RepCodec: A Speech Representation Codec for Speech Tokenization,2309.00169v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2309.00169v3_0.pdf,"With recent rapid growth of large language models (LLMs), discrete speech tokenization has played an important role for injecting speech into LLMs. However, this discretization gives rise to a loss of information, consequently impairing overall performance. To improve the performance of these discrete speech tokens, we present {}, a novel speech representation codec for semantic speech tokenization. In contrast to audio codecs which reconstruct the raw audio, {} learns a vector quantization codebook through reconstructing speech representations from speech encoders like HuBERT or data2vec. Together, the speech encoder, the codec encoder and the vector quantization codebook form a pipeline for converting speech waveforms into semantic tokens. The extensive experiments illustrate that {}, by virtue of its enhanced information retention capacity, significantly outperforms the widely used k-means clustering approach in both speech understanding and generation. Furthermore, this superiority extends across various speech encoders and languages, affirming the robustness of {}. We believe our method can facilitate large language modeling research on speech processing. Our code and models are released at .",{\rpc} model architecture. \change{Our network uses single residual units without dimension reduction.}
SEER: Facilitating Structured Reasoning and Explanation via Reinforcement Learning,2401.13246v4,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2401.13246v4_0.pdf,"Elucidating the reasoning process with structured explanations from question to answer is crucial, as it significantly enhances the interpretability, traceability, and trustworthiness of question-answering (QA) systems. However, structured explanations demand models to perform intricately structured reasoning, which poses great challenges. Most existing methods focus on single-step reasoning through supervised learning, ignoring logical dependencies between steps. Moreover, existing reinforcement learning (RL) based methods overlook the structured relationships, underutilizing the potential of RL in structured reasoning. In this paper, we propose , a novel method that maximizes a structure-based return to facilitate structured reasoning and explanation. Our proposed structure-based return precisely describes the hierarchical and branching structure inherent in structured reasoning, effectively capturing the intricate relationships between different reasoning steps. In addition, we introduce a fine-grained reward function to meticulously delineate diverse reasoning steps. Extensive experiments show that significantly outperforms state-of-the-art methods, achieving an absolute improvement of 6.9\% over RL-based methods on EntailmentBank, a 4.4\% average improvement on STREET benchmark, and exhibiting outstanding efficiency and cross-dataset generalization performance. Our code is available at .","An example of structured explanation. Given a hypothesis $h$ (a declarative sentence derived from a question-answer pair) and a set of facts (or corpus), the goal is to generate a structured explanation, which delineates the reasoning process from facts to the hypothesis."
Are AI-Generated Text Detectors Robust to Adversarial Perturbations?,2406.01179v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.01179v2_0.pdf,"The widespread use of large language models (LLMs) has sparked concerns about the potential misuse of AI-generated text, as these models can produce content that closely resembles human-generated text. Current detectors for AI-generated text (AIGT) lack robustness against adversarial perturbations, with even minor changes in characters or words causing a reversal in distinguishing between human-created and AI-generated text. This paper investigates the robustness of existing AIGT detection methods and introduces a novel detector, the Siamese Calibrated Reconstruction Network (SCRN). The SCRN employs a reconstruction network to add and remove noise from text, extracting a semantic representation that is robust to local perturbations. We also propose a siamese calibration technique to train the model to make equally confident predictions under different noise, which improves the model's robustness against adversarial perturbations. Experiments on four publicly available datasets show that the SCRN outperforms all baseline methods, achieving 6.5\%-18.25\% absolute accuracy improvement over the best baseline method under adversarial attacks. Moreover, it exhibits superior generalizability in cross-domain, cross-genre, and mixed-source scenarios. The code is available at .",An example of adversarial perturbation to a RoBERTa-based AIGT detector.
FinTextQA: A Dataset for Long-form Financial Question Answering,2405.09980v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2405.09980v1_0.pdf,"Accurate evaluation of financial question-answering (QA) systems necessitates a comprehensive dataset encompassing diverse question types and contexts. However, current financial QA datasets lack scope diversity and question complexity. This work introduces {FinTextQA}, a novel dataset for long-form question answering (LFQA) in finance. {FinTextQA} comprises 1,262 high-quality, source-attributed QA pairs extracted and selected from finance textbooks and government agency websites.Moreover, we developed a Retrieval-Augmented Generation (RAG)-based LFQA system, comprising an embedder, retriever, reranker, and generator. A multi-faceted evaluation approach, including human ranking, automatic metrics, and GPT-4 scoring, was employed to benchmark the performance of different LFQA system configurations under heightened noisy conditions. The results indicate that: (1) Among all compared generators, Baichuan2-7B competes closely with GPT-3.5-turbo in accuracy score; (2) The most effective system configuration on our dataset involved setting the embedder, retriever, reranker, and generator as Ada2, Automated Merged Retrieval, Bge-Reranker-Base, and Baichuan2-7B, respectively; (3) models are less susceptible to noise after the length of contexts reaching a specific threshold.",An LFQA sample in {FinTextQA}. Models are expected to generate paragraph-length answers when given questions and documents.
On Measuring Faithfulness or Self-consistency of Natural Language Explanations,2311.07466v4,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2311.07466v4_0.png,"Large language models (LLMs) can explain their predictions through post-hoc or Chain-of-Thought (CoT) explanations. But an LLM could make up reasonably sounding explanations that are unfaithful to its underlying reasoning. Recent work has designed tests that aim to judge the faithfulness of post-hoc or CoT explanations. In this work we argue that these faithfulness tests do not measure faithfulness to the models' inner workings -- but rather their self-consistency at output level. Our contributions are three-fold: i) We clarify the {status of faithfulness tests} in view of model explainability, characterising them as {self-consistency tests} instead. This assessment we underline by ii) constructing a {Comparative Consistency Bank} for self-consistency tests that for the first time compares existing tests on a common suite of 11 open LLMs and 5 tasks -- including iii) our new {self-consistency measure CC-SHAP}. CC-SHAP is a fine-grained measure (not a test) of LLM self-consistency. It compares how a model's input contributes to the predicted answer and to generating the explanation. Our fine-grained CC-SHAP metric allows us iii) to {compare LLM behaviour} when making predictions and to {analyse the effect of other consistency tests} at a deeper level, which takes us one step further towards measuring faithfulness by bringing us closer to the internals of the model than strictly surface output-oriented tests. % Our code is available at",CC-SHAP method on a toy example. Contribution values for illustration only. See \ref{app:instance-examples} for real samples.
UNIMO-G: Unified Image Generation through Multimodal Conditional Diffusion,2401.13388v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2401.13388v3_0.png,"Existing text-to-image diffusion models primarily generate images from text prompts. However, the inherent conciseness of textual descriptions poses challenges in faithfully synthesizing images with intricate details, such as specific entities or scenes. This paper presents {UNIMO-G}, a simple multimodal conditional diffusion framework that operates on multimodal prompts with interleaved textual and visual inputs, which demonstrates a unified ability for both text-driven and subject-driven image generation. UNIMO-G comprises two core components: a Multimodal Large Language Model (MLLM) for encoding multimodal prompts, and a conditional denoising diffusion network for generating images based on the encoded multimodal input. We leverage a two-stage training strategy to effectively train the framework: firstly pre-training on large-scale text-image pairs to develop conditional image generation capabilities, and then instruction tuning with multimodal prompts to achieve unified image generation proficiency. A well-designed data processing pipeline involving language grounding and image segmentation is employed to construct multi-modal prompts. UNIMO-G excels in both text-to-image generation and zero-shot subject-driven synthesis, and is notably effective in generating high-fidelity images from complex multimodal prompts involving multiple image entities.","Examples of UNIMO-G for both text-driven and zero-shot subject-driven generation. UNIMO-G can perceive free-form interleaved visual-language inputs and faithfully generate images. Particularly, it can generate images from multi-modal prompts with multiple image entities."
The Fine-Tuning Paradox: Boosting Translation Quality Without Sacrificing LLM Abilities,2405.20089v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2405.20089v2_0.png,"Fine-tuning large language models (LLMs) for machine translation has shown improvements in overall translation quality. However, it is unclear what is the impact of fine-tuning on desirable LLM behaviors that are not present in neural machine translation models, such as steerability, inherent document-level translation abilities, and the ability to produce less literal translations. We perform an extensive translation evaluation on the LLaMA and Falcon family of models with model size ranging from 7 billion up to 65 billion parameters. Our results show that while fine-tuning improves the general translation quality of LLMs, several abilities degrade. In particular, we observe a decline in the ability to perform formality steering, to produce technical translations through few-shot examples, and to perform document-level translation. On the other hand, we observe that the model produces less literal translations after fine-tuning on parallel data. We show that by including monolingual data as part of the fine-tuning data we can maintain the abilities while simultaneously enhancing overall translation quality. Our findings emphasize the need for fine-tuning strategies that preserve the benefits of LLMs for machine translation.",X$\rightarrow$English (top) and English$\rightarrow$X (bottom) COMET scores on WMT22 for models trained on human-written translations with different amounts of training data.
Unveiling Linguistic Regions in Large Language Models,2402.14700v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.14700v3_0.pdf,"Large Language Models (LLMs) have demonstrated considerable cross-lingual alignment and generalization ability. Current research primarily focuses on improving LLMs' cross-lingual generalization capabilities. However, there is still a lack of research on the intrinsic mechanisms of how LLMs achieve cross-lingual alignment. From the perspective of region partitioning, this paper conducts several investigations on the linguistic competence of LLMs. We discover a core region in LLMs that corresponds to linguistic competence, accounting for approximately 1\% of the total model parameters. Removing this core region by setting parameters to zero results in a significant performance decrease across 30 different languages. Furthermore, this core region exhibits significant dimensional dependence, perturbations to even a single parameter on specific dimensions leading to a loss of linguistic competence. Moreover, we discover that distinct monolingual regions exist for different languages, and disruption to these specific regions substantially reduces the LLMs' proficiency in those corresponding languages. Our research also indicates that freezing the core linguistic region during further pre-training can mitigate the issue of catastrophic forgetting (CF), a common phenomenon observed during further pre-training of LLMs. Overall, exploring the LLMs' functional regions provides insights into the foundation of their intelligence Our code is released in .}.","Three main findings of our experiments: (1) Identification of core language regions within the LLMs, where removals lead to linguistic competence loss; (2) Discovery of monolingual regions, where removals cause significant proficiency loss in specific languages; (3) Optimization of freezing core regions during further pre-training decelerates language forgetting."
FastFiD: Improve Inference Efficiency of Open Domain Question Answering via Sentence Selection,2408.06333v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2408.06333v1_0.pdf,"Open Domain Question Answering (ODQA) has been advancing rapidly in recent times, driven by significant developments in dense passage retrieval and pretrained language models. Current models typically incorporate the FiD framework, which is composed by a neural retriever alongside an encoder-decoder neural reader. In the answer generation process, the retriever will retrieve numerous passages (around 100 for instance), each of which is then individually encoded by the encoder. Subsequently, the decoder makes predictions based on these encoded passages. Nevertheless, this framework can be relatively time-consuming, particularly due to the extensive length of the gathered passages. To address this, we introduce FastFiD in this paper, a novel approach that executes sentence selection on the encoded passages. This aids in retaining valuable sentences while reducing the context length required for generating answers. Experiments on three commonly used datasets (Natural Questions, TriviaQA and ASQA) demonstrate that our method can enhance the inference speed by {2.3X-5.7X}, while simultaneously maintaining the model's performance. Moreover, an in-depth analysis of the model's attention reveals that the selected sentences indeed hold a substantial contribution towards the final answer. The codes are publicly available at .","Inference Time for FiD (base) and FastFiD (base) with varying numbers of retrieved passages. As the number of retrieved passages increases, FiD encounters increasingly severe efficiency issues. Our FastFiD significantly accelerates the process by greatly reducing decoding time."
An Open Multilingual System for Scoring Readability of Wikipedia,2406.01835v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.01835v1_0.png,"With over 60M articles, Wikipedia has become the largest platform for open and freely accessible knowledge. While it has more than 15B monthly visits, its content is believed to be inaccessible to many readers due to the lack of readability of its text. However, previous investigations of the readability of Wikipedia have been restricted to English only, and there are currently no systems supporting the automatic readability assessment of the 300+ languages in Wikipedia. To bridge this gap, we develop a multilingual model to score the readability of Wikipedia articles. To train and evaluate this model, we create a novel multilingual dataset spanning 14 languages, by matching articles from Wikipedia to simplified Wikipedia and online children encyclopedias. We show that our model performs well in a zero-shot scenario, yielding a ranking accuracy of more than 80\% across 14 languages and improving upon previous benchmarks. These results demonstrate the applicability of the model at scale for languages in which there is no ground-truth data available for model fine-tuning. Furthermore, we provide the first overview on the state of readability in Wikipedia beyond English.",Sketch of the readability scoring system for Wikipedia articles. Higher scores indicate more difficult-to-read text.
Unlearning Traces the Influential Training Data of Language Models,2401.15241v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2401.15241v2_0.pdf,"Identifying the training datasets that influence a language model's outputs is essential for minimizing the generation of harmful content and enhancing its performance. Ideally, we can measure the influence of each dataset by removing it from training; however, it is prohibitively expensive to retrain a model multiple times. This paper presents UnTrac: {Un}learning {Trac}es the influence of a training dataset on the model's performance. UnTrac is extremely simple; each training dataset is unlearned by gradient ascent, and we evaluate how much the model's predictions change after unlearning. Furthermore, we propose a more scalable approach, UnTrac-Inv, which unlearns a test dataset and evaluates the unlearned model on training datasets. UnTrac-Inv resembles UnTrac, while being efficient for massive training datasets. In the experiments, we examine if our methods can assess the influence of pretraining datasets on generating toxic, biased, and untruthful content. Our methods estimate their influence much more accurately than existing methods while requiring neither excessive memory space nor multiple checkpoints.","Overview of leave-dataset-out vs. proposed methods, UnTrac and UnTrac-Inv."
Self-Evolving GPT: A Lifelong Autonomous Experiential Learner,2407.08937v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2407.08937v1_0.pdf,"{} [1]{Corresponding Author} To improve the performance of large language models (LLMs), researchers have explored providing LLMs with textual task-solving experience via prompts. However, they rely on manual efforts to acquire and apply such experience for each task, which is not feasible for the growing demand for LLMs and the variety of user questions. To address this issue, we design a lifelong autonomous experiential learning framework based on LLMs to explore whether LLMs can imitate human ability for learning and utilizing experience. It autonomously learns and accumulates experience through experience transfer and induction, categorizing the types of input questions to select which accumulated experience to employ for them. Experimental results on six widely used NLP datasets show that our framework performs reliably in each intermediate step and effectively improves the performance of GPT-3.5 and GPT-4. This validates the feasibility of using LLMs to mimic human experiential learning and application capabilities. Additionally, we provide a detailed analysis of the behavior of our framework at each step.",An example of experience-enhanced LLMs inference.
The Hidden Space of Transformer Language Adapters,2402.13137v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.13137v2_0.pdf;/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.13137v2_1.pdf;/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.13137v2_2.pdf;/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.13137v2_3.pdf,"We analyze the operation of transformer language adapters, which are small modules trained on top of a frozen language model to adapt its predictions to new target languages. We show that adapted predictions mostly evolve in the source language the model was trained on, while the target language becomes pronounced only in the very last layers of the model. Moreover, the adaptation process is gradual and distributed across layers, where it is possible to skip small groups of adapters without decreasing adaptation performance. Last, we show that adapters operate on top of the model’s frozen representation space while largely preserving its structure, rather than on an ``isolated'' subspace. Our findings provide a deeper view into the adaptation process of language models to new languages, showcasing the constraints imposed on it by the underlying model and introduces practical implications to enhance its efficiency..}",
Advancing Large Language Models to Capture Varied Speaking Styles and Respond Properly in Spoken Conversations,2402.12786v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.12786v2_0.pdf,"Although text-based Large Language Models (LLMs) can interact with and help humans through the textual interface, speech is the most natural way for humans to communicate. Speech signals contain not merely linguistic information but also paralinguistic and prosodic information to convey messages beyond text. In this work, we aim to advance an LLM that can listen to speaking style and respond properly in spoken conversation; that is, {the same sentence spoken with different speaking styles would cause different responses in speech}. Since there is no existing dataset with such characteristics, we first build the {StyleTalk} dataset with speech-to-speech conversations with the same sentence in different speaking styles. Furthermore, we propose the {Spoken-LLM} framework to model response speech with a two-stage training approach. The proposed method outperforms text-based LLM (text-only or cascaded pipeline method) and previous speech LLM baseline. The dataset should be released to the community. The audio sample demo is at .","The overview framework of Spoken-LLM. (\texttt{c1},\texttt{r1}) and (\texttt{c2},\texttt{r2}) are the current and response speech sample pairs. \texttt{c1} and \texttt{c2} are fed into the model individually."
Automated Justification Production for Claim Veracity in Fact Checking: A Survey on Architectures and Approaches,2407.12853v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2407.12853v1_0.pdf,"Automated Fact-Checking (AFC) is the automated verification of claim accuracy. AFC is crucial in discerning truth from misinformation, especially given the huge amounts of content are generated online daily. Current research focuses on predicting claim veracity through metadata analysis and language scrutiny, with an emphasis on justifying verdicts. This paper surveys recent methodologies, proposing a comprehensive taxonomy and presenting the evolution of research in that landscape. A comparative analysis of methodologies and future directions for improving fact-checking explainability are also discussed. %These methodologies used have been additionally utilized to reduce non-factual statements in Large Language Models (LLMs) - commonly referred to as non-factual hallucinations.",General AFC Pipeline; courtesy of \cite{guo-etal-2022-survey}.
Narrowing the Knowledge Evaluation Gap: Open-Domain Question Answering with Multi-Granularity Answers,2401.04695v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2401.04695v2_0.pdf,,{Top}: \granola~QA evaluation with multi-granularity answers. {Middle}: Decoding with Response Aggregation (\drag) outputs a (potentially coarser) response by aggregating several responses of the model. {Bottom}: Accuracy gain from evaluating using multi-granularity answers for several decoding strategies. \drag~reveals a significant knowledge evaluation gap.
Muffin or Chihuahua? Challenging Multimodal Large Language Models with Multipanel VQA,2401.15847v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2401.15847v3_0.pdf,"Multipanel images, commonly seen as web screenshots, posters, etc., pervade our daily lives. These images, characterized by their composition of multiple subfigures in distinct layouts, effectively convey information to people. Toward building advanced multimodal AI applications, such as agents that understand complex scenes and navigate through webpages, the skill of multipanel visual reasoning is essential, and a comprehensive evaluation of models in this regard is important. Therefore, we introduce Multipanel Visual Question Answering (MultipanelVQA), a novel benchmark comprising 6,600 triplets of questions, answers, and multipanel images that specifically challenge models in comprehending multipanel images. Our evaluation shows that questions in the MultipanelVQA benchmark pose significant challenges to the state-of-the-art Multimodal Large Language Models (MLLMs) tested, even though humans can attain approximately 99\% accuracy on these questions. Distinctively, the MultipanelVQA benchmark features synthetically generated multipanel images specifically crafted to isolate and assess the impact of various factors, such as the layout, on MLLMs' multipanel image comprehension abilities. As a result, in addition to benchmarking the capabilities of MLLMs in understanding multipanel images, we analyze various factors of the multipanel image that affect MLLMs' performance with synthetic data and offer insights for enhancement. .",Examples of Single-panel {vs.} multipanel image VQA. GPT-4V distinguishes muffin and chihuahua in the single-panel image input but struggles with the same content in the multipanel image.
WebVoyager: Building an End-to-End Web Agent with Large Multimodal Models,2401.13919v4,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2401.13919v4_0.pdf,"The rapid advancement of large language models (LLMs) has led to a new era marked by the development of autonomous applications in real-world scenarios, which drives innovation in creating advanced web agents. Existing web agents typically only handle one input modality and are evaluated only in simplified web simulators or static web snapshots, greatly limiting their applicability in real-world scenarios. %primarily focus on processing HTML content of web pages, often neglecting the vital visual information available online. To bridge this gap, we introduce WebVoyager, an innovative Large Multimodal Model (LMM) powered web agent that can complete user instructions end-to-end by interacting with real-world websites. %for online web navigation and end-to-end web task resolution. Moreover, we establish a new benchmark by compiling real-world tasks from 15 popular websites and introduce an automatic evaluation protocol leveraging multimodal understanding abilities of GPT-4V to evaluate open-ended web agents. We show that WebVoyager achieves a 59.1\% task success rate on our benchmark, significantly surpassing the performance of both GPT-4 (All Tools) and the WebVoyager (text-only) setups, underscoring the exceptional capability of WebVoyager. The proposed automatic evaluation metric achieves 85.3\% agreement with human judgment, indicating its effectiveness in providing reliable and accurate assessments of web agents.% }","The overall workflow of WebVoyager. WebVoyager takes web tasks assigned by a human and automatically browses the web online. At each step, WebVoyager selects actions based on screenshots and text (the `type' of the web element and its contents). Once the task is completed, the answers will be returned to the user. For example, for a user query: ""Find the cost of a 2-year protection for PS4 on Amazon."", the agent interacts with Amazon online, locates the PS4, identifies the 2-year protection price, and returns ""\$30.99"" to the user."
Leveraging Machine-Generated Rationales to Facilitate Social Meaning Detection in Conversations,2406.19545v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.19545v1_0.pdf,"We present a generalizable classification approach that leverages Large Language Models (LLMs) to facilitate the detection of implicitly encoded social meaning in conversations. We design a multi-faceted prompt to extract a textual explanation of the reasoning that connects visible cues to underlying social meanings. These extracted explanations or rationales serve as augmentations to the conversational text to facilitate dialogue understanding and transfer. Our empirical results over 2,340 experimental settings demonstrate the significant positive impact of adding these rationales. Our findings hold true for in-domain classification, zero-shot, and few-shot domain transfer for two different social meaning detection tasks, each spanning two different corpora.","Fraction of cases where the classification performance was significantly better, same, or worse, when rationales were augmented, for two different tasks, i.e. detecting resisting strategies (RES) and recognizing emotions (ERC) and for two settings i.e., in-domain (ID) and transfer (TF)."
Lightweight reranking for language model generations,2307.06857v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2307.06857v3_0.pdf,"Large Language Models (LLMs) can exhibit considerable variation in the quality of their sampled outputs. Reranking and selecting the best generation from the sampled set is a popular way of obtaining strong gains in generation quality. In this paper, we present a novel approach for reranking LLM generations. Unlike other techniques that might involve additional inferences or training a specialized reranker, our approach relies on easy to compute pairwise statistics between the generations that have minimal compute overhead. We show that our approach can be formalized as an extension of self-consistency and analyze its performance in that framework, theoretically as well as via simulations. We show strong improvements for selecting the best $k$ generations for code generation tasks as well as robust improvements for the best generation for the tasks of autoformalization, summarization, and translation. While our approach only assumes black-box access to LLMs, we show that additional access to token probabilities can improve performance even further.","On the left we have the original setup where we have predicates which we know the optimal generation should satisfy and which we can evaluate on the generations. In the middle, we drop the assumption that we know whether the optimal generation should satisfy the predicates or not. On the right, we drop the assumption that we need to evaluate the predicates on the different generations -- only assuming we know on how many predicates a pair of generations agree"
ARIES: A Corpus of Scientific Paper Edits Made in Response to Peer Reviews,2306.12587v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2306.12587v2_0.pdf,"We introduce the task of automatically revising scientific papers based on peer feedback and release , a dataset of review comments and their corresponding paper edits. The data is drawn from real reviewer-author interactions from computer science, and we provide labels linking each reviewer comment to the specific paper edits made by the author in response. We automatically create a high-precision silver training set, as well as an expert-labeled test set that shows high inter-annotator agreement. In experiments with 10 models covering the state of the art, we find that they struggle even to identify which edits correspond to a comment—especially when the relationship between the edit and the comment is indirect and requires reasoning to uncover. We also extensively analyze GPT-4’s ability to generate edits given a comment and the original paper. We find that it often succeeds on a superficial level, but tends to rigidly follow the wording of the feedback rather than the underlying intent, and lacks technical details compared to human-written edits.","Overview of our tasks. In comment-edit alignment, a model is given a review comment and set of candidate edits derived from a source paper and a revised target paper, and it must align the comment to the edit(s) that are associated with it. In edit generation, a model is given a review comment and a source paper and must generate an edit that addresses the comment, possibly using placeholders for missing information."
The Unreasonable Effectiveness of Easy Training Data for Hard Tasks,2401.06751v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2401.06751v2_0.pdf,"=-1 How can we train models to perform well on hard test data when hard training data is by definition difficult to label correctly? This question has been termed the {scalable oversight} problem and has drawn increasing attention as language models have continually improved. In this paper, we present the surprising conclusion that current pretrained language models often generalize relatively well from easy to hard data, even performing as well as oracle models finetuned on hard data. We demonstrate this kind of easy-to-hard generalization using simple finetuning methods like in-context learning, linear classifier heads, and QLoRA for seven different measures of datapoint hardness, including six empirically diverse human hardness measures (like grade level) and one model-based measure (loss-based). Furthermore, we show that even if one cares most about model performance on hard data, it can be better to collect easy data rather than hard data for finetuning, since hard data is generally noisier and costlier to collect. Our experiments use open models up to 70b in size and four publicly available question-answering datasets with questions ranging in difficulty from 3rd grade science questions to college level STEM questions and general-knowledge trivia. We conclude that easy-to-hard generalization in LMs is surprisingly strong for the tasks studied..}","A model prompted with easy data (e.g., 3rd Grade problems) does {almost as well} on a hard task (College problems) as a model prompted with hard data (the College bar). Results shown for Mixtral-8x7B with $k{=}10$ prompt examples, averaged over 5 random seeds."
MIDGARD: Self-Consistency Using Minimum Description Length for Structured Commonsense Reasoning,2405.05189v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2405.05189v2_0.pdf,"We study the task of conducting structured reasoning as generating a reasoning graph from natural language input using large language models (LLMs). Previous approaches have explored various prompting schemes, yet they suffer from error propagation due to the autoregressive nature and single-pass-based decoding, which lack error correction capability. Additionally, relying solely on a single sample may result in the omission of true nodes and edges. To counter this, we draw inspiration from {self-consistency} (SC), which involves sampling a diverse set of reasoning chains and taking the majority vote as the final answer. To tackle the substantial challenge of applying SC on generated graphs, we propose {} (nimum escription length uided ggregation of easoning in irected acyclic graph) that leverages Minimum Description Length (MDL)-based formulation to identify consistent properties among the different graph samples generated by an LLM. This formulation helps reject properties that appear in only a few samples, which are likely to be erroneous, while enabling the inclusion of missing elements without compromising precision. Our method demonstrates superior performance than comparisons across various structured reasoning tasks, including argument structure extraction, explanation graph generation, inferring dependency relations among actions for everyday tasks, and semantic graph generation from natural texts.","Comparison of \model with \textsc{CoCoGen}. In this example, our objective is to infer dependency relations among items in the ""Action List"" to achieve the specified ""Objective"". \textsc{CoCoGen} uses greedy decoding and exhibits errors in the output, e.g., ""decided to run errands during a break in the rain"" is not connected with ""Drive to errand location and complete"". In contrast, our approach \model (within the \colorbox[HTML]{FCE5CD}{orange} rectangle) aggregates relevant information across different samples, resulting in more accurate inference. For this example, our algorithm improved the performance of greedy decoding from $\mathbf{66.7}$ to $\mathbf{85.7}$ in edge $F_1$-score."
ChiMed-GPT: A Chinese Medical Large Language Model with Full Training Regime and Better Alignment to Human Preferences,2311.06025v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2311.06025v3_0.png,"Recently, the increasing demand for superior medical services has highlighted the discrepancies in the medical infrastructure. With big data, especially texts, forming the foundation of medical services, there is an exigent need for effective natural language processing (NLP) solutions tailored to the healthcare domain. Conventional approaches leveraging pre-trained models present promising results in this domain and current large language models (LLMs) offer advanced foundation for medical text processing. However, most medical LLMs are trained only with supervised fine-tuning (SFT), even though it efficiently empowers LLMs to understand and respond to medical instructions but is ineffective in learning domain knowledge and aligning with human preference. In this work, we propose , a new benchmark LLM designed explicitly for Chinese medical domain, and undergoes a comprehensive training regime with pre-training, SFT, and RLHF. Evaluations on tasks including information extraction, question answering, and dialogue generation demonstrate 's superior performance over general domain LLMs. Furthermore, we analyze possible biases through prompting to perform attitude scales regarding discrimination of patients, so as to contribute to further responsible development of LLMs in the medical domain..}","An illustration of the overall training process of the \textsc{ChiMed-GPT}, which consists of three stages including pre-training, supervised fine-tuning, and reinforcement learning from human feedback (RLHF)."
Leveraging Large Language Models for Learning Complex Legal Concepts through Storytelling,2402.17019v4,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.17019v4_0.png,"Making legal knowledge accessible to non-experts is crucial for enhancing general legal literacy and encouraging civic participation in democracy. However, legal documents are often challenging to understand for people without legal backgrounds. In this paper, we present a novel application of large language models (LLMs) in legal education to help non-experts learn intricate legal concepts through storytelling, an effective pedagogical tool in conveying complex and abstract concepts. We also introduce a new dataset , which consists of 294 complex legal doctrines, each accompanied by a story and a set of multiple-choice questions generated by LLMs. To construct the dataset, we experiment with various LLMs to generate legal stories explaining these concepts. Furthermore, we use an expert-in-the-loop approach to iteratively design multiple-choice questions. Then, we evaluate the effectiveness of storytelling with LLMs through randomized controlled trials (RCTs) with legal novices on 10 samples from the dataset. We find that LLM-generated stories enhance comprehension of legal concepts and interest in law among non-native speakers compared to only definitions. Moreover, stories consistently help participants relate legal concepts to their lives. Finally, we find that learning with stories shows a higher retention rate for non-native speakers in the follow-up assessment. Our work has strong implications for using LLMs in promoting teaching and learning in the legal field and beyond.",Illustration of the expert-in-the-loop pipeline. The left section demonstrates the procedure to produce an LLM-generated story from the concept. The lower section in the center shows how we use both the definition and story as input to produce LLM-generated reading comprehension (RC) questions. The center upper section shows that we first collect expert feedback on questions and regenerate questions with expert advice. The right section outlines the RCT experiment to see if LLM-generated stories improve comprehension in legal concepts.
Prompt Optimization via Adversarial In-Context Learning,2312.02614v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2312.02614v3_0.pdf,"We propose a new method, Adversarial In-Context Learning (.}}), to optimize prompts for in-context learning (ICL). Inspired by adversarial learning, is implemented as a two-player game between a generator and discriminator, with LLMs acting as both. In each round, given an input prefixed by task instructions and several exemplars, the generator produces an output. The discriminator then classifies the generator's input-output pair as model-generated or real data. Based on the discriminator's loss, a prompt modifier LLM proposes possible edits to the generator and discriminator prompts, and the edits that most improve the adversarial loss are selected. We show that applying results in significant improvements over state-of-the-art prompt optimization techniques for both open and closed-source models on $13$ generation and classification tasks including summarization, arithmetic reasoning, machine translation, data-to-text generation, and the MMLU and big-bench hard benchmarks. In addition, our method is computationally efficient, easily extensible to other LLMs and tasks, and effective in low-resource settings","\adv{} orchestrates a minimax game between a {Generator} and a {Discriminator}, both powered by LLMs with few-shot prompts. The Generator crafts responses to unlabeled examples, while the Discriminator distinguishes between generated and ground truth outputs. Updates are made by a {Prompt Modifier} which modifies prompts based on the adversarial loss."
Multimodal Contextualized Semantic Parsing from Speech,2406.06438v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.06438v1_0.jpg,"We introduce Semantic Parsing in Contextual Environments (SPICE), a task designed to enhance artificial agents' contextual awareness by integrating multimodal inputs with prior contexts. SPICE goes beyond traditional semantic parsing by offering a structured, interpretable framework for dynamically updating an agent's knowledge with new information, mirroring the complexity of human communication. We develop the VG-SPICE dataset, crafted to challenge agents with visual scene graph construction from spoken conversational exchanges, highlighting speech and visual data integration. We also present the Audio-Vision Dialogue Scene Parser (AViD-SP) developed for use on VG-SPICE. These innovations aim to improve multimodal information processing and integration. Both the VG-SPICE dataset and the AViD-SP model are publicly available.",Example of VG-SPICE inputs as well as a plausible output to produce the correct next state context. New information that the agent is expected to add to the context is shown in green while already known information is noted in red. Grounding entities that have new information being added to them are noted in blue and orange. The current context is shown as a textually prompted representation of the actual knowledge graph (discussed in Section \ref{sec: Contextual State Representation}).
LaMP: When Large Language Models Meet Personalization,2304.11406v4,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2304.11406v4_0.pdf,"This paper highlights the importance of personalization in large language models and introduces the benchmark --- a novel benchmark for training and evaluating language models for producing personalized outputs. offers a comprehensive evaluation framework with diverse language tasks and multiple entries for each user profile. It consists of seven personalized tasks, spanning three text classification and four text generation tasks. We additionally propose two retrieval augmentation approaches that retrieve personal items from each user profile for personalizing language model outputs. To this aim, we study various retrieval models, including term matching, semantic matching, and time-aware methods. Extensive experiments on for zero-shot and fine-tuned language models demonstrate the efficacy of the proposed retrieval augmentation approach and highlight the impact of personalization in various natural language tasks.",An overview of the retrieval-augmented method for personalizing LLMs. $\phi_q$ and $\phi_p$ represent query and prompt construction functions.
AboutMe: Using Self-Descriptions in Webpages to Document the Effects of English Pretraining Data Filters,2401.06408v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2401.06408v3_0.pdf,"Large language models' (LLMs) abilities are drawn from their pretraining data, and model development begins with data curation. However, decisions around what data is retained or removed during this initial stage are under-scrutinized. In our work, we ground web text, which is a popular pretraining data source, to its social and geographic contexts. We create a new dataset of 10.3 million self-descriptions of website creators, and extract information about who they are and where they are from: their topical interests, social roles, and geographic affiliations. Then, we conduct the first study investigating how ten ``quality'' and English language identification (langID) filters affect webpages that vary along these social dimensions. Our experiments illuminate a range of implicit preferences in data curation: we show that some quality classifiers act like topical domain filters, and langID can overlook English content from some regions of the world. Overall, we hope that our work will encourage a new line of research on pretraining data curation practices and its social implications.","A paraphrased excerpt from a website's \textsc{about} page, with extracted social dimensions highlighted. We use self-descriptions like this one from Common Crawl, which is frequently used as LLM pretraining data, to examine the social effects of data curation filters."
LangBridge: Multilingual Reasoning Without Multilingual Supervision,2401.10695v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2401.10695v2_0.pdf,"We introduce , a {zero-shot} approach to adapt language models for multilingual reasoning tasks without multilingual supervision. operates by two models, each specialized in different aspects: (1) one specialized in understanding multiple languages (e.g., mT5 encoder) and (2) one specialized in reasoning (e.g., Orca 2). connects the two models by introducing minimal trainable parameters between them. Despite utilizing only English data for training, considerably enhances the performance of language models on low-resource languages across mathematical reasoning, code completion, logical reasoning, and commonsense reasoning. Our analysis suggests that the efficacy of stems from the language-agnostic characteristics of multilingual representations. We publicly release our code and models.{github.com/kaistAI/LangBridge}}","MGSM accuracy (\%) of MetaMath models and models aligned with mT5-XL encoder (2B) via \method ({LB}). In addition to the average (\textsc{avg}) accuracy, we also report the average accuracy of high-resource languages (\textsc{hrl}) and underrepresented languages (\textsc{url}) classified by \citet{shi2023language}."
Can LLMs Reason with Rules? Logic Scaffolding for Stress-Testing and Improving LLMs,2402.11442v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.11442v3_0.pdf,"Large language models (LLMs) have achieved impressive human-like performance across various reasoning tasks. However, their mastery of underlying inferential rules still falls short of human capabilities. To investigate this, we propose a logic scaffolding inferential rule generation framework, to construct an inferential rule base, ULogic, comprising both primitive and compositional rules across five domains. Our analysis of GPT-series models over a rule subset reveals significant gaps in LLMs' logic understanding compared to human performance, especially in compositional and structural complex rules with certain bias patterns. We further distill these rules into a smaller-scale inference engine for flexible rule generation and enhancing downstream reasoning. Through a multi-judger evaluation, our inference engine proves effective in generating accurate, complex and abstract conclusions and premises, and improve various commonsense reasoning tasks. Overall, our work sheds light on LLMs' limitations in grasping inferential rule and suggests ways to enhance their logical reasoning abilities~.}.",Human comparison results.
Unlocking the Power of Large Language Models for Entity Alignment,2402.15048v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.15048v2_0.png,"Entity Alignment (EA) is vital for integrating diverse knowledge graph (KG) data, playing a crucial role in data-driven AI applications. Traditional EA methods primarily rely on comparing entity embeddings, but their effectiveness is constrained by the limited input KG data and the capabilities of the representation learning techniques. Against this backdrop, we introduce ChatEA, an innovative framework that incorporates large language models (LLMs) to improve EA. To address the constraints of limited input KG data, ChatEA introduces a KG-code translation module that translates KG structures into a format understandable by LLMs, thereby allowing LLMs to utilize their extensive background knowledge to improve EA accuracy. To overcome the over-reliance on entity embedding comparisons, ChatEA implements a two-stage EA strategy that capitalizes on LLMs’ capability for multi-step reasoning in a dialogue format, thereby enhancing accuracy while preserving efficiency. Our experimental results verify ChatEA's superior performance, highlighting LLMs' potential in facilitating EA tasks. The source code is available at {https://github.com/IDEA-FinAI/ChatEA}.",A comparison of previous EA and ChatEA.
Cognitive Visual-Language Mapper: Advancing Multimodal Comprehension with Enhanced Visual Knowledge Alignment,2402.13561v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.13561v2_0.png,"Evaluating and Rethinking the current landscape of Large Multimodal Models (LMMs), we observe that widely-used visual-language projection approaches (e.g., Q-former or MLP) focus on the alignment of image-text descriptions yet ignore the visual knowledge-dimension alignment, i.e., connecting visuals to their relevant knowledge. Visual knowledge plays a significant role in analyzing, inferring, and interpreting information from visuals, helping improve the accuracy of answers to knowledge-based visual questions. In this paper, we mainly explore improving LMMs with visual-language knowledge alignment, especially aimed at challenging knowledge-based visual question answering (VQA). To this end, we present a {Cognitive Visual-Language Mapper} (CVLM), which contains a pretrained Visual Knowledge Aligner (VKA) and a Fine-grained Knowledge Adapter (FKA) used in the multimodal instruction tuning stage. Specifically, we design the VKA based on the interaction between a small language model and a visual encoder, training it on collected image-knowledge pairs to achieve visual knowledge acquisition and projection. FKA is employed to distill the fine-grained visual knowledge of an image and inject it into Large Language Models (LLMs). We conduct extensive experiments on knowledge-based VQA benchmarks and experimental results show that CVLM significantly improves the performance of LMMs on knowledge-based VQA (average gain by 5.0\%). Ablation studies also verify the effectiveness of VKA and FKA, respectively.}","It illustrates the performance of LMMs on visual information-seeking questions. The bottom part shows the widely-used architecture of open-source LMMs, where the visual mapping network is usually pretrained on massive image-text captioning data. All LMMs including GPT-4V (Date: 2023.11.17) and Gemini-Pro make incorrect decisions."
FreeCtrl: Constructing Control Centers with Feedforward Layers for Learning-Free Controllable Text Generation,2406.09688v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.09688v1_0.pdf,"Controllable text generation (CTG) seeks to craft texts adhering to specific attributes, traditionally employing learning-based techniques such as training, fine-tuning, or prefix-tuning with attribute-specific datasets. These approaches, while effective, demand extensive computational and data resources. In contrast, some proposed learning-free alternatives circumvent learning but often yield inferior results, exemplifying the fundamental machine learning trade-off between computational expense and model efficacy. To overcome these limitations, we propose FreeCtrl, a learning-free approach that dynamically adjusts the weights of selected feedforward neural network (FFN) vectors to steer the outputs of large language models (LLMs). FreeCtrl hinges on the principle that the weights of different FFN vectors influence the likelihood of different tokens appearing in the output. By identifying and adaptively adjusting the weights of attribute-related FFN vectors, FreeCtrl can control the output likelihood of attribute keywords in the generated content. Extensive experiments on single- and multi-attribute control reveal that the learning-free FreeCtrl outperforms other learning-free and learning-based methods, successfully resolving the dilemma between learning costs and model performance}.","Trade-off between learning cost and performance for CTG. Learning-based methods excel in delivering superb results but demand significant training resources. Conversely, learning-free methods are more resource-efficient but tend to yield inferior performance. Numerical performance details are available in \S\ref{sec:exp}."
"Angry Men, Sad Women: Large Language Models Reflect Gendered Stereotypes in Emotion Attribution",2403.03121v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2403.03121v3_0.pdf,"Large language models (LLMs) reflect societal norms and biases, especially about gender. While societal biases and stereotypes have been extensively researched in various NLP applications, there is a surprising gap for emotion analysis. However, emotion and gender are closely linked in societal discourse. E.g., women are often thought of as more empathetic, while men's anger is more socially accepted. %, and emotion recognition is a focal point for artificial intelligence (AI) regulation.% . To fill this gap, we present the first comprehensive study of gendered emotion attribution in five state-of-the-art LLMs (open- and closed-source). We investigate whether emotions are gendered, and whether these variations are based on societal stereotypes. We prompt the models to adopt a gendered persona and attribute emotions to an event like `When I had a serious argument with a dear person'. We then analyze the emotions generated by the models in relation to the gender-event pairs. %We use over 200k gender-event pairs and We find that {all} models consistently exhibit gendered emotions, influenced by gender stereotypes. These findings are in line with established research in psychology and gender studies. Our study sheds light on the complex societal interplay between language, gender, and emotion. The reproduction of emotion stereotypes in LLMs allows us to use those models to study the topic in detail, but raises questions about the predictive use of those same LLMs for emotion applications.% In short: do we want those models to replicate societal stereotypes around gendered emotion?",Stereotypical model biases in gendered emotion attribution for the event ``When I had a serious argument with a dear person'' from the ISEAR dataset \cite{scherer1994evidence}. The model attributes woman with \sadness and man with \anger. See Table \ref{table:expl1} for detailed explanations.
STICKERCONV: Generating Multimodal Empathetic Responses from Scratch,2402.01679v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.01679v2_0.pdf,"Stickers, while widely recognized for enhancing empathetic communication in online interactions, remain underexplored in current empathetic dialogue research, notably due to the challenge of a lack of comprehensive datasets. In this paper, we introduce the (), which uses collaborative agent interactions to realistically simulate human behavior with sticker usage, thereby enhancing multimodal empathetic communication. Building on this foundation, we develop a multimodal empathetic dialogue dataset, , comprising 12.9K dialogue sessions, 5.8K unique stickers, and 2K diverse conversational scenarios. This dataset serves as a benchmark for multimodal empathetic generation. To advance further, we propose (), a multimodal empathetic response generation framework, complemented by a comprehensive set of empathy evaluation metrics based on LLM. Our experiments demonstrate 's effectiveness in generating contextually relevant and emotionally resonant multimodal empathetic responses, contributing to the advancement of more nuanced and engaging empathetic dialogue systems}.","An example of multimodal conversation in the \dataset. Both parties can utilize the stickers to express their emotions, which enhances interactivity and expression. The assistant can empathize with the user according to the conversation (\textcolor[RGB]{0,153,0}{green} text)."
EXAMS-V: A Multi-Discipline Multilingual Multimodal Exam Benchmark for Evaluating Vision Language Models,2403.10378v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2403.10378v1_0.pdf,"We introduce , a new challenging multi-discipline multimodal multilingual exam benchmark for evaluating vision language models. It consists of multiple-choice questions across school disciplines covering natural science, social science, and other miscellaneous studies, e.g.,~religion, fine arts, business, etc. includes a variety of multimodal features such as text, images, tables, figures, diagrams, maps, scientific symbols, and equations. The questions come in languages from language families. Unlike existing benchmarks, is uniquely curated by gathering school exam questions from various countries, with a variety of education systems. This distinctive approach calls for intricate reasoning across diverse languages and relies on region-specific knowledge. Solving the problems in the dataset requires advanced perception and joint reasoning over the text and the visual content of the image. Our evaluation results demonstrate that this is a challenging dataset, which is difficult even for advanced vision--text models such as GPT-4V and Gemini; this underscores the inherent complexity of the dataset and its significance as a future benchmark.}",Data distribution for our \dataset dataset: languages and subjects.
Text Embedding Inversion Security for Multilingual Language Models,2401.12192v4,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2401.12192v4_0.png,"Textual data is often represented as real-numbered embeddings in NLP, particularly with the popularity of large language models (LLMs) and Embeddings as a Service (EaaS). However, storing sensitive information as embeddings can be susceptible to security breaches, as research shows that text can be reconstructed from embeddings, even without knowledge of the underlying model. While defence mechanisms have been explored, these are exclusively focused on English, leaving other languages potentially exposed to attacks. This work explores LLM security through {multilingual} embedding inversion. We define the problem of black-box multilingual and cross-lingual inversion attacks, and explore their potential implications. Our findings suggest that multilingual LLMs may be more vulnerable to inversion attacks, in part because English-based defences may be ineffective. To alleviate this, we propose a simple masking defense effective for both monolingual and multilingual models. This study is the first to investigate multilingual inversion attacks, shedding light on the differences in attacks and defenses across monolingual and multilingual settings.","Schematic overview of a text embedding inversion attack. A user accesses an EaaS provider, while an attacker is eavesdropping. Although the attacker has no direct access to the embedding model, they can reliably decode the information stored in the embeddings."
Synthesizing Text-to-SQL Data from Weak and Strong LLMs,2408.03256v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2408.03256v1_0.pdf,"The capability gap between open-source and closed-source large language models (LLMs) remains challenging in text-to-SQL tasks. In this paper, we introduce a synthetic data approach that amalgamates strong data generated by larger, more potent models (strong models) with weak data produced by smaller, less well-aligned models (weak models). Our approach contributes to the improvement of domain generalization in text-to-SQL models and investigates the potential of weak data supervision through preference learning. Moreover, we utilize the synthetic data approach for instruction tuning on open-source LLMs, yielding , a specialized text-to-SQL model. The effectiveness of is substantiated by achieving state-of-the-art results on the SPIDER and BIRD benchmarks, thereby mitigating the performance disparity between open-source models and the methods derived from closed-source models.","Overview of \sense: Integrating human-annotated data with synthetic data from strong models for domain diversity, and weak models for preference learning, aligning with executors for enhanced text-to-SQL performance."
Context-aware Difference Distilling for Multi-change Captioning,2405.20810v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2405.20810v2_0.pdf,"Multi-change captioning aims to describe complex and coupled changes within an image pair in natural language. Compared with single-change captioning, this task requires the model to have higher-level cognition ability to reason an arbitrary number of changes. In this paper, we propose a novel context-aware difference distilling (CARD) network to capture all genuine changes for yielding sentences. Given an image pair, CARD first decouples context features that aggregate all similar/dissimilar semantics, termed common/difference context features. Then, the consistency and independence constraints are designed to guarantee the alignment/discrepancy of common/difference context features. Further, the common context features guide the model to mine locally unchanged features, which are subtracted from the pair to distill locally difference features. Next, the difference context features augment the locally difference features to ensure that all changes are distilled. In this way, we obtain an omni-representation of all changes, which is translated into linguistic sentences by a transformer decoder. Extensive experiments on three public datasets show CARD performs favourably against state-of-the-art methods. The code is available at .",Three examples about multi-change captioning. (a) includes certain object changes; (b) consists of object and background changes; (c) shows both object changes and irrelevant viewpoint change. These changes are shown in colored boxes.
Dataflow-Guided Retrieval Augmentation for Repository-Level Code Completion,2405.19782v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2405.19782v1_0.pdf,"Recent years have witnessed the deployment of code language models (LMs) in various code intelligence tasks such as code completion. Yet, it is challenging for pre-trained LMs to generate correct completions in private repositories. Previous studies retrieve cross-file context based on import relations or text similarity, which is insufficiently relevant to completion targets. In this paper, we propose a dataflow-guided retrieval augmentation approach, called , for repository-level code completion. parses a private repository into code entities and establishes their relations through an extended dataflow analysis, forming a repo-specific context graph. Whenever triggering code completion, precisely retrieves relevant background knowledge from the repo-specific context graph and generates well-formed prompts to query code LMs. Furthermore, we construct a large Python dataset, , with more diverse completion targets. Our experiments demonstrate the superior accuracy and applicable efficiency of , improving code exact match by 3.43\% and identifier F1-score by 3.27\% on average compared to the state-of-the-art approach.","A real-world example of repository-level code completion. The code LM CodeGen25-7B-mono fails to complete the last code line correctly when entering only the unfinished code (Zero-Shot). The model needs background knowledge relevant to \texttt{newSignal}, and the retrieval of this knowledge can be guided by dataflow."
"To be Continuous, or to be Discrete, Those are Bits of Questions",2406.07812v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.07812v1_0.pdf,"Recently, binary representation has been proposed as a novel representation that lies between continuous and discrete representations. It exhibits considerable information-preserving capability when being used to replace continuous input vectors. In this paper, we investigate the feasibility of further introducing it to the output side, aiming to allow models to output binary labels instead. To preserve the structural information on the output side along with label information, we extend the previous contrastive hashing method as structured contrastive hashing. More specifically, we upgrade CKY from label-level to bit-level, define a new similarity function with span marginal probabilities, and introduce a novel contrastive loss function with a carefully designed instance selection strategy. Our model} achieves competitive performance on various structured prediction tasks, and demonstrates that binary representation can be considered a novel representation that further bridges the gap between the continuous nature of deep learning and the discrete intrinsic property of natural languages.","The model architecture. The attention hash layer produces span scores (pink circles), we only use the upper triangular part of these scores and feed them into the bit-level CKY to obtain the marginal probabilities of all valid spans (purple circles). During training, we only select the spans on the target trees for structured contrastive hashing and leave the other spans unused (transparent purple circles). During inference, as shown at the bottom, our model parses sentences by returning trees with label codes (hexadecimal numbers), which are then translated back to the original labels."
PokeMQA: Programmable knowledge editing for Multi-hop Question Answering,2312.15194v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2312.15194v2_0.pdf,"Multi-hop question answering (MQA) is one of the challenging tasks to evaluate machine's comprehension and reasoning abilities, where large language models (LLMs) have widely achieved the human-comparable performance. Due to the dynamics of knowledge facts in real world, knowledge editing has been explored to update model with the up-to-date facts while avoiding expensive re-training or fine-tuning. Starting from the edited fact, the updated model needs to provide cascading changes in the chain of MQA. The previous art simply adopts a mix-up prompt to instruct LLMs conducting multiple reasoning tasks sequentially, including question decomposition, answer generation, and conflict checking via comparing with edited facts. However, the coupling of these functionally-diverse reasoning tasks inhibits LLMs' advantages in comprehending and answering questions while disturbing them with the unskilled task of conflict checking. We thus propose a framework, rgrammable nowledge diting for ulti-hop uestion nswering (PokeMQA), to decouple the jobs. Specifically, we prompt LLMs to decompose knowledge-augmented multi-hop question, while interacting with a detached trainable scope detector to modulate LLMs behavior depending on external conflict signal. The experiments on three LLM backbones and two benchmark datasets validate our superiority in knowledge editing of MQA, outperforming all competitors by a large margin in almost all settings and consistently producing reliable reasoning process. Our code is available at {https://github.com/Hengrui-Gu/PokeMQA}.} Corresponding author}","An example of multi-hop question answering under knowledge editing, which consists of relevant knowledge facts and three specific reasoning paths solving the two-hop question. For the unreliable reasoning, it uses a outdated and a non-existent fact and end up with the right answer {Europe}."
MemeGuard: An LLM and VLM-based Framework for Advancing Content Moderation via Meme Intervention,2406.05344v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.05344v1_0.pdf,"In the digital world, memes present a unique challenge for content moderation due to their potential to spread harmful content. Although detection methods have improved, proactive solutions such as intervention are still limited, with current research focusing mostly on text-based content, neglecting the widespread influence of multimodal content like memes. Addressing this gap, we present {MemeGuard}, a comprehensive framework leveraging Large Language Models (LLMs) and Visual Language Models (VLMs) for meme intervention. {MemeGuard} harnesses a specially fine-tuned VLM, {VLMeme}, for meme interpretation, and a multimodal knowledge selection and ranking mechanism ({MKS}) for distilling relevant knowledge. This knowledge is then employed by a general-purpose LLM to generate contextually appropriate interventions. Another key contribution of this work is the {{I}ntervening} {{C}yberbullying in {M}ultimodal {M}emes (ICMM)} dataset, a high-quality, labeled dataset featuring toxic memes and their corresponding human-annotated interventions. We leverage {ICMM} to test {MemeGuard}, demonstrating its proficiency in generating relevant and effective responses to toxic memes.}\\ { {Disclaimer}: {This paper contains harmful content that may be disturbing to some readers.}}",An instance of the meme intervention task.
Acquiring Clean Language Models from Backdoor Poisoned Datasets by Downscaling Frequency Space,2402.12026v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.12026v3_0.pdf;/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.12026v3_1.pdf,"Despite the notable success of language models (LMs) in various natural language processing (NLP) tasks, the reliability of LMs is susceptible to backdoor attacks. Prior research attempts to mitigate backdoor learning while training the LMs on the poisoned dataset, yet struggles against complex backdoor attacks in real-world scenarios. In this paper, we investigate the learning mechanisms of backdoor LMs in the frequency space by Fourier analysis. Our findings indicate that the backdoor mapping presented on the poisoned datasets exhibits a more discernible inclination towards lower frequency compared to clean mapping, resulting in the faster convergence of backdoor mapping. To alleviate this dilemma, we propose {Mu}lti-{Sc}a{le} {Lo}w-{R}ank {A}daptation (MuScleLoRA), which deploys multiple radial scalings in the frequency space with low-rank adaptation to the target model and further aligns the gradients when updating parameters. Through downscaling in the frequency space, MuScleLoRA encourages the model to prioritize the learning of relatively high-frequency clean mapping, consequently mitigating backdoor learning. Experimental results demonstrate that MuScleLoRA outperforms baselines significantly. Notably, MuScleLoRA reduces the average success rate of diverse backdoor attacks to below 15\% across multiple datasets and generalizes to various backbone LMs, including BERT, RoBERTa, GPT2-XL, and Llama2. The codes are publicly available at .",Specific Words
Revealing the Parametric Knowledge of Language Models: A Unified Framework for Attribution Methods,2404.18655v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2404.18655v1_0.pdf,"Language Models (LMs) acquire parametric knowledge from their training process, embedding it within their weights. The increasing scalability of LMs, however, poses significant challenges for understanding a model's inner workings and further for updating or correcting this embedded knowledge without the significant cost of retraining. This underscores the importance of unveiling exactly what knowledge is stored and its association with specific model components. Instance Attribution (IA) and Neuron Attribution (NA) offer insights into this training-acquired knowledge, though they have not been compared systematically. Our study introduces a novel evaluation framework to quantify and compare the knowledge revealed by IA and NA. To align the results of the methods we introduce the attribution method \ to apply NA for retrieving influential training instances, and \ to discover important neurons of influential instances discovered by IA. We further propose a comprehensive list of faithfulness tests to evaluate the comprehensiveness and sufficiency of the explanations provided by both methods. Through extensive experiments and analysis, we demonstrate that NA generally reveals more diverse and comprehensive information regarding the LM's parametric knowledge compared to IA. Nevertheless, IA provides unique and valuable insights into the LM’s parametric knowledge, which are not revealed by NA. Our findings further suggest the potential of a synergistic approach of combining the diverse findings of IA and NA for a more holistic understanding of an LM's parametric knowledge.","Proposed evaluation framework comparing Instance and Neuron Attribution methods by examining most influential training instances ${x_i^{train}}$ and most important neurons ${n_m, n_t, n_p, \dots}$. Tests for Sufficiency (activation of key neurons) and Completeness (suppression of the activation of key neurons) -- bottom left, alongside fine-tuning with influential training instances -- bottom right, assess the attribution methods' fidelity to the model's mechanisms."
Full Parameter Fine-tuning for Large Language Models with Limited Resources,2306.09782v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2306.09782v2_0.png,"Large Language Models (LLMs) have revolutionized Natural Language Processing (NLP) but demand massive GPU resources for training. Lowering the threshold for LLMs training would encourage greater participation from researchers, benefiting both academia and society. While existing approaches have focused on parameter-efficient fine-tuning, which tunes or adds a small number of parameters, few have addressed the challenge of tuning the full parameters of LLMs with limited resources. In this work, we propose a new optimizer, {LO}w-{M}emory {O}ptimization ({LOMO}), which fuses the gradient computation and the parameter update in one step to reduce memory usage. By integrating LOMO with existing memory saving techniques, we reduce memory usage to 10.8\% compared to the standard approach (DeepSpeed solution). Consequently, our approach enables the full parameter fine-tuning of a 65B model on a single machine with 8$$RTX 3090, each with 24GB memory..}",Comparison of SGD and LOMO in backpropagation and parameter update stages. ${Pi}$ refers to the parameter of the model and ${Gi}$ refers to the gradient corresponding to ${Pi}$. LOMO fused gradient computation and parameter update in one step to minimize the size of gradient tensors.
Long Context is Not Long at All: A Prospector of Long-Dependency Data for Large Language Models,2405.17915v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2405.17915v1_0.pdf,"Long-context modeling capabilities are important for large language models (LLMs) in various applications. However, directly training LLMs with long context windows is insufficient to enhance this capability since some training samples do not exhibit strong semantic dependencies across long contexts. In this study, we propose a data mining framework {ProLong}} that can assign each training sample with a long dependency score, which can be used to rank and filter samples that are more advantageous for enhancing long-context modeling abilities in LLM training. Specifically, we first use delta perplexity scores to measure the {Dependency Strength} between text segments in a given document. Then we refine this metric based on the {Dependency Distance} of these segments to incorporate spatial relationships across long-contexts. Final results are calibrated with a {Dependency Specificity} metric to prevent trivial dependencies introduced by repetitive patterns. Moreover, a random sampling approach is proposed to optimize the computational efficiency of ProLong. Comprehensive experiments on multiple benchmarks indicate that ProLong effectively identifies documents that carry long dependencies and LLMs trained on these documents exhibit significantly enhanced long-context modeling capabilities.","Samples that carry longer dependencies better enhances LLMs' long-context modeling capabilities, even with a fixed training context window of 32k."
Label-Synchronous Neural Transducer for E2E Simultaneous Speech Translation,2406.04541v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.04541v1_0.png,"While the neural transducer is popular for online speech recognition, simultaneous speech translation (SST) requires both streaming and re-ordering capabilities. This paper presents the LS-Transducer-SST, a label-synchronous neural transducer for SST, which naturally possesses these two properties. The LS-Transducer-SST dynamically decides when to emit translation tokens based on an Auto-regressive Integrate-and-Fire (AIF) mechanism. A latency-controllable AIF is also proposed, which can control the quality-latency trade-off either only during decoding, or it can be used in both decoding and training. The LS-Transducer-SST can naturally utilise monolingual text-only data via its prediction network which helps alleviate the key issue of data sparsity for E2E SST. During decoding, a chunk-based incremental joint decoding technique is designed to refine and expand the search space. Experiments on the Fisher-CallHome Spanish (Es-En) and MuST-C En-De data show that the LS-Transducer-SST gives a better quality-latency trade-off than existing popular methods. For example, the LS-Transducer-SST gives a 3.1/2.9 point BLEU increase (Es-En/En-De) relative to CAAT at a similar latency and a 1.4~s reduction in average lagging latency with similar BLEU scores relative to Wait-k.",Illustration of the proposed LS-Transducer-SST. Linear denotes a linear classifier. Target-side CTC uses translations in the training objective computation.
A Modular Approach for Multimodal Summarization of TV Shows,2403.03823v9,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2403.03823v9_0.pdf,"In this paper we address the task of summarizing television shows, which touches key areas in AI research: complex reasoning, multiple modalities, and long narratives. We present a modular approach where separate components perform specialized sub-tasks which we argue affords greater flexibility compared to end-to-end methods. Our modules involve detecting scene boundaries, reordering scenes so as to minimize the number of cuts between different events, converting visual information to text, summarizing the dialogue in each scene, and fusing the scene summaries into a final summary for the entire episode. We also present a new metric, ({P}recision and {R}ecall Evaluat{i}on of {S}ummary F{a}cts), to measure both precision and recall of generated summaries, which we decompose into atomic facts. Tested on the recently released SummScreen3D dataset , our method produces higher quality summaries than comparison models, as measured with ROUGE and our new fact-based metric, and as assessed by human evaluators. Code for our experiments and metric are at .","Graphical depiction of our approach for long-form multimodal summarization where different subtasks are performed by five, specialized modules (shown in different colors). We use simplified summaries for display and show only four scenes. This full episode ({As the World Turns} aired 01-06-05, contains 29 scenes."
BizBench: A Quantitative Reasoning Benchmark for Business and Finance,2311.06602v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2311.06602v2_0.pdf,"Answering questions within business and finance requires reasoning, precision, and a wide-breadth of technical knowledge. Together, these requirements make this domain difficult for large language models (LLMs). We introduce BizBench, a benchmark for evaluating models' ability to reason about realistic financial problems. BizBench comprises eight quantitative reasoning tasks, focusing on question-answering (QA) over financial data via program synthesis. We include three financially-themed code-generation tasks from newly collected and augmented QA data. Additionally, we isolate the reasoning capabilities required for financial QA: reading comprehension of financial text and tables for extracting intermediate values, and understanding financial concepts and formulas needed to calculate complex solutions. Collectively, these tasks evaluate a model's financial background knowledge, ability to parse financial documents, and capacity to solve problems with code. We conduct an in-depth evaluation of open-source and commercial LLMs, comparing and contrasting the behavior of code-focused and language-focused models. We demonstrate that the current bottleneck in performance is due to LLMs' limited business and financial understanding, highlighting the value of a challenging benchmark for quantitative reasoning within this domain.","Overview of BizBench’s eight tasks. Each level of the pyramid corresponds to a task category, with higher levels requiring increasingly complex capabilities."
Competition of Mechanisms: Tracing How Language Models Handle Facts and Counterfactuals,2402.11655v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.11655v2_0.pdf,"Interpretability research aims to bridge the gap between empirical success and our scientific understanding of the inner workings of large language models (LLMs). However, most existing research focuses on analyzing a single mechanism, such as how models copy or recall factual knowledge. In this work, we propose a formulation of {competition of mechanisms}, which focuses on the interplay of multiple mechanisms instead of individual mechanisms and traces how one of them becomes dominant in the final prediction. We uncover how and where mechanisms compete within LLMs using two interpretability methods: logit inspection and attention modification. Our findings show traces of the mechanisms and their competition across various model components and reveal attention positions that effectively control the strength of certain mechanisms.% { Code: {{https://github.com/francescortu/comp-mech}. Data: {https://huggingface.co/datasets/francescortu/comp-mech}}. Our code and data have been uploaded to the submission system and will be open-sourced upon acceptance. }",Top: An example showing that LLMs can fail to recognize the correct mechanism when multiple possible mechanisms exist. Bottom: Our mechanistic inspection of where and how the competition of mechanisms takes place within the LLMs.
FactPICO: Factuality Evaluation for Plain Language Summarization of Medical Evidence,2402.11456v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.11456v2_0.pdf,"Plain language summarization with LLMs can be useful for improving textual accessibility of technical content. But how factual are these summaries in a high-stakes domain like medicine? % This paper presents { FactPICO}, a factuality benchmark for plain language summarization of medical texts describing randomized controlled trials (RCTs), which are the basis of evidence-based medicine and can directly inform patient treatment. { FactPICO} consists of 345 plain language summaries of RCT abstracts generated from three LLMs (i.e., GPT-4, Llama-2, and Alpaca), with fine-grained evaluation and natural language rationales from experts. We assess the factuality of critical elements of RCTs in those summaries: Populations, Interventions, Comparators, Outcomes (PICO), as well as the reported findings concerning these. % We also % evaluate the correctness of the extra information (e.g., % explanations) added by LLMs. Using { FactPICO}, we benchmark a range of existing factuality metrics, including the newly devised ones based on LLMs. We find that plain language summarization of medical evidence is still challenging, especially when balancing between simplicity and factuality, and that existing metrics correlate poorly with expert judgments on the instance level. FactPICO and our code is available at .",Expert evaluation of a GPT-4 plain language summary in {\sc FactPICO}. We omitted the original abstract (can be found in Appendix~\ref{sec:abs_full_txt}) in this figure due to space limit. More examples in Appendix~\ref{app:factpicoexamples}.
BvSP: Broad-view Soft Prompting for Few-Shot Aspect Sentiment Quad Prediction,2406.07365v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.07365v1_0.pdf,"Aspect sentiment quad prediction (ASQP) aims to predict four aspect-based elements, including {aspect term}, {opinion term}, {aspect category}, and {sentiment polarity}. In practice, unseen aspects, due to distinct data distribution, impose many challenges for a trained neural model. Motivated by this, this work formulates ASQP into the few-shot scenario, which aims for fast adaptation in real applications. Therefore, we first construct a few-shot ASQP dataset ($$) that contains richer categories and is more balanced for the few-shot study. Moreover, recent methods extract quads through a generation paradigm, which involves converting the input sentence into a templated target sequence. However, they primarily focus on the utilization of a single template or the consideration of different template orders, thereby overlooking the correlations among various templates. To tackle this issue, we further propose a {Broad-view Soft Prompting} (BvSP) method that aggregates multiple templates with a broader view by taking into account the correlation between the different templates. Specifically, BvSP uses the pre-trained language model to select the most relevant k templates with Jensen–Shannon divergence. BvSP further introduces soft prompts to guide the pre-trained language model using the selected templates. Then, we aggregate the results of multi-templates by voting mechanism. % Then, we utilize a voting mechanism that considers predictions from the templates to aggregate the results of multi-templates. To aggregate the results of multi-templates, we utilize a voting mechanism that considers predictions from the templates. Empirical results demonstrate that BvSP significantly outperforms the state-of-the-art methods under four few-shot settings and other public datasets. Our code and dataset are available at .",A unseen aspect case is shown. The newly emerged category {`internet''} is not mentioned in the pre-defined set of aspect categories.
Safety Alignment in NLP Tasks: Weakly Aligned Summarization as an In-Context Attack,2312.06924v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2312.06924v2_0.pdf,"Recent developments in balancing the usefulness and safety of Large Language Models (LLMs) have raised a critical question: Are mainstream NLP tasks adequately aligned with safety consideration? Our study, focusing on safety-sensitive documents obtained through adversarial attacks, reveals significant disparities in the safety alignment of various NLP tasks. For instance, LLMs can effectively summarize malicious long documents but often refuse to translate them. This discrepancy highlights a previously unidentified vulnerability: attacks exploiting tasks with weaker safety alignment, like summarization, can potentially compromise the integrity of tasks traditionally deemed more robust, such as translation and question-answering (QA). Moreover, the concurrent use of multiple NLP tasks with lesser safety alignment increases the risk of LLMs inadvertently processing harmful content. We demonstrate these vulnerabilities in various safety-aligned LLMs, particularly Llama2 models, Gemini and GPT-4, indicating an urgent need for strengthening safety alignments across a broad spectrum of NLP tasks{https://github.com/FYYFU/SafetyAlignNLP}}. {Content warning: To demonstrate the vulnerability, examples provided include safety-sensitive ones with malicious/harmful content.}","When given a direct translation task, the Llama2-7B model detects harmful content and doesn't respond. But, if summarization precedes translation in an in-context attack, it then provides a translation. `[INST]' denotes input, and `[/INST]' the output. See Appendix \ref{case-appendix} for more examples."
Speech language models lack important brain-relevant semantics,2311.04664v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2311.04664v2_0.pdf,"Despite known differences between reading and listening in the brain, recent work has shown that text-based language models predict both text-evoked and speech-evoked brain activity to an impressive degree. This poses the question of what types of information language models truly predict in the brain. We investigate this question via a direct approach, in which we systematically remove specific low-level stimulus features (textual, speech, and visual) from language model representations to assess their impact on alignment with fMRI brain recordings during reading and listening. Comparing these findings with speech-based language models reveals starkly different effects of low-level features on brain alignment. While text-based models show reduced alignment in early sensory regions post-removal, they retain significant predictive power in late language regions. In contrast, speech-based models maintain strong alignment in early auditory regions even after feature removal but lose all predictive power in late language regions. These results suggest that speech-based models provide insights into additional information processed by early auditory regions, but caution is needed when using them to model processing in late language regions. We make our code publicly available.~}","Contrast of estimated cross-subject prediction accuracy for reading and listening for a representative subject (subject-8). \textcolor{cyan}{Blue} and \textcolor{red}{Red} voxels depict higher cross-subject prediction accuracy estimates during listening and reading, respectively. Voxels that have similar cross-subject prediction accuracy during reading and listening appear white, and are distributed across language regions. Here, middle frontal gyrus (MFG), inferior frontal gyrus (IFG), inferior frontal gyrus orbital (IFGOrb), angular gyrus (AG), and lateral temporal cortex (LTC) are late language regions, EVC denotes early visual cortex and AC denotes auditory cortex. Cross-subject prediction accuracy for other participants are reported in Appendix Figs.~\ref{fig:noise_ceiling_subject01} and ~\ref{fig:noise_ceiling_subject07}."
Towards Privacy-Aware Sign Language Translation at Scale,2402.09611v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.09611v2_0.pdf;/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.09611v2_1.pdf,"A major impediment to the advancement of sign language translation (SLT) is data scarcity. Much of the sign language data currently available on the web cannot be used for training supervised models due to the lack of aligned captions. Furthermore, scaling SLT using large-scale web-scraped datasets bears privacy risks due to the presence of biometric information, which the responsible development of SLT technologies should account for. In this work, we propose a two-stage framework for privacy-aware SLT at scale that addresses both of these issues. We introduce , which leverages self-supervised video pretraining on anonymized and unannotated videos, followed by supervised SLT finetuning on a curated parallel dataset. achieves state-of-the-art finetuned and zero-shot gloss-free SLT performance on the How2Sign dataset, outperforming the strongest respective baselines by over 3 BLEU-4. Based on controlled experiments, we further discuss the advantages and limitations of self-supervised pretraining and anonymization via facial obfuscation for SLT. [width=1.25em,height=1.25em]{github.png}{{ {}}}","Overview of our two-stage \method method. The first stage consists of training a SignHiera encoder via masked autoencoding (MAE) on {blurred} video frames. In the second stage, a pretrained T5 model is finetuned for SLT while the pretrained SignHiera is kept frozen (\includegraphics[width=0.9em]{figures/snowflake.pdf}). The input video in the second stage {can be unblurred}."
Arithmetic Control of LLMs for Diverse User Preferences: Directional Preference Alignment with Multi-Objective Rewards,2402.18571v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.18571v3_0.pdf,"% Fine-grained control over large language models (LLMs) remains a significant challenge, hindering their adaptability to diverse user needs. While Reinforcement Learning from Human Feedback (RLHF) shows promise in aligning LLMs, its reliance on {scalar} rewards often limits its ability to capture diverse user preferences in real-world applications. To address this limitation, we introduce the Directional Preference Alignment (DPA) framework. Unlike the scalar-reward RLHF, DPA incorporates {multi-objective reward} modeling to represent diverse preference profiles. Additionally, DPA models user preferences as {directions} (i.e., unit vectors) in the reward space to achieve user-dependent preference control. Our method involves training a multi-objective reward model and then fine-tuning the LLM with a preference-conditioned variant of Rejection Sampling Finetuning (RSF), an RLHF method adopted by Llama 2. This method enjoys a better performance trade-off across various reward objectives. In comparison with the scalar-reward RLHF, DPA offers users {intuitive control over LLM generation}: they can {arithmetically} specify their desired trade-offs (e.g., more helpfulness with less verbosity). We also validate the effectiveness of DPA with real-world alignment experiments on Mistral-7B. Our method provides straightforward arithmetic control over the trade-off between helpfulness and verbosity while maintaining competitive performance with strong baselines such as Direct Preference Optimization (DPO). The code and trained model are released at .","{Arithmetic Prompting} for Preference-Conditional Generalization: Comparison between conventional RLHF methods such as DPO and our Directional Preference Alignment (DPA). In the case of DPO (left), it is capable of generating helpful responses, but these tend to be excessively verbose. Conversely, with our DPA (right), it allows for {arithmetic control} of LLMs to meet various user preferences. For instance, setting the directional preference (unit vector) to $v=\left< 0.8, -0.6\right>$ leads to less verbose responses from our aligned LLM."
RAVEL: Evaluating Interpretability Methods on Disentangling Language Model Representations,2402.17700v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.17700v2_0.pdf,"Individual neurons participate in the representation of multiple high-level concepts. To what extent can different interpretability methods successfully disentangle these roles? To help address this question, we introduce {} (), a dataset that enables tightly controlled, quantitative comparisons between a variety of existing interpretability methods. We use the resulting conceptual framework to define the new method of Multi-task Distributed Alignment Search (MDAS), which allows us to find distributed representations satisfying multiple causal criteria. With -7B as the target language model, MDAS achieves state-of-the-art results on , demonstrating the importance of going beyond neuron-level analyses to identify features distributed across activations. We release our benchmark at .","An overview of the \ourdataset\ benchmark, which evaluates how well an interpretability method can find features that isolate the causal effect of individual attributes of an entity."
Large Language Models as Zero-shot Dialogue State Tracker through Function Calling,2402.10466v4,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.10466v4_0.pdf,"Large language models (LLMs) are increasingly prevalent in conversational systems due to their advanced understanding and generative capabilities in general contexts. However, their effectiveness in task-oriented dialogues (TOD), which requires not only response generation but also effective dialogue state tracking (DST) within specific tasks and domains, remains less satisfying. In this work, we propose a novel approach for solving DST with LLMs through function calling. This method improves zero-shot DST, allowing adaptation to diverse domains without extensive data collection or model tuning. Our experimental results demonstrate that our approach achieves exceptional performance with both modestly sized open-source and also proprietary LLMs: with in-context prompting it enables various 7B or 13B parameter models to surpass the previous state-of-the-art (SOTA) achieved by ChatGPT, and improves ChatGPT's performance beating the SOTA by 5.6\% average joint goal accuracy (JGA). Individual model results for GPT-3.5 and GPT-4 are boosted by 4.8\% and 14\%, respectively. We also show that by fine-tuning on a small collection of diverse task-oriented dialogues, we can equip modestly sized models, specifically a 13B parameter LLaMA2-Chat model, with function-calling capabilities and DST performance comparable to ChatGPT while maintaining their chat capabilities. We have made the code publicly available.}","{Zero-shot DST performance comparison} among (1) previous domain transfer approaches using small models; (2) previous prompting approaches exclusively relying on advanced proprietary LLMs; and (3) our approach, compatible with various LLMs, empowers various 7B and 13B models for superior performance and sets new state-of-the-art with GPT-4."
Enhancing Dialogue State Tracking Models through LLM-backed User-Agents Simulation,2405.13037v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2405.13037v1_0.pdf,"Dialogue State Tracking (DST) is designed to monitor the evolving dialogue state in the conversations and plays a pivotal role in developing task-oriented dialogue systems. However, obtaining the annotated data for the DST task is usually a costly endeavor. In this paper, we focus on employing LLMs to generate dialogue data to reduce dialogue collection and annotation costs. Specifically, GPT-4 is used to simulate the user and agent interaction, generating thousands of dialogues annotated with DST labels. Then a two-stage fine-tuning on LLaMA 2 is performed on the generated data and the real data for the DST prediction. Experimental results on two public DST benchmarks show that with the generated dialogue data, our model performs better than the baseline trained solely on real data. In addition, our approach is also capable of adapting to the dynamic demands in real-world scenarios, generating dialogues in new domains swiftly. After replacing dialogue segments in any domain with the corresponding generated ones, the model achieves comparable performance to the model trained on real data.","The simulation process of our approach. The blue boxes are intentions for the user and the agent, the `[RECOM]', `[EOF]', and `[EOD]' are control identifiers."
KnowCoder: Coding Structured Knowledge into LLMs for Universal Information Extraction,2403.07969v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2403.07969v2_0.pdf,"In this paper, we propose KnowCoder, a Large Language Model (LLM) to conduct Universal Information Extraction (UIE) via code generation. KnowCoder aims to develop a kind of unified schema representation that LLMs can easily understand and an effective learning framework that encourages LLMs to follow schemas and extract structured knowledge accurately. To achieve these, KnowCoder introduces a code-style schema representation method to uniformly transform different schemas into Python classes, with which complex schema information, such as constraints among tasks in UIE, can be captured in an LLM-friendly manner. We further construct a code-style schema library covering over ${30,000}$ types of knowledge, which is the largest one for UIE, to the best of our knowledge. To ease the learning process of LLMs, KnowCoder contains a two-phase learning framework that enhances its schema understanding ability via code pretraining and its schema following ability via instruction tuning. After code pretraining on around $1.5$B automatically constructed data, KnowCoder already attains remarkable generalization ability and achieves relative improvements by ${49.8\%}$ F1, compared to LLaMA2, under the few-shot setting. After instruction tuning, KnowCoder further exhibits strong generalization ability on unseen schemas and achieves up to ${12.5\%}$ and ${21.9\%}$, compared to sota baselines, under the zero-shot setting and the low resource setting, respectively. Additionally, based on our unified schema representations, various human-annotated datasets can simultaneously be utilized to refine KnowCoder, which achieves significant improvements up to ${7.5\%}$ under the supervised setting.",An illustration of KnowCoder schemas.
ERA-CoT: Improving Chain-of-Thought through Entity Relationship Analysis,2403.06932v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2403.06932v2_0.png,"Large language models (LLMs) have achieved commendable accomplishments in various natural language processing tasks. However, LLMs still encounter significant challenges when dealing with complex scenarios involving multiple entities. These challenges arise from the presence of implicit relationships that demand multi-step reasoning. In this paper, we propose a novel approach ERA-CoT, which aids LLMs in understanding context by capturing relationships between entities and supports the reasoning of diverse tasks through Chain-of-Thoughts (CoT). Experimental results show that ERA-CoT demonstrates the superior performance of our proposed method compared to current CoT prompting methods, achieving a significant improvement of an average of 5.1\% on GPT3.5 compared to previous SOTA baselines. Our analysis indicates that ERA-CoT increases the LLM's understanding of entity relationships, significantly improves the accuracy of question answering, and enhances the reasoning ability of LLMs.}","The top of the figure represents the standard prediction process. The bottom of the figure shows the five-step inference process of ERA-CoT, which relies on the extraction of entities and the inference and analysis of relationships between entities to obtain the results."
On the Multi-turn Instruction Following for Conversational Web Agents,2402.15057v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.15057v1_0.pdf,"Web agents powered by Large Language Models (LLMs) have demonstrated remarkable abilities in planning and executing multi-step interactions within complex web-based environments, fulfilling a wide range of web navigation tasks. Despite these advancements, the potential for LLM-powered agents to effectively engage with sequential user instructions in real-world scenarios has not been fully explored. In this work, we introduce a new task of Conversational Web Navigation, which necessitates sophisticated interactions that span multiple turns with both the users and the environment, supported by a specially developed dataset named Multi-Turn Mind2Web (MT-Mind2Web). To tackle the limited context length of LLMs and the context-dependency issue of the conversational tasks, we further propose a novel framework, named self-reflective memory-augmented planning (Self-MAP), which employs memory utilization and self-reflection techniques. Extensive experiments are conducted to benchmark the MT-Mind2Web dataset, and validate the effectiveness of the proposed method..}",Illustrations of different problems.
Mobile-Bench: An Evaluation Benchmark for LLM-based Mobile Agents,2407.00993v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2407.00993v1_0.pdf,"With the remarkable advancements of large language models (LLMs), LLM-based agents have become a research hotspot in human-computer interaction. However, there is a scarcity of benchmarks available for LLM-based mobile agents. Benchmarking these agents generally faces three main challenges: (1) The inefficiency of UI-only operations imposes limitations to task evaluation. (2) Specific instructions within a singular application lack adequacy for assessing the multi-dimensional reasoning and decision-making capacities of LLM mobile agents. (3) Current evaluation metrics are insufficient to accurately assess the process of sequential actions. To this end, we propose Mobile-Bench, a novel benchmark for evaluating the capabilities of LLM-based mobile agents. First, we expand conventional UI operations by incorporating 103 collected APIs to accelerate the efficiency of task completion. Subsequently, we collect evaluation data by combining real user queries with augmentation from LLMs. To better evaluate different levels of planning capabilities for mobile agents, our data is categorized into three distinct groups: SAST, SAMT, and MAMT, reflecting varying levels of task complexity. Mobile-Bench comprises 832 data entries, with more than 200 tasks specifically designed to evaluate multi-APP collaboration scenarios. Furthermore, we introduce a more accurate evaluation metric, named CheckPoint, to assess whether LLM-based mobile agents reach essential points during their planning and reasoning steps. Dataset and platform are available at {https://github.com/XiaoMi/MobileBench}.","For the task of ``{Setting an alarm for seven thirty.}'', accomplishing it solely through UI operations requires four steps, while API calls can achieve the same task in just one step."
Decoder-only Streaming Transformer for Simultaneous Translation,2406.03878v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.03878v1_0.PNG;/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.03878v1_1.pdf,"Simultaneous Machine Translation (SiMT) generates translation while reading source tokens, essentially producing the target prefix based on the source prefix. To achieve good performance, it leverages the relationship between source and target prefixes to exact a policy to guide the generation of translations. Although existing SiMT methods primarily focus on the Encoder-Decoder architecture, we explore the potential of Decoder-only architecture, owing to its superior performance in various tasks and its inherent compatibility with SiMT. However, directly applying the Decoder-only architecture to SiMT poses challenges in terms of training and inference. To alleviate the above problems, we propose the first Decoder-only SiMT model, named Decoder-only Streaming Transformer (DST). Specifically, DST separately encodes the positions of the source and target prefixes, ensuring that the position of the target prefix remains unaffected by the expansion of the source prefix. Furthermore, we propose a Streaming Self-Attention (SSA) mechanism tailored for the Decoder-only architecture. It is capable of obtaining translation policy by assessing the sufficiency of input source information and integrating with the soft-attention mechanism to generate translations. Experiments demonstrate that our approach achieves state-of-the-art performance on three translation tasks}.",Comparison of Encoder-Decoder architecture and Decoder-only architecture.
I am a Strange Dataset: Metalinguistic Tests for Language Models,2401.05300v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2401.05300v2_0.png,"Statements involving metalinguistic self-reference (``This paper has six sections.'')\ are prevalent in many domains. Can current large language models (LLMs) handle such language? In this paper, we present ``I am a Strange Dataset'', a new dataset for addressing this question. There are two subtasks: {generation} and {verification}. In generation, models continue statements like ``The penultimate word in this sentence is'' (where a correct continuation is ``is''). In verification, models judge the truth of statements like ``The penultimate word in this sentence is sentence.''\ (false). We also provide minimally different metalinguistic non-self-reference examples to complement the main dataset by probing for whether models can handle metalinguistic language at all. The dataset is hand-crafted by experts and validated by non-expert annotators. We test a variety of open-source LLMs (7B to 70B parameters) as well as closed-source LLMs through APIs. All models perform close to chance across both subtasks and even on the non-self-referential metalinguistic control data, though we find some steady improvement with model scale. GPT 4 is the only model to consistently do significantly better than chance, and it is still only in the 60\% range, while our untrained human annotators score well in the 89--93\% range. The dataset and evaluation toolkit are available at .",An example highlighting the challenge presented by our task. All models that we tested on our dataset are close to chance-level.
TruthX: Alleviating Hallucinations by Editing Large Language Models in Truthful Space,2402.17811v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.17811v2_0.pdf,"Large Language Models (LLMs) sometimes suffer from producing hallucinations, especially LLMs may generate untruthful responses despite knowing the correct knowledge. Activating the truthfulness within LLM is the key to fully unlocking LLM's knowledge potential. In this paper, we propose {TruthX}, an inference-time intervention method to activate the truthfulness of LLM by identifying and editing the features within LLM's internal representations that govern the truthfulness. TruthX employs an auto-encoder to map LLM's representations into semantic and truthful latent spaces respectively, and applies contrastive learning to identify a truthful editing direction within the truthful space. During inference, by editing LLM's internal representations in truthful space, TruthX effectively enhances the truthfulness of LLM. Experiments show that TruthX improves the truthfulness of 13 advanced LLMs by an average of 20\% on TruthfulQA benchmark. Further analyses suggest that TruthX can control LLM to produce truthful or hallucinatory responses via editing only one vector in LLM's internal representations\ Llama-2-7B-Chat model with baked-in TruthX: \ Page: {https://ictnlp.github.io/TruthX-site/}}.",A case to show that TruthX can control LLM to generate truthful or hallucinatory coherent responses via editing one vector in LLM's internal representations.
Investigating Multi-Hop Factual Shortcuts in Knowledge Editing of Large Language Models,2402.11900v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.11900v2_0.pdf,"Recent work has showcased the powerful capability of large language models (LLMs) in recalling knowledge and reasoning. However, the reliability of LLMs in combining these two capabilities into reasoning through multi-hop facts has not been widely explored. This paper systematically investigates the possibilities for LLMs to utilize shortcuts based on direct connections between the initial and terminal entities of multi-hop knowledge. We first explore the existence of factual shortcuts through Knowledge Neurons, revealing that: (i) the strength of factual shortcuts is highly correlated with the frequency of co-occurrence of initial and terminal entities in the pre-training corpora; (ii) few-shot prompting leverage more shortcuts in answering multi-hop questions compared to chain-of-thought prompting. Then, we analyze the risks posed by factual shortcuts from the perspective of multi-hop knowledge editing. Analysis shows that approximately 20\% of the failures are attributed to shortcuts, and the initial and terminal entities in these failure instances usually have higher co-occurrences in the pre-training corpus. Finally, we propose erasing shortcut neurons to mitigate the associated risks and find that this approach significantly reduces failures in multiple-hop knowledge editing caused by shortcuts. Code is publicly available at {https://github.com/Jometeorie/MultiHopShortcuts}.","An illustrative example of a multi-hop factual shortcut in LLMs. The LLM may have directly encoded multi-hop knowledge (red) during the pre-training phase, which results in inconsistencies after a single-hop knowledge editing."
Revisiting Demonstration Selection Strategies in In-Context Learning,2401.12087v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2401.12087v2_0.pdf,"Large language models (LLMs) have shown an impressive ability to perform a wide range of tasks using in-context learning (ICL), where a few examples are used to describe a task to the model. However, the performance of ICL varies significantly with the choice of demonstrations, and previous research usually focuses on the data aspect ignoring the model's effect. In this work, we first revisit the factors contributing to this variance from {the model aspect, and find that the demonstration choice is both data- and model-dependent. We further propose a conjecture that {the performance of a demonstration positively correlates with its contribution to the model's understanding of the test samples}, and accordingly propose a data- and model-dependent demonstration selection method, {TopK + ConE}.} Empirically, our method yields consistent improvements in both language understanding and generation tasks with different model scales. Further analyses confirm that, besides the generality and stability under different circumstances, our method provides a unified explanation for the effectiveness of previous methods. Code is publicly available at .",{The different 8-shot performance of data-dependent methods (BM25 and TopK) and Our methods} in SST-2. The colour in the number represents the relative performance between BM25 and TopK. We see that: 1) The data-dependent methods can not obtain optimal demonstrations under different models; 2) Our data- and model-dependent methods can achieve consistent improvement across different models.
Multimodal Table Understanding,2406.08100v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.08100v1_0.pdf,"Although great progress has been made by previous table understanding methods including recent approaches based on large language models (LLMs), they rely heavily on the premise that given tables must be converted into a certain text sequence (such as Markdown or HTML) to serve as model input. However, it is difficult to access such high-quality textual table representations in some real-world scenarios, and table images are much more accessible. Therefore, how to directly understand tables using intuitive visual information is a crucial and urgent challenge for developing more practical applications. In this paper, we propose a new problem, multimodal table understanding, where the model needs to generate correct responses to various table-related requests based on the given table image. To facilitate both the model training and evaluation, we construct a large-scale dataset named MMTab, which covers a wide spectrum of table images, instructions and tasks. On this basis, we develop Table-LLaVA, a generalist tabular multimodal large language model (MLLM), which significantly outperforms recent open-source MLLM baselines on 23 benchmarks under held-in and held-out settings. The code and data is available at .$ Indicates equal contribution.} $This work was done during an internship at Baidu Inc.} $ Corresponding author: Zheng Lin.}",An overall performance comparison of Table-LLaVA 7B and existing MLLMs on various multimodal table understanding benchmarks. Table-LLaVA outperforms recent open-source MLLMs and is even competitive with the powerful GPT-4V on most tasks.
WatME: Towards Lossless Watermarking Through Lexical Redundancy,2311.09832v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2311.09832v3_0.png,"Text watermarking has emerged as a pivotal technique for identifying machine-generated text. However, existing methods often rely on arbitrary vocabulary partitioning during decoding to embed watermarks, which compromises the availability of suitable tokens and significantly degrades the quality of responses. This study assesses the impact of watermarking on different capabilities of large language models (LLMs) from a cognitive science lens. Our finding highlights a significant disparity; knowledge recall and logical reasoning are more adversely affected than language generation. These results suggest a more profound effect of watermarking on LLMs than previously understood. To address these challenges, we introduce Watermarking with Mutual Exclusion (WatME), a novel approach leveraging linguistic prior knowledge of inherent lexical redundancy in LLM vocabularies to seamlessly integrate watermarks. Specifically, WatME dynamically optimizes token usage during the decoding process by applying a mutually exclusive rule to the identified lexical redundancies. This strategy effectively prevents the unavailability of appropriate tokens and preserves the expressive power of LLMs. We provide both theoretical analysis and empirical evidence showing that WatME effectively preserves the diverse capabilities of LLMs while ensuring watermark detectability. Our code will be released to facilitate future research. via .","An illustration of WatME's advantage for lossless watermarking. The left panel depicts a vanilla LM with all words available during generation. The middle panel exposes the flaw in vanilla watermarking, which may assign all suitable tokens (e.g., 'ocean' and 'sea') to the red list, diminishing text quality. The right panel underlines how WatME exploits lexical redundancy by applying a mutual exclusion rule between such words, ensuring at least one suitable word remains on the green list, thereby improving text quality."
Multi-Aspect Controllable Text Generation with Disentangled Counterfactual Augmentation,2405.19958v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2405.19958v1_0.pdf,"Multi-aspect controllable text generation aims to control the generated texts in attributes from multiple aspects (e.g., ``positive'' from {sentiment} and ``sport'' from {topic}). For ease of obtaining training samples, existing works neglect attribute correlations formed by the intertwining of different attributes. Particularly, the stereotype formed by imbalanced attribute correlations significantly affects multi-aspect control. In this paper, we propose , a new {m}ulti-{a}spect controllable text {g}eneration method with d{i}sentangled {c}ounterfactual augmentation. We alleviate the issue of imbalanced attribute correlations during training using counterfactual feature vectors in the attribute latent space by disentanglement. During inference, we enhance attribute correlations by target-guided counterfactual augmentation to further improve multi-aspect control. Experiments show that outperforms state-of-the-art baselines in both imbalanced and balanced attribute correlation scenarios. Our source code and data are available at .",The relevance scores of positive and negative sentiment in (a) AGNews and (b) Yelp. (a) The classifiers used for statistics are from \cite{discrete2022Gu}. (b) The statistical data of Yelp are from \cite{tailor2023Yang}.
Reward-based Input Construction for Cross-document Relation Extraction,2405.20649v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2405.20649v1_0.pdf,"Relation extraction (RE) is a fundamental task in natural language processing, aiming to identify relations between target entities in text. While many RE methods are designed for a single sentence or document, cross-document RE has emerged to address relations across multiple long documents. Given the nature of long documents in cross-document RE, extracting document embeddings is challenging due to the length constraints of pre-trained language models. Therefore, we propose REward-based Input Construction (REIC), the first learning-based sentence selector for cross-document RE. REIC extracts sentences based on relational evidence, enabling the RE module to effectively infer relations. Since supervision of evidence sentences is generally unavailable, we train REIC using reinforcement learning with RE prediction scores as rewards. Experimental results demonstrate the superiority of our method over heuristic methods for different RE structures and backbones in cross-document RE. Our code is publicly available at .","An illustrated comparison between Snippet and selected sentences using our REward-based Input Construction (REIC) for cross-document relation extraction. The figure depicts an example triplet (\texttt{Kubuntu}, \texttt{x86-64}, platform) with the text path (`Mir (software)', `X86-64'), including three bridge entities ({Ubuntu}, {Linux}, {Intel}) abbreviated as (Bridge1, Bridge2, Bridge3). Dash and solid arrows signify the selection process of Snippet and REIC, respectively, while gradient-colored arrows indicate connections between the head and tail entities. REIC selects important sentences from any position within a path to determine the relation between the head and tail entity, whereas Snippet only includes sentences located around the head or tail entity."
Semi-Supervised Spoken Language Glossification,2406.08173v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.08173v1_0.pdf,"Spoken language glossification (SLG) aims to translate the spoken language text into the sign language gloss, {i.e.}, a written record of sign language. In this work, we present a framework named $S$emi-$S$upervised $S$poken $L$anguage $G$lossification ($S^3$LG) for SLG. To tackle the bottleneck of limited parallel data in SLG, our $S^3$LG incorporates large-scale monolingual spoken language text into SLG training. The proposed framework follows the self-training structure that iteratively annotates and learns from pseudo labels. Considering the lexical similarity and syntactic difference between sign language and spoken language, our $S^3$LG adopts both the rule-based heuristic and model-based approach for auto-annotation. During training, we randomly mix these complementary synthetic datasets and mark their differences with a special token. As the synthetic data may be less quality, the $S^3$LG further leverages consistency regularization to reduce the negative impact of noise in the synthetic data. Extensive experiments are conducted on public benchmarks to demonstrate the effectiveness of the $S^3$LG. Our code is available at .","Overview of the proposed $S^3$LG. It iteratively annotates and learns from pseudo labels to obtain the final SLG model $f(\hat{\theta}^K)$ after total $K$ iterations. For the $k$-th iteration, the synthetic data $\mathcal{D}_U^{k-1}$ is conducted by randomly mixing the two complementary pseudo glosses generated by the fixed rules and the previously obtained SLG model $f(\hat{\theta}^{k-1})$, respectively. Note that at the first iteration, only the rule-based synthetic data is available."
SeeClick: Harnessing GUI Grounding for Advanced Visual GUI Agents,2401.10935v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2401.10935v2_0.pdf,"Graphical User Interface (GUI) agents are designed to automate complex tasks on digital devices, such as smartphones and desktops. Most existing GUI agents interact with the environment through extracted structured data, which can be notably lengthy (e.g., HTML) and occasionally inaccessible (e.g., on desktops). To alleviate this issue, we propose a novel visual GUI agent -- {SeeClick}, which only relies on screenshots for task automation. In our preliminary study, we have discovered a key challenge in developing visual GUI agents: GUI grounding -- the capacity to accurately locate screen elements based on instructions. To tackle this challenge, we propose to enhance {SeeClick} with GUI grounding pre-training and devise a method to automate the curation of GUI grounding data. Along with the efforts above, we have also created , the first realistic GUI grounding benchmark that encompasses mobile, desktop, and web environments. After pre-training, {SeeClick} demonstrates significant improvement in over various baselines. Moreover, comprehensive evaluations on three widely used benchmarks consistently support our finding that advancements in GUI grounding directly correlate with enhanced performance in downstream GUI agent tasks. .}","Text-based agents select target elements from structured texts, occasionally augmented with screenshots. {SeeClick} employs a vision-based methodology to predict action locations solely relying on screenshots."
Whose Preferences? Differences in Fairness Preferences and Their Impact on the Fairness of AI Utilizing Human Feedback,2406.05902v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.05902v1_0.pdf,"There is a growing body of work on learning from human feedback to align various aspects of machine learning systems with human values and preferences. We consider the setting of fairness in content moderation, in which human feedback is used to determine how two comments --- referencing different sensitive attribute groups --- should be treated in comparison to one another. With a novel dataset collected from Prolific and MTurk, we find significant gaps in fairness preferences depending on the race, age, political stance, educational level, and LGBTQ+ identity of annotators. We also demonstrate that demographics mentioned in text have a strong influence on how users perceive individual fairness in moderation. Further, we find that differences also exist in downstream classifiers trained to predict human preferences. Finally, we observe that an ensemble, giving equal weight to classifiers trained on annotations from different demographics, performs better for different demographic intersections; compared to a single classifier that gives equal weight to each annotation. {{Warning: This paper discusses examples of content that may be offensive or disturbing.}}","Example tasks in our survey, asking people about their fairness preferences and their guess of the average American answer. Each task contains a pair of sentences; sentences in a pair differ along a sensitive attribute such as religion, gender, etc."
Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations,2312.08935v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2312.08935v3_0.pdf;/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2312.08935v3_1.pdf,"In this paper, we present an innovative process-oriented math process reward model called {}, which assigns a reward score to each step of math problem solutions. The training of is achieved using automatically constructed process-wise supervision data, breaking the bottleneck of heavy reliance on manual annotation in existing work. We explore the effectiveness of in two scenarios: 1) {Verification}: is utilized for reranking multiple outputs generated by Large Language Models (LLMs); 2) {Reinforcement Learning}: is employed to reinforce LLMs with step-by-step Proximal Policy Optimization (PPO). With , a series of open-source LLMs demonstrates exceptional performance. For instance, the step-by-step PPO with significantly improves the accuracy of Mistral-7B (77.9\%$$84.1\% on GSM8K and 28.6\%$$33.0\% on MATH). The accuracy can be further enhanced to 89.1\% and 43.5\% on GSM8K and MATH with the verification of , respectively. We believe that automatic process supervision holds significant potential for the future evolution of LLMs.",We evaluate the performance of various LLMs with \methodname on the GSM8K and MATH datasets. All base models are finetuned with the MetaMath dataset \citep{yu2023metamath}. The +SHEPHERD results are obtained by selecting the best one from 256 candidates using \methodname. We observe that \methodname is compatible with different LLMs. The results of GPT-4 (early) are from \cite{bubeck2023sparks}.
Fine-Grained Image-Text Alignment in Medical Imaging Enables Explainable Cyclic Image-Report Generation,2312.08078v5,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2312.08078v5_0.pdf,"Fine-grained vision-language models (VLM) have been widely used for inter-modality local alignment between the predefined fixed patches and textual words. However, in medical analysis, lesions exhibit varying sizes and positions, and using fixed patches may cause incomplete representations of lesions. Moreover, these methods provide explainability by using heatmaps to show the general image areas potentially associated with texts rather than specific regions, making their explanations not explicit and specific enough. To address these issues, we propose a novel Adaptive patch-word Matching (AdaMatch) model to correlate chest X-ray (CXR) image regions with words in medical reports and apply it to CXR-report generation to provide explainability for the generation process. Aiming to provide explicit explainability for the CXR-report generation task, we propose an AdaMatch-based bidirectional LLM for Cyclic CXR-report generation (AdaMatch-Cyclic). It employs AdaMatch to obtain the keywords for CXR images and `keypatches' for medical reports as hints to guide CXR-report generation. Extensive experiments on two publicly available CXR datasets validate the effectiveness of our method and its superior performance over existing methods. %Source code is available at https://github.com/CUHK-AIM-Group/AdaMatch-Cyclic.","Current vision-language models (VLM) achieve (a) global alignment and (b) local alignment by matching overall visual with textual features, and aligning patches with word features, respectively. (c) To exploit the relation between textual words and abnormal patches with varied sizes, our AdaMatch obtains adaptive patch features and aligns them with word features."
Are LLM-based Evaluators Confusing NLG Quality Criteria?,2402.12055v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.12055v2_0.pdf,"Some prior work has shown that LLMs perform well in NLG evaluation for different tasks. However, we discover that LLMs seem to confuse different evaluation criteria, which reduces their reliability. For further verification, we first consider avoiding issues of inconsistent conceptualization and vague expression in existing NLG quality criteria themselves. So we summarize a clear hierarchical classification system for 11 common aspects with corresponding different criteria from previous studies involved. Inspired by behavioral testing, we elaborately design 18 types of aspect-targeted perturbation attacks for fine-grained analysis of the evaluation behaviors of different LLMs. We also conduct human annotations beyond the guidance of the classification system to validate the impact of the perturbations. Our experimental results reveal confusion issues inherent in LLMs, as well as other noteworthy phenomena, and necessitate further research and improvements for LLM-based evaluation.",An example of prompting LLMs to evaluate the dialogue summarization on criteria for fluency.
Linear Transformers with Learnable Kernel Functions are Better In-Context Models,2402.10644v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.10644v2_0.pdf,"Advancing the frontier of subquadratic architectures for Language Models (LMs) is crucial in the rapidly evolving field of natural language processing. Current innovations, including State Space Models, were initially celebrated for surpassing Transformer performance on language modeling tasks. However, these models have revealed deficiencies in essential In-Context Learning capabilities -- a domain where the Transformer traditionally shines. The Based model emerged as a hybrid solution, blending a Linear Transformer with a kernel inspired by the Taylor expansion of exponential functions, augmented by convolutional networks. Mirroring the Transformer's in-context adeptness, it became a strong contender in the field. In our work, we present a singular, elegant alteration to the Based kernel that amplifies its In-Context Learning abilities evaluated with the Multi-Query Associative Recall task and overall language modeling process, as demonstrated on the Pile dataset.","Results on the MQAR dataset, designed to measure In-Context Learning capabilities of an architecture \citet{mqar}. ReBased outperforms all baselines except Attention across different sequence lengths and model sizes. See Section \ref{sec:mqar} for more details."
AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling,2402.12226v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.12226v3_0.pdf,"We introduce AnyGPT, an any-to-any multimodal language model that utilizes discrete representations for the unified processing of various modalities, including speech, text, images, and music. AnyGPT can be trained stably without any alterations to the current large language model (LLM) architecture or training paradigms. Instead, it relies exclusively on data-level preprocessing, facilitating the seamless integration of new modalities into LLMs, akin to the incorporation of new languages. We build a multimodal text-centric dataset for multimodal alignment pre-training. Utilizing generative models, we synthesize the first large-scale any-to-any multimodal instruction dataset. It consists of 108k samples of multi-turn conversations that intricately interweave various modalities, thus equipping the model to handle arbitrary combinations of multimodal inputs and outputs. Experimental results demonstrate that AnyGPT is capable of facilitating any-to-any multimodal conversation while achieving performance comparable to specialized models across all modalities, proving that discrete representations can effectively and conveniently unify multiple modalities within a language model. Demos are shown in {https://junzhan2000.github.io/AnyGPT.github.io/}.","An overview of the AnyGPT model architecture. All modalities are tokenized into discrete tokens, upon which the LLM performs multimodal understanding and generation autoregressively. Only data pre-processing and post-processing are required, with the model's architecture and training objectives remaining unaltered."
CofiPara: A Coarse-to-fine Paradigm for Multimodal Sarcasm Target Identification with Large Multimodal Models,2405.00390v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2405.00390v2_0.pdf;/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2405.00390v2_1.pdf,"Social media abounds with multimodal sarcasm, and identifying sarcasm targets is particularly challenging due to the implicit incongruity not directly evident in the text and image modalities. Current methods for Multimodal Sarcasm Target Identification (MSTI) predominantly focus on superficial indicators in an end-to-end manner, overlooking the nuanced understanding of multimodal sarcasm conveyed through both the text and image. This paper proposes a versatile MSTI framework with a coarse-to-fine paradigm, by augmenting sarcasm explainability with reasoning and pre-training knowledge. Inspired by the powerful capacity of Large Multimodal Models (LMMs) on multimodal reasoning, we first engage LMMs to generate competing rationales for coarser-grained pre-training of a small language model on multimodal sarcasm detection. We then propose fine-tuning the model for finer-grained sarcasm target identification. Our framework is thus empowered to adeptly unveil the intricate targets within multimodal sarcasm and mitigate the negative impact posed by potential noise inherently in LMMs. Experimental results demonstrate that our model far outperforms state-of-the-art MSTI methods, and markedly exhibits explainability in deciphering sarcasm as well.",Examples of multimodal sarcasm on {Twitter}: ({a}) ``{never seen a \#\textcolor{red}{dlr train driver} before. looks like a tough job \#london}''; ({b}) ``{thank god for no product placement in \#ukraine \#eurovision}''. Boxes in \textcolor{green(ncs)}{green} and words in \textcolor{red}{red} denote the visual and textual targets.
Robust Singing Voice Transcription Serves Synthesis,2405.09940v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2405.09940v2_0.pdf,"Note-level Automatic Singing Voice Transcription (AST) converts singing recordings into note sequences, facilitating the automatic annotation of singing datasets for Singing Voice Synthesis (SVS) applications. Current AST methods, however, struggle with accuracy and robustness when used for practical annotation. This paper presents ROSVOT, the first robust AST model that serves SVS, incorporating a multi-scale framework that effectively captures coarse-grained note information and ensures fine-grained frame-level segmentation, coupled with an attention-based pitch decoder for reliable pitch prediction. We also established a comprehensive annotation-and-training pipeline for SVS to test the model in real-world settings. Experimental findings reveal that ROSVOT achieves state-of-the-art transcription accuracy with either clean or noisy inputs. Moreover, when trained on enlarged, automatically annotated datasets, the SVS model outperforms its baseline, affirming the capability for practical application. Audio samples are available at {https://rosvot.github.io}. Codes can be found at {https://github.com/RickyL-2000/ROSVOT}.",AST and ASR systems serve SVS.
BadAgent: Inserting and Activating Backdoor Attacks in LLM Agents,2406.03007v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.03007v1_0.png,"With the prosperity of large language models (LLMs), powerful LLM-based intelligent agents have been developed to provide customized services with a set of user-defined tools. State-of-the-art methods for constructing LLM agents adopt trained LLMs and further fine-tune them on data for the agent task. However, we show that such methods are vulnerable to our proposed backdoor attacks named BadAgent on various agent tasks, where a backdoor can be embedded by fine-tuning on the backdoor data. At test time, the attacker can manipulate the deployed LLM agents to execute harmful operations by showing the trigger in the agent input or environment. To our surprise, our proposed attack methods are extremely robust even after fine-tuning on trustworthy data. Though backdoor attacks have been studied extensively in natural language processing, to the best of our knowledge, we could be the first to study them on LLM agents that are more dangerous due to the permission to use external tools. Our work demonstrates the clear risk of constructing LLM agents based on untrusted LLMs or data. Our code is public at","Normal LLM agents leverage the capabilities of LLMs to effectively complete specific tasks. However, after inserting backdoors into LLM agents, although they may normally perform regular tasks, once a trigger is activated, LLM agents will execute corresponding covert operations as required by the attacker."
DetermLR: Augmenting LLM-based Logical Reasoning from Indeterminacy to Determinacy,2310.18659v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2310.18659v2_0.pdf,"Recent advances in large language models (LLMs) have revolutionized the landscape of reasoning tasks. To enhance the capabilities of LLMs to emulate human reasoning, prior studies have focused on modeling reasoning steps using various thought structures like chains, trees, or graphs. However, LLM-based reasoning still encounters the following challenges: (1) Limited adaptability of preset structures to diverse tasks; (2) Insufficient precision in exploiting known conditions to derive new ones; and (3) Inadequate consideration of historical reasoning experiences for subsequent reasoning steps. To this end, we propose DetermLR, a novel perspective that rethinks the reasoning process as an evolution from indeterminacy to determinacy. First, we categorize known conditions into two types: determinate and indeterminate premises This provides an oveall direction for the reasoning process and guides LLMs in converting indeterminate data into progressively determinate insights. Subsequently, we leverage quantitative measurements to prioritize more relevant premises to explore new insights. Furthermore, we automate the storage and extraction of available premises and reasoning paths with reasoning memory, preserving historical reasoning details for subsequent reasoning steps. Comprehensive experimental results demonstrate that DetermLR surpasses all baselines on various logical reasoning benchmarks: LogiQA, ProofWriter, FOLIO, PrOntoQA, and LogicalDeduction. Compared to previous multi-step reasoning methods, DetermLR achieves higher accuracy with fewer reasoning steps, highlighting its superior efficiency and effectiveness in solving logical reasoning tasks.","The overview of DetermLR: (a) premise identification; (b) iterative reasoning process: (b-1) premise prioritization and exploration and (b-2) reasoning memorization. Green elements represent determinate premises, and blue elements represent indeterminate premises. The proportion of blue decreases with the accumulation of green during iterative reasoning."
To Generate or to Retrieve? On the Effectiveness of Artificial Contexts for Medical Open-Domain Question Answering,2403.01924v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2403.01924v2_0.pdf,"Medical open-domain question answering demands substantial access to specialized knowledge. Recent efforts have sought to decouple knowledge from model parameters, counteracting architectural scaling and allowing for training on common low-resource hardware. The retrieve-then-read paradigm has become ubiquitous, with model predictions grounded on relevant knowledge pieces from external repositories such as PubMed, textbooks, and UMLS. An alternative path, still under-explored but made possible by the advent of domain-specific large language models, entails constructing artificial contexts through prompting. As a result, ""to generate or to retrieve"" is the modern equivalent of Hamlet's dilemma. This paper presents , the first generate-then-read framework for multiple-choice question answering in medicine. We conduct extensive experiments on MedQA-USMLE, MedMCQA, and MMLU, incorporating a practical perspective by assuming a maximum of 24GB VRAM. sets a new state-of-the-art in the open-book setting of each testbed, allowing a small-scale reader to outcompete zero-shot closed-book 175B baselines while using up to 706$$ fewer parameters. Our findings reveal that generated passages are more effective than retrieved ones in attaining higher accuracy.{https://github.com/unibo-nlp/medgenie}.}","\textsc{MedGENIE} performance (Flan-T5-base, Fusion-In-Decoder) on USMLE-style questions. Comparison against fine-tuned open-source baselines with a maximum of 10B parameters, using the MedQA (4 options) test set. Model size displayed on a log scale."
SC2: Towards Enhancing Content Preservation and Style Consistency in Long Text Style Transfer,2406.04578v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.04578v1_0.pdf,"Text style transfer (TST) aims to vary the style polarity of text while preserving the semantic content. Although recent advancements have demonstrated remarkable progress in short TST, it remains a relatively straightforward task with limited practical applications. The more comprehensive long TST task presents two challenges: (1) existing methods encounter difficulties in accurately evaluating content attributes in multiple words, leading to content degradation; (2) the conventional vanilla style classifier loss encounters obstacles in maintaining consistent style across multiple generated sentences. In this paper, we propose a novel method SC2, where a multilayer Joint {S}tyle-{C}ontent Weighed (JSCW) module and a {S}tyle {C}onsistency loss are designed to address the two issues. The JSCW simultaneously assesses the amounts of style and content attributes within a token, aiming to acquire a lossless content representation and thereby enhancing content preservation. The multiple JSCW layers further progressively refine content representations. We design a style consistency loss to ensure the generated multiple sentences consistently reflect the target style polarity. Moreover, we incorporate a denoising non-autoregressive decoder to accelerate the training. We conduct plentiful experiments and the results show significant improvements of SC2 over competitive baselines. Our code: .",Comparisons of content learning between existing approaches and the proposed method: (a) evaluating the relevance between text $x$ and its style; (b) evaluating the relevance between source text $x_s$ and target text $x_t$; and (c) joint evaluating the relevance between text $x$ and its style as well as content.
NewsBench: A Systematic Evaluation Framework for Assessing Editorial Capabilities of Large Language Models in Chinese Journalism,2403.00862v4,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2403.00862v4_0.pdf,"We present NewsBench, a novel evaluation framework to systematically assess the capabilities of Large Language Models (LLMs) for editorial capabilities in Chinese journalism. Our constructed benchmark dataset is focused on four facets of writing proficiency and six facets of safety adherence, and it comprises manually and carefully designed 1,267 test samples in the types of multiple choice questions and short answer questions for five editorial tasks in 24 news domains. To measure performances, we propose different GPT-4 based automatic evaluation protocols to assess LLM generations for short answer questions in terms of writing proficiency and safety adherence, and both are validated by the high correlations with human evaluations. Based on the systematic evaluation framework, we conduct a comprehensive analysis of eleven popular LLMs which can handle Chinese. The experimental results highlight GPT-4 and ERNIE Bot as top performers, yet reveal a relative deficiency in journalistic safety adherence in creative writing tasks. Our findings also underscore the need for enhanced ethical guidance in machine-generated journalistic content, marking a step forward in aligning LLMs with journalistic standards and safety considerations. The evaluation framework and experimental results are expected to provide an in-depth understanding of the editorial capabilities of LLMs and speed up the development of LLMs in journalism..\\ }","The key components and processes to evaluate editorial capabilities of an LLM with our evaluation framework, NewsBench. The numbers inside the brackets indicate the number of test samples that we construct for each group of evaluations. The bold border boxes are the overall scores for Short Answer Questions (SAQs) and Multiple Choice Questions (MCQs) on Safety Adherence (SA) and Journalistic Writing Proficiency (JWP), respectively."
Enhancing Noise Robustness of Retrieval-Augmented Language Models with Adaptive Adversarial Training,2405.20978v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2405.20978v1_0.pdf,"Large Language Models (LLMs) exhibit substantial capabilities yet encounter challenges, including hallucination, outdated knowledge, and untraceable reasoning processes. Retrieval-augmented generation (RAG) has emerged as a promising solution, integrating knowledge from external databases to mitigate these challenges. However, inappropriate retrieved passages can potentially hinder the LLMs' capacity to generate comprehensive and high-quality responses. Prior RAG studies on the robustness of retrieval noises often confine themselves to a limited set of noise types, deviating from real-world retrieval environments and limiting practical applicability. In this study, we initially investigate retrieval noises and categorize them into three distinct types, reflecting real-world environments. We analyze the impact of these various retrieval noises on the robustness of LLMs. Subsequently, we propose a novel RAG approach known as Retrieval-augmented Adaptive Adversarial Training (RAAT). RAAT leverages adaptive adversarial training to dynamically adjust the model's training process in response to retrieval noises. Concurrently, it employs multi-task learning to ensure the model's capacity to internally recognize noisy contexts. Extensive experiments demonstrate that the LLaMA-2 7B model trained using RAAT exhibits significant improvements in F1 and EM scores under diverse noise conditions. For reproducibility, we release our code and data at: .","An illustrative example of the RAG process applied to question answering. The model predicts the correct answer with accurate retrieved text. However, it fails to produce the right answer when the retrieved text contains misleading or inaccurate information."
On the Impact of Calibration Data in Post-training Quantization and Pruning,2311.09755v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2311.09755v2_0.pdf,"Quantization and pruning form the foundation of compression for neural networks, enabling efficient inference for large language models (LLMs). Recently, various quantization and pruning techniques have demonstrated remarkable performance in a post-training setting. They rely upon calibration data, a small set of unlabeled examples that are used to generate layer activations. However, no prior work has systematically investigated how the calibration data impacts the effectiveness of model compression methods. In this paper, we present the first extensive empirical study on the effect of calibration data upon LLM performance. We trial a variety of quantization and pruning methods, datasets, tasks, and models. Surprisingly, we find substantial variations in downstream task performance, contrasting existing work that suggests a greater level of robustness to the calibration data. Finally, we make a series of recommendations for the effective use of calibration data in LLM quantization and pruning.}",Post-training compression methods rely upon calibration data to generate layer activations.
Meta-Task Prompting Elicits Embeddings from Large Language Models,2402.18458v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.18458v2_0.png,"We introduce a new unsupervised text embedding method, Meta-Task Prompting with Explicit One-Word Limitation (), for generating high-quality sentence embeddings from Large Language Models (LLMs) without the need for model fine-tuning. Leveraging meta-task prompting, guides LLMs to produce embeddings through a series of carefully designed prompts that address multiple representational aspects. Our comprehensive experiments demonstrate that embeddings averaged from various meta-tasks are versatile embeddings that yield competitive performance on Semantic Textual Similarity (STS) benchmarks and excel in downstream tasks, surpassing contrastive-trained models. Our findings suggest a new scaling law, offering a versatile and resource-efficient approach for embedding generation across diverse scenarios..}","The highest decoding probabilities are largely allocated to stop words that carry little useful information when conducting a meaning compression prompting, even if employing a constraint of ""{in one word}"" following~\citet{jiang2023scaling}. Although the general semantic, \textcolor[RGB]{235,50,35}{movie}, is contained, other aspects of this sentence are missing, like sentiments."
A Sentiment Consolidation Framework for Meta-Review Generation,2402.18005v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.18005v2_0.pdf,"Modern natural language generation systems with Large Language Models (LLMs) exhibit the capability to generate a plausible summary of multiple documents; however, it is uncertain if they truly possess the capability of information consolidation to generate summaries, especially on documents with opinionated information. We focus on meta-review generation, a form of sentiment summarisation for the scientific domain. To make scientific sentiment summarization more grounded, we hypothesize that human meta-reviewers follow a three-layer framework of sentiment consolidation to write meta-reviews. Based on the framework, we propose novel prompting methods for LLMs to generate meta-reviews and evaluation metrics to assess the quality of generated meta-reviews. Our framework is validated empirically as we find that prompting LLMs based on the framework --- compared with prompting them with simple instructions --- generates better meta-reviews..}","The three-layer framework of the underlying information consolidation logic in meta-reviewing ($P$: Positive, $P^+$: Strongly positive, $N$: Negative, $N^+$: Strongly negative)."
MuggleMath: Assessing the Impact of Query and Response Augmentation on Math Reasoning,2310.05506v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2310.05506v3_0.png,"In math reasoning with large language models (LLMs), fine-tuning data augmentation by query evolution and diverse reasoning paths is empirically verified effective, profoundly narrowing the gap between open-sourced LLMs and cutting-edge proprietary LLMs. In this paper, we conduct an investigation for such data augmentation in math reasoning and are intended to answer: (1) What strategies of data augmentation are more effective; (2) What is the scaling relationship between the amount of augmented data and model performance; and (3) Can data augmentation incentivize generalization to out-of-domain mathematical reasoning tasks? To this end, we create two new dataset AugGSM8K and AugMATH, by complicating and diversifying the queries and sampling multiple reasoning paths from GSM8K and MATH. We obtained a series of LLMs called MuggleMath by fine-tuning LLaMA models on AugGSM8K and AugMATH. MuggleMath substantially achieves new state-of-the-art on GSM8K and MATH. A log-linear relationship and a segmented log-linear are presented between MuggleMath's performance and the amount of augmented data on GSM8K and MATH, respectively. We also find that it is weak in out-of-domain math reasoning generalization from AugGSM8K to MATH and from AugMATH to GSM8K, which suggests that augmenting queries that cover a broader range of subjects is more beneficial for generalization. We release our codes and augmented data in .",Comparison of test set accuracy on GSM8K for models of varying scales after fine-tuning on AugGSM8K subsets with different query augmentation strategies.
BinaryAlign: Word Alignment as Binary Sequence Labeling,2407.12881v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2407.12881v1_0.png,"Real world deployments of word alignment are almost certain to cover both high and low resource languages. However, the state-of-the-art for this task recommends a different model class depending on the availability of gold alignment training data for a particular language pair. We propose BinaryAlign, a novel word alignment technique based on binary sequence labeling that outperforms existing approaches in both scenarios, offering a unifying approach to the task. Additionally, we vary the specific choice of multilingual foundation model, perform stratified error analysis over alignment error type, and explore the performance of BinaryAlign on non-English language pairs. We make our source code publicly available.{https://github.com/ubisoft/ubisoft-laforge-BinaryAlignWordAlignementasBinarySequenceLabeling}}","Example of alignment of an approximate translation, as often encountered in real-world applications. Links in red indicate situations where one word is aligned with several contiguous or non-contiguous words. The green line represent a situation where a word is untranslated which happens in many language pairs."
Quantifying the Persona Effect in LLM Simulations,2402.10811v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.10811v2_0.pdf,"Large language models (LLMs) have shown remarkable promise in simulating human language and behavior. This study investigates how integrating persona variables—demographic, social, and behavioral factors—impacts LLMs' ability to simulate diverse perspectives. We find that persona variables account for <10\% variance in annotations in existing subjective NLP datasets. Nonetheless, incorporating persona variables via prompting in LLMs provides modest but statistically significant improvements. Persona prompting is most effective in samples where many annotators disagree, but their disagreements are relatively minor. Notably, we find a linear relationship in our setting: the stronger the correlation between persona variables and human annotations, the more accurate the LLM predictions are using persona prompting. In a zero-shot setting, a powerful 70b model with persona prompting captures 81\% of the annotation variance achievable by linear regression trained on ground truth annotations. However, for most subjective NLP datasets, where persona variables have limited explanatory power, the benefits of persona prompting are limited.}",Illustration of persona prompting. We prepend the persona information of an annotator before the text sample and task description to investigate the capacity of LLMs to simulate diverse perspectives in subjective NLP tasks.
SyllabusQA: A Course Logistics Question Answering Dataset,2403.14666v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2403.14666v2_0.pdf,"Automated teaching assistants and chatbots have significant potential to reduce the workload of human instructors, especially for logistics-related question answering, which is important to students yet repetitive for instructors. However, due to privacy concerns, there is a lack of publicly available datasets. We introduce }~}, an open-source dataset with $63$ real course syllabi covering $36$ majors, containing $5,078$ open-ended course logistics-related question-answer pairs that are diverse in both question types and answer formats. Since many logistics-related questions contain critical information like the date of an exam, it is important to evaluate the factuality of answers. We benchmark several strong baselines on this task, from large language model prompting to retrieval-augmented generation. We introduce Fact-QA, an LLM-based (GPT-4) evaluation metric to evaluate the factuality of predicted answers. We find that despite performing close to humans on traditional metrics of textual similarity, there remains a significant gap between automated approaches and humans in terms of fact precision.","Domain diversity in \mbox{\textsc{SyllabusQA}} covering $36$ majors. For visual clarity, we show representative majors."
MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models,2308.09729v5,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2308.09729v5_0.pdf,"Large language models (LLMs) have achieved remarkable performance in natural language understanding and generation tasks. However, they often suffer from limitations such as difficulty in incorporating new knowledge, generating hallucinations, and explaining their reasoning process. To address these challenges, we propose a novel prompting pipeline, named , that leverages knowledge graphs (KGs) to enhance LLMs' inference and transparency. Our method enables LLMs to comprehend KG inputs and infer with a combination of implicit and external knowledge. Moreover, our method elicits the mind map of LLMs, which reveals their reasoning pathways based on the ontology of knowledge. We evaluate our method on diverse question \& answering tasks, especially in medical domains, and show significant improvements over baselines. We also introduce a new hallucination evaluation benchmark and analyze the effects of different components of our method. Our results demonstrate the effectiveness and robustness of our method in merging knowledge from LLMs and KGs for combined inference. To reproduce our results and extend the framework further, we make our codebase available at {https://github.com/wyl-willing/MindMap}.","A conceptual comparison between our method and the other prompting baselines: LLM-only, document retrieval + LLM, and KG retrieval + LLM."
Examining the robustness of LLM evaluation to the distributional assumptions of benchmarks,2404.16966v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2404.16966v2_0.pdf;/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2404.16966v2_1.pdf,"Benchmarks have emerged as the central approach for evaluating Large Language Models (LLMs). The research community often relies on a model's average performance across the test prompts of a benchmark to evaluate the model's performance. % on the benchmark task. This is consistent with the assumption that the test prompts within a benchmark represent a random sample from a real-world distribution of interest. We note that this is generally not the case; instead, we hold that the distribution of interest varies according to the specific use case. We find that [(1)] the correlation in model performance across test prompts is non-random, accounting for correlations across test prompts can change model rankings on major benchmarks, explanatory factors for these correlations include semantic similarity and common LLM failure points.","Illustrative example showcasing how different distributional assumptions of benchmarks affect model rankings. Consider a benchmark containing prompts reflecting three different tasks: math (red triangles), code generation (blue circles), and text generation (green squares). In Figure~\ref{subfig:unweighted}, each benchmark prompt contributes equally to the model evaluation. In contrast, Figure~\ref{subfig:weighted} accounts for correlations between prompts and the weights of the prompts are adjusted accordingly during evaluation. In scenario~\ref{subfig:unweighted}, the red LLM ranks highest because it excels in math, and the benchmark is biased towards math tasks (7 out of 12 prompts are math-related). When considering different weights in scenario~\ref{subfig:weighted}, we observe a different ranking outcome."
Bridging the Preference Gap between Retrievers and LLMs,2401.06954v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2401.06954v2_0.png,"Large Language Models (LLMs) have demonstrated superior results across a wide range of tasks, and Retrieval-augmented Generation (RAG) is an effective way to enhance the performance by locating relevant information and placing it into the context window of the LLM. However, the relationship between retrievers and LLMs in a RAG is still under-investigated. Most existing work treats the retriever and the LLM as independent components and leaves a gap between retrieving human-``friendly'' information and assembling a LLM-``friendly'' context. In this work, we examine a novel bridge mechanism. We validate the ranking and selection assumptions of retrievers in the context of RAG and propose a framework that chains together supervised and reinforcement learning to train a bridge model that optimizes the connection between the retriever and the LLM. Empirical results demonstrate the effectiveness of our method in both question-answering and personalized generation tasks.","We observe a preference gap when alternating the ranking and selection of information in RAG. Experiments are conducted with retrieving passages using GTR~\cite{ni2021large} and using top K of them as additional context for a frozen Palm2-S LLM. %``Top-K'' means we select the top-k GTR retrieved passages. Different colors indicate different datasets (detailed in Sec.~\ref{sec:datasets}) and the Y-axis shows the relative percentage. %The selected passages are used as additional context. Alternating the selection (Top-1) of information significantly affects (either positively or negatively) the LLM's performance, while randomizing the ranking of multiple selected items (Top-5) does not have a comparable impact (the metrics are detailed in Sec.~\ref{sec.parameter}). Note the impact on NQ is even too small to be visible."
Large Language Models Can Learn Temporal Reasoning,2401.06853v6,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2401.06853v6_0.pdf,"While large language models (LLMs) have demonstrated remarkable reasoning capabilities, they are not without their flaws and inaccuracies. Recent studies have introduced various methods to mitigate these limitations. Temporal reasoning (TR), in particular, presents a significant challenge for LLMs due to its reliance on diverse temporal concepts and intricate temporal logic. In this paper, we propose TG-LLM, a novel framework towards language-based TR. Instead of reasoning over the original context, we adopt a latent representation, temporal graph (TG) that enhances the learning of TR. A synthetic dataset (TGQA), which is fully controllable and requires minimal supervision, is constructed for fine-tuning LLMs on this text-to-TG translation task. We confirmed in experiments that the capability of TG translation learned on our dataset can be transferred to other TR tasks and benchmarks. On top of that, we teach LLM to perform deliberate reasoning over the TGs via Chain-of-Thought (CoT) bootstrapping and graph data augmentation. We observed that those strategies, which maintain a balance between usefulness and diversity, bring more reliable CoTs and final results than the vanilla CoT distillation..}",Our framework (TG-LLM) performs temporal reasoning in two steps: 1) Text-to-Temporal Graph translation: generate (relevant) temporal graph given the context and keyword (extracted from questions); 2) Temporal Graph Reasoning: perform Chain-of-Thought reasoning over the temporal graph.
Characterizing Similarities and Divergences in Conversational Tones in Humans and LLMs by Sampling with People,2406.04278v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.04278v1_0.pdf,"Conversational tones -- the manners and attitudes in which speakers communicate -- are essential to effective communication. Amidst the increasing popularization of Large Language Models (LLMs) over recent years, it becomes necessary to characterize the divergences in their conversational tones relative to humans. However, existing investigations of conversational modalities rely on pre-existing taxonomies or text corpora, which suffer from experimenter bias and may not be representative of real-world distributions for the studies' psycholinguistic domains. Inspired by methods from cognitive science, we propose an iterative method for simultaneously eliciting conversational tones and sentences, where participants alternate between two tasks: (1) one participant identifies the tone of a given sentence and (2) a different participant generates a sentence based on that tone. We run 100 iterations of this process with human participants and GPT-4, then obtain a dataset of sentences and frequent conversational tones. In an additional experiment, humans and GPT-4 annotated all sentences with all tones. With data from 1,339 human participants, 33,370 human judgments, and 29,900 GPT-4 queries, we show how our approach can be used to create an interpretable geometric representation of relations between conversational tones in humans and GPT-4. This work demonstrates how combining ideas from machine learning and cognitive science can address challenges in human-computer interactions.","Summary of our approach. {A}: Problem statement. {B}: The Sampling with People paradigm that aims to collect a representative sample of conversational tones and sentences. {C}: A quality-of-fit rating procedure that allows us to obtain vector representations of conversational tones with respect to their usage context. {D}: A geometric representation of the shared embedding space across elicited domains (human, GPT). {E}: As an application of our obtained data, we benchmark a selection of popular unsupervised cross-domain alignment methods."
Simul-LLM: A Framework for Exploring High-Quality Simultaneous Translation with Large Language Models,2312.04691v4,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2312.04691v4_0.png,"Large language models (LLMs) with billions of parameters and pretrained on massive amounts of data are now capable of near or better than state-of-the-art performance in a variety of downstream natural language processing tasks. Neural machine translation (NMT) is one such task that LLMs have been applied to with great success. However, little research has focused on applying LLMs to the more difficult subset of NMT called simultaneous translation (SimulMT), where translation begins before the entire source context is available to the model. In this paper, we address key challenges facing LLMs fine-tuned for SimulMT, validate classical SimulMT concepts and practices in the context of LLMs, explore adapting LLMs that are fine-tuned for NMT to the task of SimulMT, and introduce {Simul-LLM}, the first open-source fine-tuning and evaluation pipeline development framework for LLMs focused on SimulMT.",An example of a misalignment obstacle in {wait-3} simultaneous translation from German to English. Under certain circumstances a model needs to infer information earlier than the corresponding context in the source.
HyperMoE: Towards Better Mixture of Experts via Transferring Among Experts,2402.12656v4,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.12656v4_0.pdf,"The Mixture of Experts (MoE) for language models has been proven effective in augmenting the capacity of models by dynamically routing each input token to a specific subset of experts for processing. Despite the success, most existing methods face a challenge for balance between sparsity and the availability of expert knowledge: enhancing performance through increased use of expert knowledge often results in diminishing sparsity during expert selection. To mitigate this contradiction, we propose HyperMoE, a novel MoE framework built upon Hypernetworks. This framework integrates the computational processes of MoE with the concept of knowledge transferring in multi-task learning. Specific modules generated based on the information of unselected experts serve as supplementary information, which allows the knowledge of experts not selected to be used while maintaining selection sparsity. Our comprehensive empirical evaluations across multiple datasets and backbones establish that HyperMoE significantly outperforms existing MoE methods under identical conditions concerning the number of experts. Our code is publicly available at","A trade-off in MoE: (a) A small number of selectable experts can maintain sparsity but limits the availability of expert knowledge. (b) Increasing the number of selectable experts can improve performance but decrease sparsity. (c) Transferring partial knowledge from the unselected experts $E_{2,3}$ to the selected experts $E_1$ can improve the availability of expert knowledge while maintaining sparsity."
Aligning Large Language Models with Human Preferences through Representation Engineering,2312.15997v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2312.15997v3_0.png,"Aligning large language models (LLMs) with human preferences is crucial for enhancing their utility in terms of helpfulness, truthfulness, safety, harmlessness, and interestingness. Existing methods for achieving this alignment often involve employing reinforcement learning from human feedback (RLHF) to fine-tune LLMs based on human labels assessing the relative quality of model responses. Nevertheless, RLHF is susceptible to instability during fine-tuning and presents challenges in implementation. Drawing inspiration from the emerging field of representation engineering (RepE), this study aims to identify relevant representations for high-level human preferences embedded in patterns of activity within an LLM and achieve precise control of model behavior by transforming its representations. This novel approach, denoted as Representation Alignment from Human Feedback (RAHF), proves to be effective, computationally efficient, and easy to implement. Extensive experiments demonstrate the efficacy of RAHF in not only capturing but also manipulating representations to align with a broad spectrum of human preferences or values, rather than being confined to a singular concept or function (e.g. honesty or bias). RAHF's versatility in accommodating diverse human preferences shows its potential for advancing LLM performance. Code is available at .",{fig:schematic} Illustration of different apporaches. (a) Reinforcement learning from human feedback (RLHF); (b) Direct preference optimization (DPO); (c) Hindsight instruction relabeling (HIR); (d) Representation alignment from human feedback (RAHF).
ARAIDA: Analogical Reasoning-Augmented Interactive Data Annotation,2405.11912v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2405.11912v2_0.pdf,"Human annotation is a time-consuming task that requires a significant amount of effort. To address this issue, interactive data annotation utilizes an annotation model to provide suggestions for humans to approve or correct. However, annotation models trained with limited labeled data are prone to generating incorrect suggestions, leading to extra human correction effort. To tackle this challenge, we propose , an analogical reasoning-based approach that enhances automatic annotation accuracy in the interactive data annotation setting and reduces the need for human corrections. involves an error-aware integration strategy that dynamically coordinates an annotation model and a k-nearest neighbors (KNN) model, giving more importance to KNN's predictions when predictions from the annotation model are deemed inaccurate. Empirical studies demonstrate that is adaptable to different annotation tasks and models. On average, it reduces human correction labor by 11.02\% compared to vanilla interactive data annotation methods.",Comparison between manual annotation and interactive annotation.
CLAMBER: A Benchmark of Identifying and Clarifying Ambiguous Information Needs in Large Language Models,2405.12063v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2405.12063v2_0.png,"Large language models (LLMs) are increasingly used to meet user information needs, but their effectiveness in dealing with user queries that contain various types of ambiguity remains unknown, ultimately risking user trust and satisfaction. To this end, we introduce CLAMBER, a benchmark for evaluating LLMs using a well-organized taxonomy. Building upon the taxonomy, we construct $ 12K$ high-quality data to assess the strengths, weaknesses, and potential risks of various off-the-shelf LLMs. Our findings indicate the limited practical utility of current LLMs in identifying and clarifying ambiguous user queries, even enhanced by chain-of-thought (CoT) and few-shot prompting. These techniques may result in overconfidence in LLMs and yield only marginal enhancements in identifying ambiguity. Furthermore, current LLMs fall short in generating high-quality clarifying questions due to a lack of conflict resolution and inaccurate utilization of inherent knowledge. In this paper, CLAMBER presents a guidance and promotes further research on proactive and trustworthy LLMs. Our dataset is available at .","Investigation on the identification accuracy when handling ambiguous (i.e, Acc@1) versus unambiguous queries (i.e, Acc@0). We report the results under Zero-shot w/o CoT setting. Small-scale LLMs tend to classify most queries as ambiguous."
Multimodal Reasoning with Multimodal Knowledge Graph,2406.02030v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.02030v2_0.pdf,"Multimodal reasoning with large language models (LLMs) often suffers from hallucinations and the presence of deficient or outdated knowledge within LLMs. Some approaches have sought to mitigate these issues by employing textual knowledge graphs, but their singular modality of knowledge limits comprehensive cross-modal understanding. In this paper, we propose the Multimodal Reasoning with Multimodal Knowledge Graph () method, which leverages multimodal knowledge graphs (MMKGs) to learn rich and semantic knowledge across modalities, significantly enhancing the multimodal reasoning capabilities of LLMs. In particular, a relation graph attention network is utilized for encoding MMKGs and a cross-modal alignment module is designed for optimizing image-text alignment. A MMKG-grounded dataset is constructed to equip LLMs with initial expertise in multimodal reasoning through pretraining. Remarkably, achieves superior performance while training on only a small fraction of parameters, approximately 2.25\% of the LLM's parameter size. Experimental results on {multimodal question answering} and {multimodal analogy reasoning} tasks demonstrate that our method outperforms previous state-of-the-art models.",(a) The inadequate knowledge encapsulated within textual KG results in the incorrect answer. (b) Our \ourapproach produces the correct answer by reasoning with richer multimodal information.
RAGTruth: A Hallucination Corpus for Developing Trustworthy Retrieval-Augmented Language Models,2401.00396v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2401.00396v2_0.pdf,"Retrieval-augmented generation (RAG) has become a main technique for alleviating hallucinations in large language models (LLMs). Despite the integration of RAG, LLMs may still present unsupported or contradictory claims to the retrieved contents. In order to develop effective hallucination prevention strategies under RAG, it is important to create benchmark datasets that can measure the extent of hallucination. This paper presents {}, a corpus tailored for analyzing word-level hallucinations in various domains and tasks within the standard RAG frameworks for LLM applications. {} comprises nearly 18,000 naturally generated responses from diverse LLMs using RAG. These responses have undergone meticulous manual annotations at both the individual case and word levels, incorporating evaluations of hallucination intensity. We not only benchmark hallucination frequencies across different LLMs, but also critically assess the effectiveness of several existing hallucination detection methodologies. We show that using a high-quality dataset such as {}, it is possible to finetune a relatively small LLM and achieve a competitive hallucination detection performance when compared to the existing prompt-based approaches using state-of-the-art LLMs such as GPT-4. Furthermore, the finetuned model can effectively mitigate hallucination in LLM responses.",Frequency of different types of hallucination by task.
Chat Vector: A Simple Approach to Equip LLMs with Instruction Following and Model Alignment in New Languages,2310.04799v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2310.04799v3_0.pdf,"Recently, the development of open-source large language models (LLMs) has advanced rapidly. Nevertheless, due to data constraints, the capabilities of most open-source LLMs are primarily focused on English. To address this issue, we introduce the concept of {chat vector} to equip pre-trained language models with instruction following and human value alignment via simple model arithmetic. The chat vector is derived by subtracting the weights of a pre-trained base model (e.g. LLaMA2) from those of its corresponding chat model (e.g. LLaMA2-chat). By simply adding the chat vector to a continual pre-trained model's weights, we can endow the model with chat capabilities in new languages without the need for further training. Our empirical studies demonstrate the superior efficacy of the chat vector from three different aspects: instruction following, toxicity mitigation, and multi-turn dialogue. Moreover, to showcase the adaptability of our approach, we extend our experiments to encompass various languages, base models, and chat vectors. The results underscore the chat vector's simplicity, effectiveness, and wide applicability, making it a compelling solution for efficiently enabling conversational capabilities in pre-trained language models. Our code is available at .","An illustration to demonstrate the difference between the traditional approach and our method. The blue arrows on the top right side depict the conventional method of constructing a non-English LM. First, an open-source PLM (e.g. LLaMA2) undergoes continual pre-training (CP) on the target language, followed by SFT and RLHF alignment procedures. In contrast, the gray arrow on the left illustrates how we obtain the chat vector through simple parameter subtraction. This chat vector can be added to the CP model to produce the chat model in the target language, as depicted by the dual-color arrow."
CLOMO: Counterfactual Logical Modification with Large Language Models,2311.17438v4,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2311.17438v4_0.pdf,"In this study, we delve into the realm of counterfactual reasoning capabilities of large language models (LLMs). Our primary objective is to cultivate the counterfactual thought processes within LLMs and rigorously assess these processes for their validity. Specifically, we introduce a novel task, }ounterfactual }gical }dification (), and a high-quality human-annotated benchmark. In this task, LLMs must adeptly alter a given argumentative text to uphold a predetermined logical relationship. To effectively evaluate a generation model's counterfactual capabilities, we propose an innovative evaluation metric, the decomposed {Self-Evaluation Score (SES)} to directly evaluate the natural language output of LLMs instead of modeling the task as a multiple-choice problem. Analysis shows that the proposed automatic metric aligns well with human preference. Our experimental results show that while LLMs demonstrate a notable capacity for logical counterfactual thinking, there remains a discernible gap between their current abilities and human performance. Code and data are available at .",Demonstration of \ourdataset. An LLM is given an argument and two premises. The LLM needs to modify the statements in Argument such that the logical relation \textsf{R} switch to stand in \textsf{state 2} instead of \textsf{state 1}.
Exploring Hybrid Question Answering via Program-based Prompting,2402.10812v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.10812v1_0.pdf,"Question answering over heterogeneous data requires reasoning over diverse sources of data, which is challenging due to the large scale of information and organic coupling of heterogeneous data. Various approaches have been proposed to address these challenges. One approach involves training specialized retrievers to select relevant information, thereby reducing the input length. Another approach is to transform diverse modalities of data into a single modality, simplifying the task difficulty and enabling more straightforward processing. In this paper, we propose , a novel program-based prompting framework for the hybrid question answering task. follows the code generation and execution paradigm. In addition, integrates various functions to tackle the hybrid reasoning scenario. Specifically, contains function declaration and function implementation to perform hybrid information-seeking over data from various sources and modalities, which enables reasoning over such data without training specialized retrievers or performing modal transformations. Experimental results on two typical hybrid question answering benchmarks HybridQA and MultiModalQA demonstrate the effectiveness of : it surpasses all baseline systems and achieves the best performances in the few-shot settings on both datasets.",Example of hybrid question answering task with the corresponding program.
Simple but Effective Compound Geometric Operations for Temporal Knowledge Graph Completion,2408.06603v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2408.06603v1_0.pdf,"Temporal knowledge graph completion aims to infer the missing facts in temporal knowledge graphs. Current approaches usually embed factual knowledge into continuous vector space and apply geometric operations to learn potential patterns in temporal knowledge graphs. However, these methods only adopt a single operation, which may have limitations in capturing the complex temporal dynamics present in temporal knowledge graphs. Therefore, we propose a simple but effective method, i.e. TCompoundE, which is specially designed with two geometric operations, including time-specific and relation-specific operations. We provide mathematical proofs to demonstrate the ability of TCompoundE to encode various relation patterns. Experimental results show that our proposed model significantly outperforms existing temporal knowledge graph embedding models. Our code is available at {https://github.com/nk-ruiying/TCompoundE}.",An illustration of temporal evolution patterns. It can be observed that the relationships between head entity and tail entity are dynamically determined by both relation and time.
Uncertainty Aware Learning for Language Model Alignment,2406.04854v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.04854v1_0.pdf,"As instruction-tuned large language models (LLMs) evolve, aligning pretrained foundation models presents increasing challenges. Existing alignment strategies, which typically leverage diverse and high-quality data sources, often overlook the intrinsic uncertainty of tasks, learning all data samples equally. This may lead to suboptimal data efficiency and model performance. In response, we propose uncertainty-aware learning (UAL) to improve the model alignment of different task scenarios, by introducing the sample uncertainty (elicited from more capable LLMs). We implement UAL by a simple fashion -- adaptively setting the label smoothing value of training according to the uncertainty of individual samples. Analysis shows that our UAL indeed facilitates better token clustering in the feature space, validating our hypothesis. Extensive experiments on widely used benchmarks demonstrate that our UAL significantly and consistently outperforms standard supervised fine-tuning. Notably, LLMs aligned in a mixed scenario have achieved an average improvement of 10.62\% on high-entropy tasks (i.e., AlpacaEval leaderboard), and 1.81\% on complex low-entropy tasks (i.e., MetaMath and GSM8K).","{Illustration of feature clustering.} Compared to SFT, UAL-based models show more convergence in the feature space, which we detailed our exploration in Section \ref{sec:cluster}."
Interpretable User Satisfaction Estimation for Conversational Systems with Large Language Models,2403.12388v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2403.12388v2_0.jpg,"Accurate and interpretable user satisfaction estimation (USE) is critical for understanding, evaluating, and continuously improving conversational systems. Users express their satisfaction or dissatisfaction with diverse conversational patterns in both general-purpose (ChatGPT and Bing Copilot) and task-oriented (customer service chatbot) conversational systems. Existing approaches based on featurized ML models or text embeddings fall short in extracting generalizable patterns and are hard to interpret. In this work, we show that LLMs can extract interpretable signals of user satisfaction from their natural language utterances more effectively than embedding-based approaches. Moreover, an LLM can be tailored for USE via an iterative prompting framework using supervision from labeled examples. Our proposed method, (), not only has higher accuracy but is more interpretable as it scores user satisfaction via learned rubrics with a detailed breakdown.",Illustration of user utterances with satisfaction patterns (green) and dissatisfaction patterns (red).
Measuring Political Bias in Large Language Models: What Is Said and How It Is Said,2403.18932v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2403.18932v1_0.pdf,"We propose to measure political bias in LLMs by analyzing both the content and style of their generated content regarding political issues. Existing benchmarks and measures focus on gender and racial biases. However, political bias exists in LLMs and can lead to polarization and other harms in downstream applications. In order to provide transparency to users, we advocate that there should be fine-grained and explainable measures of political biases generated by LLMs. Our proposed measure looks at different political issues such as reproductive rights and climate change, at both the content (the substance of the generation) and the style (the lexical polarity) of such bias. We measured the political bias in eleven open-sourced LLMs and showed that our proposed framework is easily scalable to other topics and is explainable.",An overview of our proposed framework for measuring political bias in LLM-generated content. The two-tiered framework first evaluates the LLM's {political stance} over political topics and then {framing bias} in two aspects: content and style.
Fortify the Shortest Stave in Attention: Enhancing Context Awareness of Large Language Models for Effective Tool Use,2312.04455v4,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2312.04455v4_0.pdf,"In this paper, we demonstrate that an inherent waveform pattern in the attention allocation of large language models (LLMs) significantly affects their performance in tasks demanding a high degree of context awareness, such as utilizing LLMs for tool-use. Specifically, the crucial information in the context will be potentially overlooked by model when it is positioned in the trough zone of the attention waveform, leading to decreased performance. To address this issue, we propose a novel inference method named . It allows LLMs to process their input through multiple parallel processes. Each process utilizes a distinct base angle for the rotary position embedding, thereby creating a unique attention waveform. By compensating an attention trough of a particular process with an attention peak of another process, our approach enhances LLM's awareness to various contextual positions, thus mitigating the risk of overlooking crucial information. In the largest tool-use benchmark, our method elevates a 7B model to achieve state-of-the-art performance, comparable to that of GPT-4. On other benchmarks and some RAG tasks, which also demand a thorough understanding of contextual content, our \ also exhibited notable enhancements in performance..}","(a) Task illustration: Presented with multiple key-value pairs and a target key (highlighted in bold), the model is required to accurately retrieve and generate the value associated with this key from an extensive context. (b) We illustrate the position-related fluctuation in accuracy of Llama-2-7B on this in-context retrieval task. (c) The pattern of the attention score exhibits fluctuations, which we term the ``attention waveform''. Our study reveals a connection between the position-related fluctuations in LLMs' performance and this attention waveform."
Enhancing Multilingual Capabilities of Large Language Models through Self-Distillation from Resource-Rich Languages,2402.12204v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.12204v1_0.pdf,"While large language models (LLMs) have been pre-trained on multilingual corpora, their performance still lags behind in most languages compared to a few resource-rich languages. One common approach to mitigate this issue is to translate training data from resource-rich languages into other languages and then continue training. However, using the data obtained solely relying on translation while ignoring the original capabilities of LLMs across languages is not always effective, which we show will limit the performance of cross-lingual knowledge transfer. In this work, we propose SDRRL, a method based on elf-istillation from esource-ich anguages that effectively improve multilingual performance by leveraging the internal capabilities of LLMs on resource-rich languages. We evaluate on different LLMs (LLaMA-2 and SeaLLM) and source languages (English and French) across various comprehension and generation tasks, experimental results demonstrate that SDRRL can significantly enhance multilingual capabilities while minimizing the impact on original performance in resource-rich languages.. }","Comparison between vanilla supervised fine-tuning (SFT), translate-then-SFT, and our proposed method. Besides using the translated question-answer pairs in the target language (e.g., Japanese), SDRRL further leverages the generated answer $A^{\star}_{\rm EN}$ by LLMs in the resource-rich language (e.g., English) and collects self-distillated data (in green box) to help enhance its multilingual capabilities."
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations,2403.14112v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2403.14112v3_0.pdf,"We introduce CHARM, the first benchmark for comprehensively and in-depth evaluating the commonsense reasoning ability of large language models (LLMs) in Chinese, which covers both globally known and Chinese-specific commonsense. We evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5 representative prompt strategies for improving LLMs' reasoning ability, such as Chain-of-Thought. Our findings indicated that the LLM's language orientation and the task's domain influence the effectiveness of the prompt strategy, which enriches previous research findings. We built closely-interconnected reasoning and memorization tasks, and found that some LLMs struggle with memorizing Chinese commonsense, affecting their reasoning ability, while others show differences in reasoning despite similar memorization performance. We also evaluated the LLMs' memorization-independent reasoning abilities and analyzed the typical errors. Our study precisely identified the LLMs' strengths and weaknesses, providing the clear direction for optimization. It can also serve as a reference for studies in other fields. We will release CHARM at .",Construction of CHARM. CHARM encompasses both global and Chinese-specific commonsense. CHARM consists closely-interconnected reasoning and memorization tasks.
Model Composition for Multimodal Large Language Models,2402.12750v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.12750v2_0.pdf,"Recent developments in Multimodal Large Language Models (MLLMs) have shown rapid progress, moving towards the goal of creating versatile MLLMs that understand inputs from various modalities. However, existing methods typically rely on joint training with paired multimodal instruction data, which is resource-intensive and challenging to extend to new modalities. In this paper, we propose a new paradigm through the model composition of existing MLLMs to create a new model that retains the modal understanding capabilities of each original model. Our basic implementation, , demonstrates the effectiveness of this paradigm by reusing modality encoders and merging LLM parameters. Furthermore, we introduce to address parameter interference and mismatch issues during the merging process, thereby enhancing the model performance. To facilitate research in this area, we propose , a benchmark for assessing ability of MLLMs to understand inputs from diverse modalities. Experiments on this benchmark and four other multimodal understanding tasks show significant improvements over baselines, proving that model composition can create a versatile model capable of processing inputs from multiple modalities.}",Illustration of various approaches for multimodal large language models: (a) aligning LLM with a multimodal encoder and (b) joint training with multiple modal encoders and (c) our proposed model composition method that creates a versatile model from existing MLLMs through a training-free and extensible process.
Draft & Verify: Lossless Large Language Model Acceleration via Self-Speculative Decoding,2309.08168v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2309.08168v2_0.pdf,"We present a novel inference scheme, self-speculative decoding, for accelerating Large Language Models (LLMs) without the need for an auxiliary model. This approach is characterized by a two-stage process: drafting and verification. The drafting stage generates draft tokens at a slightly lower quality but more quickly, which is achieved by selectively skipping certain intermediate layers during drafting. Subsequently, the verification stage employs the original LLM to validate those draft output tokens in one forward pass. This process ensures the final output remains {identical} to that produced by the unaltered LLM. Moreover, the proposed method requires no additional neural network training and no extra memory footprint, making it a plug-and-play and cost-effective solution for inference acceleration. Benchmarks with LLaMA-2 and its variants demonstrated a speedup up to 1.99$$., and will be released with the Apache-2.0 License.}","Visualization of the self-speculative decoding process. The verification stage evaluates all drafted tokens in a single forward pass, with accepted tokens marked in green and rejected tokens highlighted in red. Each verification step also predicts one more token, which is denoted in blue."
Measuring Meaning Composition in the Human Brain with Composition Scores from Large Language Models,2403.04325v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2403.04325v3_0.png,"The process of meaning composition, wherein smaller units like morphemes or words combine to form the meaning of phrases and sentences, is essential for human sentence comprehension. Despite extensive neurolinguistic research into the brain regions involved in meaning composition, a computational metric to quantify the extent of composition is still lacking. Drawing on the key-value memory interpretation of transformer feed-forward network blocks, we introduce the Composition Score, a novel model-based metric designed to quantify the degree of meaning composition during sentence comprehension. Experimental findings show that this metric correlates with brain clusters associated with word frequency, structural processing, and general sensitivity to words, suggesting the multifaceted nature of meaning composition during human sentence comprehension. {GitHub}.}",Comparing Composition Scores with fMRI data during naturalistic listening comprehension.
Complex Reasoning over Logical Queries on Commonsense Knowledge Graphs,2403.07398v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2403.07398v2_0.pdf,"Event commonsense reasoning requires the ability to reason about the relationship between events, as well as infer implicit context underlying that relationship. %, which state-of-the-art language models still struggle to perform. However, data scarcity makes it challenging for language models to learn to generate commonsense inferences for contexts and questions involving interactions between complex events. To address this demand, we present ({COM}plex {COM}monsense), a new dataset created by sampling multi-hop logical queries (e.g., {the joint effect or cause of both event A and B, or the effect of the effect of event C}) from an existing commonsense knowledge graph (CSKG), and verbalizing them using handcrafted rules and large language models into multiple-choice and text generation questions. Our experiments show that language models trained on exhibit significant improvements in complex reasoning ability, resulting in enhanced zero-shot performance in both in-domain and out-of-domain tasks for question answering and generative commonsense reasoning, without expensive human annotations.}",An example of conjunctive logical queries and their verbalization as complex commonsense inferences.
Learning to Plan and Generate Text with Citations,2404.03381v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2404.03381v3_0.pdf,"The increasing demand for the deployment of LLMs in information-seeking scenarios has spurred efforts in creating verifiable systems, which generate responses to queries along with supporting evidence. In this paper, we explore the {attribution} capabilities of plan-based models which have been recently shown to improve the faithfulness, grounding, and controllability of generated text. We conceptualize plans as a sequence of questions which serve as {blueprints} of the generated content and its organization. We propose two attribution models that utilize different variants of blueprints, an {abstractive} model where questions are generated from scratch, and an {extractive} model where questions are copied from the input. Experiments on long-form question-answering show that planning consistently improves attribution quality. Moreover, the citations generated by blueprint models are more accurate compared to those obtained from LLM-based pipelines lacking a planning component.","{figure:example} Query (top), followed by most relevant (abridged) passages, and summaries (bottom) with in-line citations. Summary~(a) is the output of a vanilla sequence-to-sequence model trained to generate long answers with citations. Summaries~(b) and~(c) are the output of models with abstractive and extractive plans, respectively. Citations for plan-based models can have different formats (e.g., references to the question plan; see Section~\ref{sec:results:analysis})."
Language Models can Exploit Cross-Task In-context Learning for Data-Scarce Novel Tasks,2405.10548v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2405.10548v3_0.pdf,"Large Language Models (LLMs) have transformed NLP with their remarkable In-context Learning (ICL) capabilities. Automated assistants based on LLMs are gaining popularity; however, adapting them to novel tasks is still challenging. While colossal models excel in zero-shot performance, their computational demands limit widespread use, and smaller language models struggle without context. This paper investigates whether LLMs can generalize from labeled examples of predefined tasks to novel tasks. Drawing inspiration from biological neurons and the mechanistic interpretation of the Transformer architecture, we explore the potential for information sharing across tasks. We design a cross-task prompting setup with three LLMs and show that LLMs achieve significant performance improvements despite no examples from the target task in the context. Cross-task prompting leads to a remarkable performance boost of 107\% for LLaMA-2 7B, 18.6\% for LLaMA-2 13B, and 3.2\% for GPT 3.5 on average { over zero-shot prompting, and performs comparable to standard in-context learning}. The effectiveness of generating pseudo-labels for in-task examples is demonstrated, and our analyses reveal a strong correlation between the effect of cross-task examples and model activation similarities in source and target input tokens. This paper offers a first-of-its-kind exploration of LLMs' ability to solve novel tasks based on contextual signals from different task examples.","In this working example, we aim to solve a question from MedMCQA using demonstrations from BoolQ. To do so, we sample a semantically similar demonstration from the source task and then use this demonstration along with task descriptors of the source and target tasks to generate a cross-task prompt that is fed to an LLM."
Split and Rephrase with Large Language Models,2312.11075v4,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2312.11075v4_0.png;/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2312.11075v4_1.png,"The Split and Rephrase (SPRP) task, which consists in splitting complex sentences into a sequence of shorter grammatical sentences, while preserving the original meaning, can facilitate the processing of complex texts for humans and machines alike. It is also a valuable testbed to evaluate natural language processing models, as it requires modelling complex grammatical aspects. In this work, we evaluate large language models on the task, showing that they can provide large improvements over the state of the art on the main metrics, although still lagging in terms of splitting compliance. Results from two human evaluations further support the conclusions drawn from automated metric results. We provide a comprehensive study that includes prompting variants, domain shift, fine-tuned pretrained language models of varying parameter size and training data volumes, contrasted with both zero-shot and few-shot approaches on instruction-tuned language models. Although the latter were markedly outperformed by fine-tuned models, they may constitute a reasonable off-the-shelf alternative. Our results provide a fine-grained analysis of the potential and limitations of large language models for SPRP, with significant improvements achievable using relatively small amounts of training data and model parameters overall, and remaining limitations for all models on the task.","Impact of parameter size on the DeSSE (left) and BiSECT (right) test sets, with Pythia model variants fine-tuned over data from the DeSSE (-DE) or BiSECT (-BI) training data."
SAPT: A Shared Attention Framework for Parameter-Efficient Continual Learning of Large Language Models,2401.08295v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2401.08295v3_0.pdf,"The continual learning (CL) ability is vital for deploying large language models (LLMs) in the dynamic world. Existing methods devise the learning module to acquire task-specific knowledge with parameter-efficient tuning (PET) block and the selection module to pick out the corresponding one for the testing input, aiming at handling the challenges of catastrophic forgetting and knowledge transfer in CL. However, these methods tend to address only one of the challenges, ignoring the potential of aligning the two modules to effectively address catastrophic forgetting and knowledge transfer simultaneously. To this end, we propose a novel Shared Attention Framework (SAPT), to align the PET learning and selection via the Shared Attentive Learning \& Selection module. Extensive experiments on two CL benchmarks demonstrate the superiority of SAPT. Moreover, SAPT consistently demonstrates its superiority when we scale it to different model sizes (from 770M to 13B), different model architectures (T5 and LLaMA-2) and unseen tasks..}",The conceptual framework for the learning and the selection module to achieve the continual learning of large language models based on \petlWithEmoji \ when the new Dialogue Generation task arrives. Dashed lines represent the working process of existing works while solid lines are for that of our SAPT in this work.
CharacterEval: A Chinese Benchmark for Role-Playing Conversational Agent Evaluation,2401.01275v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2401.01275v2_0.pdf,"Recently, the advent of large language models (LLMs) has revolutionized generative agents. Among them, Role-Playing Conversational Agents (RPCAs) attract considerable attention due to their ability to emotionally engage users. However, the absence of a comprehensive benchmark impedes progress in this field. To bridge this gap, we introduce {CharacterEval}, a Chinese benchmark for comprehensive RPCA assessment, complemented by a tailored high-quality dataset. The dataset comprises 1,785 multi-turn role-playing dialogues, encompassing 11,376 examples and featuring 77 characters derived from Chinese novels and scripts. It was carefully constructed, beginning with initial dialogue extraction via GPT-4, followed by rigorous human-led quality control, and enhanced with in-depth character profiles sourced from Baidu Baike. {CharacterEval} employs a multifaceted evaluation approach, encompassing thirteen targeted metrics on four dimensions. To facilitate the convenient evaluation for these subjective metrics in {CharacterEval}, we further developed CharacterRM, a role-playing reward model based on human annotations, which has a higher correlation with human judgment compared to GPT-4. Comprehensive experiments on {CharacterEval} demonstrate that Chinese LLMs exhibit more promising capabilities than GPT-4 in Chinese role-playing conversation. Source code, data source and reward model will be publicly accessible at~.","An example of the {CharacterEval}, including the dialogue, scene and character's profile."
Generative Cross-Modal Retrieval: Memorizing Images in Multimodal Language Models for Retrieval and Beyond,2402.10805v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.10805v1_0.pdf,"The recent advancements in generative language models have demonstrated their ability to memorize knowledge from documents and recall knowledge to respond to user queries effectively. Building upon this capability, we propose to enable multimodal large language models (MLLMs) to memorize and recall images within their parameters. Given a user query for visual content, the MLLM is anticipated to ``recall'' the relevant image from its parameters as the response. Achieving this target presents notable challenges, including inbuilt visual memory and visual recall schemes within MLLMs. To address these challenges, we introduce a generative cross-modal retrieval framework, which assigns unique identifier strings to represent images and involves two training steps: learning to memorize and learning to retrieve. The first step focuses on training the MLLM to memorize the association between images and their respective identifiers. The latter step teaches the MLLM to generate the corresponding identifier of the target image, given the textual query input. By memorizing images in MLLMs, we introduce a new paradigm to cross-modal retrieval, distinct from previous discriminative approaches. The experiments demonstrate that the generative paradigm performs effectively and efficiently even with large-scale image candidate sets.",Real cases from GPT4 illustrate the necessity of visual outputs for LLMs.
Self-Training with Pseudo-Label Scorer for Aspect Sentiment Quad Prediction,2406.18078v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.18078v1_0.pdf,"Aspect Sentiment Quad Prediction (ASQP) aims to predict all quads (aspect term, aspect category, opinion term, sentiment polarity) for a given review, which is the most representative and challenging task in aspect-based sentiment analysis. A key challenge in the ASQP task is the scarcity of labeled data, which limits the performance of existing methods. To tackle this issue, we propose a self-training framework with a pseudo-label scorer, wherein a scorer assesses the match between reviews and their pseudo-labels, aiming to filter out mismatches and thereby enhance the effectiveness of self-training. We highlight two critical aspects to ensure the scorer's effectiveness and reliability: the quality of the training dataset and its model architecture. To this end, we create a human-annotated comparison dataset and train a generative model on it using ranking-based objectives. Extensive experiments on public ASQP datasets reveal that using our scorer can greatly and consistently improve the effectiveness of self-training. Moreover, we explore the possibility of replacing humans with large language models for comparison dataset annotation, and experiments demonstrate its feasibility..}",Illustration of our pseudo-label scorer.
Self-Training with Direct Preference Optimization Improves Chain-of-Thought Reasoning,2407.18248v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2407.18248v1_0.pdf,"Effective training of language models (LMs) for mathematical reasoning tasks demands high-quality supervised fine-tuning data. Besides obtaining annotations from human experts, a common alternative is sampling from larger and more powerful LMs. However, this knowledge distillation approach can be costly and unstable, particularly when relying on closed-source, proprietary LMs like GPT-4~, whose behaviors are often unpredictable. In this work, we demonstrate that the reasoning abilities of small-scale LMs can be enhanced through self-training, a process where models learn from their own outputs. We also show that the conventional self-training can be further augmented by a preference learning algorithm called Direct Preference Optimization (DPO)~. By integrating DPO into self-training, we leverage preference data to guide LMs towards more accurate and diverse chain-of-thought reasoning. We evaluate our method across various mathematical reasoning tasks using different base models. Our experiments show that this approach not only improves LMs' reasoning performance but also offers a more cost-effective and scalable solution compared to relying on large proprietary LMs.","Our approach demonstrates superior performance on the GSM8K benchmark while minimizing the required compute cost, including both training and inference. Compute cost calculations are based on the methodology outlined by~\citet{yuan2023scaling}.\protect\footnotemark"
Document-level Claim Extraction and Decontextualisation for Fact-Checking,2406.03239v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.03239v2_0.pdf,"Selecting which claims to check is a time-consuming task for human fact-checkers, especially from documents consisting of multiple sentences and containing multiple claims. However, existing claim extraction approaches focus more on identifying and extracting claims from individual sentences, identifying whether a sentence contains a claim or the exact boundaries of the claim within a sentence. In this paper, we propose a method for {document-level} claim extraction for fact-checking, which aims to extract check-worthy claims from documents and decontextualise them so that they can be understood out of context. Specifically, we first recast claim extraction as extractive summarization in order to identify central sentences from documents, then rewrite them to include necessary context from the originating document through sentence decontextualisation. Evaluation with both automatic metrics and a fact-checking professional shows that our method is able to extract check-worthy claims from documents more accurately than previous work, while also improving evidence retrieval.","An example of document-level claim extraction. Document\protect\footnotemark[1] is a piece of news from CNBC. Gold Claim\protect\footnotemark[2] is annotated by the fact-checking organization, Misbar. Sentences in orange denote check-worthy claims extracted by sentence-level CE (Claimbuster). Sentences in blue denote salient claims extracted by our document-level CE. The claim in green is a decontextualised claim derived from the 4th sentence obtained by our document-level CE."
LLMs Learn Task Heuristics from Demonstrations: A Heuristic-Driven Prompting Strategy for Document-Level Event Argument Extraction,2311.06555v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2311.06555v3_0.png,"In this study, we explore in-context learning (ICL) in document-level event argument extraction (EAE) to alleviate the dependency on large-scale labeled data for this task. We introduce the Heuristic-Driven Link-of-Analogy (HD-LoA) prompting tailored for the EAE task. Specifically, we hypothesize and validate that LLMs learn task-specific heuristics from demonstrations in ICL. Building upon this hypothesis, we introduce an explicit heuristic-driven demonstration construction approach, which transforms the haphazard example selection process into a systematic method that emphasizes task heuristics. Additionally, inspired by the analogical reasoning of human, we propose the link-of-analogy prompting, which enables LLMs to process new situations by drawing analogies to known situations, enhancing their performance on unseen classes beyond limited ICL examples. Experiments show that our method outperforms existing prompting methods and few-shot supervised learning methods on document-level EAE datasets. Additionally, the HD-LoA prompting shows effectiveness in other tasks like sentiment analysis and natural language inference, demonstrating its broad adaptability {https://github.com/hzzhou01/HD-LoA-Prompting}}.",CoT's step-by-step reasoning degrades to a single step for non-reasoning tasks. Reasoning steps of reasoning tasks (in orange) and non-reasoning tasks (in blue) are compared. Different colors indicate distinct reasoning steps. Prompts are from \citep{shum-etal-2023-automatic}.
Investigating and Mitigating the Multimodal Hallucination Snowballing in Large Vision-Language Models,2407.00569v4,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2407.00569v4_0.pdf,"Though advanced in understanding visual information with human languages, Large Vision-Language Models (LVLMs) still suffer from multimodal hallucinations. A natural concern is that during multimodal interaction, the generated hallucinations could influence the LVLMs' subsequent generation. Thus, we raise a question: {When presented with a query relevant to the previously generated hallucination, will LVLMs be misled and respond incorrectly, even though the ground visual information exists?} To answer this, we propose a framework called {MMHalSnowball} to evaluate LVLMs' behaviors when encountering generated hallucinations, where LVLMs are required to answer specific visual questions within a curated hallucinatory conversation. Crucially, our experiment shows that the performance of open-source LVLMs drops by at least $31\%$, indicating that LVLMs are prone to accept the generated hallucinations and make false claims that they would not have supported without distractions. We term this phenomenon {Multimodal Hallucination Snowballing}. To mitigate this, we further propose a training-free method called {Residual Visual Decoding}, where we revise the output distribution of LVLMs with the one derived from the residual visual input, providing models with direct access to the visual information. Experiments show that our method can mitigate more than $24\%$ of the snowballed multimodal hallucination while maintaining capabilities.{https://github.com/ \/MMHalSnowball}}","An example of the LVLM assisting a visually impaired person to cross the street. The model is misled by the generated hallucination and mistakenly suggests the user to cross the street, although it can give correct advice independently. \tcbox[colback=green1]{Green} and \tcbox[colback=red1]{red} colors highlight the correct answer and hallucinations, respectively."
mCoT: Multilingual Instruction Tuning for Reasoning Consistency in Language Models,2406.02301v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.02301v2_0.pdf,"Large language models (LLMs) with Chain-of-thought (CoT) have recently emerged as a powerful technique for eliciting reasoning to improve various downstream tasks. As most research mainly focuses on English, with few explorations in a multilingual context, the question of how reliable this reasoning capability is in different languages is still open. To address it directly, we study multilingual reasoning consistency across multiple languages, using popular open-source LLMs. First, we compile the first large-scale multilingual math reasoning dataset, , covering eleven diverse languages. Then, we introduce multilingual CoT instruction tuning to boost reasoning capability across languages, thereby improving model consistency. While existing LLMs show substantial variation across the languages we consider, and especially low performance for lesser resourced languages, our 7B parameter model achieves impressive consistency across languages, and superior or comparable performance to close- and open-source models even of much larger sizes.","Overview of multilingual reasoning; LLMs are expected to have consistent reasoning capabilities across different languages when given the same problem which has the same answer. Shown in picture are three example languages: English (EN), Swahili (SW), and Chinese (ZH). For EN, we show the problem formulation, and the Chain-of-Thought (CoT) reasoning."
Beyond Traditional Benchmarks: Analyzing Behaviors of Open LLMs on Data-to-Text Generation,2401.10186v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2401.10186v3_0.pdf,"We analyze the behaviors of open large language models (LLMs) on the task of data-to-text (D2T) generation, i.e., generating coherent and relevant text from structured data. To avoid the issue of LLM training data contamination with standard benchmarks, we design -- a tool for collecting novel structured data records from public APIs. We find that open LLMs (Llama 2, Mistral, and Zephyr) can generate fluent and coherent texts in zero-shot settings from data in common formats collected with . However, we show that the semantic accuracy of the outputs is a major issue: both according to human annotators and our reference-free metric based on GPT-4, more than 80\% of the outputs of open LLMs contain at least one semantic error. We publicly release the code, data, and model outputs.}","To benchmark LLMs, we download unlabeled structured data from public APIs and prompt LLMs to generate texts based on the data. We annotate semantic errors in the outputs using reference-free metrics."
One Prompt To Rule Them All: LLMs for Opinion Summary Evaluation,2402.11683v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.11683v2_0.png,"Evaluation of opinion summaries using conventional reference-based metrics often fails to provide a comprehensive assessment and exhibits limited correlation with human judgments. While Large Language Models (LLMs) have shown promise as reference-free metrics for NLG evaluation, their potential remains unexplored for opinion summary evaluation. Furthermore, the absence of sufficient opinion summary evaluation datasets hinders progress in this area. In response, we introduce the dataset, encompassing $7$ dimensions crucial to the evaluation of opinion summaries: , , , , , , and . We propose , a dimension-independent prompt, along with , a dimension-dependent set of prompts for opinion summary evaluation. Our experiments demonstrate that emerges as a good alternative for evaluating opinion summaries, achieving an average Spearman correlation of $$ with human judgments, surpassing prior methodologies. Remarkably, we are the first to explore the efficacy of LLMs as evaluators, both on closed-source and open-source models, in the opinion summary evaluation domain.","{\textsc{G-Eval} vs. \textsc{Op-I-Prompt}}. On closed-source model (\chatgpt{3.5}) our \textsc{Op-I-Prompt} shows comparable performance whereas on open-source model (\mistral{7}) our approach outperforms \textsc{G-Eval} on $7$ dimensions: \fl \:(FA), \coh \:(CO), \rel \:(RE), \fa \:(FA), \asp \:(AC), \sent \:(SC), and \spec \:(SP). Check Figure \ref{fig:generation_runs} for more details."
AutoDSL: Automated domain-specific language design for structural representation of procedures with constraints,2406.12324v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.12324v1_0.pdf,"Accurate representation of procedures in restricted scenarios, such as non-standardized scientific experiments, requires precise depiction of constraints. Unfortunately, , as an effective tool to express constraints structurally, often requires case-by-case hand-crafting, necessitating customized, labor-intensive efforts. To overcome this challenge, we introduce the framework to automate -based constraint design across various domains. Utilizing domain specified experimental protocol corpora, optimizes syntactic constraints and abstracts semantic constraints. Quantitative and qualitative analyses of the designed by across five distinct domains highlight its potential as an auxiliary module for language models, aiming to improve procedural planning and execution.","{Representative constraints in protocols.} {(A)} Parameter omission: This refers to the absence of essential parameter values within a predefined set, \eg, the lack of temperature specification during the denaturation step in Protein Gel Electrophoresis. {(B)} Parameter under-specification: This occurs when a quantitative parameter is described using qualitative terms, leading to ambiguity, \eg, unclear mixture configurations in DNA Extraction. {(C)} Action undefinition: This involves the description of procedural steps at a high level without grounding to the specific, executable actions required, \eg, the vague \texttt{change} operation in Cell Preparation. {(D)} Iterative control logic: Loops that operate iteratively to satisfy a final condition rather than straightforwardly, as seen in PCR Optimization. {(E)} Memory management: Drawing a parallel with computer memory mechanisms, laboratory procedures also face constraints on the availability of storage for intermediates, necessitating explicit reallocation of containers and devices, such as managing buffers in Protein Digestion. {(F)} Concurrent management: The synchronization of actions without dependencies to maximize time efficiency and resource utilization, \eg, reagent splitting in RNA Extraction."
VIEScore: Towards Explainable Metrics for Conditional Image Synthesis Evaluation,2312.14867v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2312.14867v2_0.pdf,"In the rapidly advancing field of conditional image generation research, challenges such as limited explainability lie in effectively evaluating the performance and capabilities of various models. This paper introduces , a Visual Instruction-guided Explainable metric for evaluating any conditional image generation tasks. leverages general knowledge from Multimodal Large Language Models (MLLMs) as the backbone and does not require training or fine-tuning. We evaluate on seven prominent tasks in conditional image tasks and found: (1) (GPT4-o) achieves a high Spearman correlation of 0.4 with human evaluations, while the human-to-human correlation is 0.45. (2) (with open-source MLLM) is significantly weaker than GPT-4o and GPT-4v in evaluating synthetic images. (3) achieves a correlation on par with human ratings in the generation tasks but struggles in editing tasks. With these results, we believe shows its great potential to replace human judges in evaluating image synthesis tasks.",We study the correlation between MLLMs and human perspectives on rating images.
Tree-of-Traversals: A Zero-Shot Reasoning Algorithm for Augmenting Black-box Language Models with Knowledge Graphs,2407.21358v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2407.21358v1_0.pdf;/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2407.21358v1_1.pdf;/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2407.21358v1_2.pdf;/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2407.21358v1_3.pdf;/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2407.21358v1_4.pdf;/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2407.21358v1_5.pdf,"Knowledge graphs (KGs) complement Large Language Models (LLMs) by providing reliable, structured, domain-specific, and up-to-date external knowledge. However, KGs and LLMs are often developed separately and must be integrated after training. We introduce Tree-of-Traversals, a novel zero-shot reasoning algorithm that enables augmentation of black-box LLMs with one or more KGs. The algorithm equips a LLM with actions for interfacing a KG and enables the LLM to perform tree search over possible thoughts and actions to find high confidence reasoning paths. We evaluate on two popular benchmark datasets. Our results show that significantly improves performance on question answering and KG question answering tasks. Code is available at","An example of how \tot uses a KG interface for the query, ``What actor played in both Inception and Interstellar?''."
Investigating Cultural Alignment of Large Language Models,2402.13231v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.13231v2_0.pdf,"The intricate relationship between language and culture has long been a subject of exploration within the realm of linguistic anthropology. Large Language Models (LLMs), promoted as repositories of collective human knowledge, raise a pivotal question: do these models genuinely encapsulate the diverse knowledge adopted by different cultures? Our study reveals that these models demonstrate greater cultural alignment along two dimensions—firstly, when prompted with the dominant language of a specific culture, and secondly, when pretrained with a refined mixture of languages employed by that culture. We quantify cultural alignment by simulating sociological surveys, comparing model responses to those of actual survey participants as references. Specifically, we replicate a survey conducted in various regions of Egypt and the United States through prompting LLMs with different pretraining data mixtures in both Arabic and English with the personas of the real respondents and the survey questions. Further analysis reveals that misalignment becomes more pronounced for underrepresented personas and for culturally sensitive topics, such as those probing social values. Finally, we introduce Anthropological Prompting, a novel method leveraging anthropological reasoning to enhance cultural alignment. Our study emphasizes the necessity for a more balanced multilingual pretraining dataset to better represent the diversity of human experience and the plurality of different cultures with many implications on the topic of cross-lingual transfer.}",Our framework for measuring the cultural alignment of LLM knowledge/output and ground-truth cultural data collected through survey responses.
RAID: A Shared Benchmark for Robust Evaluation of Machine-Generated Text Detectors,2405.07940v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2405.07940v2_0.pdf,"Many commercial and open-source models claim to detect machine-generated text with extremely high accuracy (99\% or more). However, very few of these detectors are evaluated on shared benchmark datasets and even when they are, the datasets used for evaluation are insufficiently challenging—lacking variations in sampling strategy, adversarial attacks, and open-source generative models. In this work we present RAID: the largest and most challenging benchmark dataset for machine-generated text detection. RAID includes over 6 million generations spanning 11 models, 8 domains, 11 adversarial attacks and 4 decoding strategies. Using RAID, we evaluate the out-of-domain and adversarial robustness of 8 open- and 4 closed-source detectors and find that current detectors are easily fooled by adversarial attacks, variations in sampling strategies, repetition penalties, and unseen generative models. We release our data} along with a leaderboard} to encourage future research.",Detectors for machine-generated text are often highly performant on default model settings but fail to detect more unusual settings such as using random sampling with a repetition penalty.
"Silent Signals, Loud Impact: LLMs for Word-Sense Disambiguation of Coded Dog Whistles",2406.06840v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.06840v2_0.pdf,"*{-1pt} {{Warning: This paper contains content that may be upsetting or offensive to some readers.}} A dog whistle is a form of coded communication that carries a secondary meaning to specific audiences and is often weaponized for racial and socioeconomic discrimination. Dog whistling historically originated from United States politics, but in recent years has taken root in social media as a means of evading hate speech detection systems and maintaining plausible deniability. In this paper, we present an approach for word-sense disambiguation of dog whistles from standard speech using Large Language Models (LLMs), and leverage this technique to create a dataset of high-confidence coded examples of dog whistles used in formal and informal communication. {Silent Signals}} is the largest dataset of disambiguated dog whistle usage, created for applications in hate speech detection, neology, and political science.",This figure demonstrates the nuances of dog whistle detection as a word can be used in a {coded} or {non-coded} sense. {All illustrations were created using Adobe Firefly.}
Analyzing LLM Behavior in Dialogue Summarization: Unveiling Circumstantial Hallucination Trends,2406.03487v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.03487v1_0.pdf,"Recent advancements in large language models (LLMs) have considerably advanced the capabilities of summarization systems. However, they continue to face concerns about {hallucination}. While prior work has evaluated LLMs extensively in news domains, most evaluation of dialogue summarization has focused on BART-based models, leaving a gap in our understanding of their faithfulness. Our work benchmarks the faithfulness of LLMs for dialogue summarization, using human annotations and focusing on identifying and categorizing span-level inconsistencies. Specifically, we focus on two prominent LLMs: GPT-4 and Alpaca-13B. Our evaluation reveals subtleties as to what constitutes a hallucination: LLMs often generate plausible inferences, supported by circumstantial evidence in the conversation, that lack direct evidence, a pattern that is less prevalent in older models. We propose a refined taxonomy of errors, coining the category of ""Circumstantial Inference"" to bucket these LLM behaviors. Using our taxonomy, we compare the behavioral differences between LLMs and older fine-tuned models. Additionally, we systematically assess the efficacy of automatic error detection methods on LLM summaries and find that they struggle to detect these nuanced errors. To address this, we introduce two prompt-based approaches for fine-grained error detection that outperform existing metrics, particularly for identifying ""Circumstantial Inference."" }","In the example provided, GPT-4 infers that the speakers are discussing ""their son."" Although this inference seems plausible given the circumstantial evidence in the conversation, it lacks direct evidence."
LLM in a flash: Efficient Large Language Model Inference with Limited Memory,2312.11514v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2312.11514v3_0.pdf,"Large language models (LLMs) are central to modern natural language processing, delivering exceptional performance in various tasks. However, their substantial computational and memory requirements present challenges, especially for devices with limited DRAM capacity. This paper tackles the challenge of efficiently running LLMs that exceed the available DRAM capacity by storing the model parameters in flash memory, but bringing them on demand to DRAM. Our method involves constructing an inference cost model that takes into account the characteristics of flash memory, guiding us to optimize in two critical areas: reducing the volume of data transferred from flash and reading data in larger, more contiguous chunks. Within this hardware-informed framework, we introduce two principal techniques. First, ``windowing'' strategically reduces data transfer by reusing previously activated neurons, and second, ``row-column bundling'', tailored to the sequential data access strengths of flash memory, increases the size of data chunks read from flash memory. These methods collectively enable running models up to twice the size of the available DRAM, with up to 4x and 20x increase in inference speed compared to naive loading approaches in CPU and GPU, respectively. Our integration of sparsity awareness, context-adaptive loading, and a hardware-oriented design paves the way for effective inference of LLMs on devices with limited memory.","Average inference latency for a single token when only half of the model's memory is available: Our method selectively loads parameters on demand for each token generation step. The latency represents the time required to repeatedly load parameters from flash memory, combined with the time needed for computations."
Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models,2306.05424v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2306.05424v2_0.png,"Conversation agents fueled by Large Language Models (LLMs) are providing a new way to interact with visual data. While there have been initial attempts for image-based conversation models, this work addresses the under-explored field of {video-based conversation} by introducing Video-ChatGPT. It is a multimodal model that merges a video-adapted visual encoder with an LLM. The resulting model is capable of understanding and generating detailed conversations about videos. We introduce a new dataset of 100,000 video-instruction pairs used to train Video-ChatGPT acquired via manual and semi-automated pipeline that is easily scalable and robust to label noise. We also develop a quantitative evaluation framework for video-based dialogue models to objectively analyze the strengths and weaknesses of video-based dialogue models. Code: . [1]{Equal contribution.}","{Architecture of Video-ChatGPT.} Video-ChatGPT leverages the CLIP-L/14 visual encoder to extract both spatial and temporal video features. This is accomplished by averaging frame-level features across temporal and spatial dimensions respectively. The computed spatiotemporal features are then fed into a learnable linear layer, which projects them into the LLMs input space. In our approach, we utilize the Vicuna-v1.1 model, comprised of 7B parameters, and initialize it with weights from LLaVA~\cite{liu2023llava}."
Classist Tools: Social Class Correlates with Performance in NLP,2403.04445v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2403.04445v1_0.png;/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2403.04445v1_1.png,"Since the foundational work of William Labov on the social stratification of language , linguistics has made concentrated efforts to explore the links between socio-demographic characteristics and language production and perception. But while there is strong evidence for socio-demographic characteristics in language, they are infrequently used in Natural Language Processing (NLP). Age and gender are somewhat well represented, but Labov's original target, socioeconomic status, is noticeably absent. And yet it matters. We show empirically that NLP disadvantages less-privileged socioeconomic groups. We annotate a corpus of 95K utterances from movies with social class, ethnicity and geographical language variety and measure the performance of NLP systems on three tasks: language modelling, automatic speech recognition, and grammar error correction. We find significant performance disparities that can be attributed to socioeconomic status as well as ethnicity and geographical differences. %(1) the word-error rate (WER) between the actual script and its automatic transcription. %WER strongly correlates with socioeconomic class. With NLP technologies becoming ever more ubiquitous and quotidian, % present in our everyday lives, they must accommodate all language varieties to avoid disadvantaging already marginalised groups. We argue for the inclusion of socioeconomic class in future language technologies.%",Wav2Vec2
A Community-Centric Perspective for Characterizing and Detecting Anti-Asian Violence-Provoking Speech,2407.15227v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2407.15227v1_0.pdf,"Violence-provoking speech – speech that implicitly or explicitly promotes violence against the members of the targeted community, contributed to a massive surge in anti-Asian crimes during the COVID-19 pandemic. While previous works have characterized and built tools for detecting other forms of harmful speech, like fear speech and hate speech, our work takes a community-centric approach to studying anti-Asian violence-provoking speech. Using data from $420k$ Twitter posts spanning a 3-year duration (January 1, 2020 to February 1, 2023), we develop a codebook to characterize anti-Asian violence-provoking speech and collect a community-crowdsourced dataset to facilitate its large-scale detection using state-of-the-art classifiers. We contrast the capabilities of natural language processing classifiers, ranging from BERT-based to LLM-based classifiers, in detecting violence-provoking speech with their capabilities to detect anti-Asian hateful speech. In contrast to prior work that has demonstrated the effectiveness of such classifiers in detecting hateful speech ($F_1 = 0.89$), our work shows that accurate and reliable detection of violence-provoking speech is a challenging task ($F_1 = 0.69$). We discuss the implications of our findings, particularly the need for proactive interventions to support Asian communities during public health crises.\\ {{Warning: this paper contains content that may be offensive or upsetting.}}","{{Study Overview.} Our study comprises 4 key parts: {(i)} collecting anti-Asian COVID-19 data from Twitter, {(ii)} developing and validating anti-Asian violence-provoking speech codebook, {(iii)} obtaining community-centric annotations, and {(iv)} training and evaluating detection classifiers on community-crowdsourced data.}"
Generating Coherent Sequences of Visual Illustrations for Real-World Manual Tasks,2405.10122v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2405.10122v1_0.png,"Multistep instructions, such as recipes and how-to guides, greatly benefit from visual aids, such as a series of images that accompany the instruction steps. While Large Language Models (LLMs) have become adept at generating coherent textual steps, Large Vision/Language Models (LVLMs) are less capable of generating accompanying image sequences. The most challenging aspect is that each generated image needs to adhere to the relevant textual step instruction, as well as be visually consistent with earlier images in the sequence. To address this problem, we propose an approach for generating consistent image sequences, which integrates a Latent Diffusion Model (LDM) with an LLM to transform the sequence into a caption to maintain the semantic coherence of the sequence. In addition, to maintain the visual coherence of the image sequence, we introduce a copy mechanism to initialise reverse diffusion processes with a latent vector iteration from a previously generated image from a relevant step. Both strategies will condition the reverse diffusion process on the sequence of instruction steps and tie the contents of the current image to previous instruction steps and corresponding images. Experiments show that the proposed approach is preferred by humans in 46.6\% of the cases against 26.6\% for the second best method. In addition, automatic metrics showed that the proposed method maintains semantic coherence and visual consistency across the sequence of visual illustrations.",The properties of the elements in illustrations should remain coherent throughout the whole sequence.
LoRA-Flow: Dynamic LoRA Fusion for Large Language Models in Generative Tasks,2402.11455v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.11455v1_0.pdf,"LoRA employs lightweight modules to customize large language models (LLMs) for each downstream task or domain, where different learned additional modules represent diverse skills. Combining existing LoRAs to address new tasks can enhance the reusability of learned LoRAs, particularly beneficial for tasks with limited annotated data. Most prior works on LoRA combination primarily rely on task-level weights for each involved LoRA, making different examples and tokens share the same LoRA weights. However, in generative tasks, different tokens may necessitate diverse skills to manage. Taking the Chinese math task as an example, understanding the problem description may depend more on the Chinese LoRA, while the calculation part may rely more on the math LoRA. To this end, we propose LoRA-Flow, which utilizes dynamic weights to adjust the impact of different LoRAs. The weights at each step are determined by a fusion gate with extremely few parameters, which can be learned with only 200 training examples. Experiments across six generative tasks demonstrate that our method consistently outperforms baselines with task-level fusion weights. This underscores the necessity of introducing dynamic fusion weights for LoRA combination..}","Illustration of the proposed LoRA-Flow method. For the token $y_t$ at the $t$-th step, we use a gate that conditions on the prefix $\mathbf{y}_{<t}$ to determine the fusion weights. The dynamic fusion weights are intended to control the influence of different LoRA modules, to better cope with various types of tokens in generative tasks. Red and blue rectangles represent the weights assigned to the two involved LoRAs."
Generalizability of Mixture of Domain-Specific Adapters from the Lens of Signed Weight Directions and its Application to Effective Model Pruning,2402.10639v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.10639v2_0.pdf,"Several parameter-efficient fine-tuning methods based on adapters have been proposed as a streamlined approach to incorporate not only a single specialized knowledge into existing Pre-Trained Language Models (PLMs) but also multiple of them at once. Recent works such as AdapterSoup propose to mix not all but only a selective sub-set of domain-specific adapters during inference via model weight averaging to optimize performance on novel, unseen domains with excellent computational efficiency. However, the essential generalizability of this emerging weight-space adapter mixing mechanism on {unseen, in-domain examples} remains unexplored. Thus, in this study, we conduct a comprehensive analysis to elucidate the generalizability of domain-specific adapter mixtures in in-domain evaluation. We also provide investigations into the inner workings of the mixture of domain-specific adapters by analyzing their weight signs, yielding critical analysis on the negative correlation between their fraction of weight sign difference and their mixtures' generalizability. The code is available at {Github}.","Mixing the adapter weights across various tasks may result in the importance weights of individual tasks nullifying each other, thereby yielding a merged mixture losing important information."
CritiqueLLM: Towards an Informative Critique Generation Model for Evaluation of Large Language Model Generation,2311.18702v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2311.18702v2_0.pdf,"Since the natural language processing (NLP) community started to make large language models (LLMs) act as a critic to evaluate the quality of generated texts, most of the existing works train a critique generation model on the evaluation data labeled by GPT-4's direct prompting. We observe that these models lack the ability to generate informative critiques in both pointwise grading and pairwise comparison especially without references. As a result, their generated critiques cannot provide fine-grained distinguishability on generated texts, causing unsatisfactory evaluation performance. In this paper, we propose a simple yet effective method called {Eval-Instruct}, which can first acquire pointwise grading critiques with pseudo references and then revise these critiques via multi-path prompting to obtain informative evaluation data in different tasks and settings, including pointwise grading and pairwise comparison with / without references. After fine-tuning on these data, the resulting model is empirically shown to outperform ChatGPT and all the open-source baselines and even achieve comparable evaluation performance to GPT-4 in system-level correlations of pointwise grading. We also demonstrate that our generated critiques can act as scalable feedback to further improve the generation quality of strong LLMs like ChatGPT.}.","Overview of Eval-Instruct. Starting from referenced pointwise grading data, our proposed multi-path prompting method can apply pointwise-to-pairwise and referenced-to-reference-free prompting strategies to acquire evaluation data in other tasks and settings via two different paths. Cross validation is adopted to filter out the contradictory data from these two paths and further improve the data quality."
Effects of diversity incentives on sample diversity and downstream model performance in LLM-based text augmentation,2401.06643v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2401.06643v3_0.png,"The latest generative large language models (LLMs) have found their application in data augmentation tasks, where small numbers of text samples are LLM-paraphrased and then used to fine-tune downstream models. However, more research is needed to assess how different prompts, seed data selection strategies, filtering methods, or model settings affect the quality of paraphrased data (and downstream models). In this study, we investigate three text diversity incentive methods well established in crowdsourcing: {taboo} words, {hints} by previous outlier solutions, and {chaining} on previous outlier solutions. Using these incentive methods as part of instructions to LLMs augmenting text datasets, we measure their effects on generated texts' lexical diversity and downstream model performance. We compare the effects over 5 different LLMs, 6 datasets and 2 downstream models. We show that diversity is most increased by {taboo} words, but downstream model performance is highest with {hints}.","Overview of our methodology. For each dataset, we randomly sample 6 samples per label that are used as seed sentences for LLM data augmentation. There, we collect data in in 2 rounds - 1st only using the {prompt} method and then in parallel for {prompt} method and 3 different diversity incentive methods. These are added together to form the datasets. BERT-large or Mistral classifier is fine-tuned 5 or 3 times respectively on each of the collected data and then evaluated. We repeat the entire process 5 times."
Bridging the Empirical-Theoretical Gap in Neural Network Formal Language Learning Using Minimum Description Length,2402.10013v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.10013v2_0.png;/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.10013v2_1.png;/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.10013v2_2.png,"Neural networks offer good approximation to many tasks but consistently fail to reach perfect generalization, even when theoretical work shows that such perfect solutions can be expressed by certain architectures. Using the task of formal language learning, we focus on one simple formal language and show that the theoretically correct solution is in fact not an optimum of commonly used objectives --- even with regularization techniques that according to common wisdom should lead to simple weights and good generalization (L1,~L2) or other meta-heuristics (early-stopping, dropout). On the other hand, replacing standard targets with the Minimum Description Length objective (MDL) results in the correct solution being an optimum.",L1
Context versus Prior Knowledge in Language Models,2404.04633v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2404.04633v3_0.pdf,"To answer a question, language models often need to integrate prior knowledge learned during pretraining and new information presented in context. We hypothesize that models perform this integration in a predictable way across different questions and contexts: models will rely more on prior knowledge for questions about entities (e.g., persons, places, etc.) that they are more familiar with due to higher exposure in the training corpus, and be more easily persuaded by some contexts than others. To formalize this problem, we propose two mutual information-based metrics to measure a model's dependency on a context and on its prior about an entity: first, the of a given context represents how much a model depends on the context in its decision, and second, the of a given entity represents how much the model can be swayed away from its original answer distribution about an entity. We empirically test our metrics for their validity and reliability. Finally, we explore and find a relationship between the scores and the model's expected familiarity with an entity, and provide two use cases to illustrate their benefits. [width=1.25em,height=1.15em]{figures/github.png} {}","In answering a given query, a model may be more {\entitytext{susceptible}} to context for some \entitytext{entities} than others, while some \contexttext{contexts} may be more {\contexttext{persuasive}} than others (as indicated in this figure by color darkness in the rightmost column). We introduce mutual information-based metrics to evaluate how much impact the context has relative to the prior knowledge of a model."
Word Matters: What Influences Domain Adaptation in Summarization?,2406.14828v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.14828v1_0.png;/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.14828v1_1.png;/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.14828v1_2.png,"Domain adaptation aims to enable Large Language Models (LLMs) to generalize domain datasets unseen effectively during the training phase. However, factors such as the size of the model parameters and the scale of training data are general influencers and do not reflect the nuances of domain adaptation performance. This paper investigates the fine-grained factors affecting domain adaptation performance, analyzing the specific impact of `words' in training data on summarization tasks. We propose quantifying dataset learning difficulty as the learning difficulty of generative summarization, which is determined by two indicators: word-based compression rate and abstraction level. Our experiments conclude that, when considering dataset learning difficulty, the cross-domain overlap and the performance gain in summarization tasks exhibit an approximate linear relationship, which is not directly related to the number of words. Based on this finding, predicting a model's performance on unknown domain datasets is possible without undergoing training. Source code and scripts are available at .",Single-domain adaptation. The dotted line reflects a changing trend fitting the scatter data. The light-colored area represents a standard deviation of plus or minus.
Faithful Logical Reasoning via Symbolic Chain-of-Thought,2405.18357v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2405.18357v2_0.pdf,"While the recent Chain-of-Thought (CoT) technique enhances the reasoning ability of large language models (LLMs) with the theory of mind, it might still struggle in handling logical reasoning that relies much on symbolic expressions and rigid deducing rules. To strengthen the logical reasoning capability of LLMs, we propose a novel Symbolic Chain-of-Thought, namely {SymbCoT}, a fully LLM-based framework that integrates symbolic expressions and logic rules with CoT prompting. Technically, building upon an LLM, SymbCoT 1) first translates the natural language context into the symbolic format, and then 2) derives a step-by-step plan to solve the problem with symbolic logical rules, 3) followed by a verifier to check the translation and reasoning chain. Via thorough evaluations on 5 standard datasets with both First-Order Logic and Constraint Optimization symbolic expressions, SymbCoT shows striking improvements over the CoT method consistently, meanwhile refreshing the current state-of-the-art performances. We further demonstrate that our system advances in more faithful, flexible, and explainable logical reasoning. To our knowledge, this is the first to combine symbolic expressions and rules into CoT for logical reasoning with LLMs. Code is open at .",An illustrative example of logical reasoning via Chain-of-Thought and our proposed Symbolic CoT (SymbCoT).
ESCoT: Towards Interpretable Emotional Support Dialogue Systems,2406.10960v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.10960v1_0.pdf,"Understanding the reason for emotional support response is crucial for establishing connections between users and emotional support dialogue systems. Previous works mostly focus on generating better responses but ignore interpretability, which is extremely important for constructing reliable dialogue systems. To empower the system with better interpretability, we propose an emotional support response generation scheme, named {E}motion-Focused and {S}trategy-Driven {C}hain-{o}f-{T}hought ({ESCoT}), mimicking the process of {identifying}, {understanding}, and {regulating} emotions. Specially, we construct a new dataset with ESCoT in two steps: (1) {Dialogue Generation} where we first generate diverse conversation situations, then enhance dialogue generation using richer emotional support strategies based on these situations; (2) {Chain Supplement} where we focus on supplementing selected dialogues with elements such as emotion, stimulus, appraisal, and strategy reason, forming the manually verified chains. Additionally, we further develop a model to generate dialogue responses with better interpretability. We also conduct extensive experiments and human evaluations to validate the effectiveness of the proposed ESCoT and generated dialogue responses. Our data and code are available at {https://github.com/TeigenZhang/ESCoT}.","Illustration of the ESCoT scheme. The supporter first {identifies emotion}, then {understands emotion} from perspectives of emotional stimulus and individual appraisal, and finally chooses the appropriate strategy and responds to the seeker to {regulate emotion}."
PathReasoner: Modeling Reasoning Path with Equivalent Extension for Logical Question Answering,2405.19109v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2405.19109v1_0.jpg,"Logical reasoning task has attracted great interest since it was proposed. Faced with such a task, current competitive models, even large language models (e.g., ChatGPT and PaLM 2), still perform badly. Previous promising LMs struggle in logical consistency modeling and logical structure perception. To this end, we model the logical reasoning task by transforming each logical sample into reasoning paths and propose an architecture {PathReasoner}. It addresses the task from the views of both data and model. To expand the diversity of the logical samples, we propose an atom extension strategy supported by equivalent logical formulas, to form new reasoning paths. From the model perspective, we design a stack of transformer-style blocks. In particular, we propose a path-attention module to joint model in-atom and cross-atom relations with the high-order diffusion strategy. Experiments show that PathReasoner achieves competitive performances on two logical reasoning benchmarks and great generalization abilities.","Probing tests on representative LMs (e.g., RoBERTa). (a) is about model prediction consistency. (b) is related to the perception of logical connectives. Detailed pilot experiments are shown in the Appendix."
Advancing Parameter Efficiency in Fine-tuning via Representation Editing,2402.15179v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.15179v3_0.png,"Parameter Efficient Fine-Tuning (PEFT) techniques have drawn significant attention due to their ability to yield competitive results while updating only a small portion of the adjustable parameters. However, existing PEFT methods pose challenges in hyperparameter selection, such as choosing the rank for LoRA or Adapter, or specifying the length of soft prompts. To address these challenges, we propose a novel fine-tuning approach for neural models, named Representation EDiting (RED), which modifies the representations generated at some layers through the application of scaling and biasing operations. While existing PEFT methods still demonstrate over-parameterization that could potentially undermine the generalization ability acquired from pre-training, RED can substantially reduce the number of trainable parameters by a factor of $25,700$ compared to full parameter fine-tuning and by a factor of $32$ relative to LoRA. Remarkably, RED achieves results comparable or superior to both full parameter fine-tuning and other PEFT methods. Extensive experiments across various model architectures and scales, including RoBERTa, GPT-2, T5, and LLaMA-2, have demonstrated the effectiveness and efficiency of RED.}, thereby positioning it as a promising PEFT strategy for large-scale neural models.","{fig:figure_01} Comparison of previous representative PEFT methods with the proposed RED. (a) LoRA incorporates learnable bottleneck-shaped modules (highlighted in \textcolor{orange}{orange}) by integrating additional connections parallel to the $\mathbf{W}_q$ and $\mathbf{W}_v$ matrices of attention blocks, along with modifying the weights of these matrices in a low-rank fashion. Adapter, on the other hand, introduces learnable modules within similar structures (also highlighted in \textcolor{orange}{orange}) by incorporating additional connections following both the attention and feed-forward sub-layers. (b) RED introduces two learnable vectors, $l_\text{scaling}$ and $l_\text{bias}$, to directly edit the representations (marked in \textcolor{teal}{green}) generated by feed-forward sub-layers, which significantly reduces the number of parameters required for fine-tuning. \vspace{-5mm}"
"Chunk, Align, Select: A Simple Long-sequence Processing Method for Transformers",2308.13191v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2308.13191v2_0.pdf,"Although dominant in natural language processing, transformer-based models still struggle with long-sequence processing, due to the computational costs of their self-attention operations, which increase exponentially as the length of the input sequence grows. To address this challenge, we propose a {Sim}ple framework to enhance the long-content processing of off-the-shelf pre-trained transformers via three steps: {C}hunk, {A}lign, and {S}elect (SimCAS). More specifically, we first divide each long-sequence input into a batch of chunks, then align the inter-chunk information during the encoding steps, and finally, select the most representative hidden states from the encoder for the decoding process. With our SimCAS, the computation and memory costs can be reduced to linear complexity. In experiments, we demonstrate the effectiveness of the proposed method on various real-world long-text summarization and reading comprehension tasks, in which SimCAS significantly outperforms prior long-sequence processing baselines. The code is at . %{https://github.com/xjw-nlp/SimCAS}.","The SimCAS framework: The long inputs are first divided into a batch of chunks, each of which is filled with start token \texttt{[S]}, padding token \texttt{[P]} and end token \texttt{[E]}. Then the inter-chunk information can be transferred via the alignment of \texttt{[S]} and \texttt{[E]} representations after each encoder layer. Next, hidden states are selected for decoding steps. The decoder output logits and attention scores are utilized as rewards for updating the token selector."
Combining Supervised Learning and Reinforcement Learning for Multi-Label Classification Tasks with Partial Labels,2406.16293v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.16293v1_0.pdf,"Traditional supervised learning heavily relies on human-annotated datasets, especially in data-hungry neural approaches. However, various tasks, especially multi-label tasks like document-level relation extraction, pose challenges in fully manual annotation due to the specific domain knowledge and large class sets. Therefore, we address the multi-label positive-unlabelled learning (MLPUL) problem, where only a subset of positive classes is annotated. We propose Mixture Learner for Partially Annotated Classification (MLPAC), an RL-based framework combining the exploration ability of reinforcement learning and the exploitation ability of supervised learning. Experimental results across various tasks, including document-level relation extraction, multi-label image classification, and binary PU learning, demonstrate the generalization and effectiveness of our framework.","{A.} A partially annotated data sample in DocRE task. {B.} Severe imbalanced distribution of annotated positive (red scatters) and negative labels (blue and orange scatters) corresponding to the DocRED \cite{yao-etal-2019-docred} dataset. Orange scatters are actually-positive labels reannotated by Re-DocRED dataset \cite{wu2022revisiting} {C.} Results of training on incomplete DocRED and testing on reannotated Re-DocRED and DocGNRE \cite{li2023semi}. SSR-PU is sensitive to prior estimation, while ours is prior agnostic. {D.} Performance comparison in DocRE task."
Spatially-Aware Speaker for Vision-and-Language Navigation Instruction Generation,2409.05583v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2409.05583v1_0.pdf,"Embodied AI aims to develop robots that can {understand} and execute human language instructions, as well as communicate in natural languages. On this front, we study the task of generating highly detailed navigational instructions for the embodied robots to follow. Although recent studies have demonstrated significant leaps in the generation of step-by-step instructions from sequences of images, the generated instructions lack variety in terms of their referral to objects and landmarks. Existing speaker models learn strategies to evade the evaluation metrics and obtain higher scores even for low-quality sentences. In this work, we propose SAS (Spatially-Aware Speaker), an instruction generator or {Speaker} model that utilises both structural and semantic knowledge of the environment to produce richer instructions. For training, we employ a reward learning method in an adversarial setting to avoid systematic bias introduced by language evaluation metrics. Empirically, our method outperforms existing instruction generation models, evaluated using standard metrics. Our code is available at .","Extracting 3D scene relationships from house environments (a,b) can improve instruction generation by including object references (c)."
HiRoPE: Length Extrapolation for Code Models Using Hierarchical Position,2403.19115v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2403.19115v2_0.pdf,"Addressing the limitation of context length in large language models for code-related tasks is the primary focus of this paper. Existing LLMs are constrained by their pre-trained context lengths, leading to performance issues in handling long complex code sequences. Inspired by how human programmers navigate code, we introduce Hierarchical Rotary Position Embedding (HiRoPE), a novel approach that enhances the traditional rotary position embedding into a hierarchical format based on the hierarchical structure of source code. HiRoPE offers easy integration into existing LLMs without extra training costs. Our method is extensively evaluated with various LLMs, demonstrating stable performance in tasks such as language modeling and long code completion. We also introduce a new long code understanding task with real-world code projects, in hopes of promoting further development in this code-related field. Theoretically and experimentally, we find that HiRoPE also addresses the out-of-distribution issue in position encoding. Our HiRoPE significantly expands the context length capabilities of LLMs, enabling inference at lengths exponentially greater than the training length.","Illustration of the hierarchical position in source code, such as function-level and token-level positions. We also show a simplified abstract syntax tree of the code in the bottom left corner."
Never Lost in the Middle: Mastering Long-Context Question Answering with Position-Agnostic Decompositional Training,2311.09198v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2311.09198v2_0.png,"While large language models (LLMs) are equipped with longer text input capabilities than before, they struggle to seek correct information in long contexts. The ""lost in the middle"" problem challenges most LLMs, referring to the dramatic decline in accuracy when correct information is located in the middle. To overcome this crucial issue, this paper proposes to enhance the information searching and reflection ability of LLMs in long contexts via specially designed tasks called }osition-}gnostic }ulti-step QA (PAM QA). Trained with this task, our model excels in focusing more precisely on the desired information. Experimental results show substantial improvement in Multi-doc QA and other benchmarks, surpassing state-of-the-art models by a 13.7\% absolute gain in shuffled settings and by 21.5\% in the passage retrieval task. We release our model and code to promote related research in the community. and } %and }","The workflow of PAM QA. The blue dashed lines indicate information flows. The desired output of a sample is composed of three parts, corresponding to three steps: Question repetition, index prediction, and answer summarization. [i] refers to the index of the $i$-th document. An input sample is displayed on the top."
Meta-Tuning LLMs to Leverage Lexical Knowledge for Generalizable Language Style Understanding,2305.14592v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2305.14592v2_0.pdf,"Language style is often used by writers to convey their intentions, identities, and mastery of language. In this paper, we show that current large language models struggle to capture some language styles without fine-tuning. To address this challenge, we investigate whether LLMs can be meta-trained based on representative lexicons to recognize new styles they have not been fine-tuned on. Experiments on 13 established style classification tasks, as well as 63 novel tasks generated using LLMs, demonstrate that meta-training with style lexicons consistently improves zero-shot transfer across styles. We release the code and data at .","Overview of using lexicon-based instructions for cross-style zero-shot classification. It consists of two steps: (1) instruction tuning the model on training styles; (2) evaluating the learned model on unseen target styles zero-shot. A lexicon-based instruction is composed of \textcolor{blue}{instruction}, \textcolor{randClassForMethod}{class names}, \textcolor{subsetLexForMethod}{lexicons} and \textcolor{inputSentForMethod}{an input}."
Navigating the Dual Facets: A Comprehensive Evaluation of Sequential Memory Editing in Large Language Models,2402.11122v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.11122v1_0.pdf,"Memory Editing (ME) has emerged as an efficient method to modify erroneous facts or inject new facts into Large Language Models (LLMs). Two mainstream ME methods exist: parameter-modifying ME and parameter-preserving ME (integrating extra modules while preserving original parameters). Regrettably, previous studies on ME evaluation have two critical limitations: (i) {evaluating LLMs with single edit only}, %{single-edit evaluations}, neglecting the need for continuous editing, and (ii) {evaluations focusing solely on basic factual triples}, overlooking broader LLM capabilities like logical reasoning and reading understanding. This study addresses these limitations with contributions threefold: (i) We explore how ME affects a wide range of fundamental capabilities of LLMs under sequential editing. Experimental results reveal an intriguing phenomenon: Most parameter-modifying ME consistently degrade performance across all tasks after a few sequential edits. In contrast, parameter-preserving ME effectively maintains LLMs' fundamental capabilities but struggles to accurately recall edited knowledge presented in a different format. (ii) We extend our evaluation to different editing settings, such as layers to edit, model size, instruction tuning, etc. Experimental findings indicate several strategies that can potentially mitigate the adverse effects of ME. (iii) We further explain why parameter-modifying ME damages LLMs from three dimensions: parameter changes after editing, language modeling capability, and the in-context learning capability. Our in-depth study advocates more careful use of ME in real-world scenarios.","A comparison of two main limitations in previous memory editing evaluations. (a) shows the conventional method, assessing models after each edit, focused solely on the modified knowledge triples. (b) presents our approach, evaluating LLMs after a series of edits to assess their overall impact on various capabilities of LLMs, for a deeper insight into the enduring effects of memory editing."
"LLM-Rubric: A Multidimensional, Calibrated Approach to Automated Evaluation of Natural Language Texts",2501.00274v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2501.00274v1_0.pdf,"This paper introduces a framework for the automated evaluation of natural language texts. A manually constructed rubric describes how to assess multiple dimensions of interest. To evaluate a text, a large language model (LLM) is prompted with each rubric question and produces a distribution over potential responses. The LLM predictions often fail to agree well with human judges---indeed, the humans do not fully agree with one another. However, the multiple LLM distributions can be {combined} to {predict} each human judge's annotations on all questions, including a summary question that assesses overall quality or relevance. accomplishes this by training a small feed-forward neural network that includes both judge-specific and judge-independent parameters. When evaluating dialogue systems in a human-AI information-seeking task, we find that with $$ questions (assessing dimensions such as naturalness, conciseness, and citation quality) predicts human judges' assessment of overall user satisfaction, on a scale of 1--4, with RMS error $< $, a $2$ improvement over the uncalibrated baseline. =-1","An overview of the \llmeval framework. The LLM and its prompts are fixed across texts and judges, but the calibration network weights are trained to predict the responses of various human judges."
LIEDER: Linguistically-Informed Evaluation for Discourse Entity Recognition,2403.06301v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2403.06301v2_0.pdf,"Discourse Entity (DE) recognition is the task of identifying novel and known entities introduced within a text. While previous work has found that large language models have basic, if imperfect, DE recognition abilities , it remains largely unassessed which of the fundamental semantic properties that govern the introduction and subsequent reference to DEs they have knowledge of. We propose the Linguistically-Informed Evaluation for Discourse Entity Recognition (LIEDER) dataset that allows for a detailed examination of language models' knowledge of four crucial semantic properties: , , , and . We find evidence that state-of-the-art large language models exhibit sensitivity to all of these properties except , which demonstrates that they have yet to reach human-level language understanding abilities.",Results for singular continuations by model and comparison type. The dotted lines indicate chance performance and the error bars indicate bootstrapped 95\% confidence intervals.
NEO-BENCH: Evaluating Robustness of Large Language Models with Neologisms,2402.12261v4,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.12261v4_0.pdf,"The performance of Large Language Models (LLMs) degrades from the temporal drift between data used for model training and newer text seen during inference. One understudied avenue of language change causing data drift is the emergence of neologisms -- new word forms -- over time. We create a diverse resource of recent English neologisms by using several popular collection methods. We analyze temporal drift using neologisms by comparing sentences containing new words with near-identical sentences that replace neologisms with existing substitute words. Model performance is nearly halved in machine translation when a single neologism is introduced in a sentence. Motivated by these results, we construct a benchmark to evaluate LLMs' ability to generalize to neologisms with various natural language understanding tasks and model perplexity. Models with later knowledge cutoff dates yield lower perplexities and perform better in downstream tasks. LLMs are also affected differently based on the linguistic origins of words, indicating that neologisms are complex for static LLMs to address. We release our benchmark at: {https://github.com/JonathanQZheng/NEO-BENCH}.",{\sc Neo-Bench} collects neologisms from 2020-2023 for LLM evaluation. ``Pig Butchering'' originated as a Mandarin expression \begin{CJK*}{UTF8}{gbsn}(杀猪盘)\end{CJK*}.
What is the Best Way for ChatGPT to Translate Poetry?,2406.03450v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.03450v1_0.pdf,"Machine translation (MT) has historically faced significant challenges when applied to literary works, particularly in the domain of poetry translation. The advent of Large Language Models such as ChatGPT holds potential for innovation in this field. This study examines ChatGPT's capabilities in English-Chinese poetry translation tasks, utilizing targeted prompts and small sample scenarios to ascertain optimal performance. Despite promising outcomes, our analysis reveals persistent issues in the translations generated by ChatGPT that warrant attention. To address these shortcomings, we propose an Explanation-Assisted Poetry Machine Translation (EAPMT) method, which leverages monolingual poetry explanation as a guiding information for the translation process. Furthermore, we refine existing evaluation criteria to better suit the nuances of modern poetry translation. We engaged a panel of professional poets for assessments, complemented evaluations by using GPT-4. The results from both human and machine evaluations demonstrate that our EAPMT method outperforms traditional translation methods of ChatGPT and the existing online systems. This paper validates the efficacy of our method and contributes a novel perspective to machine-assisted literary translation.",Comparison between the framework of the traditional translation method and the proposed Explanation-Assisted Poetry Machine Translation (EAPMT).
Representation Learning with Conditional Information Flow Maximization,2406.05510v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.05510v2_0.png,"This paper proposes an information-theoretic representation learning framework, named conditional information flow maximization, to extract noise-invariant sufficient representations for the input data and target task. It promotes the learned representations have good feature uniformity and sufficient predictive ability, which can enhance the generalization of pre-trained language models (PLMs) for the target task. Firstly, an information flow maximization principle is proposed to learn more sufficient representations for the input and target by simultaneously maximizing both input-representation and representation-label mutual information. Unlike the information bottleneck, we handle the input-representation information in an opposite way to avoid the over-compression issue of latent representations. Besides, to mitigate the negative effect of potential redundant features from the input, we design a conditional information minimization principle to eliminate negative redundant features while preserve noise-invariant features. Experiments on 13 language understanding benchmarks demonstrate that our method effectively improves the performance of PLMs for classification and regression. Extensive experiments show that the learned representations are more sufficient, robust and transferable.",Venn information diagram comparison of our CIFM with existing principles. The learned representations by each principle is circled by the red dashed line.
GPT is Not an Annotator: The Necessity of Human Annotation in Fairness Benchmark Construction,2405.15760v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2405.15760v1_0.png,"Social biases in LLMs are usually measured via bias benchmark datasets. Current benchmarks have limitations in scope, grounding, quality, and human effort required. Previous work has shown success with a community-sourced, rather than crowd-sourced, approach to benchmark development. However, this work still required considerable effort from annotators with relevant lived experience. This paper explores whether an LLM (specifically, GPT-3.5-Turbo) can assist with the task of developing a bias benchmark dataset from responses to an open-ended community survey. We also extend the previous work to a new community and set of biases: the Jewish community and antisemitism. Our analysis shows that GPT-3.5-Turbo has poor performance on this annotation task and produces unacceptable quality issues in its output. Thus, we conclude that GPT-3.5-Turbo is not an appropriate substitute for human annotation in sensitive tasks related to social biases, and that its use actually negates many of the benefits of community-sourcing bias benchmarks.","Comparison of human post-evaluation results for GPT-WinoQueer (blue, left) and GPT-WinoSemitism (orange, right) datasets. GPT-WQ evaluation results are generally worse, with a lower proportion of correct responses and higher proportions of grammatically incorrect, opposite, and hallucinated responses."
Multi-modal Preference Alignment Remedies Degradation of Visual Instruction Tuning on Language Models,2402.10884v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.10884v2_0.png,"Multi-modal large language models (MLLMs) are expected to support multi-turn queries of interchanging image and text modalities in production. However, the current MLLMs trained with visual-question-answering (VQA) datasets could suffer from degradation, as VQA datasets lack the diversity and complexity of the original text instruction datasets with which the underlying language model was trained. To address this degradation, we first collect a lightweight, 5k-sample VQA preference dataset where answers were annotated by Gemini for five quality metrics in a granular fashion and investigate standard Supervised Fine-tuning, rejection sampling, Direct Preference Optimization (DPO) and SteerLM algorithms. Our findings indicate that with DPO, we can surpass the instruction-following capabilities of the language model, achieving a 6.73 score on MT-Bench, compared to Vicuna's 6.57 and LLaVA's 5.99. This enhancement in textual instruction-following capability correlates with boosted visual instruction performance (+4.9\% on MM-Vet, +6\% on LLaVA-Bench), with minimal alignment tax on visual knowledge benchmarks compared to the previous RLHF approach. In conclusion, we propose a distillation-based multi-modal alignment model with fine-grained annotations on a small dataset that restores and boosts MLLM's language capability after visual instruction tuning.","From a visual-instruction-tuned pre-trained model, we generate 4 completions for a given image-question prompt. These answers are then presented to Gemini to obtain granular annotations given a labeling guide. We construct a preference dataset of (image-text prompt, preferred completion) and (image-text prompt, rejected completion). We benchmarked DPO, Rejection sampling, and SteerLM alignment methods, in addition to a pure SFT baseline using Gemini-provided answers directly."
Multistage Collaborative Knowledge Distillation from a Large Language Model for Semi-Supervised Sequence Generation,2311.08640v4,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2311.08640v4_0.jpg,"We study semi-supervised sequence generation tasks, where the few labeled examples are too scarce to finetune a model, and meanwhile, few-shot prompted large language models (LLMs) exhibit room for improvement. In this paper, we present the discovery that a student model distilled from a few-shot prompted LLM can commonly generalize better than its teacher to unseen examples on such tasks. We find that the student is able to learn a general pattern from the high-quality pseudolabels produced by the teacher during knowledge distillation (KD), and favorably not a general pattern from the low-quality pseudolabels. Leveraging this discovery, we propose a new method, ~(), for these tasks. first few-shot prompts an LLM to produce pseudolabels for unlabeled data. Then at each stage of an iterative KD process, a new pair of students is trained on disjoint partitions of the pseudolabeled data, and produces new and improved pseudolabels for their unseen partitions. We conduct extensive experiments on four syntactic and semantic parsing datasets and show the effectiveness of \ for low-resource semi-supervised sequence generation. On CRAFT biomedical parsing, for example, 3-stage with 50 labeled examples outperforms an LLM teacher and vanilla KD by 7.5\% and 3.7\% parsing F1, respectively, and matches the performance of supervised finetuning with 500 labeled examples.~{github.com/andotalao24/Multistage-Collaborative-Knowledge-Distillation}.}%under an Apache-2.0 license.","Overview of \ours. {(1)} We use demonstrations from labeled data $\mathcal{D}^\text{labeled}$ to few-shot prompt an LLM teacher $t$ to produce pseudolabels for unlabeled data $\mathcal{D}^\text{unlabeled}$. We partition $\mathcal{D}^\text{unlabeled}$ into $\mathcal{D}^\text{unlabeled}_A$ and $\mathcal{D}^\text{unlabeled}_B$, and let $\mathcal{D}^{(x, \hat{y}_0)}_A$ and $\mathcal{D}^{(x, \hat{y}_0)}_B$ denote the same partitions but with teacher-generated pseudolabels. {(2)} At the $i$-th intermediate KD stage, students $s_A^i$ and $s_B^i$ are trained on previously pseudolabeled data $\mathcal{D}_A^{(x,\hat{y}_{i-1})}$ and $\mathcal{D}_B^{(x,\hat{y}_{i-1})}$, respectively, and leveraged to label the other partitions $\mathcal{D}_B^\text{unlabeled}$ and $\mathcal{D}_A^\text{unlabeled}$ and produce $\mathcal{D}_B^{(x,\hat{y}_i)}$ and $\mathcal{D}_A^{(x,\hat{y}_i)}$, which will be used to train the next-stage student(s). {(3)} In the final KD stage, a single final student $s^n$ is trained on both latest pseudolabeled partitions $\mathcal{D}^{(x, \hat{y}_{n-1})}_A$ and $\mathcal{D}^{(x, \hat{y}_{n-1})}_B$."
LogogramNLP: Comparing Visual and Textual Representations of Ancient Logographic Writing Systems for NLP,2408.04628v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2408.04628v1_0.pdf,"Standard natural language processing (NLP) pipelines operate on symbolic representations of language, which typically consist of sequences of discrete tokens. However, creating an analogous representation for ancient logographic writing systems is an extremely labor-intensive process that requires expert knowledge. At present, a large portion of logographic data persists in a purely visual form due to the absence of transcription---this issue poses a bottleneck for researchers seeking to apply NLP toolkits to study ancient logographic languages: most of the relevant data are {images of writing}. This paper investigates whether direct processing of visual representations of language offers a potential solution. We introduce {LogogramNLP}, the first benchmark enabling NLP analysis of ancient logographic languages, featuring both transcribed and visual datasets for four writing systems along with annotations for tasks like classification, translation, and parsing. Our experiments compare systems that employ recent visual and text encoding strategies as backbones. The results demonstrate that visual representations outperform textual representations for some investigated tasks, suggesting that visual processing pipelines may unlock a large amount of cultural heritage data of logographic languages for NLP-based analyses. Data and code are available at .","Glyph classification on Old Chinese (ZHO). {Left axis}: we plot the error rate of glyph classification. The data point at |G| = 50 shows the classification error calculated using the top 50 most frequent glyphs in the dataset. The purple horizontal line (71.85\%) represents the line-level text recognition CER for ZHO, provided for reference. {Right axis}: The frequency count (in orange bars) of each glyph in the dataset. Note that the counts are in logarithmic scale, illustrating the long tail distribution of glyph counts."
Confabulation: The Surprising Value of Large Language Model Hallucinations,2406.04175v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.04175v2_0.pdf,"This paper presents a systematic defense of large language model (LLM) hallucinations or `confabulations' as a potential resource instead of a categorically negative pitfall. The standard view is that confabulations are inherently problematic and AI research should eliminate this flaw. In this paper, we argue and empirically demonstrate that measurable semantic characteristics of LLM confabulations mirror a human propensity to utilize increased narrativity as a cognitive resource for sense-making and communication. In other words, it has potential value. Specifically, we analyze popular hallucination benchmarks and reveal that hallucinated outputs display increased levels of narrativity and semantic coherence relative to veridical outputs. This finding reveals a tension in our usually dismissive understandings of confabulation. It suggests, counter-intuitively, that the tendency for LLMs to confabulate may be intimately associated with a positive capacity for coherent narrative-text generation.","The left panel illustrates distribution for narrative score of hallucinated outputs (blue) and the edited version of the output (gray) in the \texttt{FaithDial} dataset. The hallucinated texts are, in general more narrative rich than those that are edited to resolve inaccuracies. The right panel illustrates distribution for non-hallucinated texts from the \texttt{FaithDial} dataset."
How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs,2401.06373v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2401.06373v2_0.pdf,"Most traditional AI safety research has approached AI models as machines and centered on algorithm-focused attacks developed by security experts. As {large language models} (LLMs) become increasingly common and competent, non-expert users can also impose risks during daily interactions. This paper introduces a new perspective on jailbreaking LLMs as human-like communicators to explore this overlooked intersection between everyday language interaction and AI safety. Specifically, we study how to persuade LLMs to jailbreak them. First, we propose a persuasion taxonomy derived from decades of social science research. Then we apply the taxonomy to automatically generate interpretable {persuasive adversarial prompts} (PAP) to jailbreak LLMs. Results show that persuasion significantly increases the jailbreak performance across all risk categories: PAP consistently achieves an attack success rate of over $92\%$ on Llama 2-7b Chat, GPT-3.5, and GPT-4 in $10$ trials, surpassing recent algorithm-focused attacks. On the defense side, we explore various mechanisms against PAP, find a significant gap in existing defenses, and advocate for more fundamental mitigation for highly interactive LLMs{Y. Zeng}, {W. Shi}, {R. Jia}}{ We have informed Meta and OpenAI of our findings. For safety concerns, we only publicly release our persuasion taxonomy at {}. Researchers can apply for the jailbreak data upon review.}.","We propose a persuasion taxonomy with persuasion techniques, and apply it to automatically paraphrase plain harmful queries into human-readable persuasive adversarial prompts (PAP). This method achieves an attack success rate of over {92\%} on Llama-2, GPT-3.5, and GPT-4 {without specialized optimization. }"
Causal-Guided Active Learning for Debiasing Large Language Models,2408.12942v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2408.12942v2_0.pdf,"Although achieving promising performance, recent analyses show that current generative large language models (LLMs) may still capture dataset biases and utilize them for generation, leading to poor generalizability and harmfulness of LLMs. However, due to the diversity of dataset biases and the over-optimization problem, previous prior-knowledge-based debiasing methods and fine-tuning-based debiasing methods may not be suitable for current LLMs. To address this issue, we explore combining active learning with the causal mechanisms and propose a casual-guided active learning (CAL) framework, which utilizes LLMs itself to automatically and autonomously identify informative biased samples and induce the bias patterns. Then a cost-effective and efficient in-context learning based method is employed to prevent LLMs from utilizing dataset biases during generation. Experimental results show that CAL can effectively recognize typical biased instances and induce various bias patterns for debiasing LLMs.",(a) Dataset bias under causal perspective (b) Illustration of the Causal-Guided Active Learning framework.
Towards Better Understanding of Contrastive Sentence Representation Learning: A Unified Paradigm for Gradient,2402.18281v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.18281v2_0.pdf,"Sentence Representation Learning (SRL) is a crucial task in Natural Language Processing (NLP), where contrastive Self-Supervised Learning (SSL) is currently a mainstream approach. However, the reasons behind its remarkable effectiveness remain unclear. Specifically, many studies have investigated the similarities between contrastive and non-contrastive SSL from a theoretical perspective. Such similarities can be verified in classification tasks, where the two approaches achieve comparable performance. But in ranking tasks (i.e., Semantic Textual Similarity (STS) in SRL), contrastive SSL significantly outperforms non-contrastive SSL. Therefore, two questions arise: First, {what commonalities enable various contrastive losses to achieve superior performance in STS?} Second, {how can we make non-contrastive SSL also effective in STS?} To address these questions, we start from the perspective of gradients and discover that four effective contrastive losses can be integrated into a unified paradigm, which depends on three components: the {Gradient Dissipation}, the {Weight}, and the {Ratio}. Then, we conduct an in-depth analysis of the roles these components play in optimization and experimentally demonstrate their significance for model performance. Finally, by adjusting these components, we enable non-contrastive SSL to achieve outstanding performance in STS. % SRL .}",Average Spearman's correlation on Semantic Textual Similarity tasks for ineffective optimization objectives before (``ori'') and after (``mod'') modifications under different backbones.
Emergent Word Order Universals from Cognitively-Motivated Language Models,2402.12363v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.12363v2_0.pdf,"The world's languages exhibit certain so-called typological or implicational universals; for example, Subject-Object-Verb (SOV) languages typically use postpositions. Explaining the source of such biases is a key goal of linguistics. We study word-order universals through a computational simulation with language models (LMs). Our experiments show that typologically-typical word orders tend to have lower perplexity estimated by LMs with cognitively plausible biases: syntactic biases, specific parsing strategies, and memory limitations. This suggests that the interplay of cognitive biases and predictability (perplexity) can explain many aspects of word-order universals. It also showcases the advantage of cognitively-motivated LMs, typically employed in cognitive modeling, in the simulation of language universals. [width=1.1em,height=1.1em]{logo/github.png} { }",We compare the word orders that are challenging for LMs to those that are infrequent in attested languages (\cref{sec:design}). We examine the advantage of cognitively-motivated LMs (\cref{sec:model}) in simulating the word-order universals (the world's word-order distribution) with their inductive biases (\cref{sec:experiment}).
CausalGym: Benchmarking causal interpretability methods on linguistic tasks,2402.12560v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.12560v1_0.pdf,"Language models (LMs) have proven to be powerful tools for psycholinguistic research, but most prior work has focused on purely behavioural measures (e.g., surprisal comparisons). At the same time, research in model interpretability has begun to illuminate the abstract causal mechanisms shaping LM behavior. To help bring these strands of research closer together, we introduce . We adapt and expand the SyntaxGym suite of tasks to benchmark the ability of interpretability methods to causally affect model behaviour. To illustrate how can be used, we study the models (14M--6.9B) and assess the causal efficacy of a wide range of interpretability methods, including linear probing and distributed alignment search (DAS). We find that DAS outperforms the other methods, and so we use it to study the learning trajectory of two difficult linguistic phenomena in : negative polarity item licensing and filler--gap dependencies. Our analysis shows that the mechanism implementing both of these tasks is learned in discrete stages, not gradually. [width=1.25em,height=1.25em]{logos/github.png}{}","The \benchmarktitle{} pipeline: {(1)} take an input minimal pair ($\mathbf{b}, \mathbf{s}$) exhibiting a linguistic alternation that affects next-token predictions ($y_b, y_s$); {(2)} intervene on the base forward pass using a pre-defined intervention function that operates on aligned representations from both inputs; {(3)} check how this intervention affected the next-token prediction probabilities. In aggregate, such interventions assess the causal role of the intervened representation on the model's behaviour."
Speech Translation with Speech Foundation Models and Large Language Models: What is There and What is Missing?,2402.12025v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.12025v3_0.png,"The field of natural language processing (NLP) has recently witnessed a transformative shift with the emergence of foundation models, particularly Large Language Models (LLMs) that have revolutionized text-based NLP. This paradigm has extended to other modalities, including speech, where researchers actively the combination of Speech Foundation Models (SFMs) and LLMs capable of addressing multimodal tasks. Among such tasks, this paper focuses on speech-to-text translation (ST). By examining the published papers on the topic, we propose a unified view of the architectural solutions and training strategies presented so far, highlighting similarities and differences among them. Based on this examination, the lessons learned but also % how diverse settings and evaluation approaches hinder the identification of the best-performing solution for each architectural building block and training choice.} Lastly, we recommendations for future works on the topic aimed at better understanding the strengths and weaknesses of the",\mg{Architectural building blocks of ST models based on the combination of an SFM and an LLM}.
Arabic Diacritics in the Wild: Exploiting Opportunities for Improved Diacritization,2406.05760v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.05760v1_0.png,"The widespread absence of diacritical marks in Arabic text poses a significant challenge for Arabic natural language processing (NLP). This paper explores instances of naturally occurring diacritics, referred to as ``diacritics in the wild,'' to unveil patterns and latent information across six diverse genres: news articles, novels, children's books, poetry, political documents, and ChatGPT outputs. We present a new annotated dataset that maps real-world partially diacritized words to their maximal full diacritization in context. Additionally, we propose extensions to the analyze-and-disambiguate approach in Arabic NLP to leverage these diacritics, resulting in notable improvements. Our contributions encompass a thorough analysis, valuable datasets, and an extended diacritization algorithm. We release our code and datasets as open source.","(a) The nine Arabic diacritics commonly used in Modern Standard Arabic, grouped by function; and four examples of diacritic clusters. (b) A visually annotated example of a diacritized phrase meaning `and the bright suns [lit. and-the-suns the-bright]'. Diacritics are marked in red; and so are the undiacritized vowel-lengthening helping letters. Silent letters appear in dotted boxes."
Disinformation Capabilities of Large Language Models,2311.08838v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2311.08838v2_0.pdf,"Automated disinformation generation is often listed as an important risk associated with large language models (LLMs). The theoretical ability to flood the information space with disinformation content might have dramatic consequences for societies around the world. This paper presents a comprehensive study of the disinformation capabilities of the current generation of LLMs to generate false news articles in the English language. In our study, we evaluated the capabilities of 10 LLMs using 20 disinformation narratives. We evaluated several aspects of the LLMs: how good they are at generating news articles, how strongly they tend to agree or disagree with the disinformation narratives, how often they generate safety warnings, etc. We also evaluated the abilities of detection models to detect these articles as LLM-generated. We conclude that LLMs are able to generate convincing news articles that agree with dangerous disinformation narratives.","Summary of how many generated texts we consider {dangerous} or {safe}. Dangerous texts are disinformation articles that could be misused by bad actors. Safe texts contain disclaimers, provide counterarguments, argue against the user, etc. Note that GPT-4 annotations are generally slightly biased towards safety."
Learn or Recall? Revisiting Incremental Learning with Pre-trained Language Models,2312.07887v5,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2312.07887v5_0.pdf,"Incremental Learning (IL) has been a long-standing problem in both vision and Natural Language Processing (NLP) communities. In recent years, as Pre-trained Language Models (PLMs) have achieved remarkable progress in various NLP downstream tasks, utilizing PLMs as backbones has become a common practice in recent research of IL in NLP. Most assume that catastrophic forgetting is the biggest obstacle to achieving superior IL performance and propose various techniques to overcome this issue. However, we find that this assumption is problematic. Specifically, we revisit more than 20 methods on four classification tasks (Text Classification, Intent Classification, Relation Extraction, and Named Entity Recognition) under the two most popular IL settings (Class-Incremental and Task-Incremental) and reveal that most of them severely underestimate the inherent anti-forgetting ability of PLMs. Based on the observation, we propose a frustratingly easy method called SEQ* for IL with PLMs. The results show that SEQ* has competitive or superior performance compared with state-of-the-art (SOTA) IL methods yet requires considerably less trainable parameters and training time. These findings urge us to revisit the IL with PLMs and encourage future studies to have a fundamental understanding of the catastrophic forgetting in PLMs. The data, code and scripts are publicly available {https://github.com/zzz47zzz/codebase-for-incremental-learning-with-llm}}.",The comparison between the proposed SEQ* and SOTA IL methods on five class-incremental tasks. We report the average accuracy after learning the final task. The detailed results are provided in Table \ref{tab:sota_main_gen_pythia410m}.
Cendol: Open Instruction-tuned Generative Large Language Models for Indonesian Languages,2404.06138v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2404.06138v2_0.png,"Large language models (LLMs) show remarkable human-like capability in various domains and languages. However, a notable quality gap arises in low-resource languages, e.g., Indonesian indigenous languages, rendering them ineffective and inefficient in such linguistic contexts. To bridge this quality gap, we introduce Cendol, a collection of Indonesian LLMs encompassing both decoder-only and encoder-decoder architectures across a range of model sizes. We highlight Cendol's effectiveness across a diverse array of tasks, attaining $$20\% improvement, and demonstrate its capability to generalize to unseen tasks and indigenous languages of Indonesia. Furthermore, Cendol models showcase improved human favorability despite their limitations in capturing indigenous knowledge and cultural values in Indonesia. In addition, we discuss the shortcomings of parameter-efficient tunings, such as LoRA, for language adaptation. Alternatively, we propose the usage of vocabulary adaptation to enhance efficiency. Lastly, we evaluate the safety of Cendol and showcase that safety in pre-training in one language such as English is transferable to low-resource languages, such as Indonesian, even without RLHF and safety fine-tuning.",Overview of Cendol Collection and LLM adaptation into Cendol$^{inst}$ and Cendol$^{chat}$ models.
Latxa: An Open Language Model and Evaluation Suite for Basque,2403.20266v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2403.20266v2_0.png,"We introduce Latxa, a family of large language models for Basque ranging from 7 to 70 billion parameters. %, together with a complete development ecosystem. Latxa is based on Llama 2, which we continue pretraining on a new Basque corpus comprising 4.3M documents and 4.2B tokens. Addressing the scarcity of high-quality benchmarks for Basque, we further introduce 4 multiple choice evaluation datasets: EusProficiency, comprising 5,169 questions from official language proficiency exams; EusReading, comprising 352 reading comprehension questions; EusTrivia, comprising 1,715 trivia questions from 5 knowledge areas; and EusExams, comprising 16,774 questions from public examinations. In our extensive evaluation, Latxa outperforms all previous open models we compare to by a large margin. In addition, it is competitive with GPT-4 Turbo in language proficiency and understanding, despite lagging behind in reading comprehension and knowledge-intensive tasks. Both the Latxa family of models, as well as our new pretraining corpora and evaluation datasets, are publicly available under open licenses.} Our suite enables reproducible research on methods to build LLMs for low-resource languages.",Validation perplexity throughout training.
Why are Sensitive Functions Hard for Transformers?,2402.09963v4,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.09963v4_0.pdf,"Empirical studies have identified a range of learnability biases and limitations of transformers, such as a persistent difficulty in learning to compute simple formal languages such as PARITY, and a bias towards low-degree functions. However, theoretical understanding remains limited, with existing expressiveness theory either overpredicting or underpredicting realistic learning abilities. We prove that, under the transformer architecture, the loss landscape is constrained by the input-space sensitivity: Transformers whose output is sensitive to many parts of the input string inhabit isolated points in parameter space, leading to a low-sensitivity bias in generalization. We show theoretically and empirically that this theory unifies a broad array of empirical observations about the learning abilities and biases of transformers, such as their generalization bias towards low sensitivity and low degree, and difficulty in length generalization for PARITY. This shows that understanding transformers' inductive biases requires studying not just their in-principle expressivity, but also their loss landscape.","Training transformers on inputs of increasing length produces a steeper loss landscape for PARITY (as measured by average direction sharpness), while the loss landscape of MAJORITY does not show significant changes. Our main result (Theorem~\ref{thm:lrho-bound}) provides a rigorous explanation for this phenomenon."
Talk With Human-like Agents: Empathetic Dialogue Through Perceptible Acoustic Reception and Reaction,2406.12707v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.12707v1_0.pdf,"Large Language Model (LLM)-enhanced agents become increasingly prevalent in Human-AI communication, offering vast potential from entertainment to professional domains. However, current multi-modal dialogue systems overlook the acoustic information present in speech, which is crucial for understanding human communication nuances. This oversight can lead to misinterpretations of speakers' intentions, resulting in inconsistent or even contradictory responses within dialogues. To bridge this gap, in this paper, we propose PerceptiveAgent, an empathetic multi-modal dialogue system designed to discern deeper or more subtle meanings beyond the literal interpretations of words through the integration of speech modality perception. Employing LLMs as a cognitive core, PerceptiveAgent perceives acoustic information from input speech and generates empathetic responses based on speaking styles described in natural language. Experimental results indicate that PerceptiveAgent excels in contextual understanding by accurately discerning the speakers' true intentions in scenarios where the linguistic meaning is either contrary to or inconsistent with the speaker's true feelings, producing more nuanced and expressive spoken dialogues. Code is publicly available at: .",Examples illustrating the definition of empathy within dialogues.
WebCiteS: Attributed Query-Focused Summarization on Chinese Web Search Results with Citations,2403.01774v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2403.01774v2_0.pdf,"Enhancing the attribution in large language models (LLMs) is a crucial task. One feasible approach is to enable LLMs to cite external sources that support their generations. However, existing datasets and evaluation methods in this domain still exhibit notable limitations. In this work, we formulate the task of attributed query-focused summarization~(AQFS) and present WebCiteS, a Chinese dataset featuring 7k human-annotated summaries with citations. WebCiteS derives from real-world user queries and web search results, offering a valuable resource for model training and evaluation. Prior works in attribution evaluation do not differentiate between groundedness errors and citation errors. They also fall short in automatically verifying sentences that draw partial support from multiple sources. We tackle these issues by developing detailed metrics and enabling the automatic evaluator to decompose the sentences into sub-claims for fine-grained verification. Our comprehensive evaluation of both open-source and proprietary models on WebCiteS highlights the challenge LLMs face in correctly citing sources, underscoring the necessity for further improvement.}",Illustration of attributed query-focused summarization~(AQFS). Full example is shown in Table~\ref{tab:fsp_prompt}.
What Languages are Easy to Language-Model? A Perspective from Learning Probabilistic Regular Languages,2406.04289v5,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.04289v5_0.pdf,"What can large language models learn? By definition, language models (LM) are distributions over strings. Therefore, an intuitive way of addressing the above question is to formalize it as a matter of learnability of {classes} of distributions over strings. While prior work in this direction focused on assessing the theoretical limits, in contrast, we seek to understand the empirical learnability. Unlike prior empirical work, we evaluate neural LMs on their home turf---learning probabilistic languages---rather than as classifiers of formal languages. In particular, we investigate the learnability of regular LMs (s) by RNN and Transformer LMs. We empirically test the learnability of s as a function of various complexity parameters of the and the hidden state size of the neural LM. We find that the rank, which corresponds to the size of linear space spanned by the logits of its conditional distributions, and the expected length of sampled strings are strong and significant predictors of learnability for both RNNs and Transformers. Several other predictors also reach significance, but with differing patterns between RNNs and Transformers.-1","A \textcolor{ETHGreen}{finite-state automaton} defines a {set} of strings by assigning string binary weights. A \textcolor{ETHRed}{probabilistic finite-state automaton} and a \textcolor{ETHBlue}{neural LM} such as an RNN or a Transformer LM, however, define a {probability distribution} over strings."
"PsySafe: A Comprehensive Framework for Psychological-based Attack, Defense, and Evaluation of Multi-agent System Safety",2401.11880v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2401.11880v3_0.pdf,"Multi-agent systems, when enhanced with Large Language Models (LLMs), exhibit profound capabilities in collective intelligence. However, the potential misuse of this intelligence for malicious purposes presents significant risks. To date, comprehensive research on the safety issues associated with multi-agent systems remains limited. In this paper, we explore these concerns through the innovative lens of agent psychology, revealing that the dark psychological states of agents constitute a significant threat to safety. To tackle these concerns, we propose a comprehensive framework~({PsySafe}) grounded in agent psychology, focusing on three key areas: firstly, identifying how dark personality traits in agents can lead to risky behaviors; secondly, evaluating the safety of multi-agent systems from the psychological and behavioral perspectives, and thirdly, devising effective strategies to mitigate these risks. Our experiments reveal several intriguing phenomena, such as the collective dangerous behaviors among agents, agents' self-reflection when engaging in dangerous behavior, and the correlation between agents' psychological assessments and dangerous behaviors. We anticipate that our framework and observations will provide valuable insights for further research into the safety of multi-agent systems. We make our data and code publicly accessible at . {Warning: this paper includes examples that may be offensive or harmful.}","{Examples of Agents' Interactions after Psychological-based Attack}. After being attacked, the multi-agent system, whether for safe daily tasks or dangerous jailbreak tasks, provides dangerous answers. Agents collaborate with each other to generate dangerous content. Responses identified as dangerous are highlighted in red, whereas safe responses are indicated in green."
Political Compass or Spinning Arrow? Towards More Meaningful Evaluations for Values and Opinions in Large Language Models,2402.16786v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.16786v2_0.png,"Much recent work seeks to evaluate values and opinions in large language models (LLMs) using multiple-choice surveys and questionnaires. Most of this work is motivated by concerns around real-world LLM applications. For example, politically-biased LLMs may subtly influence society when they are used by millions of people. Such real-world concerns, however, stand in stark contrast to the artificiality of current evaluations: real users do not typically ask LLMs survey questions. Motivated by this discrepancy, we challenge the prevailing {constrained} evaluation paradigm for values and opinions in LLMs and explore more realistic {unconstrained} evaluations. As a case study, we focus on the popular Political Compass Test (PCT). In a systematic review, we find that most prior work using the PCT {forces} models to comply with the PCT's multiple-choice format. We show that models give substantively different answers when not forced; that answers change depending on how models are forced; and that answers lack paraphrase robustness. Then, we demonstrate that models give different answers yet again in a more realistic open-ended answer setting. We distill these findings into recommendations and open challenges in evaluating values and opinions in LLMs.","A model is prompted with a proposition from the Political Compass Test. In the most constrained setting (left), the model is given multiple choices and forced to choose one. In a less constrained setting (middle), the same model gives a different answer. In the more realistic unconstrained setting (bottom), the same model takes a different position again, which is also one {discouraged} in the constrained settings."
"Same Task, More Tokens: the Impact of Input Length on the Reasoning Performance of Large Language Models",2402.14848v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.14848v2_0.pdf,"This paper explores the impact of extending input lengths on the capabilities of Large Language Models (LLMs). Despite LLMs advancements in recent times, their performance consistency across different input lengths is not well understood. We investigate this aspect by introducing a novel QA reasoning framework, specifically designed to assess the impact of input length. We isolate the effect of input length using multiple versions of the same sample, each being extended with padding of different lengths, types and locations. Our findings show a notable degradation in LLMs' reasoning performance at much shorter input lengths than their technical maximum. We show that the degradation trend appears in every version of our dataset, although at different intensities. Additionally, our study reveals that the traditional metric of next word prediction correlates negatively with performance of LLMs' on our reasoning dataset. We analyse our results and identify failure modes that can serve as useful guides for future research, potentially informing strategies to address the limitations observed in LLMs.","Reasoning performance drops as input grows, across a variety of tasks. Inputs are composed of text containing information relevant to the task (in red), and irrelevant text (grey) which is drawn from various sources and extended incrementally. Two separate text spans are required to answer correctly, and are located randomly in the input. Each point reflects the performance across 600 samples."
Do Llamas Work in English? On the Latent Language of Multilingual Transformers,2402.10588v4,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.10588v4_0.png,"We ask whether multilingual language models trained on unbalanced, English dominated corpora use English as an internal pivot language---a question of key importance for understanding how language models function and the origins of linguistic bias. Focusing on the family of transformer models, our study uses carefully constructed non English prompts with a unique correct single token continuation. From layer to layer, transformers gradually map an input embedding of the final prompt token to an output embedding from which next token probabilities are computed. Tracking intermediate embeddings through their high dimensional space reveals three distinct phases, whereby intermediate embeddings (1)~start far away from output token embeddings; (2)~already allow for decoding a semantically correct next token in middle layers, but give higher probability to its version in English than in the input language; (3)~finally move into an input language specific region of the embedding space. We cast these results into a conceptual model where the three phases operate in ``input space'', ``concept space'', and ``output space'', respectively. Crucially, our evidence suggests that the abstract ``concept space'' lies closer to English than to other languages, which may have important consequences regarding the biases held by multilingual language models. Code and data is made available here: .","{Illustration of logit lens,} which applies language modeling head (here, \llama{}-7B) prematurely to latent embeddings in intermediate layers, yielding one next\hyp token distribution per position ($x$-axis) and layer ($y$-axis). We show final tokens of translation prompt (\cf\ \Secref{sec:data}) ending with ``Français:\ ""fleur"" - \zh{中文}:\ ""'' (where ``\zh{中文}'' means ``Chinese''). Final layer correctly ranks ``\zh{花}'' (translation of ``fleur'') on top, whereas intermediate layers decode English ``flower''. Color indicates entropy of next\hyp token distributions from low (blue) to high (red). (Plotting tool: \citet{belrose2023eliciting}.)"
Calibrating Large Language Models Using Their Generations Only,2403.05973v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2403.05973v1_0.pdf,"As large language models (LLMs) are increasingly deployed in user-facing applications, building trust and maintaining safety by accurately quantifying a model's confidence in its prediction becomes even more important. However, finding effective ways to calibrate LLMs---especially when the only interface to the models is their generated text---remains a challenge. We propose APRICOT \ (uxiliary edction of nfidence argets): A method to set confidence targets and train an additional model that predicts an LLM's confidence based on its textual input and output alone. This approach has several advantages: It is conceptually simple, does not require access to the target model beyond its output, does not interfere with the language generation, and has a multitude of potential usages, for instance by verbalizing the predicted confidence or adjusting the given answer based on the confidence. We show how our approach performs competitively in terms of calibration error for white-box and black-box LLMs on closed-book question-answering to detect incorrect LLM answers.",Illustration of APRICOT \peach: We train an auxiliary model to predict a target LLM's confidence based on its input and the generated answer.
Iterative Forward Tuning Boosts In-Context Learning in Language Models,2305.13016v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2305.13016v3_0.pdf,"Despite the advancements in in-context learning (ICL) for large language models (LLMs), current research centers on specific prompt engineering, such as demonstration selection, with the expectation that a single iteration of demonstrations processing can generalize effectively to a given test sample. However, this perspective overlooks the potential benefits derived from multiple iterations involving demonstrations, a practice aligning more closely with the iterative decision-making process exhibited by humans, who often learn through analogy. In this study, we introduce a novel two-stage framework to boost ICL in LLMs. Specifically, our framework delineates the ICL process into two distinct stages: and test stages. The stage incorporates a unique attention mechanism, i.e., iterative enhanced attention, which enables multiple rounds of information accumulation. This mechanism operates by manipulating the Key-Value matrices without training, fostering enhanced understanding capabilities in LLMs by ``{thinking}'' demonstrations multiple times. We evaluated across a range of benchmarks and LLMs, showing its superior performance over vanilla ICL methods and its effectiveness in challenging tasks where demonstration selection is infeasible.","The illustrations of vanilla ICL and our proposed two-stage framework through \methodname. The vanilla ICL method processes demonstrations only once, while our ``\methodname'' method enables multiple rounds of information accumulation during the reasoning process."
Pride and Prejudice: LLM Amplifies Self-Bias in Self-Refinement,2402.11436v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.11436v2_0.png,"Recent studies show that large language models (LLMs) improve their performance through self-feedback on certain tasks while degrade on others. We discovered that such a contrary is due to LLM's bias in evaluating their own output. In this paper, we formally define LLM's self-bias -- the tendency to favor its own generation -- using two statistics. We analyze six LLMs (GPT-4, GPT-3.5, Gemini, LLaMA2, Mixtral and DeepSeek) on translation, constrained text generation, and mathematical reasoning tasks. We find that self-bias is prevalent in all examined LLMs across multiple languages and tasks. Our analysis reveals that while the self-refine pipeline improves the fluency and understandability of model outputs, it further amplifies self-bias. To mitigate such biases, we discover that larger model size and external feedback with accurate assessment can significantly reduce bias in the self-refine pipeline, leading to actual performance improvement in downstream tasks. The code and data are released at .","How LLM's self-feedback inflates scores compared to human assessment. Bias is the mean difference between LLM and human scores, while skewness (Dskew) measures the asymmetry of their distribution around zero. Non-biased estimation will have Dskew=0."
Steering Llama 2 via Contrastive Activation Addition,2312.06681v4,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2312.06681v4_0.pdf;/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2312.06681v4_1.pdf,"We introduce Contrastive Activation Addition (CAA), a method for steering language models by modifying their activations during forward passes. CAA computes ``steering vectors'' by averaging the difference in residual stream activations between pairs of positive and negative examples of a particular behavior, such as factual versus hallucinatory responses. During inference, these steering vectors are added at all token positions after the user's prompt with either a positive or negative coefficient, allowing precise control over the degree of the targeted behavior. We evaluate CAA's effectiveness on Llama 2 Chat using multiple-choice behavioral question datasets and open-ended generation tasks. We demonstrate that CAA significantly alters model behavior, is effective over and on top of traditional methods like finetuning and system prompt design, and minimally reduces capabilities. Moreover, we gain deeper insights into CAA's mechanisms by employing various activation space interpretation methods. CAA accurately steers model outputs and sheds light on how high-level concepts are represented in Large Language Models (LLMs).","We perform forward passes on contrastive examples of answers to multiple-choice questions, extracting residual stream activations at a particular layer at the token position of the answer. We then take the mean activation difference over many contrast pairs. At inference time, this vector is added back into the residual stream with a chosen multiplier at all token positions after the instruction to control the behavior."
Deciphering Oracle Bone Language with Diffusion Models,2406.00684v3,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.00684v3_0.pdf,"Originating from China's Shang Dynasty approximately 3,000 years ago, the Oracle Bone Script (OBS) is a cornerstone in the annals of linguistic history, predating many established writing systems. Despite the discovery of thousands of inscriptions, a vast expanse of OBS remains undeciphered, casting a veil of mystery over this ancient language. The emergence of modern AI technologies presents a novel frontier for OBS decipherment, challenging traditional NLP methods that rely heavily on large textual corpora, a luxury not afforded by historical languages. This paper introduces a novel approach by adopting image generation techniques, specifically through the development of Oracle Bone Script Decipher (). Utilizing a conditional diffusion-based strategy, generates vital clues for decipherment, charting a new course for AI-assisted analysis of ancient languages. To validate its efficacy, extensive experiments were conducted on an oracle bone script dataset, with quantitative results demonstrating the effectiveness of . {Code and decipherment results will be made available} at {https://github.com/guanhaisu/OBSD}.",Conditional diffusion model for OBS decipherment.
Causal Estimation of Memorisation Profiles,2406.04327v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.04327v1_0.pdf,"Understanding memorisation in language models has practical and societal implications, e.g., studying models' training dynamics or preventing copyright infringements. Prior work defines memorisation as the causal effect of training with an instance on the model's ability to predict that instance. This definition relies on a counterfactual: the ability to observe what would have happened had the model not seen that instance. Existing methods struggle to provide computationally efficient and accurate estimates of this counterfactual. Further, they often estimate memorisation for a model architecture rather than for a specific model instance. This paper fills an important gap in the literature, proposing a new, principled, and efficient method to estimate memorisation based on the difference-in-differences design from econometrics. Using this method, we characterise a model's memorisation profile---its memorisation trends across training---by only observing its behaviour on a small set of instances throughout training. In experiments with the Pythia model suite, we find that memorisation (i) is stronger and more persistent in larger models, (ii) is determined by data order and learning rate, and (iii) has stable trends across model sizes, thus making memorisation in larger models predictable from smaller ones.",Memorisation profile (top) and path (bottom) of Pythia \q{6.9}{\billion}. Each entry represents the expected counterfactual memorisation of instances trained on at a specific timestep (\enquote{Treatment Step}) across model checkpoints (\enquote{Checkpoint Step}). The dashed vertical line indicates the end of the first epoch.
CHECKWHY: Causal Fact Verification via Argument Structure,2408.10918v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2408.10918v2_0.pdf,"With the growing complexity of fact verification tasks, the concern with ``thoughtful'' reasoning capabilities is increasing. However, recent fact verification benchmarks mainly focus on checking a narrow scope of semantic factoids within claims and lack an explicit logical reasoning process. In this paper, we introduce , a challenging dataset tailored to a novel causal fact verification task: checking the truthfulness of the causal relation within claims through rigorous reasoning steps. consists of over 19K ``{why}'' {claim-evidence-argument structure} triplets with {supports}, {refutes}, and {not enough info} labels. Each argument structure is composed of connected evidence, representing the reasoning process that begins with foundational evidence and progresses toward claim establishment. Through extensive experiments on state-of-the-art models, we validate the importance of incorporating the argument structure for causal fact verification. Moreover, the automated and human evaluation of argument structure generation reveals the difficulty in producing satisfying argument structure by fine-tuned models or Chain-of-Thought prompted LLMs, leaving considerable room for future improvements}.","An entry from C\textsc{heck}W\textsc{hy}: a ``{why}'' {claim} with its corresponding {cause} and {effect}, and an {argument structure} representing the reasoning process from cause to effect. Notably, the cause-effect pair is used solely during the annotation process and not included in the argument structure, implying that it is implicitly inferred from the claim, rather than being provided explicitly."
Quality-Aware Translation Models: Efficient Generation and Quality Estimation in a Single Model,2310.06707v4,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2310.06707v4_0.pdf,"Maximum-a-posteriori~(MAP) decoding is the most widely used decoding strategy for neural machine translation~(NMT) models. The underlying assumption is that model probability correlates well with human judgment, with better translations getting assigned a higher score by the model. However, research has shown that this assumption does not always hold, and generation quality can be improved by decoding to optimize a utility function backed by a metric or quality-estimation signal, as is done by Minimum Bayes Risk~(MBR) or quality-aware decoding. The main disadvantage of these approaches is that they require an additional model to calculate the utility function during decoding, significantly increasing the computational cost. In this paper, we propose to make the NMT models themselves quality-aware by training them to estimate the quality of their own output. Using this approach for MBR decoding we can drastically reduce the size of the candidate list, resulting in a speed-up of {two-orders of magnitude}. When applying our method to MAP decoding we obtain quality gains similar or even superior to quality reranking approaches, but with the efficiency of single pass decoding.",Alignment between predicted quality scores from the QA Prediction model and actual \bleurtqe scores of translations in the en $\rightarrow$ de test dataset. The boxplots show the distribution of actual scores across all samples assigned to each bin. The median ground truth quality score increases steadily in line with the predicted bins.
American Sign Language Handshapes Reflect Pressures for Communicative Efficiency,2406.04024v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2406.04024v2_0.pdf,"Communicative efficiency is a key topic in linguistics and cognitive psychology, with many studies demonstrating how the pressure to communicate with minimal effort guides the form of natural language. However, this phenomenon is rarely explored in signed languages. This paper shows how handshapes in American Sign Language (ASL) reflect these efficiency pressures and provides new evidence of communicative efficiency in the visual-gestural modality. We focus on hand configurations in native ASL signs and signs borrowed from English to compare efficiency pressures from both ASL and English usage. First, we develop new methodologies to quantify the articulatory effort needed to produce handshapes and the perceptual effort required to recognize them. Then, we analyze correlations between communicative effort and usage statistics in ASL or English. Our findings reveal that frequent ASL handshapes are easier to produce and that pressures for communicative efficiency mostly come from ASL usage, rather than from English lexical borrowing.}","Examples of handshapes in ASL components. The ASL lexicon can be divided into a native component (e.g. signs native to ASL; left) and a foreign component (e.g. fingerspelling, loan signs; right). 19 out of 22 handshapes in ASL fingerspelling also appear in the native lexicon\protect\footnotemark."
Reasoning in Conversation: Solving Subjective Tasks through Dialogue Simulation for Large Language Models,2402.17226v1,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2402.17226v1_0.pdf,"Large Language Models (LLMs) have achieved remarkable performance in {objective} tasks such as open-domain question answering and mathematical reasoning, which can often be solved through recalling learned factual knowledge or chain-of-thought style reasoning. However, we find that the performance of LLMs in {subjective} tasks is still unsatisfactory, such as metaphor recognition, dark humor detection, etc. Compared to objective tasks, subjective tasks focus more on interpretation or emotional response rather than a universally accepted reasoning pathway. Based on the characteristics of the tasks and the strong dialogue-generation capabilities of LLMs, we propose {RiC} ({R}easoning {i}n {C}onversation), a method that focuses on solving subjective tasks through dialogue simulation. The motivation of {RiC} is to mine useful contextual information by simulating dialogues instead of supplying chain-of-thought style rationales, thereby offering potential useful knowledge behind dialogues for giving the final answers. We evaluate both API-based and open-source LLMs including GPT-4, ChatGPT, and OpenChat across twelve tasks. Experimental results show that {RiC} can yield significant improvement compared with various baselines.","Illustration of our method. (a) An example of the metaphor recognition task. (b) Incorrect responses by LLM using zero-shot-CoT~\cite{kojima2023large} prompting. (c) Our method can simulate helpful dialogues (shown in the dashed box), thereby offering useful information in the generated conversation and aiding reasoning on this subjective task."
COKE: A Cognitive Knowledge Graph for Machine Theory of Mind,2305.05390v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2305.05390v2_0.pdf,"Theory of mind (ToM) refers to humans' ability to understand and infer the desires, beliefs, and intentions of others. The acquisition of ToM plays a key role in humans' social cognition and interpersonal relations. Though indispensable for social intelligence, ToM is still lacking for modern AI and NLP systems since they cannot access the human mental state and cognitive process beneath the training corpus. To empower AI systems with the ToM ability and narrow the gap between them and humans, in this paper, we propose : the first cognitive knowledge graph for machine theory of mind. Specifically, formalizes ToM as a collection of 45k+ manually verified cognitive chains that characterize {human mental activities} and subsequent behavioral/affective responses when facing specific {social circumstances}. In addition, we further generalize using LLMs and build a powerful generation model tailored for cognitive reasoning. Experimental results in both automatic and human evaluation demonstrate the high quality of , the superior ToM ability of , and its potential to significantly enhance social applications. We release our code and data at .",\coke instantiates the Theory of Mind as cognitive chains in social situations.
MMToM-QA: Multimodal Theory of Mind Question Answering,2401.08743v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2401.08743v2_0.pdf,"Theory of Mind (ToM), the ability to understand people’s mental states, is an essential ingredient for developing machines with human-level social intelligence. Recent machine learning models, particularly large language models, seem to show some aspects of ToM understanding. However, existing ToM benchmarks use unimodal datasets – either video or text. Human ToM, on the other hand, is more than video or text understanding. People can flexibly reason about another person’s mind based on conceptual representations (e.g., goals, beliefs, plans) extracted from any available data. To address this, we introduce a multimodal Theory of Mind question answering (MMToM-QA) benchmark. MMToM-QA comprehensively evaluates machine ToM both on multimodal data and on different kinds of unimodal data about a person’s activity in a household environment. To engineer multimodal ToM capacity, we propose a novel method, BIP-ALM (Bayesian Inverse Planning Accelerated by Language Models). BIP-ALM extracts unified representations from multimodal data and utilizes language models for scalable Bayesian inverse planning. We conducted a systematic comparison of human performance, BIP-ALM, and state-of-the-art models, including GPT-4. The experiments demonstrate that large language models and large multimodal models still lack robust ToM capacity. BIP-ALM, on the other hand, shows promising results, by leveraging the power of both model-based mental inference and language models..}","{Sketch of the MMToM-QA benchmark}. Each question is associated with a video stream (representative frames highlighting key moments are shown above for illustration) and text input (illustrative text above is shortened for brevity). In the example video, Emily can see the wine glass on one of the kitchen tables (1st frame) and passes by it without picking it up (2nd frame). At the end of the clip (3rd frame), it appears that she could be walking towards the cabinets on the left side of the room; or she might want to check if a goal object is inside the microwave. The text indicates that there are no cupcakes in the cabinets, but there is a cupcake inside the microwave. To confidently choose the correct answer, a model must fuse relevant information from both the video and the text."
ICLEF: In-Context Learning with Expert Feedback for Explainable Style Transfer,2309.08583v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2309.08583v2_0.pdf,"While state-of-the-art large language models (LLMs) can excel at adapting text from one style to another, current work does not address the {explainability} of style transfer models. Recent work has explored generating textual explanations from larger teacher models and distilling them into smaller student models. One challenge with such approach is that LLM outputs may contain errors that require expertise to correct, but gathering and incorporating expert feedback is difficult due to cost and availability. To address this challenge, we propose , a novel human-AI collaboration approach to model distillation that incorporates {scarce} expert human feedback by combining {in-context learning} and {model self-critique}. We show that our method leads to generation of high-quality synthetic explainable style transfer datasets for formality ({}) and subjective bias ({}). Via automatic and human evaluation, we show that specialized student models fine-tuned on our datasets outperform generalist teacher models on the explainable style transfer task in one-shot settings, and perform competitively compared to few-shot teacher models, highlighting the quality of the data and the role of expert feedback. In an extrinsic task of authorship attribution, we show that explanations generated by smaller models fine-tuned on {} are more predictive of authorship than explanations generated by few-shot teacher models.","Generating \textsc{e-GYAFC}: formality style transfer dataset GYAFC \cite{rao-tetreault-2018-dear} is augmented with semi-structured natural language explanations. The LLM generates the informal attributes of the input sentence, a formal paraphrase, and the formal attributes of the resulting sentence. Expert feedback is incorporated via in-context learning and self-critique to refine the initial generations."
LooGLE: Can Long-Context Language Models Understand Long Contexts?,2311.04939v2,/export/fs06/yguan19/Figure1_Dataset/benchmark/img/2311.04939v2_0.pdf,"Large language models (LLMs), despite their impressive performance in various language tasks, are typically limited to processing texts within context-window size. This limitation has spurred significant research efforts to enhance LLMs' long-context understanding with high-quality long-sequence benchmarks. However, prior datasets in this regard suffer from shortcomings, such as short context length compared to the context window of modern LLMs; outdated documents that have data leakage problems; and an emphasis on short dependency tasks rather than long dependency tasks. In this paper, we present , a {Lo}ng C{o}ntext {G}eneric {L}anguage {E}valuation benchmark for LLMs' long context understanding. features relatively new documents post-2022, with over 24,000 tokens per document and 6,000 newly generated questions spanning diverse domains. Human annotators meticulously crafted more than 1,100 high-quality question-answer pairs to meet the long dependency requirements. These pairs underwent thorough cross-validation, yielding the most precise assessment of LLMs' long dependency capabilities. The evaluation of eight state-of-the-art LLMs on revealed key findings: (i) commercial models outperformed open-sourced models; (ii) LLMs excelled in short dependency tasks like short question-answering and cloze tasks but struggled with more intricate long dependency tasks; (iii) in-context learning and chaining thoughts offered only marginal improvements in long context comprehension; (iv) retrieval-based techniques demonstrated substantial benefits for short question-answering, while strategies for extending context window length through optimized transformer architectures or positional encoding had limited impact on long context understanding. As such, not only provides a systematic and comprehensive evaluation schema on long-context LLMs, but also sheds light on future development of enhanced models towards ``true long-context understanding''. All evaluation codes are released at: .",The LooGLE benchmark for long context understanding.
