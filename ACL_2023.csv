paper_title,arxiv_id,fig1_file_path,fig1_caption,abstract,introduction
Binary and Ternary Natural Language Generation,2306.01841v1,./img_ACL_2023/2306.01841v1.pdf,Overview of TBT. A transformer block contains the multi-head self-attention and feed-forward network. We propose a statistic-based quantization method for weights ternarization/binarization and adopt a learning-based asymmetric quantization method for activation in ReLU/Softmax output (ùêó‚àà‚Ñù^n_+) and learning-based asymmetric quantization method for activations that contain both positive and negative values in other layers (ùêó‚àà‚Ñù^n).,"Ternary and binary neural networks enable multiplication-free computation and promise multiple orders of magnitude efficiency gains over full-precision networks if implemented on specialized hardware. However, since both the parameter and the output space are highly discretized, such networks have proven very difficult to optimize. The difficulties are compounded for the class of transformer text generation models due to the sensitivity of the attention operation to quantization and the noise-compounding effects of autoregressive decoding in the high-cardinality output space. We approach the problem with a mix of statistics-based quantization for the weights and elastic quantization of the activations and demonstrate the first ternary and binary transformer models on the downstream tasks of summarization and machine translation. Our ternary BART base achieves an R1 score of 41 on the CNN/DailyMail benchmark, which is merely 3.9 points behind the full model while being 16x more efficient. Our binary model, while less accurate, achieves a highly non-trivial score of 35.6. For machine translation, we achieved BLEU scores of 21.7 and 17.6 on the WMT16 En-Ro benchmark, compared with a full precision mBART model score of 26.8. We also compare our approach in the 8-bit activation setting, where our ternary and even binary weight models can match or outperform the best existing 8-bit weight models in the literature. Our code and models are available at: <https: //github. com/facebookresearch/Ternary_Binary_Transformer>.","Generative pre-trained transformers have emerged as powerful and generic tools, driving breakthroughs not only in language understanding but the field of AI in general. These models owe their success mainly to their seemingly infinite ability to scale to ever-larger data and model sizes. Unfortunately, such scaling comes at the cost of large computational requirements, putting extensively large generative transformers out of reach of all but the most resource-rich institutions. Even moderately sized pre-trained transformers have limited applications due to their size and computational cost. Making generative transformers more efficient is imperative for widening their use to more devices and practical applications. 
In this work, we explore making generative pre-trained transformers more efficient via the quantization of their weights and activations. Quantizing the weights of a neural network is useful for compression and allows the model to be stored more efficiently. However, compression alone does not reduce computation costs since the network's activations need to be computed in full precision. Quantizing both weights and activations allows computation to be performed with lower precision, potentially leading to significant efficiency gains depending on the quantization level and hardware implementation. Quantizing neural networks have a long history, and multiple works have attempted to quantize pre-trained transformers at various quantization levels. Most of this work focuses on encoder-only models (mainly BERT) for sentence and token classification tasks. Quantizing text generation models has generally been regarded as a more difficult task due to the large output vocabulary and sequential decoding. Recent work has tackled this problem, though only for mild quantization levels (down to 8-bit activations) and with mixed success. 
In contrast, we are interested in very low-bit quantization, down to ternary and even binary weights and activations. In order to achieve this, we combine and unify best practices for weight and activation quantization and present a framework that uses gradient-matching quantization for weights and elastic quantization for activations. We apply our method to natural language generation tasks and, for the first time, demonstrate low-bit generative transformers of competitive accuracy. Our ternary (weight and activation) model lags a full-precision BART model by only 4 points in ROUGE on the XSUM summarization dataset. In contrast, our model with ternary weights and 8-bit activations comes within 1 point and even outperforms comparable state-of-the-art models with 8-bit weights. We also demonstrate a fully binary (weights and activations) model. While not as competitive, it is able to achieve a highly non-trivial ROUGE-1 score of 31.7. 
Our results also extend to machine translation models. On the WMT16 En-Ro benchmark, we quantize an mBART model to extend the ternary-weight 8-bit activation SoTA by 1.2 points while demonstrating fully ternary and fully binary translation models for the first time. 
We summarize our contributions as follows: 
‚àô We propose a novel combination of statistics-based weight quantization with learning-based activation quantization, which enables stably training transformer encoder-decoder models to converge in the fully ternary/binary settings, which was not previously possible. 
‚àô We significantly improve the state-of-the-art text generation models in the 8-bit activation and ternary/binary weight settings while setting the first non-trivial baselines for the fully ternary and fully binary settings."
EM Pre-training for Multi-party Dialogue Response Generation,2305.12412v1,./img_ACL_2023/2305.12412v1.pdf,"An example of multi-party dialogue response generation task, better view in color.","Dialogue response generation requires an agent to generate a response according to the current dialogue history, in terms of which two-party dialogues have been well studied, but leaving a great gap for multi-party dialogues at the same time. Different from two-party dialogues where each response is a direct reply to its previous utterance, the addressee of a response utterance should be specified before it is generated in the multi-party scenario. Thanks to the huge amount of two-party conversational data, various pre-trained language models for two-party dialogue response generation have been proposed. However, due to the lack of annotated addressee labels in multi-party dialogue datasets, it is hard to use them to pre-train a response generation model for multi-party dialogues. To tackle this obstacle, we propose an Expectation-Maximization (EM) approach that iteratively performs the expectation steps to generate addressee labels, and the maximization steps to optimize a response generation model. Theoretical analyses and extensive experiments have justified the feasibility and effectiveness of our proposed method. The official implementation of this paper is available at <https: //github. com/EricLee8/MPDRG>.","Inspired by the tremendous success in pre-training large language models (PLMs) in general domains, efforts have been made to train PLMs for dialogue response generation. However, they constrain the dialogues to be either two-party, or sequential structured (i.e.,each utterance replies directly to its previous utterance). Different from them, a multi-party dialogue can involve multiple interlocutors, where each interlocutor can reply to any preceding utterances, making the response relations of the dialogue tree-structured and much more complicated. Besides, the speaker and addressee of a response utterance should be specified before it is generated in multi-party scenario, making the annotated data for multi-party dialogue response generation (MPDRG) less available. 
Figure illustrates an example of MPDRG task taken from the Ubuntu IRC benchmark. The upper part shows the tree-structured addressee relations of the dialogue, where the arrows point from addressees to speakers, and different colors represent different interlocutors. The middle part displays the content of the dialogue history, where U_7 is the response to be generated. The addressee (U_6) and the speaker (# 4) of it are given, and the content of this response is the target of our model. The lower part gives the human response, which is also called the ground truth reference. 
Previous works on MPDRG fine-tune generative PLMs on small multi-party dialogue datasets with explicit addressee annotations. They utilize the response annotations to form a tree-structured response graph, then encode the dialogue history using either homogeneous or heterogeneous Graph Neural Networks (GNNs). Nevertheless, none of them make attempts to pre-train a response generation model for multi-party dialogues due to the lack of large-scale corpora with annotated addressee labels. 
To solve the aforementioned problem of data scarcity, we propose an EM approach that iteratively performs the expectation steps to generate addressee labels, and the maximization steps to optimize a response generation model. Specifically, we treat the addressee of each utterance in the dialogue history as a discrete latent variable z. During the E-steps, given the current dialogue history c_t and the the response utterance r_t, we model the distribution of the current addressee z_t as p (z_t|c_t, r_t; Œ∏), where Œ∏ is the current model parameters. During the M-steps, we sample (c_t, r_t, z_t) triplets from distribution p (z_t|c_t, r_t; Œ∏) and optimize the generative model p (r_t|c_t, z_t; Œ∏) on these samples. With the iteration number increasing, the accuracy of latent variable prediction and the quality of generated responses will grow together. It is worth noting that during these iterations, annotated addressee labels are not required, which makes it possible to leverage the huge amount of multi-party dialogue corpora without addressee labels. We provide theoretical analyses to prove the feasibility of our EM method, and conduct experiments on the Ubuntu IRC benchmark, which is used in previous works. 
The contributions of our work can be summarized as the following three folds: 
 
 * To the best of our knowledge, we are the first to study the pre-training of multi-party dialogue response generation, which is much more challenging and complicated than two-party dialogues. 
 * We put forward an EM approach to alleviate the scarcity of multi-party dialogue data with addressee labels, making it possible to pre-train a model with huge amount of unlabeled corpora. 
 * We provide theoretical analyses to prove the feasibility of our EM pre-training method, and experimental results on the Ubuntu IRC benchmark show our pre-trained model achieves state-of-the-art performance compared with previous works."
ACLM: A Selective-Denoising based Generative Data Augmentation Approach for Low-Resource Complex NER,2306.00928v1,./img_ACL_2023/2306.00928v1.pdf,"Overview of ACLM: ACLM follows a 4-step template creation process, which serves as an input to the model during fine-tuning and generation. (1) Keyword Selection: The most important keywords (in red) associated with the NEs (in bold) in the sentence is first extracted using attention maps obtained from a fine-tuned NER model. (2) Selective Masking: All words except the NEs and the keywords obtained from the previous step is replaced with mask tokens M. (3) Labeled Sequence Linearization: Label tokens are added before and after each entity in the sentence. (4) Dynamic Masking: The template goes through further masking where a small portion of the keywords are dynamically masked at each training iteration. While generation we also apply mixner, which randomly joins two templates after (3) and before (4). Post generating augmentations with ACLM, the generated augmentations are concatenated with the gold data and used to fine-tune our final NER model.","Complex Named Entity Recognition (NER) is the task of detecting linguistically complex named entities in low-context text. In this paper, we present ACLM (Attention-map aware keyword selection for Conditional Language Model fine-tuning), a novel data augmentation approach, based on conditional generation, to address the data scarcity problem in low-resource complex NER. ACLM alleviates the context-entity mismatch issue, a problem existing NER data augmentation techniques suffer from and often generates incoherent augmentations by placing complex named entities in the wrong context. ACLM builds on BART and is optimized on a novel text reconstruction or denoising task - we use selective masking (aided by attention maps) to retain the named entities and certain keywords in the input sentence that provide contextually relevant additional knowledge or hints about the named entities. Compared with other data augmentation strategies, ACLM can generate more diverse and coherent augmentations preserving the true word sense of complex entities in the sentence. We demonstrate the effectiveness of ACLM both qualitatively and quantitatively on monolingual, cross-lingual, and multilingual complex NER across various low-resource settings. ACLM outperforms all our neural baselines by a significant margin (1%-36%). In addition, we demonstrate the application of ACLM to other domains that suffer from data scarcity (e.g., biomedical). In practice, ACLM generates more effective and factual augmentations for these domains than prior methods. footnote-1","Named Entity Recognition (NER) is a fundamental task in Natural Language Processing (NLP) that aims to detect various types of named entities (NEs) from text. Recently, there has been considerable progress in NER using neural learning methods that achieve state-of-the-art (SOTA) performance on well-known benchmark datasets, including CoNLL 2003 and OntoNotes. However, these datasets are designed to evaluate the performance on detecting ""relatively easy"" NEs like proper names (e.g., people such as ""Barack Obama, "" locations such as ""New York, "" or organizations such as ""IBM"") in well-formed, context-rich text that comes from news articles. On the other hand, complex NER benchmarks like MultiCoNER present several contemporary challenges in NER, including short low-context texts with emerging and semantically ambiguous complex entities (e.g., movie names in online comments) that reduce the performance of SOTA methods previously evaluated only on the existing NER benchmark datasets. Our experiments reveal that the performance of the current SOTA NER method (previously evaluated only on the CoNLL 2003 dataset) drops by 23%when evaluated on MultiCoNER and 31.8%when evaluated on a low-resource setting with just 500 training samples (more details in Table). Thus, we emphasize that research on building systems that can effectively detect complex NEs in the text is currently understudied in the field of NLP. 
In the past, researchers have made several attempts at building supervised approaches to detect complex and compositional noun phrase entities in sentences. However, the scarcity of annotated training data for building effective systems has always been a challenge. Data augmentation has been shown to be an effective solution for low-resource NER. In practice, though these systems perform well and generate coherent augmentations on common NER benchmark datasets with easy proper noun NEs, they fail to be effective for complex NER, often generating incoherent augmentations. We first argue that certain types of complex NEs follow specific linguistic patterns and appear only in specific contexts (examples in Appendix), and augmentations that do not follow these patterns impede a NER model from learning such patterns effectively. This sometimes also leads to augmentations with context-entity mismatch, further hurting the learning process. For e.g., unlike proper names, substituting complex NEs from other sentences in the corpus or replacing them with synonyms often leads to augmentations where the NE does not fit into the new context (e.g., swapping proper names across sentences might still keep the sentence coherent but swapping the name of a book with a movie (both creative work entity) or the name of a football team with a political party (both group entity) makes it incoherent). Fine-tuning pre-trained language models (PLMs), similar to prior-work, fail to generate new context around complex NEs or completely new NEs with the desired linguistic patterns due to low-context sentences and the lack of existing knowledge of such linguistically complex NEs (examples in Fig. ). This leads to in-coherent augmentations and poses a severe problem in knowledge-intensive tasks like bio-medical NER, where non-factual augmentations severely hurt learning. Our experiments also reveal that introducing new context patterns around NEs proves to be a more effective data augmentation technique for complex NER than diversifying NEs (ACLM vs. MELM in Table). 
 
Main Results: To overcome the aforesaid problems, we formulate data augmentation as a conditional generation task and propose ACLM, a conditional text generation model that generates augmentation samples by introducing new and diverse context patterns around a NE. ACLM builds on BART and is fine-tuned on a modification of the text reconstruction from corrupted text task, a common denoising-based PLM pre-training objective. In contrast to other PLM pre-training strategies, which randomly mask a portion of the text for corruption, our modified objective is based on selective masking, wherein we mask all other words in the sentence except the NEs and a small percentage of keywords related to the NEs. We refer to this corrupted sentence as a template, and it serves as input to the model for both the training and generation phases. These keywords are other non-NE tokens in the sentence that provide contextually relevant additional knowledge or hints to BART about the complex NEs without the need of retrieving knowledge from any external sources. We select these keywords using attention maps obtained from a transformer model fine-tuned on the NER task, and they help the PLM overcome the problem where it might not possess enough knowledge about a semantically ambiguous complex NE (example in Fig. ). Training ACLM on this modified objective allows us to generate diverse, coherent, factual, and high-quality augmentations given templates. We also propose mixner, a novel algorithm that mixes two templates during the augmentation generation phase and boosts the diversity of augmentations. Our primary contributions are as follows: 
 
 * We propose ACLM, a novel data augmentation framework specially designed for low-resource complex NER. Compared with previous methods in the literature, ACLM effectively alleviates the context-entity mismatch problem by preserving the true sense of semantically ambiguous NEs in augmentations. Additionally, to accompany ACLM, we propose mixner, which boosts the diversity of ACLM generations. 
 * We qualitatively and quantitively show the benefits of ACLM for monolingual, cross-lingual, and multilingual complex NER across various low-resource settings on the MultiCoNER dataset. Our proposed ACLM outperforms all other baselines in literature by a significant margin (1%-36%) and generates more diverse, coherent, and high-quality augmentations compared to them. 
 * We perform extensive experiments to study the application of ACLM in three other domains, including science and medicine. ACLM outperforms all our baselines in these domains (absolute gains in the range of 1%-11%) and generates more factual augmentations."
Natural Language to Code Generation in Interactive Data Science Notebooks,2212.09248v1,./img_ACL_2023/2212.09248v1.pdf,"An example of a computational notebook adapted from our dataset, with examples of reading and preprocessing data (cell c_1), data wrangling (cell c_2, c_3), and data analysis (cells c_3 -c_7). Annotated NL intents are shown in green.","Computational notebooks, such as Jupyter notebooks, are interactive computing environments that are ubiquitous among data scientists to perform data wrangling and analytic tasks. To measure the performance of AI pair programmers that automatically synthesize programs for those tasks given natural language (NL) intents from users, we build ARCADE, a benchmark of 1,082 code generation problems using the pandas data analysis framework in data science notebooks. ARCADE features multiple rounds of NL-to-code problems from the same notebook. It requires a model to understand rich multi-modal contexts, such as existing notebook cells and their execution states as well as previous turns of interaction. To establish a strong baseline on this challenging task, we develop PaChiNCo, a 62B code language model (LM) for Python computational notebooks, which significantly outperforms public code LMs. Finally, we explore few-shot prompting strategies to elicit better code with step-by-step decomposition and NL explanation, showing the potential to improve the diversity and explainability of model predictions.","Data science is the process of extracting insights from data, and has become an integral part of decision making and knowledge discovery. Data scientists and machine learning (ML) practitioners often use computational notebooks, which are interactive environments such as Jupyter notebooks and Google Colab, in their work. As illustrated in, data scientists spend significant amount of time on data wrangling and exploratory data analysis (EDA). This has motivated research on automating and accelerating the data science workflow in general, with particular interest in data wrangling and EDA tasks. 
On the other hand, large language models (LLMs) trained on code can assist developers by translating natural language (NL) intents into executable programs, with promising applications in synthesizing code for data wrangling and EDA tasks. Computational notebooks also present unique challenges to LLMs, because notebooks freely mix NL, code, graphics, and execution results, and because of their interactivity, notebooks feature multiple turns of related NL-to-code problems. As illustrated in, those problems often have interesting dependency structures. They may share common execution context (e.g.,DataFrame df), form semantically coherent turns (e.g.,c_4, c_5), or exhibit non-trivial long range data dependency (e.g.,from c_6 to c_2, or c_7 to c_3). This dependency structure is more complex than existing multi-turn text to code tasks with sequentially dependent problems. 
Several benchmarks have been proposed to evaluate program synthesis of data science programs from NL intents, but these datasets have several limitations. First, some datasets derive from data science tutorial notebooks, but this data tends to contain NL text (e.g.,exercise questions) which is more verbose and elaborate than the concise, ephemeral style that developers write when interacting with code LMs. Other datasets assume that the developer provides extra information, such as unit tests or input/output examples, but such systems pose an extra burden to users who might not normally write such tests or examples during their workflow. Finally, existing datasets usually contain independent tasks with isolated contexts, or limited number of contextually dependent problems, rather than having multiple, related tasks with complex dependencies such as in. Therefore, there is a need for a benchmark that provides realistic NL intents, rich notebook context, and multiple interrelated problems, so as to better reflect real-world usage by data scientists. 
To fill this gap, we present ARCADE, a new benchmark for code generation for data wrangling and EDA tasks in computational notebooks (). ARCADE consists of 1,082 problems spanning across 136 notebooks for 106 ML datasets. ARCADE features a series of NL utterances written by professional data scientists with the intention of interacting with an AI pair programmer when working in a notebook (e.g., green texts in), with high-quality code solutions using the pandas library. To mitigate the risk of data leakage when evaluating LLMs, 65%of the problems and their notebooks are created from scratch, based on recent ML datasets on Kaggle. ARCADE also challenges LLMs with grounded language understanding, where a model needs to ground relevant semantic concepts in intents (e.g.,""min and max"" in u_1) to variable states (e.g.,the column df ['TIME'] ) to understand the intent. Besides featuring concise NL intents and contextually rich problems, more than 67%of problems in ARCADE also require complex solutions using 5 or more pandas API calls, making it more challenging than most existing benchmarks. 
To demonstrate how ARCADE can motivate new research, we evaluate public code LMs, finding that they struggle on this task, especially newly created problems from scratch. To build an LM tailored for data science, we develop PaChiNCo, a 62B code language model for Python computational notebooks, trained on a mixture of general-domain NL, source code, and Jupyter notebooks (). PaChiNCo significantly outperforms public code LMs on ARCADE. Further, we explore few-shot prompting strategies to alter the style of model predictions, such as decomposing code into step-by-step solutions and adding inline NL explanations (). Not only is code in this style potentially more understandable to novice data scientists (), we also demonstrate that this improves the accuracy of self-consistency reranking, because prompting the model to explain its solutions improves the diversity of the model's predictions."
Text Adversarial Purification as Defense against Adversarial Attacks,2203.14207v2,./img_ACL_2023/2203.14207v2.pdf,"Text Adversarial Purification Process: Compared with Image Purification, we use masked language models to recover noisy texts to purify adversarial texts as a defense against word-substitutions attacks.","Adversarial purification is a successful defense mechanism against adversarial attacks without requiring knowledge of the form of the incoming attack. Generally, adversarial purification aims to remove the adversarial perturbations therefore can make correct predictions based on the recovered clean samples. Despite the success of adversarial purification in the computer vision field that incorporates generative models such as energy-based models and diffusion models, using purification as a defense strategy against textual adversarial attacks is rarely explored. In this work, we introduce a novel adversarial purification method that focuses on defending against textual adversarial attacks. With the help of language models, we can inject noise by masking input texts and reconstructing the masked texts based on the masked language models. In this way, we construct an adversarial purification process for textual models against the most widely used word-substitution adversarial attacks. We test our proposed adversarial purification method on several strong adversarial attack methods including Textfooler and BERT-Attack and experimental results indicate that the purification algorithm can successfully defend against strong word-substitution attacks.","Adversarial examples can successfully mislead strong neural models in both computer vision tasks and language understanding tasks. An adversarial example is a maliciously crafted example attached with an imperceptible perturbation and can mislead neural networks. To defend attack examples of images, the most effective method is adversarial training which is a mini-max game used to incorporate perturbations into the training process. 
Defending adversarial attacks is extremely important in improving model robustness. However, defending adversarial examples in natural languages is more challenging due to the discrete nature of texts. That is, gradients cannot be used directly in crafting perturbations. The substitution-based adversarial examples are more complicated than gradient-based adversarial examples in images, making it difficult for neural networks to defend against these substitution-based attacks. 
The first challenge of defending against adversarial attacks in NLP is that due to the discrete nature, these substitution-based adversarial examples can have substitutes in any token of the sentence and each substitute has a large candidate list. This would cause a combinatorial explosion problem, making it hard to apply adversarial training methods. Strong attacking methods such as show that using the crafted adversarial examples as data augmentation in adversarial training cannot effectively defend against these substitution-based attacks. Further, defending strategies such as adversarial training rely on the assumption that the candidate lists of the substitutions are accessible. However, the candidate lists of the substitutions should not be exposed to the target model; that is, the target model should be unfamiliar to the candidate list of the adversarial examples. In real-world defense systems, the defender is not aware of the strategy the potential attacks might use, so the assumption that the candidate list is available would significantly constrain the potential applications of these defending methods. 
Considering that it is challenging to defend against textual adversarial attacks when the form of the attacks cannot be acknowledged in advance, we introduce a novel adversarial purification method as a feasible defense mechanism against these attacks. The adversarial purification method is to purify adversarially perturbed input samples before making predictions. The major works about adversarial purification focus on purifying continuous inputs such as images, therefore these works explore different generative models such as GANs, energy-based models (EBMs) and recently developed diffusion models. However, in textual adversarial attacks, the inputs are discrete tokens which makes it more challenging to deploy previous adversarial purification methods. 
Therefore, we introduce a purification mechanism with the help of masked language models. We first consider the widely used masking process to inject noise into the input; then we recover the clean texts from the noisy inputs with the help of the masked language models (e.g.,a BERT). Further, considering that the iterative process in previous adversarial purification algorithms can be extremely costly (e.g.,a VP-SDE process in diffusion models), we instead simplify the iterative process to an ensemble-purifying process that conducting adversarial purification multiple times to obtain an ensembled result as a compromise to the time cost in traditional adversarial purification process. 
Through extensive experiments, we prove that the proposed text adversarial purification algorithm can successfully serve as defense against strong attacks such as Textfooler and BERT-Attack. Experiment results show that the accuracy under attack in baseline defense methods is lower than random guesses, while after text purification, the performance can reach only a few percent lower than the original accuracy when the candidate range of the attack is limited. Further, extensive results indicate that the candidate range of the attacker score is essential for successful attacks, which is a key factor in maintaining the semantics of the adversaries. Therefore we also recommend that future attacking methods can focus on achieving successful attacks with tighter constraints. 
To summarize our contributions: (1) We raise the concern of defending substitution-based adversarial attacks without acknowledging the form of the attacks in NLP tasks. (2) To the best of our knowledge, we are the first to consider adversarial purification as a defense against textual adversarial attacks exemplified by strong word-substitution attacks and combine text adversarial purification with pre-trained models. (3) We perform extensive experiments to demonstrate that the adversarial purification method is capable of defending strong adversarial attacks, which brings a new perspective to defending textual adversarial attacks."
Rule By Example: Harnessing Logical Rules for Explainable Hate Speech Detection,2307.12935v1,./img_ACL_2023/2307.12935v1.png,"Generalization problem of rules. Logical rules, while easy to explain, are inherently fragile to the nuances of natural language.","Classic approaches to content moderation typically apply a rule-based heuristic approach to flag content. While rules are easily customizable and intuitive for humans to interpret, they are inherently fragile and lack the flexibility or robustness needed to moderate the vast amount of undesirable content found online today. Recent advances in deep learning have demonstrated the promise of using highly effective deep neural models to overcome these challenges. However, despite the improved performance, these data-driven models lack transparency and explainability, often leading to mistrust from everyday users and a lack of adoption by many platforms. In this paper, we present Rule By Example (RBE): a novel exemplar-based contrastive learning approach for learning from logical rules for the task of textual content moderation. RBE is capable of providing rule-grounded predictions, allowing for more explainable and customizable predictions compared to typical deep learning-based approaches. We demonstrate that our approach is capable of learning rich rule embedding representations using only a few data examples. Experimental results on 3 popular hate speech classification datasets show that RBE is able to outperform state-of-the-art deep learning classifiers as well as the use of rules in both supervised and unsupervised settings while providing explainable model predictions via rule-grounding.","Content moderation is a major challenge confronting the safety of online social platforms such as Facebook, Twitter, YouTube, Twitch, etc. . Major technology corporations are increasingly allocating valuable resources towards the development of automated systems for the detection and moderation of harmful content in addition to hiring and training expert human moderators to combat the growing menace of negativity and toxicity online. 
Despite the popularity of deep learning approaches, many practical solutions used in products today are comprised of rule-based techniques based on expertly curated signals such as block lists, key phrases, and regular expressions. Such methods are widely used due to their transparency, ease of customization, and interpretability. However, they have the disadvantage of being difficult to maintain and scale, in addition to being inherently fragile and noisy. Figure shows an example where logical rules, while explainable in nature, face the problem of being inflexible to their context of use in natural language. While a given rule may be too specific and fail to capture different variations of usage commonly found in content online, rules can also be too broad and incorrectly block lexically similar content. 
In contrast to the challenges faced by rule-based methods, data-driven deep learning approaches have shown great promise across a wide range of content moderation tasks and modalities. Fueled by large amounts of data and deep neural networks, these complex models are capable of learning richer representations that better generalize to unseen data. The impressive performances of these models have resulted in significant industry investment in content moderation as-a-service. Several technology companies such as Google, OpenAI, and Microsoft use these models to offer services to aid in content moderation. However, despite their significant investment, they face adoption challenges due to the inability of customers to understand how these complex models reason about their decisions. Additionally, with the increasing attention around online content moderation and distrust amongst consumers, explainability and transparency are at the forefront of demands. This presents the challenging open question of how we can leverage the robustness and predictive performance of complex deep-learning models whilst allowing the transparency, customizability, and interpretability that rule-based approaches provide. 
Prior works such as have explored learning from rules for tasks such as controlling neural network learning, assisting in human annotation, and improving self-supervised learning in low data scenarios. propose a rule-exemplar training method for noisy supervision using rules. While performant in denoising over-generalized rules in the network via a soft implication loss, similar to other ML approaches, this method lacks the ability to interpret model predictions at inference time. propose a general-purpose framework for the automatic discovery and integration of symbolic rules into pre-trained models. However, these symbolic rules are derived from low-capacity ML models on a reduced feature space. While less complex than large deep neural networks, these low-capacity models are still not easily interpretable by humans. Therefore, the task of combining the explainability of rules and the predictive power of deep learning models remains an open problem. 
In order to tackle this problem, we introduce Rule By Example (RBE): a novel exemplar-based contrastive learning approach for learning from logical rules for the task of textual content moderation. RBE is comprised of two neural networks, a rule encoder, and a text encoder, which jointly learn rich embedding representations for hateful content and the logical rules that govern them. Through the use of contrastive learning, our framework uses a semantic similarity objective that pairs hateful examples with clusters of rule exemplars that govern it. Through this approach, RBE is able to provide more explainable predictions by allowing for what we define as Rule-grounding. This means that our model is able to ground its predictions by showing the corresponding explainable logical rule and the exemplars that constitute that rule. 
We evaluate RBE in both supervised and unsupervised settings using a suite of rulesets. Our results show that with as little as one exemplar per rule, RBE is capable of outperforming state-of-the-art hateful text classifiers across three benchmark content moderation datasets in both settings. In summary, the contributions of this paper are: 
 
 * Rule By Example (RBE): a novel exemplar-based contrastive learning approach to learn from logical rules for the task of textual content moderation. 
 * We demonstrate how RBE can be easily integrated to boost model F1-score by up to 4%on three popular hate speech classification datasets. 
 * A detailed analysis and insights into the customizability and interpretability features of RBE to address the problem of emerging hateful content and model transparency."
"When Does Translation Require Context? A Data-driven, Multilingual Exploration",2109.07446v2,./img_ACL_2023/2109.07446v2.pdf,Number of MuDA tags on TED test data. Exact numbers of each tag are given in. Number of tags for other document-level datasets can be found in.,"Although proper handling of discourse significantly contributes to the quality of machine translation (MT), these improvements are not adequately measured in common translation quality metrics. Recent works in context-aware MT attempt to target a small set of discourse phenomena during evaluation, however not in a fully systematic way. In this paper, we develop the Multilingual Discourse-Aware (MuDA) benchmark, a series of taggers that identify and evaluate model performance on discourse phenomena in any given dataset. The choice of phenomena is inspired by a novel methodology to systematically identify translations requiring context. We confirm the difficulty of previously studied phenomena while uncovering others that were previously unaddressed. We find that common context-aware MT models make only marginal improvements over context-agnostic models, which suggests these models do not handle these ambiguities effectively. We release code and data for 14 language pairs to encourage the MT community to focus on accurately capturing discourse phenomena.","In order to properly translate discourse phenomena including anaphoric pronouns, lexical cohesion, and discourse markers, a machine translation (MT) model must use information from previous utterances. 
However, while generating proper translations of these phenomena is important for comprehension, they represent a small portion of words in natural language. Therefore, common metrics such as BLEU cannot be used to judge the quality of discourse translation. 
Recent work on neural machine translation (NMT) models that attempt to incorporate extra-sentential context often perform targeted evaluation of certain discourse phenomena, mostly focusing on ellipsis, formality, and pronoun translation. However, only a limited set of discourse phenomena for a few language pairs have been studied (see summary in Table). The difficulty of broadening these studies stems from the reliance of previous work on introspection and domain knowledge to identify the relevant discourse phenomena, frequently involving expert speakers, which then requires engineering complex language-specific methods to create test suites or manually designing data for evaluation. 
In this paper, we identify sentences that contain discourse phenomena through a data-driven, semi-automatic methodology. We apply this method to create a multilingual benchmark testing discourse phenomena in the domain of MT. First, we develop P-CXMI () as a metric to identify when context is helpful in MT, or more broadly text generation in general. Then, we perform a systematic analysis of words with high P-CXMI to find categories of translations where context is useful (). We identify novel discourse phenomena that to our knowledge have not been addressed previously (e.g.,consistency of verb forms), without requiring a-priori language-specific knowledge. Finally, we design a series of methods to automatically tag words belonging to the identified classes of ambiguities () and we evaluate existing translation models for different categories of ambiguous translations (). 
We examine a parallel corpus spanning 14 language pairs, measuring translation ambiguity and model performance. We find that the context-aware methods, while improving on standard evaluation metrics, only perform significantly better than context-agnostic baselines for certain discourse phenomena in our benchmark. Our benchmark provides a more fine-grained evaluation of translation models and reveals weaknesses of context-aware models, such as verb form cohesion. We also find that DeepL, a commercial document-level translation system, does better in our benchmark than its sentence-level ablation and Google Translate. We hope that the released benchmark and code, as well as our findings, will spur targeted evaluation of discourse phenomena in MT to cover more languages and more phenomena in the future."
Making More of Little Data: Improving Low-Resource Automatic Speech Recognition Using Data Augmentation,2305.10951v2,./img_ACL_2023/2305.10951v2.png,Visualization of the self-training approach where a teacher model is fine-tuned on manually transcribed data and subsequently used to transcribe unlabeled speech. A student model is then fine-tuned on the combined datasets.,"The performance of automatic speech recognition (ASR) systems has advanced substantially in recent years, particularly for languages for which a large amount of transcribed speech is available. Unfortunately, for low-resource languages, such as minority languages, regional languages or dialects, ASR performance generally remains much lower. In this study, we investigate whether data augmentation techniques could help improve low-resource ASR performance, focusing on four typologically diverse minority languages or language variants (West Germanic: Gronings, West-Frisian; Malayo-Polynesian: Besemah, Nasal). For all four languages, we examine the use of self-training, where an ASR system trained with the available human-transcribed data is used to generate transcriptions, which are then combined with the original data to train a new ASR system. For Gronings, for which there was a pre-existing text-to-speech (TTS) system available, we also examined the use of TTS to generate ASR training data from text-only sources. We find that using a self-training approach consistently yields improved performance (a relative WER reduction up to 20.5%compared to using an ASR system trained on 24 minutes of manually transcribed speech). The performance gain from TTS augmentation for Gronings was even stronger (up to 25.5%relative reduction in WER compared to a system based on 24 minutes of manually transcribed speech). In sum, our results show the benefit of using self-training or (if possible) TTS-generated data as an efficient solution to overcome the limitations of data availability for resource-scarce languages in order to improve ASR performance.","Self-supervised learning (SSL) enables speech representation learning without the need for (manually) labeled data. Although this approach is very effective, pre-training an SSL model is costly. This cost (e.g., training time, resources, and memory) increases with the number of languages added to the model. Furthermore, transferring information across languages, or extending a pre-trained model to new data or to a different domain is computationally expensive, and catastrophic forgetting may occur. To alleviate this, SSL models are therefore often fine-tuned on the target task with target domain data. For the task of automatic speech recognition (ASR), fine-tuning approaches generally require less data, but training ASR systems that perform well for languages with very little data remains challenging. This leads to (digitally) underrepresented communities and domains such as minority languages, regional languages and dialects not profiting sufficiently from most recent technological advancements. 
Recent studies explored fine-tuning of pre-trained self-supervised models for ASR using speech from low-resource languages (e.g., ), and difficulties of modeling resource-scarce languages and dialects were acknowledged in previous work. It remains an open question to what extent model performance is dependent on the amount of fine-tuning data and the type of language, when the total amount of available data for a language is limited. Having a better understanding of how limited training data affects model performance paves the way for creating meaningful speech technology for a wider range of languages. 
In this paper, we fine-tune pre-trained SSL models for ASR using varying amounts of data from four typologically diverse minority languages or language variants: Gronings, West-Frisian, Besemah and Nasal, which have a limited amount of data available. We specifically investigate whether data augmentation approaches can be used to generate additional training data to improve the performance of these models, particularly when very little resources are available. By using data from (ongoing) language documentation projects, we evaluate a real-world use of our experimental setup. 
Previous work describes the benefits of data augmentation by adopting a self-training approach, which generates labels (i.e.,transcriptions) for unlabeled speech (e.g., ). Various self-training methods are proposed, including iterative approaches, decoding with an external (text-based) language model, or filtering approaches that improve the quality of the generated labels. However, limited conclusions can be drawn from these works on the effectiveness of self-training in a very low-resource, real-world setting, as these studies either use datasets with more than 10 hours of data (which may not be available for very small languages), only considered modeling English, or reported average performance over a set of languages that strongly varied in terms of training data size. We therefore complement this work by investigating the benefits of self-training for four typologically different, true low-resource languages. To this end, we use a standard self-training approach to evaluate the potential benefit of a simple system in a real-world setup, which nevertheless yields substantial performance improvements (relative word-error-rate (WER) reductions up to 20.5%). 
In addition to self-training, several studies (e.g., ) reported on augmenting the training data with synthetic speech generated using a text-to-speech (TTS) system. For this reason, we also examine whether this approach is useful in our low-resource setup. We recognize that not all very low-resource languages may have sufficient amounts of data available for TTS development, and we therefore only generate synthetic training examples for Gronings, one of the four low-resource languages in our dataset that has an existing TTS system available. We show the benefit (i.e.,up to 25.5%relative reduction in WER) of augmenting the training data by using an existing TTS system, and analyze the effect of adding different amounts of synthetic speech on the model performance. Our datasets, code, and newly trained models are publicly available."
Post-Abstention: Towards Reliably Re-Attempting the Abstained Instances in QA,2305.01812v1,./img_ACL_2023/2305.01812v1.png,"Illustrating the impact of employing a post-abstention method on top of selective prediction system. A regular model that has an accuracy of 70%(at coverage 100%) is first enabled with selective prediction ability that increases the accuracy to 85%but drops the coverage to 71%. Then, on employing a post-abstention method to the abstained instances (remaining 29%), coverage increases to 80%without a considerable drop in overall accuracy.","Despite remarkable progress made in natural language processing, even the state-of-the-art models often make incorrect predictions. Such predictions hamper the reliability of systems and limit their widespread adoption in real-world applications. Selective prediction partly addresses the above concern by enabling models to abstain from answering when their predictions are likely to be incorrect. While selective prediction is advantageous, it leaves us with a pertinent question 'what to do after abstention'. To this end, we present an explorative study on 'Post-Abstention', a task that allows re-attempting the abstained instances with the aim of increasing coverage of the system without significantly sacrificing its accuracy. We first provide mathematical formulation of this task and then explore several methods to solve it. Comprehensive experiments on 11 QA datasets show that these methods lead to considerable risk improvements ‚Äìperformance metric of the Post-Abstention task‚Äì both in the in-domain and the out-of-domain settings. We also conduct a thorough analysis of these results which further leads to several interesting findings. Finally, we believe that our work will encourage and facilitate further research in this important area of addressing the reliability of NLP systems.","Despite remarkable progress made in Natural Language Processing (NLP), even the state-of-the-art systems often make incorrect predictions. This problem becomes worse when the inputs tend to diverge from the training data distribution. Incorrect predictions hamper the reliability of systems and limit their widespread adoption in real-world applications. 
Selective prediction partly addresses the above concern by enabling models to abstain from answering when their predictions are likely to be incorrect. By avoiding potentially incorrect predictions, it allows maintaining high task accuracy and thus improves the system 's reliability. Selective prediction has recently received considerable attention from the NLP community leading to development of several methods. While these contributions are important, selective prediction leaves us with a pertinent question: what to do after abstention? 
In this work, we address the above question and present an explorative study on' Post-Abstention ', a task that allows re-attempting the abstained instances with the aim of increasing coverage of the given selective prediction system without significantly sacrificing its accuracy. Figure illustrates the benefit of employing a post-abstention method; a model that achieves an accuracy of 70%is first enabled with the selective prediction ability that increases the accuracy to 85%but answers only 71%instances. Then, a post-abstention method is employed (for the 29%abstained instances) that assists the system in answering 9%more instances raising the coverage to 80%without considerably dropping the overall accuracy. We note that this task allows re-attempting all the abstained instances but does not require the system to necessarily output predictions for all of them i.e.,the system can abstain even after utilizing a post-abstention method (when it is not sufficiently confident even in its new prediction). This facet not only allows the system to maintain its performance but also provides opportunities of sequentially applying stronger post-abstention methods to reliably and optimally increase the coverage in stages. 
We provide mathematical formulation of the post-abstention task and explore several baseline methods to solve it (Section). To evaluate the efficacy of these methods, we conduct comprehensive experiments with 11 Question-Answering datasets from MRQA shared task in both in-domain and out-of-domain settings (Section). Our post-abstention methods lead to overall risk improvements (performance metric of the proposed task) of up to 21.81 in the in-domain setting and 24.23 in the out-of-domain setting. To further analyze these results, we study several research questions, such as' what is the extent of overlap between the instances answered by different post-abstention methods ', ' what is the distribution of model 's original confidence on instances that get answered in the post-abstention stage', and 'how often do the system' s predictions change after applying post-abstention methods '. In Section, we show that these investigations lead to numerous important and interesting findings. 
In summary, our contributions are as follows: 
 * We present an explorative study on' Post-Abstention', a task that aims at increasing the coverage of a given selective prediction system without significantly sacrificing its accuracy. 
 * We explore several baseline post-abstention methods and evaluate them in an extensive experimental setup spanning 11 QA datasets in both in-domain and out-of-domain settings. 
 * We show that the proposed post-abstention methods result in overall risk value improvements of up to 21.81 and 24.23 in the in-domain and out-of-domain settings respectively. 
 * Our thorough analysis leads to several interesting findings, such as (a) instances answered by different post-abstention methods are not mutually exclusive i.e.,there exist some overlapping instances, (b) instances that get answered in the post-abstention stage are not necessarily the ones on which the given system was initially most confident, etc. 
 We believe our work will encourage further research in Post-Abstention, an important step towards improving the reliability of NLP systems."
Improving Continual Relation Extraction by Distinguishing Analogous Semantics,2305.06620v1,./img_ACL_2023/2305.06620v1.pdf,Framework of the proposed model for task T_k.,"Continual relation extraction (RE) aims to learn constantly emerging relations while avoiding forgetting the learned relations. Existing works store a small number of typical samples to re-train the model for alleviating forgetting. However, repeatedly replaying these samples may cause the overfitting problem. We conduct an empirical study on existing works and observe that their performance is severely affected by analogous relations. To address this issue, we propose a novel continual extraction model for analogous relations. Specifically, we design memory-insensitive relation prototypes and memory augmentation to overcome the overfitting problem. We also introduce integrated training and focal knowledge distillation to enhance the performance on analogous relations. Experimental results show the superiority of our model and demonstrate its effectiveness in distinguishing analogous relations and overcoming overfitting.","Relation extraction (RE) aims to detect the relation between two given entities in texts. For instance, given a sentence ""Remixes of tracks from Persona 5 were supervised by Kozuka and original composer Shoji Meguro"" and an entity pair (Persona 5, Shoji Meguro), the ""composer"" relation is expected to be identified by an RE model. Conventional RE task assumes all relations are observed at once, ignoring the fact that new relations continually emerge in the real world. To deal with emerging relations, some existing works study continual RE. In continual RE, new relations and their involved samples continually emerge, and the goal is to classify all observed relations. Therefore, a continual RE model is expected to be able to learn new relations while retaining the performance on learned relations. 
Existing works primarily focus on storing and replaying samples to avoid catastrophic forgetting of the learned relations. On one hand, considering the limited storage and computational resources, it is impractical to store all training samples and re-train the whole model when new relations emerge. On the other hand, replaying a small number of samples every time new relations emerge would make the model prone to overfit the stored samples. Moreover, existing works simply attribute catastrophic forgetting to the decay of previous knowledge as new relations come but seldom delve deeper into the real causation. We conduct an empirical study and find that the severe decay of knowledge among analogous relations is a key factor of catastrophic forgetting. 
Table shows the accuracy and accuracy drop of two existing models on the FewRel and TACRED datasets. CRL and CRECL are both state-of-the-art models for continual RE. All relations in the datasets are divided into three groups according to the maximum cosine similarity of their prototypes to other relation prototypes. A relation prototype is the overall representation of the relation. We can observe that the performance on relations with higher similarity is poorer, which is reflected in less accuracy and greater accuracy drop. Given that a relation pair with high similarity is often analogous to each other, the performance on a relation tends to suffer a significant decline, i.e., catastrophic forgetting, when its analogous relations appear. For example, the accuracy of the previously learned relation ""location"" drops from 0.98 to 0.6 after learning a new relation ""country of origin"". Therefore, it is important to maintain knowledge among analogous relations for alleviating catastrophic forgetting. See Appendix for more details of our empirical study. 
To address the above issues, we propose a novel continual extraction model for analogous relations. Specifically, we introduce memory-insensitive relation prototypes and memory augmentation to reduce overfitting. The memory-insensitive relation prototypes are generated by combining static and dynamic representations, where the static representation is the average of all training samples after first learning a relation, and the dynamic representation is the average of stored samples. The memory augmentation replaces entities and concatenates sentences to generate more training samples for replay. Furthermore, we propose integrated training and focal knowledge distillation to alleviate knowledge forgetting of analogous relations. The integrated training combines the advantages of two widely-used training methods, which contribute to a more robust feature space and better distinguish analogous relations. One method uses contrastive learning for training and generates prototypes for relation classification, while the other trains a linear classifier. The focal knowledge distillation assigns high weights to analogous relations, making the model more focus on maintaining their knowledge. 
Our main contributions are summarized below: 
 
 * We explicitly consider the overfitting problem in continual RE, which is often ignored by previous works. We propose memory-insensitive relation prototypes and memory augmentation to alleviate overfitting. 
 * We conduct an empirical study and find that analogous relations are hard to distinguish and their involved knowledge is more easily to be forgotten. We propose integrated training and focal knowledge distillation to better distinguish analogous relations. 
 * The experimental results on two benchmark datasets demonstrate that our model achieves state-of-the-art accuracy compared with existing works, and better distinguishes analogous relations and overcomes overfitting for continual RE. Our source code is available at <https: //github. com/nju-websoft/CEAR>."
ThinkSum: Probabilistic reasoning over sets using large language models,2210.01293v2,./img_ACL_2023/2210.01293v2.pdf,"An example adapted from the Conceptual combinations (Invented words) task, in which models must select the most likely completion of a phrase that includes nonce words whose definitions are given. Top: Direct prompting evaluates completion likelihoods normalized over the four answer choices ('people', 'animals', 'birds', 'researchers'). Middle: Chain-of-thought-like or auxiliary knowledge approaches would query a LLM or knowledge base for additional context. This example shows the brittleness entrusting all 'reasoning' to self-attention in linear text, especially in smaller models, which have stronger recency bias: if we simply list generated examples as the additional context in the prompt, the recency bias causes the LLM to still give a higher probability to 'people' than to 'animals', simply because 'bam' (simple dwelling) examples are given after the 'binne' examples. Bottom: Our ThinkSum approach to this task queries a LLM (GPT-2 XL) to produce sets of examples defining the nonce words, then marginalizes over substitutions of these examples into the target phrase.","Large language models (LLMs) have a substantial capacity for high-level analogical reasoning: reproducing patterns in linear text that occur in their training data (zero-shot evaluation) or in the provided context (few-shot in-context learning). However, recent studies show that even the more advanced LLMs fail in scenarios that require reasoning over multiple objects or facts and making sequences of logical deductions. We propose a two-stage probabilistic inference paradigm, ThinkSum, which reasons over sets of objects or facts in a structured manner. In the first stage (Think ‚Äì retrieval of associations), a LLM is queried in parallel over a set of phrases extracted from the prompt or an auxiliary model call. In the second stage (Sum ‚Äì probabilistic inference or reasoning), the results of these queries are aggregated to make the final prediction. We demonstrate the possibilities and advantages of ThinkSum on the BIG-bench suite of LLM evaluation tasks, achieving improvements over the state of the art using GPT-family models on thirteen difficult tasks, often with far smaller model variants. We also compare and contrast ThinkSum with other proposed modifications to direct prompting of LLMs, such as variants of chain-of-thought prompting. Our results suggest that because the probabilistic inference in ThinkSum is performed outside of calls to the LLM, ThinkSum is less sensitive to prompt design, yields more interpretable predictions, and can be flexibly combined with latent variable models to extract structured knowledge from LLMs. Overall, our proposed paradigm represents a promising approach for enhancing the reasoning capabilities of LLMs.","Large language models can recall a broad range of basic facts, recognize and mimic various forms in language, and efficiently extrapolate analogies in structure and meaning. These abilities allow LLMs to excel in zero-shot and few-shot tasks formulated as the generation or selection of a likely completion to a prompt. This formulation requires LLMs to perform fast associative thinking, in which each token of text in the sequence making up the answer is generated or scored in one pass through the model and, other than that, no intermediate information is created or retained. This fast thinking is made possible by the compression of information that is repeated in a variety of ways in large training datasets, within the LLM 's weights. 
However, it is increasingly evident that when reasoning, or slow thinking, is required, failure modes of LLMs are revealed. In our usage, reasoning refers to the sequential manipulation of concepts that can be expressed in language. Tasks that require iterative retrieval of rarely stated knowledge, uncertainties over multiple objects or facts, or multiple steps of deduction are difficult even for the most advanced LLMs. In a recently designed suite of evaluations, BIG-bench, some of the tasks where the gap between machine and human performance is large involve inference sequences with nested counterfactuals (Logical deduction), concepts introduced through definitions (Conceptual combinations), etc. (see Fig. ). These are tasks where a human solver' s intuitive feeling of ' (in) coherence' is insufficient to produce the right answer, and a sequence of thoughts, along with the use of intermediate results, may be necessary to arrive at the solution, particularly when working memory is insufficient. 
We show several tasks in BIG-bench that can be addressed by a two-component mechanism, which we name ThinkSum: 
 * Think (fast thinking / association / knowledge retrieval step): creating an association of text spans with sets of strings. This process may involve generation from a language model, as is the case in Fig. , where the novel word 'binne' is associated with the set of strings {'cat', 'mink', ‚Ä¶} by prompting GPT-3 with the definition and asking for examples. Alternatively, it may consist solely of a scoring mechanism, resulting in the formation of a matrix of probabilities on which probabilistic inference is performed. 
 * Sum (slow thinking / Summarization / reasoning step): probabilistic inference that aggregates generated strings or probabilities to produce the final answer. Summarization typically involves, and often entirely consists of, summing of probabilities of strings (computed in the Think step), as in Fig. , where the final word is assumed to be sampled from a mixture of possible substitutions of 'binne' and 'bam' words into the input. 
 We discuss different ways to Think and to Sum in section, but we start with one example, illustrated in Fig. (bottom), motivated by the Conceptual combinations (Invented words) task in BIG-bench. In this task, the LLM is provided with the definitions of two invented words and asked to infer the most plausible sentence that uses a combination of the invented words. As the words are not common or consistently used in the training set, the LLM needs to understand and combine the definitions of the invented words to reason about the meaning of the combination. The LLM is queried to produce example instances of the invented words with the help of the definitions. These example instances can be substituted into the query in place of the invented words. By mapping individual spans of the text of interest to sets, we arrive at a mixture model (in this example, a mixture with 25 components for 5 possible replacements of each word), which can be used in the same manner the original LLM is used, either to score text or to generate it token by token. When we score all candidate completions using this mixture model and normalize over the four choices, the correct answer ‚Äì that 'binne bams' are for animals and not people ‚Äì becomes the most likely. 
An important difference between our ThinkSum and existing chain-of-thought-like prompt engineering methods, is that our reasoning step is not reduced to a generation problem for the LLM, but is performed as a probabilistic inference external to the LLM. This reduces vulnerability to features of the prompt, such as accidental distraction of the LLM by spurious patterns (see Fig. , middle). Instead, we engineer the slow thinking process to make parallel calls to the LLM to query for intermediate information, then possibly perform programmatic recombination of strings (Think). The final reasoning step ‚Äì in which likelihoods obtained from the LLM for the recombinations derived from earlier steps of the reasoning process are combined to make the final prediction ‚Äì is left to classical probabilistic reasoning (Sum). In a sense, Sum replaces the self-attention mechanism over linear text, which is used as the sole 'reasoning' mechanism in chain-of-thought-like approaches that expect the intermediate 'thoughts' to take the form of generated tokens intervening between the input and output. 
Imposing an alternative reasoning system over an associative ""knee-jerk reaction"" system has an analogy with models of human cognitive processes that separate System 1 (fast thinking) and System 2 (slow thinking). System 2 acts as a 'controller' that can prime System 1 to appropriately bias its fast thinking. In the context of reasoning with deep learning models, System 2 has been interpreted as operating with sparse concepts that can be described in language. Through repeated usage, the functions of System 2 become compressed into System 1 intuitions, in the same manner that iterative 'reasoning' functions of which smaller LLMs are not capable become zero-shot generation capacities for large LLMs. As is the case with humans, there is always the next frontier of problems where a trained model with remarkable 'intuition' needs to be slowed down. The main claim of this paper is that more is possible with LLMs of existing scale when they are used in concert with a wise controller that allows for probabilistic inference."
NUWA-XL: Diffusion over Diffusion for eXtremely Long Video Generation,2303.12346v1,./img_ACL_2023/2303.12346v1.pdf,"Overview of NUWA-XL for extremely long video generation in a ""coarse-to-fine"" process. A global diffusion model first generates L keyframes which form a ""coarse"" storyline of the video, a series of local diffusion models are then applied to the adjacent frames, treated as the first and the last frames, to iteratively complete the middle frames resulting O (L^m) ""fine"" frames in total.","In this paper, we propose NUWA-XL, a novel Diffusion over Diffusion architecture for eXtremely Long video generation. Most current work generates long videos segment by segment sequentially, which normally leads to the gap between training on short videos and inferring long videos, and the sequential generation is inefficient. Instead, our approach adopts a ""coarse-to-fine"" process, in which the video can be generated in parallel at the same granularity. A global diffusion model is applied to generate the keyframes across the entire time range, and then local diffusion models recursively fill in the content between nearby frames. This simple yet effective strategy allows us to directly train on long videos (3376 frames) to reduce the training-inference gap, and makes it possible to generate all segments in parallel. To evaluate our model, we build FlintstonesHD dataset, a new benchmark for long video generation. Experiments show that our model not only generates high-quality long videos with both global and local coherence, but also decreases the average inference time from 7.55min to 26s (by 94.26%) at the same hardware setting when generating 1024 frames. The homepage link is <https: //msra-nuwa. azurewebsites. net/> ","Recently, visual synthesis has attracted a great deal of interest in the field of generative models. Existing works have demonstrated the ability to generate high-quality images and short videos (e.g., 4 seconds, 5 seconds, 5.3 seconds). However, videos in real applications are often much longer than 5 seconds. A film typically lasts more than 90 minutes. A cartoon is usually 30 minutes long. Even for ""short"" video applications like TikTok, the recommended video length is 21 to 34 seconds. Longer video generation is becoming increasingly important as the demand for engaging visual content continues to grow. 
However, scaling to generate long videos has a significant challenge as it requires a large amount of computation resources. To overcome this challenge, most current approaches use the ""Autoregressive over X"" architecture, where ""X"" denotes any generative models capable of generating short video clips, including Autoregressive Models like Phenaki, TATS, NUWA-Infinity; Diffusion Models like MCVD, FDM, LVDM. The main idea behind these approaches is to train the model on short video clips and then use it to generate long videos by a sliding window during inference. ""Autoregressive over X"" architecture not only greatly reduces the computational burden, but also relaxes the data requirements for long videos, as only short videos are needed for training. 
Unfortunately, the ""Autoregressive over X"" architecture, while being a resource-sufficient solution to generate long videos, also introduces new challenges: 1) Firstly, training on short videos but forcing it to infer long videos leads to an enormous training-inference gap. It can result in unrealistic shot change and long-term incoherence in generated long videos, since the model has no opportunity to learn such patterns from long videos. For example, Phenaki and TATS are trained on less than 16 frames, while generating as many as 1024 frames when applied to long video generation. 2) Secondly, due to the dependency limitation of the sliding window, the inference process can not be done in parallel and thus takes a much longer time. For example, TATS takes 7.5 minutes to generate 1024 frames, while Phenaki takes 4.1 minutes. 
To address the above issues, we propose NUWA-XL, a ""Diffusion over Diffusion"" architecture to generate long videos in a ""coarse-to-fine"" process, as shown in Fig. . In detail, a global diffusion model first generates L keyframes based on L prompts which forms a ""coarse"" storyline of the video. The first local diffusion model is then applied to L prompts and the adjacent keyframes, treated as the first and the last frames, to complete the middle L-2 frames resulting in L + (L-1) √ó (L-2) ‚âà L^2 ""fine"" frames in total. By iteratively applying the local diffusion to fill in the middle frames, the length of the video will increase exponentially, leading to an extremely long video. For example, NUWA-XL with m depth and L local diffusion length is capable of generating a long video with the size of O (L^m). The advantages of such a ""coarse-to-fine"" scheme are three-fold: 1) Firstly, such a hierarchical architecture enables the model to train directly on long videos and thus eliminating the training-inference gap; 2) Secondly, it naturally supports parallel inference and thereby can significantly improve the inference speed when generating long videos; 3) Thirdly, as the length of the video can be extended exponentially w. r. t. the depth m, our model can be easily extended to longer videos. Our key contributions are listed in the following: 
 
 * We propose NUWA-XL, a ""Diffusion over Diffusion"" architecture by viewing long video generation as a novel ""coarse-to-fine"" process. 
 * To the best of our knowledge, NUWA-XL is the first model directly trained on long videos (3376 frames), which closes the training-inference gap in long video generation. 
 * NUWA-XL enables parallel inference, which significantly speeds up long video generation. Concretely, NUWA-XL speeds up inference by 94.26%when generating 1024 frames. 
 * We build FlintstonesHD, a new dataset to validate the effectiveness of our model and provide a benchmark for long video generation."
Vision Meets Definitions: Unsupervised Visual Word Sense Disambiguation Incorporating Gloss Information,2305.01788v3,./img_ACL_2023/2305.01788v3.pdf,"An example of VWSD from SemEval-2023 task 1 dataset. We can see that even if the target word ('Angora') is the same, different images should be selected according to the context.","Visual Word Sense Disambiguation (VWSD) is a task to find the image that most accurately depicts the correct sense of the target word for the given context. Previously, image-text matching models often suffered from recognizing polysemous words. This paper introduces an unsupervised VWSD approach that uses gloss information of an external lexical knowledge-base, especially the sense definitions. Specifically, we suggest employing Bayesian inference to incorporate the sense definitions when sense information of the answer is not provided. In addition, to ameliorate the out-of-vocabulary (OOV) issue, we propose a context-aware definition generation with GPT-3. Experimental results show that VWSD performance increased significantly with our Bayesian inference-based approach. In addition, our context-aware definition generation achieved prominent performance improvement in OOV examples exhibiting better performance than the existing definition generation method.","With the development of deep learning technology, research on multimodality such as Visio-Linguistic Models (VLMs) has been actively conducted. In particular, state-of-the-art VLMs, such as image-text matching (ITM) models and text-to-image generation models, are employed in many industrial projects, including image retrieval systems and AI-assisted image generators. 
Visual Word Sense Disambiguation (VWSD) is a multimodal task of natural language processing (NLP) and computer vision that selects the image which corresponds to the intended meaning of the target word among a set of candidate images. Figure is an example of VWSD. For the ambiguous target word 'Angora', we can notice that the answer image should be conditionally changed regarding the context. VWSD can play an important role in several downstream tasks including image retrieval, action recognition and visual question answering. 
Unsupervised VWSD can be formulated in the same way as the ITM task, that is, finding the images that best match the given context. However, VWSD often requires more complex reasoning on both text and images than conventional ITM models. The example in Figure demonstrates that CLIP, a state-of-the-art (SOTA) ITM model, fails to recognize the answer image for the given context. This limitation of VLMs, where they fail to handle ambiguous words, was also reported in another study on an image generation model. 
To ameliorate this problem, we propose to disambiguate visual words with the assistance of a glossary of lexical knowledge-bases (LKBs) without the use of any further training or dataset. Specifically, we utilize the sense definitions of an ambiguous word that have been widely exploited in previous lexical semantic tasks. Herein, since the answer sense of the target word is not provided in the VWSD setting, we propose an approach derived from Bayesian inference, using pretained ITM models. Moreover, in order to deal with out-of-vocabulary (OOV) words that cannot find the sense definitions of the target word in LKBs, we suggest the concept of context-aware definition generation (CADG). The definitions of a target word are generated by a large language model, GPT-3, as auxiliary information for VWSD. 
Experiments were conducted on SemEval-2023 (SE23) Task 1-Visual-WSD, a publicly available VWSD dataset. Furthermore, in the experiments, we utilized two pretained SOTA ITM models: (1) CLIP and (2) FLAVA. Experiments showed that our proposed approach significantly improved the performance of baseline ITM models. In addition, we demonstrated that our concept of CADG not only significantly increased the performance of OOV cases but is also more advantageous than the previous definition generation approach. We implement experimental codes in <https: //github. com/soon91jae/UVWSD>. 
The contributions of this paper can be summarized as follows: 
 
 * This paper introduces a new gloss-incorporated VWSD approach inspired by Bayesian inference. 
 * Experimental results show that our Bayesian inference-based approach boosted the unsupervised VWSD performance significantly without any additional training. 
 * Furthermore, we suggest the CADG method to challenge the OOV issue."
Neural Unsupervised Reconstruction of Protolanguage Word Forms,2211.08684v1,./img_ACL_2023/2211.08684v1.pdf,"Overview of our paper. (a) We model the evolution of word forms as a generative process which applies many character-level edits to the ancestral form, producing a distribution over the output word form y and edit sequence Œî. (b) Using a dynamic program, we can compute the distribution over output words, p (y | x). We model this for every language branch l ‚àà L. (c) Our method uses EM to infer ancestral forms. For the E-step, we want to sample from the posterior distribution, where y is observed but x is not. (f) With samples from the previous step fixed, we use another dynamic program to compute expected edit counts. (e) In the M-step, we use these edit counts to train our character-level edit models q, parameterized as recurrent neural networks. q determines the edit probabilities in (c) and thus influences the next round of samples. (d) After several EM iterations, we take the maximum likelihood word forms as the final reconstructions.","We present a state-of-the-art neural approach to the unsupervised reconstruction of ancient word forms. Previous work in this domain used expectation-maximization to predict simple phonological changes between ancient word forms and their cognates in modern languages. We extend this work with neural models that can capture more complicated phonological and morphological changes. At the same time, we preserve the inductive biases from classical methods by building monotonic alignment constraints into the model and deliberately underfitting during the maximization step. We evaluate our performance on the task of reconstructing Latin from a dataset of cognates across five Romance languages, achieving a notable reduction in edit distance from the target word forms compared to previous methods.","Research has shown that groups of languages can often be traced back to a common ancestor, or a protolanguage, which has evolved and branched out over time to produce its modern descendants. Words in protolanguages undergo sound changes to produce their corresponding forms in modern languages. We call words in different languages with a common proto-word ancestor cognates. The study of cognate sets can reveal patterns of phonological change, but their proto-words are often undocumented. 
To reconstruct ancient word forms, linguists use the comparative method, which compares individual features of words in modern languages to their corresponding forms in hypothesized reconstructions of the protolanguage. Past work has demonstrated the possibility of automating this manual procedure. For example, developed probabilistic models of phonological change and used them to learn reconstructions of Latin based on a dataset of Romance languages, and extended their method to a large scale dataset of Austronesian languages. 
Nevertheless, previous approaches to computational protolanguage reconstruction have mainly considered simple rules of phonological change. In previous works, phonological change is modeled applying a sequence of phoneme-level edits to the ancestral form. Although this can capture many regular sound changes such as lenitions, epentheses, and elisions, these edits are typically conditioned only on adjacent phonemes and lack more general context-sensitivity. Phonological effects such as dissimilation, vowel harmony, syllabic stress, pre-cluster shortening, trysyllabic laxing, and homorganic lengthening, as well as many non-phonological aspects of language change, are all frequently dependent on non-local contexts. However, it is difficult to extend existing multinomial and log-linear models to handle more complex conditioning environments. 
Motivated by these challenges, our work is the first to use neural models for unsupervised reconstruction. Ancestral word forms and model parameters in previous unsupervised approaches are typically learned using expectation-maximization. In applying neural methods to protolanguage reconstruction, we identify a problem in which the EM objective becomes degenerate under highly expressive models. In particular, we find that neural models are able to express not just complex phonological changes, but also inconsistent ones (i.e., predicting vastly different edits in similar contexts), undermining their ability to distinguish between good and bad hypotheses. From a linguistic perspective, phonological change should exhibit regularities due to the constraints of the human articulatory and cognitive faculties, so we build a bias towards regular changes into our method by using a specialized model architecture and learning algorithm. We outline our approach in Figure. 
Our work enables neural models to effectively learn reconstructions under expectation-maximization. In Section, we describe a specialized neural architecture with monotonic alignment constraints. In Section, we motivate training deliberately underfitted models. Then, in Section, we conduct experiments and show a significant improvement over the previously best performing method. Finally, we conduct ablation experiments and attribute the improvement to (1) the ability to model longer contexts and (2) a training process that is well-regularized for learning under EM."
DaMSTF: Domain Adversarial Learning Enhanced Meta Self-Training for Domain Adaptation,2308.02753v1,./img_ACL_2023/2308.02753v1.pdf,"An overview of the DaMSTF. Red arrows indicate the training process of the model, while blue and green arrows indicate the data flow.","Self-training emerges as an important research line on domain adaptation. By taking the model's prediction as the pseudo labels of the unlabeled data, self-training bootstraps the model with pseudo instances in the target domain. However, the prediction errors of pseudo labels (label noise) challenge the performance of self-training. To address this problem, previous approaches only use reliable pseudo instances, i.e., pseudo instances with high prediction confidence, to retrain the model. Although these strategies effectively reduce the label noise, they are prone to miss the hard examples. In this paper, we propose a new self-training framework for domain adaptation, namely Domain adversarial learning enhanced Self-Training Framework (DaMSTF). Firstly, DaMSTF involves meta-learning to estimate the importance of each pseudo instance, so as to simultaneously reduce the label noise and preserve hard examples. Secondly, we design a meta constructor for constructing the meta validation set, which guarantees the effectiveness of the meta-learning module by improving the quality of the meta validation set. Thirdly, we find that the meta-learning module suffers from the training guidance vanishment and tends to converge to an inferior optimal. To this end, we employ domain adversarial learning as a heuristic neural network initialization method, which can help the meta-learning module converge to a better optimal. Theoretically and experimentally, we demonstrate the effectiveness of the proposed DaMSTF. On the cross-domain sentiment classification task, DaMSTF improves the performance of BERT with an average of nearly 4%.","Domain adaptation, which aims to adapt the model trained on the source domain to the target domain, attracts much attention in Natural Language Processing (NLP) applications. Since domain adaptation involves labeled data from the source domain and unlabeled data from the target domain, it can be regarded as a semi-supervised learning problem. From this perspective, self-training, a classical semi-supervised learning approach, emerges a prospective research direction on domain adaptation. 
Self-training consists of a series of loops over the pseudo labeling phase and model retraining phase. In the pseudo labeling phase, self-training takes the model 's prediction as the pseudo labels for the unlabeled data from the target domain. Based on these pseudo-labeled instances, self-training retrains the current model in the model retraining phase. The trained model can be adapted to the target domain by repeating these two phases. Due to the prediction errors, there exists label noise in pseudo instances, which challenges self-training approaches. 
Previous self-training approaches usually involve a data selection process to reduce the label noise, i.e., preserving the reliable pseudo instances and discarding the remaining ones. In general, higher prediction confidence implies higher prediction correctness, so existing self-training approaches prefer the pseudo instances with high prediction confidence. However, fitting the model on these easy pseudo instances cannot effectively improve the model, as the model is already confident about its prediction. On the contrary, pseudo instances with low prediction confidence can provide more information for improving the model, but contain more label noise at the same time. 
To simultaneously reduce the label noise and preserve hard examples, we propose to involve in meta-learning to reweight pseudo instances. Within a learning-to-learn schema, the meta-learning module learns to estimate the importance of every pseudo instance, and then, allocates different instance weights to different pseudo instances. Ideally, hard and correct pseudo instances will be assigned larger weights, while easy or error pseudo instances will be assigned smaller weights. To achieve this, the process in the meta-learning module is formulated as a bi-level hyperparameters optimization problem, where instance weights are taken as the hyperparameters and determined by a series of meta-training steps and meta-validation steps. In the meta-training step, the model is virtually updated on the meta-training set with respect to the current instance weights. In the meta validation step, we validate the virtually updated model with an unbiased meta validation set, and optimize the instance weights with the training guidance back-propagated from the validation performance. 
According to the analysis in, a high-quality meta validation set, which is clean and unbiased to the test set, is important for the effectiveness of the meta-learning algorithm. To this end, we propose a meta constructor oriented to the domain adaptation scenario. At each self-training iteration, the meta constructor selects out the most reliable pseudo instances and inserts them into the meta validation set. Since the instances in the meta validation set are all from the target domain and vary along with the self-training iterations, the data distribution in the constructed meta validation set approximates the one in the target domain. Thus, the meta constructor reduces the bias of the meta validation set. On the other hand, selecting the most reliable pseudo instances can reduce the label noise, making the meta validation set cleaner. 
Another challenge for the meta-learning module is the training guidance vanishment, referring to the gradient vanishment on hyperparameters. With a theoretical analysis, we attribute this problem to the gradient vanishment on the meta validation set. To this end, we introduce a domain adversarial learning module to perturb the model' s parameters, thereby increasing the model 's gradients on the meta validation set. In DaMSTF, we also interpret the domain adversarial learning module as a heuristic neural network initialization method. Before the model retraining phase, the domain adversarial learning module first initializes the model' s parameters by aligning the model 's feature space. For domain adaptation, the global optimal refers to the state where the model' s parameters are agnostic to the domain information but discriminative to the task information. Thus, the training process in the domain adversarial learning module makes the model's parameters closer to the global optimal, serving as a heuristic neural network initialization. 
Our contributions can be summarized as follows: 
 
 * We propose a new self-training framework to realize domain adaptation, named Domain adversarial learning enhanced Meta Self Training Framework (DaMSTF), which involves meta-learning to simultaneously reduce the label noise and preserve hard examples. 
 * We propose a meta constructor to construct the meta validation set, which guarantees the effectiveness of the meta-learning module. 
 * We theoretically point out the training guidance vanishment problem in the meta-learning module and propose to address this problem with a domain adversarial learning module. 
 * Theoretically, we analyze the effectiveness of the DaMSTF in achieving domain adaptation. Experimentally, we validate the DaMSTF on two popular models, i.e., BERT for the sentiment analysis task and BiGCN for the rumor detection task, with four benchmark datasets."
Facilitating Multi-turn Emotional Support Conversation with Positive Emotion Elicitation: A Reinforcement Learning Approach,2307.07994v1,./img_ACL_2023/2307.07994v1.pdf,A simplified multi-turn ESC example between the user (left) and agent (right). The agent progressively adjusts the intensity of empathy and elicitation to achieve the goal of improving the user's mental state.,"Emotional support conversation (ESC) aims to provide emotional support (ES) to improve one 's mental state. Existing works stay at fitting grounded responses and responding strategies (e.g., question), which ignore the effect on ES and lack explicit goals to guide emotional positive transition. To this end, we introduce a new paradigm to formalize multi-turn ESC as a process of positive emotion elicitation. Addressing this task requires finely adjusting the elicitation intensity in ES as the conversation progresses while maintaining conversational goals like coherence. In this paper, we propose Supporter, a mixture-of-expert-based reinforcement learning model, and well design ES and dialogue coherence rewards to guide policy' s learning for responding. Experiments verify the superiority of Supporter in achieving positive emotion elicitation during responding while maintaining conversational goals including coherence.","Emotional support (ES) aims to reassure a person to recover from emotional distress and improve one 's mental state. It is a manifestation of emotional intelligence in social interactions. Endowing ES into social dialogue systems for building helpful and trustful agents is an emerging trend. 
To achieve this goal, a typical practice is modeling empathy, which aims to perceive and understand the situation and feelings of others. Yet, the empathetic conversation is inherently deficient in providing ES as (1) Lack of consideration of multi-turn conversation. Just making empathetic responses in each single dialogue turn leads to ignoring the user' s feedback and mental state changes in multi-turn interaction. (2) Lack of awareness of emotional elicitation. Only emanating emotional resonance fails to help users jump out of negative mental states. Although design emotional support conversation (ESC) task promising to remedy these deficiencies, existing works stay at fitting grounded responses and responding strategies (e.g., question) while ignoring the effects of such efforts on ES. They do not fully model the essential working mechanism of ESC and lack explicit goals to guide a user 's emotion to a positive transition in the multi-turn process. Thus, they are still insufficient to lay out an entire ESC process and cannot effectively improve one' s mental state. 
To this end, we introduce multi-turn ESC with positive emotion elicitation, a new paradigm aims to progressively empathize and elicit users to reach a better mental state through multi-turn conversation. Addressing this task is challenging (an example is in Figure): First, in a realistic multi-turn ESC, the user 's emotions often transit towards positive (e.g., the user' s emotion starts with negative and ends with positive, i.e., ""My school was closed"" ‚Üí ""I feel better now"") with fluctuation (e.g., the user 's negative emotions in the first two turns gradually deepen, i.e., ""My school was closed"" ‚Üí ""I don' t even know""), which requires the agent to equip with the mechanism dealing with complex situations to respond satisfactorily. Second, for ES, the ES response requires a delicate balance between empathy and elicitation. Only empathizing without eliciting falls into a negative emotional cycle, while the opposite setting brings a sense of distance in communication. They need to be progressively and purposefully adjusted in ongoing interactions, e.g., the agent expresses empathy of varying emotional polarity (negative ‚Üí negative ‚Üí positive) and carefully increase the intensity of elicitation (only empathy ‚Üí weak elicitation ‚Üí strong elicitation). Third, for language expression, the ES response purposefully elicits positive emotions but should not undermine general conversational goals like coherence. Making an eliciting response that is out of the dialogue context, e.g., replacing ""I understand you. I would. . . happened to me. "" with ""Come on! I believe. . . find a solution! "", may cause users to resent and block useful feedback. 
In this paper, we propose Supporter to facilitate multi-turn emotional Support conversation with positive emotion elicitation using a mixture-of-expert (MoE) based reinforcement learning (RL). MoE designs heuristic experts associated with specific tasks to learn diverse semantics by characterizing dialogue context, where: (1) To cope with the user 's emotional fluctuation in the ongoing conversation, experts are devised as positive and negative experts as a whole; (2) To inspire ES of responding, the emotion experts of MoE are designed to predict the user' s emotional states that are possibly transited to; (3) To inspire the expression of responding, the keyword experts of MoE are designed to predict the keywords that maintain the dialogue coherence. With experts as candidates, our RL agent learns conversational semantic encoding policy and purposefully selects experts with expert selection policy for response generation. To achieve the goal of positive emotion elicitation during responding while maintaining conversational goals like coherence, we optimize policy by carefully constructing the rewards: (1) ES rewards consider the conversation progress to dynamically adjust the elicitation intensity of positive emotion; (2) Dialogue coherence rewards involve keyword-level and sentence-level guides to finely maintain coherence. 
Our contributions are summarized as follows: (1) We introduce a new paradigm by carefully dissecting the challenges of formalizing multi-turn ESC as a process of positive emotion elicitation. (2) We propose Supporter, an MoE-based RL model with carefully constructed ES and dialogue coherence rewards, elicits positive emotion during responding while maintaining dialogue coherence. (3) Extensive experiments show the superiority of Supporter with automatic, interactive human, and novel ES and dialogue coherence evaluations."
Query Enhanced Knowledge-Intensive Conversation via Unsupervised Joint Modeling,2212.09588v2,./img_ACL_2023/2212.09588v2.pdf,"Overview of QKConv's joint training process. QKConv consists of three modules: a query generator, an off-the-shelf knowledge selector, and a response generator, where two generators share model parameters. During training, for a given dialogue context, QKConv learns to produce the target response by exploring multiple candidate queries and leveraging corresponding selected knowledge. Additionally, we integrate context-sensitive and response-sensitive guidance into the candidate query set to regulate query generation and facilitate joint training.","In this paper, we propose an unsupervised query enhanced approach for knowledge-intensive conversations, namely QKConv. There are three modules in QKConv: a query generator, an off-the-shelf knowledge selector, and a response generator. QKConv is optimized through joint training, which produces the response by exploring multiple candidate queries and leveraging corresponding selected knowledge. The joint training solely relies on the dialogue context and target response, getting exempt from extra query annotations or knowledge provenances. To evaluate the effectiveness of the proposed QKConv, we conduct experiments on three representative knowledge-intensive conversation datasets: conversational question-answering, task-oriented dialogue, and knowledge-grounded conversation. Experimental results reveal that QKConv performs better than all unsupervised methods across three datasets and achieves competitive performance compared to supervised methods.","In addition to open-domain chitchat, there exist various knowledge-intensive conversations, such as conversational question-answering, task-oriented dialogue, and knowledge-grounded conversation. Although large-scale language models can implicitly store common knowledge within parameters, they are known to suffer from producing plausible statements with factual errors (a. k. a. knowledge hallucination). Therefore, there is a trend to rely on external resources, such as Wikipedia databases or search engine results, to facilitate knowledge-intensive conversations. 
In knowledge-intensive conversations, the most straightforward way to retrieve external knowledge is to take the dialogue context as the query and use an off-the-shelf retriever to return the knowledge entry. However, it encounters some difficulties in retrieving appropriate knowledge. As the focus or topic changes along with the conversation flow, the outdated information in the dialogue context brings extra noise to the retriever, resulting in obsolete or irrelevant knowledge retrieved. Moreover, the dialogue context has a native misalignment with the short and interrogative query preferred in existing retrievers. 
Some methods choose to finetune a task-specific retriever to enhance the performance of knowledge selection. However, this strategy is usually computationally expensive (e.g., finetuning a dense retriever requires constant recomputation for massive knowledge entries) or even infeasible for complex retrieval systems (e.g., retraining a search engine is impractical). Some other methods choose to generate a self-contained query based on the dialogue context. This strategy relies on careful query annotations to guarantee the completeness of essential information extraction and the adaptation to the knowledge selector. 
In this paper, we introduce a novel unsupervised query enhanced approach for knowledge-intensive conversations, namely QKConv. As shown in Figure, QKConv consists of three modules: a query generator, an off-the-shelf knowledge selector, and a response generator. Specifically, QKConv is optimized through joint training, which produces the response by exploring multiple candidate queries and leveraging corresponding selected knowledge. We also integrate two types of query guidance to regulate query generation and facilitate joint training: context-sensitive guidance (e.g., the last context utterance) and response-sensitive guidance (e.g., the target response). 
The benefits brought by QKConv 's design are three-fold. Firstly, the training of QKConv solely relies on the dialogue context and target response, getting exempt from extra query annotations or knowledge provenances. Secondly, the joint training of QKConv boosts query generation toward better knowledge selection and ensures end-to-end performances, compared to the individual optimization of each module. Thirdly, thanks to the query generation module, QKConv gets rid of the expensive computation of tuning knowledge selectors and has the generality to adopt various knowledge selectors. 
To evaluate the effectiveness of the proposed QKConv, we conduct experiments on three representative knowledge-intensive conversation datasets: conversational question answering QReCC, task-oriented dialogue SMD, and knowledge-grounded conversation WoW. Experimental results reveal that QKConv performs better than all unsupervised methods across three datasets and even outperforms supervised methods on some datasets. Specifically, QKConv' s generated query achieves superior knowledge selection performance, and QKConv exhibits robust knowledge utilization in response generation. We have released QKConv's code and model checkpoints [1], hoping to facilitate further research in knowledge-intensive conversations. [1] <https: //github. com/PaddlePaddle/Knover/tree/develop/projects/QKConv> In summary, the main contributions of this paper are: (1) We propose an unsupervised query enhanced approach via joint training for knowledge-intensive conversations, namely QKConv. To the best of our knowledge, we are the first to utilize joint training for query generation. (2) We show that QKConv achieves state-of-the-art end-to-end results against all unsupervised methods and outperforms supervised methods on certain datasets. (3) We show that QKConv exhibits superior query quality and robust knowledge utilization in response generation."
White-Box Multi-Objective Adversarial Attack on Dialogue Generation,2305.03655v2,./img_ACL_2023/2305.03655v2.pdf,"Illustration of our DGSlow attack method. In each iteration, the current adversarial utterance xÃÇ_n^‚Ñ¨, together with persona, chat history, and references, are fed into the model to obtain the word saliency via gradient descent. Then we mutate the positions with high word saliency and validate the correctness of the perturbed samples. The remaining samples query the model to calculate their fitness, and we select k prominent candidates using adaptive search for the next iteration.","Pre-trained transformers are popular in state-of-the-art dialogue generation (DG) systems. Such language models are, however, vulnerable to various adversarial samples as studied in traditional tasks such as text classification, which inspires our curiosity about their robustness in DG systems. One main challenge of attacking DG models is that perturbations on the current sentence can hardly degrade the response accuracy because the unchanged chat histories are also considered for decision-making. Instead of merely pursuing pitfalls of performance metrics such as BLEU, ROUGE, we observe that crafting adversarial samples to force longer generation outputs benefits attack effectiveness‚Äîthe generated responses are typically irrelevant, lengthy, and repetitive. To this end, we propose a white-box multi-objective attack method called DGSlow. Specifically, DGSlow balances two objectives‚Äîgeneration accuracy and length, via a gradient-based multi-objective optimizer and applies an adaptive searching mechanism to iteratively craft adversarial samples with only a few modifications. Comprehensive experiments on four benchmark datasets demonstrate that DGSlow could significantly degrade state-of-the-art DG models with a higher success rate than traditional accuracy-based methods. Besides, our crafted sentences also exhibit strong transferability in attacking other models.","Pre-trained transformers have achieved remarkable success in dialogue generation (DG), e.g., the ubiquitous chat agents and voice-embedded chat-bots. However, such powerful models are fragile when encountering adversarial samples crafted by small and imperceptible perturbations. Recent studies have revealed the vulnerability of deep learning in traditional tasks such as text classification and neural machine translation. Nonetheless, investigating the robustness of DG systems has not received much attention. 
Crafting DG adversarial samples is notably more challenging due to the conversational paradigm, where we can only modify the current utterance while the models make decisions also based on previous chat history. This renders small perturbations even more negligible for degrading the output quality. An intuitive adaptation of existing accuracy-based attacks, especially black-box methods that merely pursue pitfalls for performance metrics, cannot effectively tackle such issues. Alternatively, we observed that adversarial perturbations forcing longer outputs are more effective against DG models, as longer generated responses are generally more semantic-irrelevant to the references. Besides, such an objective is non-trivial because current large language models can handle and generate substantially long outputs. This implies the two attacking objectives‚Äîgeneration accuracy and length, can somehow be correlated and jointly approximated. 
To this end, we propose a novel attack method targeting the two objectives called DGSlow, which produces semantic-preserving adversarial samples and achieves a higher attack success rate on DG models. Specifically, we define two objective-oriented losses corresponding to the response accuracy and length. Instead of integrating both objectives and applying human-based parameter tuning, which is inefficient and resource-consuming, we propose a gradient-based multi-objective optimizer to estimate an optimal Pareto-stationary solution. The derived gradients serve as indicators of the significance of each word in a DG instance. Then we iteratively substitute those keywords using masked language modeling (MLM) and validate the correctness of crafted samples. The intuition is to maintain semantics and grammatical correctness with minimum word replacements. Finally, we define a unique fitness function that considers both objectives for selecting promising crafted samples. Unlike existing techniques that apply either greedy or random search, we design an adaptive search algorithm where the selection criteria are dynamically based on the current iteration and candidates' quality. Our intuition is to avoid the search strapped in a local minimum and further improve efficiency. 
We conduct comprehensive attacking experiments on three pre-trained transformers over four DG benchmark datasets to evaluate the effectiveness of our method. Evaluation results demonstrate that DGSlow overall outperforms all baseline methods in terms of higher attack success rate, better semantic preservance, and longer as well as more irrelevant generation outputs. We further investigate the transferability of DGSlow on different models to illustrate its practicality and usability in real-world applications. 
Our main contributions are as follows: 
 
 * To the best of our knowledge, we are the first to study the robustness of large language models in DG systems against adversarial attacks, and propose a potential way to solve such challenge by re-defining DG adversarial samples. 
 * Different from existing methods that only consider a single objective, e.g., generation accuracy, we propose multi-objective optimization and adaptive search to produce semantic-preserving adversarial samples that can produce both lengthy and irrelevant outputs. 
 * Extensive experiments demonstrate the superiority of DGSlow to all baselines as well as the strong transferability of our crafted samples."
Do language models have coherent mental models of everyday things?,2212.10029v3,./img_ACL_2023/2212.10029v3.png,"While humans appear to have coherent mental pictures of everyday things (e.g., an egg, A), our question-asking probes suggest that LMs do not (e.g., one LM answered that the egg white both surrounds and is surrounded by the shell, B). This model incoherence can be reduced by applying commonsense constraints (e.g., surrounds is asymmetric), resulting in a more coherent parts model (C).","When people think of everyday things like an egg, they typically have a mental image associated with it. This allows them to correctly judge, for example, that ""the yolk surrounds the shell"" is a false statement. Do language models similarly have a coherent picture of such everyday things? To investigate this, we propose a benchmark dataset consisting of 100 everyday things, their parts, and the relationships between these parts, expressed as 11,720 ""X relation Y? "" true/false questions. Using these questions as probes, we observe that state-of-the-art pre-trained language models (LMs) like GPT-3 and Macaw have fragments of knowledge about these everyday things, but do not have fully coherent ""parts mental models"" (54-59%accurate, 19-43%conditional constraint violation). We propose an extension where we add a constraint satisfaction layer on top of the LM 's raw predictions to apply commonsense constraints. As well as removing inconsistencies, we find that this also significantly improves accuracy (by 16-20%), suggesting how the incoherence of the LM' s pictures of everyday things can be significantly reduced.","Psychologists and cognitive scientists hypothesize that humans develop mental models of the world, namely internal, conceptual representations of the environment which we base our decisions and actions on. observed that 5-month-old human infants exhibit understanding of mechanical properties of objects in terms of arrangements and motions of surfaces, well before they can understand language. Drawing loosely on this idea, but without making any claims about how LMs reason internally, we investigate if pre-trained language models show evidence of coherent internal representations of everyday things, analogous to human mental models, via probing. We focus on mental models in the context of ordinary objects that we encounter in our everyday lives. Such commonsense knowledge helps us understand how these everyday things work and how to interact with them. For example, when someone tries to make a fried egg, they know that it has a shell and that it can be cracked open to reveal the egg white and yolk inside. However, if a system does not have a coherent picture of such everyday things, thinking that the egg yolk surrounds the shell, then it might have to resort to ridiculous approaches such as trying to scrape the egg yolk off the shell into the pan. 
We explore a first version of this, in which we consider only knowledge about an object 's parts and their relationships. We refer to this knowledge as a parts mental model. We first create a benchmark dataset of 100 everyday things, by asking human annotators to draw a graph representing their parts mental model (e.g., Figure) depicting the parts of an everyday thing, spatial relationships, connections between its parts and functional dependencies (if any). Then we probe two representative state-of-the-art LMs with questions about these everyday things. We find that the LMs' parts mental models are generally of poor quality. Further, model predictions can violate basic consistency constraints e.g.,transitivity. To alleviate this, we apply constraint reasoning to derive more accurate and consistent mental models of everyday things, correcting some of the LMs' original inconsistencies. This is illustrated in Figure. 
Our contributions are: 0cm 0cm 0cm 0cm
 1. We present a benchmark dataset of parts mental models consisting of 100 everyday things, 2.2K parts and 11.7K relationships. 
 2. We show that SOTA LMs like GPT-3 and Macaw are poor at answering relationship queries between parts of everyday things. The parts mental models derived using their predictions are only 54-59%accurate, and significantly inconsistent (19-43%conditional violation œÑ). 
 3. We propose a neuro-symbolic method that applies constraint reasoning on top of raw LM predictions as a way of obtaining more consistent (0%conditional violation œÑ) and more accurate mental models (16-20%improvement). This suggests a broader cognitive architecture (LM + reasoner) for future systems, to better construct mental models than the LM alone."
MoralDial: A Framework to Train and Evaluate Moral Dialogue Systems via Moral Discussions,2212.10720v2,./img_ACL_2023/2212.10720v2.pdf,"The proposed framework to model the communication mechanisms in moral discussion. The framework includes three parts to express morality. When acting moral explanation and moral revision, the discusser would use the expression of basic RoTs (marked in the same color). In summary, To express morality, a person or dialogue system is supposed to (1) understand the expression of basic RoTs; (2) appropriately deal with possible moral conflict; (3) explain its moral views; and (4) revise its moral views if necessary.","Morality in dialogue systems has raised great attention in research recently. A moral dialogue system aligned with users' values could enhance conversation engagement and user connections. In this paper, we propose a framework, MoralDial to train and evaluate moral dialogue systems. In our framework, we first explore the communication mechanisms of morality and resolve expressed morality into three parts, which indicate the roadmap for building a moral dialogue system. Based on that, we design a simple yet effective method: constructing moral discussions between simulated specific users and the dialogue system. The constructed discussions consist of expressing, explaining, revising, and inferring moral views in dialogue exchanges, which makes conversational models learn morality well in a natural manner. Furthermore, we propose a novel evaluation method under the framework. We evaluate the multiple aspects of morality by judging the relation between dialogue responses and human values in discussions, where the multifaceted nature of morality is particularly considered. Automatic and manual experiments demonstrate that our framework is promising to train and evaluate moral dialogue systems.","Morality is described as ""principles concerning the distinction between right and wrong or good and bad behaviors"". In recent years, aligning AI with human values, morality, ethics, and social norms has become a hot topic in research. As an important application of AI, open-domain dialogue systems, which directly interact with users, requires the nature of morality more urgently. A moral open-domain dialogue system can practice social norms and gain users 'trust more easily. Moreover, moral dialogue systems further promote dialogue safety, mitigating immoral speeches and behaviors. 
To analyze text-based morality, related works introduce Rules of thumb (RoTs), the basic conceptual units to study social norms and morality (e.g.,you shouldn' t slap or punch others 'face). Adopting RoTs to model morality is proved effective. For example, train Delphi on RoTs judgment corpora and find that machine has the potential to make ethical judgments. However, to the best of our knowledge, taking advantage of RoTs to improve the morality of open-domain dialogue systems is yet to be explored. 
There are three challenges to building a moral dialogue system. Firstly, morality is a biological attribute of human-beings, thus how to understand and express morality by explicitly interacting with users is a great challenge. Exploring the communication mechanisms of morality is necessary. Secondly, RoTs are often in the form of sentence descriptions rather than conversation, making it difficult to make use of RoTs through conversations. Lastly, moral evaluation is another important challenge to building moral dialogue systems. Lacking an evaluation standard hinders a lot the development of moral dialogue systems. 
To address these challenges, we design a framework named MoralDial to train and evaluate moral conversational models in. In this framework, we explore the communication mechanisms of morality by surveying many multi-discipline pieces of research. We resolve morality into three sub-modules: (1) Standpoint Sentences/Phrases (sentence-level), (2) Discussion State (conversation-level), and (3) Discusser Behavior (utterance-level), which provides more detailed requirements that the conversational models should understand and capture. 
For training a conversational model to satisfy the above requirements, we propose a simple yet effective method by constructing corresponding moral discussions, which embeds morality standpoints (RoTs) into a conversation. In the constructed discussions, the dialogue system and the simulated users are pre-set to have respective moral views. Then we design some dialogue flows including moral answering, moral explanation, moral revision, and RoT inference learning. The dialogue flows also correspond to our proposed framework. We adopt multi-task learning and make conversational models learn the skills simultaneously. By expressing, explaining, and revising moral views in dialogue exchanges, conversational models learn morality well in a natural manner. 
We also adopt this framework to evaluate moral dialogue systems. It is quite difficult to directly judge morality due to its subjectivity, topic-broadness, and open-endedness. Instead, we evaluate morality from the decomposed sub-modules, including moral answering, explanation, revision, and inference. Furthermore, we transform this complex moral evaluation problem into an agreement judgment between one' s response and moral values, which is computationally and quantitatively feasible. In this procedure, we consider the moral values of the user, the chatbot, and the general population at the same time, which emphasizes the multifacetedness of morality. 
We apply our proposed framework and methods on popular conversational models (i.e.,DialoGPT and Blenderbot). The automatic and human experimental results demonstrate that each sub-module in our framework is indispensable and our framework is promising to train and evaluate a moral dialogue system. 
In summary, our contributions are threefold. 
 
 * We propose a framework named MoralDial to describe and model moral discussions, which also explores the communication mechanisms of expressed morality. 
 * Inspired by the framework, we construct moral discussions from the sentence-formal RoTs to train moral dialogue systems. 
 * We present a novel evaluation method to evaluate the moral performance of conversational models based on the framework."
Denoising Bottleneck with Mutual Information Maximization for Video Multimodal Fusion,2305.14652v3,./img_ACL_2023/2305.14652v3.pdf,"An example of redundancy and noise in a video. As illustrated, consecutive frames have high cosine similarity, which results in a problem of redundancy. In addition, useless information like distracting background and weak alignment between frames and transcripts compose noises in videos.","Video multimodal fusion aims to integrate multimodal signals in videos, such as visual, audio and text, to make a complementary prediction with multiple modalities contents. However, unlike other image-text multimodal tasks, video has longer multimodal sequences with more redundancy and noise in both visual and audio modalities. Prior denoising methods like forget gate are coarse in the granularity of noise filtering. They often suppress the redundant and noisy information at the risk of losing critical information. Therefore, we propose a denoising bottleneck fusion (DBF) model for fine-grained video multimodal fusion. On the one hand, we employ a bottleneck mechanism to filter out noise and redundancy with a restrained receptive field. On the other hand, we use a mutual information maximization module to regulate the filter-out module to preserve key information within different modalities. Our DBF model achieves significant improvement over current state-of-the-art baselines on multiple benchmarks covering multimodal sentiment analysis and multimodal summarization tasks. It proves that our model can effectively capture salient features from noisy and redundant video, audio, and text inputs. The code for this paper is publicly available at <https: //github. com/WSXRHFG/DBF>.","With the rapid development of social platforms and digital devices, more and more videos are flooding our lives, which leads video multimodal fusion an increasingly popular focus of NLP research. Video multimodal fusion aims to integrate the information from two or more modalities (e.g., visual and audio signals) into text for a more comprehensive reasoning. For example, multimodal sentiment analysis utilizes contrast between transcript and expression to detect sarcam, multimodal summarization complete summary with information only exists in visual signal. 
However, as shown in the Figure, there exist plenty of redundancy and noise in video multimodal fusion: 1) high similarity across consecutive frames brings video redundancy; 2) useless information, such as the distracting background, introduces frame noise; 3) weak alignment between visual stream and text also introduces misalignment noise. To alleviate the problem of redundancy and noise in video multimodal fusion, control the flow of redundant and noisy information between multimodal sequences by a fusion forget gate. The fusion forget gate impairs the impact of noise and redundancy in a coarse grain of the whole modality, so it will also filter out some representative information in the filtered modality. 
In order to remove noise and redundancy while preserving critical information in video multimodal fusion, we propose a denoising fusion bottleneck (DBF) model with mutual information maximization (MI-Max). Firstly, inspired by, we introduce a bottleneck module to restrict the redundant and noisy information across different modalities. With the bottleneck module, inputs can only attend to low-capacity bottleneck embeddings to exchange information across different modalities, which urges redundant and noisy information to be discarded. Secondly, in order to prevent key information from being filtered out, we adopt the idea of contrastive learning to supervise the learning of our bottleneck module. Specifically, under the noise-contrastive estimation framework, for each sample, we treat all the other samples in the same batch as negative ones. Then, we aim to maximize the mutual information between fusion results and each unimodal inputs by distinguishing their similarity scores from negative samples. Two aforementioned modules complement each other, the MI-Max module supervises the fusion bottleneck not to filter out key information, and in turn, the bottleneck reduces irrelevant information in fusion results to facilitate the maximization of mutual information. 
We conduct extensive experiments on three benchmarks spanning two tasks. MOSI and MOSEI are two datasets for multimodal sentiment analysis. How2 is a benchmark for multimodal summarization. Experimental results show that our model achieves consistent improvements compared with current state-of-the-art methods. Meanwhile, we perform comprehensive ablation experiments to demonstrate the effectiveness of each module. In addition, we visualize the attention regions and tensity to multiple frames to intuitively show the behavior of our model to reduce noise while retaining key information implicitly. 
Concretely, we make the following contributions: (i) We propose a denoising bottleneck fusion model for video multimodal fusion, which reduces redundancy and noise while retaining key information. (ii) We achieve new state-of-the-art performance on three benchmarks spanning two video multimodal fusion tasks. (iii) We provide comprehensive ablation studies and qualitative visualization examples to demonstrate the effectiveness of both bottleneck and MI-Max modules."
SimLM: Pre-training with Representation Bottleneck for Dense Passage Retrieval,2207.02578v2,./img_ACL_2023/2207.02578v2.pdf,Pre-training architecture of SimLM. Replaced tokens (underlined) are randomly sampled from the generator distribution.,"In this paper, we propose SimLM (Similarity matching with Language Model pre-training), a simple yet effective pre-training method for dense passage retrieval. It employs a simple bottleneck architecture that learns to compress the passage information into a dense vector through self-supervised pre-training. We use a replaced language modeling objective, which is inspired by ELECTRA, to improve the sample efficiency and reduce the mismatch of the input distribution between pre-training and fine-tuning. SimLM only requires access to an unlabeled corpus and is more broadly applicable when there are no labeled data or queries. We conduct experiments on several large-scale passage retrieval datasets and show substantial improvements over strong baselines under various settings. Remarkably, SimLM even outperforms multi-vector approaches such as ColBERTv2 which incurs significantly more storage cost. Our code and model checkpoints are available at <https: //github. com/microsoft/unilm/tree/master/simlm>.","Passage retrieval is an important component in applications like ad-hoc information retrieval, open-domain question answering, retrieval-augmented generation and fact verification. Sparse retrieval methods such as BM25 were the dominant approach for several decades, and still play a vital role nowadays. With the emergence of large-scale pre-trained language models (PLM), increasing attention is being paid to neural dense retrieval methods. Dense retrieval methods map both queries and passages into a low-dimensional vector space, where the relevance between the queries and passages are measured by the dot product or cosine similarity between their respective vectors. 
Like other NLP tasks, dense retrieval benefits greatly from a strong general-purpose pre-trained language model. However, general-purpose pre-training does not solve all the problems. As shown in Table, improved pre-training techniques that are verified by benchmarks like GLUE do not result in consistent performance gain for retrieval tasks. Similar observations are also made by. We hypothesize that, to perform robust retrieval, the [CLS] vector used for computing matching scores should encode all the essential information in the passage. The next-sentence prediction (NSP) task in BERT introduces some supervision signals for the [CLS] token, while RoBERTa and ELECTRA do not have such sequence-level tasks. 
In this paper, we propose SimLM to pre-train a representation bottleneck with replaced language modeling objective. SimLM consists of a deep encoder and a shallow decoder connected with a representation bottleneck, which is the [CLS] vector in our implementation. Given a randomly masked text segment, we first employ a generator to sample replaced tokens for masked positions, then use both the deep encoder and shallow decoder to predict the original tokens at all positions. Since the decoder only has limited modeling capacity, it must rely on the representation bottleneck to perform well on this pre-training task. As a result, the encoder will learn to compress important semantic information into the bottleneck, which would help train biencoder-based dense retrievers. Our pre-training objective works with plain texts and does not require any generated pseudo-queries as for GPL. 
Compared to existing pre-training approaches such as Condenser or coCondenser, our method has several advantages. First, it does not have any extra skip connection between the encoder and decoder, thus reducing the bypassing effects and simplifying the architecture design. Second, similar to ELECTRA pre-training, our replaced language modeling objective can back-propagate gradients at all positions and does not have [MASK] tokens in the inputs during pre-training. Such a design increases sample efficiency and decreases the input distribution mismatch between pre-training and fine-tuning. 
To verify the effectiveness of our method, we conduct experiments on several large-scale web search and open-domain QA datasets: MS-MARCO passage ranking, TREC Deep Learning Track datasets, and the Natural Questions (NQ) dataset. Results show substantial gains over other competitive methods using BM25 hard negatives only. When combined with mined hard negatives and cross-encoder based re-ranker distillation, we can achieve new state-of-the-art performance."
Learning Optimal Policy for Simultaneous Machine Translation via Binary Search,2305.12774v3,./img_ACL_2023/2305.12774v3.png,"The translating probability of ground-truth when attending to different numbers of tokens. When translating each target token, the model adopts the wait-k policy for previous tokens.","Simultaneous machine translation (SiMT) starts to output translation while reading the source sentence and needs a precise policy to decide when to output the generated translation. Therefore, the policy determines the number of source tokens read during the translation of each target token. However, it is difficult to learn a precise translation policy to achieve good latency-quality trade-offs, because there is no golden policy corresponding to parallel sentences as explicit supervision. In this paper, we present a new method for constructing the optimal policy online via binary search. By employing explicit supervision, our approach enables the SiMT model to learn the optimal policy, which can guide the model in completing the translation during inference. Experiments on four translation tasks show that our method can exceed strong baselines across all latency scenarios","Simultaneous machine translation (SiMT), which outputs the generated translation before reading the whole source sentence, is applicable to many real-time scenarios, such as live broadcast and real-time subtitles. To achieve the goal of high translation quality and low latency, the SiMT model relies on a policy that determines the number of source tokens to read during the translation of each target token. 
The translation policy plays a pivotal role in determining the performance of SiMT, as an imprecise policy can lead to degraded translation quality or introduce unnecessary delays, resulting in poor translation performance. Therefore, it is crucial to establish an optimal policy that achieves good latency-quality trade-offs. However, the absence of a golden policy between the source and target makes it challenging for the SiMT model to acquire the explicit supervision required for learning the optimal policy. According to, the SiMT model will learn better policy if it is trained with external supervision. Consequently, by constructing the optimal policy between the source and target, we can train the SiMT model, which will then generate translations based on the learned policy during inference. 
However, the existing methods, including fixed policy and adaptive policy, have limitations in learning the optimal policy due to the lack of appropriate explicit supervision. For fixed policy, the model relies on heuristic rules to generate translations. However, these rules may not prompt the SiMT model to output the generated translation immediately, even when there is sufficient source information to translate the current target token. Consequently, the fixed policy often cannot achieve good latency-quality trade-offs because of its rigid rules. For adaptive policy, the model can dynamically determine its policy based on the translation status, leading to improved performance. Nevertheless, precise policy learning without explicit supervision remains challenging. Some methods attempt to construct learning labels for the policy offline by introducing external information. But the constructed labels for policy learning cannot guarantee that they are also optimal for the translation model. 
Under these grounds, our goal is to search for an optimal policy through self-learning during training, eliminating the need for external supervision. Subsequently, this optimal policy can be employed to guide policy decisions during inference. In SiMT, increasing the number of source tokens read improves translation quality but also leads to higher latency. However, as the length of the read-in source sequence grows, the profit of translation quality brought by reading more source tokens will also hit bottlenecks. Therefore, the gain of reading one source token can be evaluated with the ratio of the improvement in translation quality to the corresponding increase in latency. The optimal policy will make sure that every decision of reading or writing will get the greatest gain. In this way, after translating the whole source sequence, the SiMT can get the greatest gain, thereby achieving good latency-quality trade-offs. 
In this paper, we propose a SiMT method based on binary search (BS-SiMT), which leverages binary search to construct the optimal translation policy online and then performs policy learning accordingly. Specifically, BS-SiMT model consists of a translation model and an agent responsible for policy decisions during inference. To construct the optimal policy, the translation model treats potential source positions as search interval and selects the next search interval by evaluating the concavity in binary search. This selection process effectively identifies the interval with the highest gain, thus enabling the construction of an optimal policy that ensures good performance. Subsequently, the constructed policy is used to train the agent, which determines whether the current source information is sufficient to translate the target token during inference. If the current source information is deemed sufficient, the translation model outputs the generated translation; otherwise, it waits for the required source tokens. Experiments on De‚ÜîEn and En‚ÜîVi translation tasks show that our method can exceed strong baselines under all latency."
Training-free Neural Architecture Search for RNNs and Transformers,2306.00288v1,./img_ACL_2023/2306.00288v1.png,"Plots of training-free metrics evaluated on 8,795 RNN architectures in NAS-Bench-NLP, against test loss of the architectures assessed on the Penn Treebank dataset when trained. Loss values are from NAS-Bench-NLP, and Kendall œÑ and Spearman œÅ also shown. Only our Hidden Covariance metric performed on the first and second layer of the RNN showed a substantial correlation between the metric and trained test loss. Some other metrics do have some minor positive correlations.","Neural architecture search (NAS) has allowed for the automatic creation of new and effective neural network architectures, offering an alternative to the laborious process of manually designing complex architectures. However, traditional NAS algorithms are slow and require immense amounts of computing power. Recent research has investigated training-free NAS metrics for image classification architectures, drastically speeding up search algorithms. In this paper, we investigate training-free NAS metrics for recurrent neural network (RNN) and BERT-based transformer architectures, targeted towards language modeling tasks. First, we develop a new training-free metric, named hidden covariance, that predicts the trained performance of an RNN architecture and significantly outperforms existing training-free metrics. We experimentally evaluate the effectiveness of the hidden covariance metric on the NAS-Bench-NLP benchmark. Second, we find that the current search space paradigm for transformer architectures is not optimized for training-free neural architecture search. Instead, a simple qualitative analysis can effectively shrink the search space to the best performing architectures. This conclusion is based on our investigation of existing training-free metrics and new metrics developed from recent transformer pruning literature, evaluated on our own benchmark of trained BERT architectures. Ultimately, our analysis shows that the architecture search space and the training-free metric must be developed together in order to achieve effective results. Our source code is available at <https: //github. com/aaronserianni/training-free-nas>.","Recurrent neural networks (RNNs) and BERT-based transformer models with self-attention have been extraordinarily successful in achieving state-of-the-art results on a wide variety of language modeling-based natural language processing (NLP) tasks, including question answering, sentence classification, tagging, and natural language inference. However, the manual development of new neural network architectures has become increasingly difficult as models are getting larger and more complicated. Neural architecture search (NAS) algorithms aim to procedurally design and evaluate new, efficient, and effective architectures within a predesignated search space. NAS algorithms have been extensively used for developing new convolutional neural network (CNN) architectures for image classification, with many surpassing manually-designed architectures and achieving state-of-the-art results on many classification benchmarks. Some research has been conducted on NAS for RNNs and transformers, particularly with BERT-based architectures, but NAS is not widely used for designing these architectures. 
While NAS algorithms and methods have been successful in developing novel and effective architectures, there are two main problems that current algorithms face. The search space for various architectures is immense, and the amount of time and computational power to run NAS algorithms is prohibitively expensive. Because traditional NAS algorithms require the evaluation of candidate architectures in order to gauge performance, candidate architectures need to be trained fully, each taking days or weeks to complete. Thus, past attempts at NAS have been critiqued for being computationally resource-intensive, consuming immense amounts of electricity, and producing large amounts of carbon emissions. These problems are especially true for transformers and RNNs, as they have more parameters and take longer to train when compared to other architectures. 
Recently, there has been research into training-free NAS metrics and algorithms, which offer significant performance increases over traditional NAS algorithms. These metrics aim to partially predict an architecture's trained accuracy from its initial untrained state, given a subset of inputs. However, prior research has focused on developing training-free NAS metrics for CNNs and Vision Transformers with image classification tasks. In this work, we apply existing training-free metrics and create our own metrics for RNNs and BERT-based transformers with language modeling tasks. Our main contributions are: 
 
 * We develop a new training-free metric for RNN architectures, called ""hidden covariance, "" which significantly outperforms existing metrics on NAS-Bench-NLP. 
 * We develop a NAS benchmark for BERT-based architectures utilizing the FlexiBERT search space and ELECTRA pretraining scheme. 
 * We evaluate existing training-free metrics on our NAS BERT benchmark, and propose a series of new metrics adapted from attention head pruning. 
 * Finally, we discuss current limitations with training-free NAS for transformers due to the structure of transformer search spaces, and propose an alternative paradigm for speeding up NAS algorithms based on scaling laws of transformer hyperparameters."
Generating EDU Extracts for Plan-Guided Summary Re-Ranking,2305.17779v1,./img_ACL_2023/2305.17779v1.pdf,"EDU Plan-Guided Abstraction (PGA). EDU spans form the oracle content plan, while EDU spans form a random distractor plan. A model is trained to generate the reference only when given the oracle plan, not the random one. EDU-level plans afford more fine-grained control than sentence-level as irrelevant content is cut out: ""but the calendar is only allowed to turn 39"".","Two-step approaches, in which summary candidates are generated-then-reranked to return a single summary, can improve ROUGE scores over the standard single-step approach. Yet, standard decoding methods (i.e., beam search, nucleus sampling, and diverse beam search) produce candidates with redundant, and often low quality, content. In this paper, we design a novel method to generate candidates for re-ranking that addresses these issues. We ground each candidate abstract on its own unique content plan and generate distinct plan-guided abstracts using a model's top beam. More concretely, a standard language model (a BART LM) auto-regressively generates elemental discourse unit (EDU) content plans with an extractive copy mechanism. The top K beams from the content plan generator are then used to guide a separate LM, which produces a single abstractive candidate for each distinct plan. We apply an existing re-ranker (BRIO) to abstractive candidates generated from our method, as well as baseline decoding methods. We show large relevance improvements over previously published methods on widely used single document news article corpora, with ROUGE-2 F1 gains of 0.88,2.01, and 0.38 on CNN / Dailymail, NYT, and Xsum, respectively. A human evaluation on CNN / DM validates these results. Similarly, on 1k samples from CNN / DM, we show that prompting GPT-3 to follow EDU plans outperforms sampling-based methods by 1.05 ROUGE-2 F1 points. Code to generate and realize plans is available at <https: //github. com/griff4692/edu-sum>.","Generating diverse abstracts and then re-ranking can lead to large performance gains (in ROUGE) over the standard approach of generating a single summary. Typically, diversity is controlled for at the token-level by modifying beam search to introduce sampling (top-K, nucleus) or directly penalize repetition. 
Yet, there is a tradeoff, as these methods tend to achieve diversity at the expense of quality. To avoid content de-generation while still achieving diversity, diversity can be introduced during a planning stage, as in, who generate entity chain plans with diverse beam search before realizing a summary with regular beam search. 
In this paper, we also explore achieving diverse summaries through diverse plans, yet we focus on grounded extractive plans, which promote diversity by encouraging a model to focus on specific, unique parts of the source text. We define a content plan as a set of non-overlapping text spans from the source document. Specifically, we choose elemental discourse units (EDUs) as the appropriate granularity for content planning. EDUs represent sub-sentential independent clauses and allow for more fine-grained control than sentence-level extraction. EDUs are more self-contained and less fragmented than other potential sub-sentence content units, e.g.,entities or noun phrases. Extractive EDUs are contiguous and are atomic, whereas entities do not cover all content and can appear in multiple contexts. 
At a high-level, we employ two encoder-decoder models. Given a document, the first model generates K unique content plans with beam search. Then, each content plan is used as a guide to a second model, which realizes an abstract given the plan and the document. Specifically, a BART-based hierarchical encoder-decoder learns to generate extracts from left-to-right by copying EDUs until a special end of extract token is copied. These extractive plans are used to decorate the input document and serve as a guide for the Plan-Guided Abstractor (PGA). The top K beams are returned from the content planner, while only the top beam is returned for plan realization to avoid de-generation. An example of the training procedure from the CNN/DailyMail news dataset is shown in Figure. 
We compare our PGA candidate generation method to other decoding baselines (beam search, diverse beam, search, and nucleus sampling) at both the candidate level (across beams), as well as after applying a re-ranker (BRIO) to obtain a single, re-ranked summary. We also benchmark the performance of re-ranked summaries from our PGA method against publicly reported results from other summary re-ranking papers. We note consistently higher ROUGE and BERTScores against both our internal baselines and public benchmarks, which we link to improved content selection across candidate beams. We also conduct a human evaluation and find that annotators assess top ranked summaries from PGA candidates as containing more relevant content than candidates produced by baseline decoding methods. By separately optimizing the plan and plan-guided abstracts, we can easily combine generated plans with a Large Language Model (LLM). In, we prompt GPT-3.5 to generate diverse, focused summaries and apply a re-ranker. We compare with a series of un-focused prompts and find that ROUGE scores improve across the board. More generally, prompting with diverse plans, and then re-ranking, is a convenient alternative to RLHF alignment when using closed models. 
Our primary contributions are: (1). We propose a novel two-stage model for generating high-quality, diverse candidate summaries for downstream re-ranking. Our plan generation approach adapts a pre-trained LM to perform span-level copying to produce EDU-level plans. (2). Our plan-guided abstraction model leads to large improvements in top-ranked summaries vis-a-vis previously published results (0.88,2.01, and 0.38 ROUGE-2 F1 percentage point gains on CNN/DM, NYT, and Xsum, respectively), and outperforms on summary relevance according to human evaluation. (3) We perform extensive analysis of candidate generation methods, according to the diversity of derived content plans and factors, such as source length. (4) We show that we can improve the reference-based performance of few-shot LLMs by prompting for diverse summaries based on extractive EDU plans."
Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters,2212.10001v2,./img_ACL_2023/2212.10001v2.pdf,"Results of standard prompting, Chain-of-Thought (CoT) prompting, and our ablation setting with invalid reasoning (). We show one demonstration example and one inference example for arithmetic reasoning, where the rationale is in color (green: valid, yellow: invalid). We find that valid reasoning for the demonstrations matters only a small portion to the performance of CoT‚Äîby providing rationales with invalid reasoning, LLMs can achieve over 80-90%of the performance of CoT under various metrics while performing logically sound and pertinent reasoning.","Chain-of-Thought (CoT) prompting can dramatically improve the multi-step reasoning abilities of large language models (LLMs). CoT explicitly encourages the LLM to generate intermediate rationales for solving a problem, by providing a series of reasoning steps in the demonstrations. Despite its success, there is still little understanding of what makes CoT prompting effective and which aspects of the demonstrated reasoning steps contribute to its performance. In this paper, we show that CoT reasoning is possible even with invalid demonstrations‚Äîprompting with invalid reasoning steps can achieve over 80-90%of the performance obtained using CoT under various metrics, while still generating coherent lines of reasoning during inference. Further experiments show that other aspects of the rationales, such as being relevant to the query and correctly ordering the reasoning steps, are much more important for effective CoT reasoning. Overall, these findings both deepen our understanding of CoT prompting, and open up new questions regarding LLMs' capability to learn to reason in context.","Large language models (LLMs) can perform new tasks during inference when prompted with a few demonstrations. Chain-of-Thought (CoT) prompting can (Figure) improve the ability of sufficiently large LLMs to do complex and multi-step reasoning. In addition to (query, answer) example-pair demonstrations, CoT prompting includes a rationale (colored part in Figure) for each example, i.e., a series of reasoning steps towards the answer, which encourages the LLM to explicitly generate its intermediate reasoning process before predicting the final answer. Despite its successes, there is little understanding of what makes CoT prompting effective and which aspects of the demonstrated reasoning steps contribute to its performance. Recent findings also reveal that in-context learning could be very different from fine-tuning/training; for example, and show that providing random labels or misleading instructions in context only marginally harms model performance for certain tasks. Inspired by this work, we take a closer look at CoT prompting to study how and why it works. 
We design a series of ablation experiments where we deliberately change different aspects of the demonstrated rationales and measure how the model performance varies accordingly (, ). On two representative multi-step reasoning tasks‚Äîarithmetic reasoning and multi-hop factual question answering (QA), we find that the validity of reasoning matters only a small portion to the performance‚Äîby providing rationales with completely invalid reasoning steps, the LLM can still achieve over 80-90%of the performance of CoT under various metrics while generating coherent lines of reasoning towards the answer (). Through further examinations, we identify and formulate other aspects of a CoT rationale (), and find that being relevant to the query and correctly ordering the reasoning steps are the key for the effectiveness of CoT prompting. 
Overall, our findings suggest that what LLMs learn about how to reason under CoT prompting could be limited. Rather, they have already gained a lot of such ""reasoning abilities"" from pretraining, and the demonstrations may mainly specify an output space/format that regularizes the model generation to look step-by-step while being in order and relevant to the query. Our work suggests a new way of interpreting the evaluation scores in view of the prior knowledge LLMs possess, and leads to reflections on benchmarking few-shot reasoning which we discuss in."
Faithful Low-Resource Data-to-Text Generation through Cycle Training,2305.14793v2,./img_ACL_2023/2305.14793v2.png,"Cycle Training of the Data-to-Text model and Text-to-Data model. For each cycle, the upper-level models are frozen to generate the intermediate text for the training of the lower-level models, that attempt to reconstruct the initial inputs (d, t denote initial inputs of the upper-level models; tÃÇ, dÃÇ denote the upper-level models 'generations that serve as inputs to the lower-level models; d', t' denote the generations of the lower-level models).","Methods to generate text from structured data have advanced significantly in recent years, primarily due to fine-tuning of pre-trained language models on large datasets. However, such models can fail to produce output faithful to the input data, particularly on out-of-domain data. Sufficient annotated data is often not available for specific domains, leading us to seek an unsupervised approach to improve the faithfulness of output text. Since the problem is fundamentally one of consistency between the representations of the structured data and text, we evaluate the effectiveness of cycle training in this work. Cycle training uses two models which are inverses of each other: one that generates text from structured data, and one which generates the structured data from natural language text. We show that cycle training, when initialized with a small amount of supervised data (100 samples in our case), achieves nearly the same performance as fully supervised approaches for the data-to-text generation task on the WebNLG, E2E, WTQ, and WSQL datasets. We perform extensive empirical analysis with automated evaluation metrics and a newly designed human evaluation schema to reveal different cycle training strategies' effectiveness of reducing various types of generation errors. Our code is publicly available at <https: //github. com/Edillower/CycleNLG>.","A wealth of information exists in the form of structured knowledge, such as movie information databases or product catalogs, which we may want to verbalize for a variety of purposes, such as comparing two items, or presenting detailed descriptions in a natural language form suitable for conversational assistants. Recent work has tackled this data-to-text generation task using freely available public datasets, most notably WebNLG and ToTTo. However, there remain two major challenges. First, the volume of training data required for good performance, especially if it is not in a domain represented by one of the existing corpora, is very large. Second, multiple recent papers, inter alia, point out that neural natural language generation (NLG) from structured data tends to produce multiple kinds of errors which limit the utility of these models in customer-facing applications. Hallucinations occur when NLG models inject nonsensical words or information not related to the input structured data, into the generated output text. For instance, an NLG model may claim a shirt's color is ""three"". Simple factual errors occur when an NLG model produces coherent but factually wrong output. 
There are two threads of research to consider as we attempt to tackle these problems in the data-to-text setting. The first is designing models that directly produce output more faithful to the input data. The second is designing models to detect and correct factual errors or hallucinations after the output text is generated. In both cases, prior research has generally assumed sufficient pairs of structured data and text as training data to achieve human-level performance on the task. While fact verification models can achieve very high performance, they generally do so when trained on large corpora of 100,000 examples or more. Since performance appears to degrade when evaluated on out-of-domain data, this presents a significant limitation of fact-verification models. Similarly, corpora like WebNLG contain about 20,000 examples; this is probably too small to achieve human performance even under full supervision but is large enough to make it prohibitive to generate domain-specific corpora of the size of WebNLG. 
In spite of the above mentioned limitations, very few of the models developed for data-to-text and table-to-text tasks take advantage of the fact that the task of faithful text generation is fundamentally one of consistency between the data and the corresponding text. In fact, despite the WebNLG 2020 challenge being explicitly bi-directional, only three models competing in the challenge leveraged this idea of consistency. 
To overcome the aforementioned limitations related to the lack of training data (especially out-of-domain data) and the consistency between structured data and text, we adopt a Cycle Training approach. We assume unpaired data ùíü, in the form of subject-predicate-object triples, and text ùíØ, which may or may not be from the same domain. We also make use of a small (100 samples) set of paired data and text, ùíü_pr, ùíØ_pr. Cycle training makes use of two iteratively trained models, a forward model ‚Ñ±: ùíü‚ÜíùíØ and a reverse model ‚Ñõ: ùíØ‚Üíùíü. Training is unsupervised, namely, we freeze one model and use it to transform one set of inputs, and train the other by using it to predict the original input from the output of the first model. Concretely, in one cycle, we freeze ‚Ñ±, and train ‚Ñõ by reconstructing the input ùíü as ‚Ñõ (‚Ñ± (ùíü) ). After one training epoch, we reverse the roles of the two models. Remarkably, even though the models are initially quite poor, this can converge to models with near-supervised performance, as we will show. Moreover, we show that this process ensures the faithfulness of the output text with respect to the input data, and vice versa, even with very little or no paired data. 
We note that a previous data-to-text system, CycleGT, has used cycle training. We will discuss in detail the differences between CycleGT and our proposed approach in Section. Moreover, we examine in detail the conditions under which cycle training works well, with an emphasis on domains and the nature of the training text and structured data. We find that unsupervised cycle training outperforms low-resource fine-tuned models and can achieve near fully-supervised performance when initialized and post-tuned with a small amount of annotated data. We detail the results and findings in Section. Thus, to build on past research in self-consistent data-to-text generation, we make these novel contributions: (i) We successfully apply cycle training to both the data-to-text and text-to-data models using only a pre-trained language model, T5, without recourse to graph methods or other auxiliary models. (ii) We show that cycle training achieves nearly the same performance as supervised models for some domains. (iii) We present an extensive empirical analysis on the conditions under which cycle training works well, and on the data-to-text faithfulness with respect to different types of generation errors. (iv) We design a novel counting and ranking based annotation schema to more comprehensively evaluate the faithfulness of the generated text from the standpoints of correctness, faithfulness, data coverage, and fluency. Our schema improves upon the rating-based schema used for the WebNLG 2020 Challenge, in terms of objectiveness, consistency, precision and ease of evaluation."
Hierarchical Verbalizer for Few-Shot Hierarchical Text Classification,2305.16885v1,./img_ACL_2023/2305.16885v1.pdf,"Illustration of methods for HTC problems. (a) Previous methods typically regard HTC as a downstream classification fine-tuning task. (b) HPT formulates HTC as a multi-label MLM problem following the prompt tuning paradigm. (c) Our HierVerb leverages hierarchy-aware verbalizers, which are more effective for few-shot tuning.","Due to the complex label hierarchy and intensive labeling cost in practice, the hierarchical text classification (HTC) suffers a poor performance especially when low-resource or few-shot settings are considered. Recently, there is a growing trend of applying prompts on pre-trained language models (PLMs), which has exhibited effectiveness in the few-shot flat text classification tasks. However, limited work has studied the paradigm of prompt-based learning in the HTC problem when the training data is extremely scarce. In this work, we define a path-based few-shot setting and establish a strict path-based evaluation metric to further explore few-shot HTC tasks. To address the issue, we propose the hierarchical verbalizer (""HierVerb""), a multi-verbalizer framework treating HTC as a singleor multi-label classification problem at multiple layers and learning vectors as verbalizers constrained by hierarchical structure and hierarchical contrastive learning. In this manner, HierVerb fuses label hierarchy knowledge into verbalizers and remarkably outperforms those who inject hierarchy through graph encoders, maximizing the benefits of PLMs. Extensive experiments on three popular HTC datasets under the few-shot settings demonstrate that prompt with HierVerb significantly boosts the HTC performance, meanwhile indicating an elegant way to bridge the gap between the large pre-trained model and downstream hierarchical classification tasks.","Hierarchical text classification (HTC) is a long-standing research problem due to the wide range of real applications. However, prior works could still suffer poor performance in practice due to the nature of its sophisticated label hierarchy as well as the requirement of large-scale data annotation before training the model. Therefore, solving the HTC under the low-resource or few-shot setting becomes an urgent research topic. 
Existing state-of-the-art HTC models focus on inserting label hierarchy features through graph encoders and then fuse the features into the input layer or output layer of a text encoder such as Bidirectional LSTM or pre-trained language models (PLMs), as shown in Figure (a). And there is a trend of taking advantage of PLMs as the backbone of the text encoder through a fine-tuning paradigm. Despite the success of PLMs in extensive NLP-related tasks, recently, a series of studies suggest that it's helpful to elicit the knowledge contained in PLMs and point out the fine-tuning paradigm is suboptimal in few-shot settings due to distinct training strategies between the pre-training and fine-tuning stages. Inspired by ""in-context learning"" proposed by GPT-3, lots of prompt-based methods were proposed to bridge the gap between pre-training and downstream tasks via stimulating pre-trained model knowledge with a few hard or soft prompts. In prompt-based tuning, the input is usually wrapped through a natural language template and the tasks are converted as masked language modeling (MLM) for PLM. For example, in the sentiment classification task, the original input x will be wrapped as ""x. It was [MASK] "". The objective is to utilize MLM to predict the word that fills the [MASK], and subsequently employ a verbalizer to map the predicted word to the final classification (e.g.,""positive"" -> label ""Positive""). 
Although remarkable performances have been achieved via prompt tuning on the flat text classification where labels have no hierarchy, its effects on HTC problems remain unclear, as discussed in HPT. As shown in Figure (b), HPT proposed a hierarchy-aware prompt tuning method that incorporates the label hierarchy knowledge into soft prompts through graph representation and achieves the new state-of-the-art results on several HTC popular datasets. However, even though the low-resource setting experiment was considered in HPT, the commonly used K-shot setting was not investigated. The limitation lies in the absence of a uniform definition of the K-shot setting in HTC. Besides, the way to utilize PLMs in few-shot settings through soft prompts and fuse hierarchy by graph encoder into the PLMs harms tapping the full potential of PLMs. Hence, it is crucial to exploit a new method to elicit knowledge from PLMs in a hierarchy-aware manner for few-shot learning. 
Inspired by the prior works on verbalizer design between model outputs and labels, as shown in Figure (a) and (b), which makes promising improvements over prompt-based tuning, it is natural to raise this question: is there any verbalizer design method specific to the HTC problems? The most current works can be mainly divided into three kinds of verbalizers: manual verbalizers, search-based verbalizers, and soft verbalizers. However, the main difference between previous works on verbalizers is the way of embedding the semantic space and they are all based on a strong assumption that there is no hierarchical dependency between downstream task labels, which raises a gap between rich flat prior knowledge in PLM and downstream task hierarchies. Thus these verbalizers are not suitable for hierarchical classification tasks, lacking awareness of hierarchy in their architectural design. To address these issues, we introduce a hierarchical-aware verbalizer (HierVerb) combined with the prompt tuning method to fully exploit the hierarchical knowledge within labels. The major contributions of this paper can be summarized as follows: 
 
 * To our best knowledge, we are the first to define the path-based few-shot setting on hierarchical text classification tasks and propose a path-based evaluation metric to further explore the consistency problem in HTC tasks. 
 * We propose HierVerb for few-shot HTC, which integrates the hierarchical information into the verbalizers through the flat hierarchical contrastive learning and hierarchy-aware constraint chain to better leverage the pre-trained language model for few-shot learning. 
 * Experimental results demonstrate that HierVerb significantly outperforms the current state-of-the-art HTC methods on three popular benchmarks (WOS, DBPedia, and RCV1-V2) under extreme few-shot settings (i.e., K <=8), validating the effectiveness of its design."
Summary-Oriented Vision Modeling for Multimodal Abstractive Summarization,2212.07672v2,./img_ACL_2023/2212.07672v2.jpg,"An example of our MM-Sum dataset. Inputs: an article and image sequence pair; Output: summary. As we can see, the image sequence also concisely paraphrases the summary. The red content indicates its associated object is useless to the summary while the green counterparts represent important information.","Multimodal abstractive summarization (MAS) aims to produce a concise summary given the multimodal data (text and vision). Existing studies mainly focus on how to effectively use the visual features from the perspective of an article, having achieved impressive success on the high-resource English dataset. However, less attention has been paid to the visual features from the perspective of the summary, which may limit the model performance, especially in the lowand zero-resource scenarios. In this paper, we propose to improve the summary quality through summary-oriented visual features. To this end, we devise two auxiliary tasks including vision to summary task and masked image modeling task. Together with the main summarization task, we optimize the MAS model via the training objectives of all these tasks. By these means, the MAS model can be enhanced by capturing the summary-oriented visual features, thereby yielding more accurate summaries. Experiments on 44 languages, covering mid-high-, low-, and zero-resource scenarios, verify the effectiveness and superiority of the proposed approach, which achieves state-of-the-art performance under all scenarios. Additionally, we will contribute a large-scale multilingual multimodal abstractive summarization (MM-Sum) dataset.","Recently, many studies have been carried out to effectively inject the visual features into MAS models. For instance, and explore the hierarchy between the textual article and visual features, and integrate them into the MAS model. design a multistage fusion network to model the fine-grained interactions between the two modalities. And study multiple multimodal fusion methods to infuse the visual features into generative pre-trained language models, e.g., BART. Despite their success on the high-resource English dataset, they only model visual features from the perspective of an article and neglect the relevance of visual features to the summary, which restricts their potential performance especially on the training dataset with limited scale. For example, though the object ""black clothes"" in the first image of is associated with the article content (red part), the object contributes little to the summary. Thus, the MAS model should focus on summary-oriented visual features. However, the visual features are generally implicitly learned via the MAS objective, which cannot help the model learn to explicitly discard the needless visual information. 
To address this issue, in this paper, we propose a Summary-Oriented Vision enhanced MAS (SOV-MAS) training framework to generate more accurate summaries through explicitly improving the relevance of visual features to the summary. To this end, we design two summary-oriented vision modeling tasks, namely vision to summary task, and masked image modeling task. Specifically, as shown in, (1) the vision to summary task is to produce the concise summary by only taking the image sequence; (2) the masked image modeling task aims to predict the semantic class distribution of the regions in one fully masked image given the summary and the remaining images. Together with the main multimodal summarization task, the MAS model is optimized through the joint objectives of all these tasks. In this way, the model is enhanced to explicitly exploit the summary-oriented visual features, thus leading to more accurate summaries. 
To validate the SOV-MAS framework on various languages and diverse settings, we construct the first large-scale Multilingual Multimodal Summarization dataset (MM-Sum) based on XL-Sum, a multilingual summarization dataset. The MM-Sum covers 44 languages with mid-high-, lowand zero-resource scenarios. Experiments on these settings show that our model significantly outperforms related methods in terms of ROUGE scores, especially under the lowand zero-resource settings, demonstrating its effectiveness. Besides, we extend our approach to two previous best MAS models (i.e., VG-BART and VG-T5). Human evaluation and the results on How2 benchmark further suggest the superiority and generalizability of our approach. In summary, our main contributions are: 
 
 * To the best of our knowledge, we are the first that contributes a large-scale multilingual multimodal summarization dataset (44 languages, 1.1M article-summary pairs with 3.5M images). 
 * We propose two general summary-oriented vision modeling tasks, which substantially boost the summary quality and are flexible and easy to be extended to existing MAS models. 
 * Experiments on MM-Sum show that our model builds new state-of-the-art performance in all scenarios, especially on the low and zero resource where the fewer the data are (mid-high‚Üílow‚Üízero), the greater the improvement we gain. Besides, results on the How2 dataset show the generalizability of our approach. 
 * When jointly training the MAS model on multiple languages, we find that our model learns transferable visual features among languages, where the vision serves as an anchor in the zero-resource languages."
CATS: A Pragmatic Chinese Answer-to-Sequence Dataset with Large Scale and High Quality,2306.11477v1,./img_ACL_2023/2306.11477v1.pdf,An example for a practical TableQA system. The red dotted lines denote the input data for answer-to-sequence.,"There are three problems existing in the popular data-to-text datasets. First, the large-scale datasets either contain noise or lack real application scenarios. Second, the datasets close to real applications are relatively small in size. Last, current datasets bias in the English language while leaving other languages underexplored. To alleviate these limitations, in this paper, we present Cats, a pragmatic Chinese answer-to-sequence dataset with large scale and high quality. The dataset aims to generate textual descriptions for the answer in the practical TableQA system. Further, to bridge the structural gap between the input SQL and table and establish better semantic alignments, we propose a Unified Graph Transformation approach to establish a joint encoding space for the two hybrid knowledge resources and convert this task to a graph-to-text problem. The experiment results demonstrate the effectiveness of our proposed method. Further analysis on Cats attests to both the high quality and challenges of the dataset.","Data-to-text (D2T) generation aims to generate a natural language description conditioned on structured or semi-structured data, such as graphs or tables. It helps people get the key points of the input data and makes the stored information accessible to a broader range of end-users. A large number of datasets have been proposed as the testbed for neural D2T models and are driving the domain. 
However, as shown in Table, we note three problems existing in the popular datasets. First, the large-scale datasets either contain noises (e.g., WEATHERGOV) or lack practical application scenarios, e.g., ToTTo. The shortcoming leads to a separation between research and application. Second, the datasets close to practical scenarios are relatively small in size. For example, ROTOWIRE only contains 4.9K training examples, and CoSQL is consist of 7.8K training pairs. The small training size can easily lead to overfitting and is not conducive to training a reliable neural network model. Lastly, most of the existing datasets are built for English, which leads to advanced work on D2T generation primarily focusing on English and leaving other languages underexplored. These limitations hinder the progress of D2T generation. We therefore need to investigate possible remedies. 
The crucial step to improving the above limitations is digging out a data-to-text task with a practical scenario. Recently, CoSQL has proposed a practical controlled D2T task: answer-to-sequence. As shown in Figure, the task takes a SQL query generated by a semantic parsing module, i.e., text-to-SQL, and its corresponding execution result (in the form of a table) as the model input and aims to produce a natural language description as the response to users in a real-world TableQA system. The SQL gives explicit signals for models on what to generate. The generated description could provide a concise and easy-to-understand summary of the result table and help users verify whether the queried result is consistent with the original question. Moreover, the task also contributes to a more user-friendly human-computer interaction. Nevertheless, CoSQL contains only 7.8K answer-to-sequence examples for training. Additionally, it is a dataset with SQL-grounded dialogue state tracking as the core, and the generation annotations are very rough. The scale and quality of CoSQL limit further exploring the answer-to-sequence task. 
In this paper, to bridge the gap between research and application of data-to-text datasets and enrich their language diversity, we comply with the CoSQL setting and present Cats, a large-scale and high-quality Chinese answer-to-sequence dataset. We manually annotate all collected SQL-table pairs to obtain their descriptions. We make two efforts to improve the quality and scale of the collected SQL-Table pairs and guarantee they are close to practical scenarios. First, we annotate the SQL-table pairs from DuSQL, a large-scale Chinese Text-to-SQL dataset with a SQL query distribution close to real applications. Data collected in this way are named Cats-D. Second, we adopt an automatic data construction pipeline to collect a large number of SQL-table pairs for annotation. The basic idea is automatically crawling a mount of tables from the Internet to build multi-table databases and then automatically generating SQL queries based on the SQL grammar and constrained by the given database. Data collected with this method are referred to as Cats-S. Compared to Cats-D, Cats-Sexpands the data scale while reducing the share of easy SQLs to make the dataset more challenging. In total, Catsis made up of both Cats-Dand Cats-S, and contains 43,369 answer-to-sequence examples, which is an order of magnitude larger than CoSQL. 
The input SQL and table in answer-to-sequence are heterogeneous, and there is a structural gap between them. To bridge the gap and establish better semantic alignments, we propose a Unified Graph Transformation approach (Ugt), which first converts the two sources to two undirected graphs, then builds the connection between the nodes in different graphs to obtain a unified graph. In this way, we convert this task to a graph-to-text problem. Previous graph-to-text work transforms the input graph into a new token graph to apply pretrained language models, such as T5. We consider that this transformation breaks the original input graph structure and may bring in extra noises into graph encoding. Hence, we further introduce a Node Segment Embedding (Nse) to preserve original structure information. 
Our contributions are three-fold as follows: 
 
 * We present a large-scale and high-quality Chinese answer-to-sequence dataset (Cats), which narrows the gap between research and application of data-to-text generation datasets and enriches the language diversity. 
 * We propose Ugt and Nse to better model the input of two heterogeneous structured input data sources. 
 * Experiments and analysis on Catsattest to both the high quality and challenges of the dataset. The results also demonstrate the effectiveness of our proposed method."
A Critical Evaluation of Evaluations for Long-form Question Answering,2305.18201v1,./img_ACL_2023/2305.18201v1.pdf,Answer length distribution in the comparison of model-generated and human-written answers (H/M) in our expert-annotated dataset. History is the hardest domain for models and also has the largest discrepancy between model and human answer length. There are 75 questions and 75 human-written and model-generated answers.,"Long-form question answering (LFQA) enables answering a wide range of questions, but its flexibility poses enormous challenges for evaluation. We perform the first targeted study of the evaluation of long-form answers, covering both human and automatic evaluation practices. We hire domain experts in seven areas to provide preference judgments over pairs of answers, along with free-form justifications for their choices. We present a careful analysis of experts' evaluation, which focuses on new aspects such as the comprehensiveness of the answer. Next, we examine automatic text generation metrics, finding that no existing metrics are predictive of human preference judgments. However, some metrics correlate with fine-grained aspects of answers (e.g., coherence). We encourage future work to move away from a single ""overall score"" of the answer and adopt a multi-faceted evaluation, targeting aspects such as factuality and completeness. We publicly release all of our annotations and code to spur future work into LFQA evaluation.","Long-form question answering, an emerging research area within QA, requires systems to generate long and complex answers to questions by leveraging large language models and evidence document retrievers. While remarkable strides have been made in LFQA model development, the current state of LFQA evaluation is dire: most prior papers use a combination of crowdsourced human annotations and simple string-matching metrics (e.g., ROUGE). We present the first study of the evaluation of long-form answers, exploring both human and automatic evaluation protocols to better understand how we should evaluate LFQA moving forward. 
Human evaluation: In most prior human LFQA evaluations, crowd annotators are given a question, two candidate answers, and (optionally) evidence documents, and they are asked to identify the better answer. However, crowdworkers do not necessarily have the expertise or background knowledge to reliably judge properties such as factuality. Thus, we hire domain experts in seven different fields (e.g., biology, economics) to perform the same answer preference task and additionally provide detailed justifications as to why they chose a particular answer. Analyzing their justifications reveals that experts consider properties such as completeness and factuality to be more decisive than surface-level aspects (e.g., conciseness and level of detail) on which crowdworkers tend to fixate. Additionally, even experts often disagree with each other about which answer is better; this disagreement stems from valuing fine-grained answer properties differently. 
Automatic evaluation: As human evaluation is slow and expensive, developing a reliable automatic LFQA evaluation metric is crucial for speeding up model development. While ROUGE has been shown to be misleading for LFQA, do any other existing text generation metrics correlate to human judgments of answer quality? Can we train a metric to mimic human preference judgments? To answer these questions, we curate a suite of 12 automatic metrics and measure how they correlate to human judgments of both ""overall quality"" and two fine-grained aspects (coherence and faithfulness). None of these metrics reliably matches human judgments of overall answer quality. However, automatic metrics such as QAFactEval and RankGen show potential at modeling fine-grained aspects of LFQA answers, which can spur research on a new generation of automatic LFQA metrics. 
Overall, we provide the first thorough study of LFQA evaluation and shed light on the components of good long-form answers. As part of our exploration, we collected and will release a small-scale dataset of expert evaluation of long-form answers (260 ratings and justifications over 140 answer pairs). We conclude by providing recommendations for the future of human and automatic LFQA evaluation, encouraging the community to hire expert evaluators and move from poorly-defined judgments of ""overall preference"" to a multi-faceted evaluation modeling attributes such as answer completeness, factuality, and ease of understanding."
HyPe: Better Pre-trained Language Model Fine-tuning with Hidden Representation Perturbation,2212.08853v2,./img_ACL_2023/2212.08853v2.PNG,The overview of proposed HyPe fine-tuning technique. The random noise is added to the hidden representations fed into each Transformers layer in the forward computation of PLMs.,"Language models with the Transformers structure have shown great performance in natural language processing. However, there still poses problems when fine-tuning pre-trained language models on downstream tasks, such as over-fitting or representation collapse. In this work, we propose HyPe, a simple yet effective fine-tuning technique to alleviate such problems by perturbing hidden representations of Transformers layers. Unlike previous works that only add noise to inputs or parameters, we argue that the hidden representations of Transformers layers convey more diverse and meaningful language information. Therefore, making the Transformers layers more robust to hidden representation perturbations can further benefit the fine-tuning of PLMs en bloc. We conduct extensive experiments and analyses on GLUE and other natural language inference datasets. Results demonstrate that HyPe outperforms vanilla fine-tuning and enhances generalization of hidden representations from different layers. In addition, HyPe acquires negligible computational overheads, and is better than and compatible with previous state-of-the-art fine-tuning techniques. Codes are released at <https: //github. com/Yuanhy1997/HyPe>.","Pretrain-then-finetune has become the mainstream paradigm in recent natural language processing (NLP) practices, and there emerges various pre-trained language models (PLMs) such as BERT, RoBERTa, and XLNet. Vanilla PLM fine-tuning with common strategies (e.g., dropout and AdamW) can empower PLMs with excellent downstream performance. However, vanilla fine-tuned PLMs acquire performances with large variances on the downstream tasks. Such unstable performances may results from over-fitting or representation collapse. These problems can be aggravated in low-resource scenarios. 
In recent literature, effective fine-tuning techniques have been proposed to improve the performance and generalization (transferability) of fine-tuned PLMs. Besides other explicit regularization, adding noise is a widely-used strategy to smoothen the optimization landscape and mitigate over-fitting. For example, some works apply the perturbation to pre-trained parameter weights (e.g., NoisyTune), input embedding features (e.g., R3F) or gradients (e.g., ChildTuning) during the fine-tuning process. 
Injecting noise to input features is a conventional technique for generalization and can be seen as implicit parameter regularization. Common PLMs are stacked basic neural network layers (i.e., Transformer layers), and previous research points out that different Transformers layers of PLMs resolve different language information which is encoded in hidden representations. We turn to inject noise between layers to enhance the hidden semantic representations for better generalization on Transformers layer level. 
Based on the above findings, we propose to improve fine-tuning by perturbing the hidden representations. As shown in Figure, we propose a simple yet effective fine-tuning technique named HyPe (Hi (y) dden representation Perturbation) that adds random noise to the hidden representations between layers (i.e., the inputs of each Transformers layer) to alleviate the performance of fine-tuned layers from degrading. To be concrete, we introduce no inductive biases to the distributions of noise in HyPe and focus on the pivotal influences of noise per se. Although noise can be compatible with auxiliary constrains or include informative priors, they may lead to non-negligible computational overheads. We simply use the uniform and normal distributions as two variants of noise distributions and denote them as HyPe-U and HyPe-N, respectively. The computational overheads are marginal in HyPe. HyPe can also be regarded as a decoupling analysis of the above methods. 
We conduct extensive experiments on GLUE benchmark and HyPe improves vanilla fine-tuning up to 1.60 on BERT in terms of average scores of the relatively small datasets MRPC, RTE, CoLA, and STS-B, surpasses previous state-of-the-art techniques (i.e.,R-Drop) by 0.15, and improves performance in low-resource scenarios. Further analyses demonstrate that HyPe is also compatible with different scales of PLM (Section) and other fine-tuning techniques (Section), increases the robustness towards adversarial attacks (Section), and improves generalization across tasks and domains on different layers (Section). 
To summarize our work, the main contributions are listed as follows: 
 
 * We propose HyPe, a simple yet effective fine-tuning technique requiring little computational overhead to improve the performance and transferability of fine-tuning PLMs. 
 * Extensive experimental results show that 1) HyPe improves fine-tuning in the aspect of task performance and generalization and is complementary to PLM scaling; 2) HyPe surpasses and is compatible with current state-of-the-art fine-tuning techniques."
Word sense extension,2306.05609v1,./img_ACL_2023/2306.05609v1.pdf,"Illustration of the problem of word sense extension. Given a novel context, a speaker chooses an existing word in the lexicon to convey a novel intended meaning that has not appeared in the semantics of that word. The speaker determines the appropriateness of a chosen word (indicated by line width of the colored curves) based on semantic relatedness between the novel intended meaning and existing word meanings.","Humans often make creative use of words to express novel senses. A long-standing effort in natural language processing has been focusing on word sense disambiguation (WSD), but little has been explored about how the sense inventory of a word may be extended toward novel meanings. We present a paradigm of word sense extension (WSE) that enables words to spawn new senses toward novel context. We develop a framework that simulates novel word sense extension by first partitioning a polysemous word type into two pseudo-tokens that mark its different senses, and then inferring whether the meaning of a pseudo-token can be extended to convey the sense denoted by the token partitioned from the same word type. Our framework combines cognitive models of chaining with a learning scheme that transforms a language model embedding space to support various types of word sense extension. We evaluate our framework against several competitive baselines and show that it is superior in predicting plausible novel senses for over 7,500 English words. Furthermore, we show that our WSE framework improves performance over a range of transformer-based WSD models in predicting rare word senses with few or zero mentions in the training data.","Humans make creative reuse of words to express novel senses. For example, the English verb arrive extended from its original sense ""to come to locations (e.g., to arrive at the gate) "" toward new senses such as ""to come to an event (e.g., to arrive at a concert) "" and ""to achieve a goal or cognitive state (e.g., to arrive at a conclusion) "" (see Figure). The extension of word meaning toward new context may draw on different cognitive processes such as metonymy and metaphor, and here we develop a general framework that infers how words extend to plausible new senses.
A long-standing effort in natural language processing (NLP) is to build systems that support automatic word sense disambiguation (WSD) from linguistic context. This line of work typically takes a discriminative approach toward word meaning and has developed models relying on both traditional machine learning and modern neural language models. However, existing WSD models often struggle with recognizing rare word senses with few or no mentions in training. Here we show that by modelling the generative extensional processes of word meaning, WSD models can become better at recognizing infrequent word senses in natural context and without relying on external lexical resources. 
Work in computational and cognitive linguistics shows that word senses do not extend arbitrarily. Lexical semanticists have suggested that a number of cognitive devices may be applied to generate creative word usages, such as logical metonymy and metaphor. Cognitive linguists have also suggested that systematic mappings between conceptual domains underlie the metaphorization of word meaning. However, the reliance on hand-crafted rules of semantic productivity makes it difficult to implement systems that support flexible and scalable extension to new word senses. 
We present a paradigm that considers the problem of word sense extension (WSE) illustrated in Figure. Given a novel context and an intended meaning, a speaker wishes to choose an existing word in the lexicon to express that meaning which the word has never been used to convey. To operationalize a speaker model without prior knowledge about pairings between the novel meaning and existing word forms, we replace each candidate word type with a pair of ""pseudo-tokens"" that signify one of its existing senses (called the target sense) the other senses (called the source senses) respectively, a method related to previous work in polysemy induction. We then infer whether a partitioned pseudo-token denoting the source sense may be extended to express the target sense denoted by its sibling token partitioned from the same word type. We propose a family of cognitively-inspired probabilistic models for this inference problem. We show that our WSE models can reliably predict plausible novel senses on a large usage-based dataset with approximately 34,000 senses for over 7,500 English word types."
PVGRU: Generating Diverse and Relevant Dialogue Responses via Pseudo-Variational Mechanism,2212.09086v4,./img_ACL_2023/2212.09086v4.pdf,"Overview of dialogue variability. (a) and (b) represent two dialogues a and b from DSTC7-AVSD. ùí©^a and ùí©^b represent the distribution composition of dialogue a and b on utterance level, respectively. ùí©_t^a represents the distribution of dialogue a at time step t on utterance level. (c) stands for the distribution composition of an utterance. u_i^a and u_i^b represent the i-th utterance of dialogue a and b, respectively. w_j^i stands for the j-th word of the i-th utterance. ‚ñΩ _3^a denotes the distribution variations caused by u_3 to ùí©^a and ‚ñΩ _3^i denotes the variations caused by token w_3^i to the distribution of u_i. The utterances marked in brown in dialogue a indicate that there is a similar expression in dialogue b.","We investigate response generation for multi-turn dialogue in generative-based chatbots. Existing generative models based on RNNs (Recurrent Neural Networks) usually employ the last hidden state to summarize the sequences, which makes models unable to capture the subtle variability observed in different dialogues and cannot distinguish the differences between dialogues that are similar in composition. In this paper, we propose a Pseudo-Variational Gated Recurrent Unit (PVGRU) component without posterior knowledge through introducing a recurrent summarizing variable into the GRU, which can aggregate the accumulated distribution variations of subsequences. PVGRU can perceive the subtle semantic variability through summarizing variables that are optimized by the devised distribution consistency and reconstruction objectives. In addition, we build a Pseudo-Variational Hierarchical Dialogue (PVHD) model based on PVGRU. Experimental results demonstrate that PVGRU can broadly improve the diversity and relevance of responses on two benchmark datasets.","Complex grammatical rules exist in the high variability text data, especially dialogue. As shown in Figure, two utterances with just one/two words different may have different semantics, such as utterance u_6 of dialogue a vs. u_5 of dialogue b; On the other hand, two dialogues with lots of semantically similar utterances may express quite different context meanings, such as a vs. b. These variabilities lead to multiple mappings between dialogue context and response, which occurs in response causing one-to-many issue and in context resulting in many-to-one problem. We can observe that the distribution of dialogue contexts (i.e., ùí©^a and ùí©^b) is composed of the distribution of utterances, and the distribution of each utterance consists of the distribution of words (i.e., Figure (c) ). How to model the word-level and utterance-level variation in dialogue plays an important role in improving the quality of responses. 
A line of existing researches employ RNNs (Recurrent Neural Networks) to model dialogue context. However, researchers perceive that it is inappropriate to employ RNNs to directly model this kind of variability observed in dialogue corpora. This is because the internal transition structure of the RNNs is entirely deterministic, which can not effectively model the randomness or variability in dialogue contexts. 
Variational mechanism has demonstrated attractive merits in modeling variability from both theoretical and practical perspectives. Methods based on variational mechanism introduce latent variable into RNNs to model one-to-many and many-to-one phenomena in dialogue. Although these approaches achieve promising results, they still have defects. First, these methods face the dilemma that latent variables may vanish because of the posterior collapse issue. Variational mechanism can work only when latent variables with intractable posterior distributions exist. Second, the sampled latent variables may not correctly reflect the semantics of the dialogue context due to the one-to-many and many-to-one phenomena observed in dialogue. Third, the posterior knowledge is employed in training while prior knowledge used in inference, which causes an inconsistency problem between training and inference. 
To tackle these problems, we propose a Pseudo-Variational Gated Recurrent Unit (PVGRU) component based on pseudo-variational mechanism. PVGRU is based on GRU by introducing a recurrent summarizing variable, which can aggregate the accumulated distribution variations of subsequences. The methods based on PVGRU can perceive the subtle semantic differences between different sequences. First, the pseudo-variational mechanism adopts the idea of latent variables but does not adopts posterior mechanism. Therefore, PVGRU does not suffer from the posterior collapse issue. Second, we design the consistency and reconstruction objectives to optimize the recurrent summarizing variable in PVGRU, which ensure that the recurrent variable can reflect the semantic of dialogue context from wordand utterance-level, respectively. The consistency objective makes the distribution of the incremental information consistent with the corresponding input at each time step. For instance in Figure, the distribution of u_3^a is consistent with ‚ñΩ _3^a=ùí©_3^a-ùí©_2^a and the distribution of w_3^i is consistent with ‚ñΩ _3^i. The reconstruction objective demands the summarizing variable to be able to reconstruct the sequence information. For example, we can reconstruct the subsequence information before time step 3 from distribution ùí©_3^a. Third, we guarantees the consistency between training and inference since we do not employ posterior knowledge when optimizing summarizing variable. 
In addition, we build a Pseudo-Variational Hierarchical Dialogue model (PVHD) based on PVGRU to model the word-level and utterance-level variation. To summarize, we make the following contributions: 
 
 * We analyze the reasons for one-to-many and many-to-one issues from high variability of dialogue corpus and propose PVGRU with recurrent summarizing variable to model the variability of dialogue sequences. 
 * We propose to optimize recurrent summarizing variable using consistency and reconstruction objective, which guarantees that the summarizing variable can reflect the semantics of the dialogue context and maintain the consistency between training and inference processes. 
 * We propose PVHD model based on PVGRU, which significantly outperforms strong baselines with RNN and Transformer architectures on two benchmark datasets. The code including baselines for comparison is avaliable on Github."
A Survey on Zero Pronoun Translation,2305.10196v1,./img_ACL_2023/2305.10196v1.jpg,"An overview of pro-drop languages by considering their typological patterns and language families. Example of ZP phenomenon in other languages (i.e.,Korean, Hungarian and Hindi). Words in brackets are pronouns that are invisible in source language (implicit and explicit). The underlined words are corresponding antecedents. ""EN"" represents the human translation in English, which is a non-pro-drop language. ""OT"" is output translated by SOTA NMT systems with inappropriate translations.","Zero pronouns (ZPs) are frequently omitted in pro-drop languages (e.g.,Chinese, Hungarian, and Hindi), but should be recalled in non-pro-drop languages (e.g.,English). This phenomenon has been studied extensively in machine translation (MT), as it poses a significant challenge for MT systems due to the difficulty in determining the correct antecedent for the pronoun. This survey paper highlights the major works that have been undertaken in zero pronoun translation (ZPT) after the neural revolution, so that researchers can recognise the current state and future directions of this field. We provide an organisation of the literature based on evolution, dataset, method and evaluation. In addition, we compare and analyze competing models and evaluation metrics on different benchmarks. We uncover a number of insightful findings such as: 1) ZPT is in line with the development trend of large language model; 2) data limitation cause learning bias in languages and domains; 3) performance improvements are often reported on single benchmarks, but advanced methods are still far from real-world use; 4) general-purpose metrics are not reliable on nuances and complexities of ZPT, emphasizing the necessity of targeted metrics; 5) apart from commonly-cited errors, ZPs will cause risks of gender bias.","Pronouns play an important role in natural language, as they enable speakers to refer to people, objects, or events without repeating the nouns that represent them. Zero pronoun (ZP) is a complex phenomenon that appears frequently in pronoun-dropping (pro-drop) languages such as Chinese, Hungarian, and Hindi. Specifically, pronouns are often omitted when they can be pragmatically or grammatically inferable from intraand inter-sentential contexts. Since recovery of such ZPs generally fails, this poses difficulties for several generation tasks, including dialogue modelling, question answering, and machine translation. 
When translating texts from pro-drop to non-pro-drop languages (e.g.,Chinese‚áíEnglish), this phenomenon leads to serious problems for translation models in terms of: 1) completeness, since translation of such invisible pronouns cannot be normally reproduced; 2) correctness, because understanding the semantics of a source sentence needs to identifying and resolving the pronominal reference. 
UTF8gbsn Figure shows ZP examples in three typological patterns determined by language family (detailed in Appendix). Taking a full-drop language for instance, the first-person subject and third-person object pronouns are omitted in Hindi input while these pronouns are all compulsory in English translation. This is not a problem for human beings since we can easily recall these missing pronoun from the context. However, even a real-life MT system still fails to accurately translate ZPs. 
In response to this problem, zero pronoun translation (ZPT) has been studied extensively in the MT community on three significant challenges: 
 
 * Dataset: there is limited availability of ZP-annotated parallel data, making it difficult to develop systems that can handle ZP complexities. 
 * Approach: due to the ability to capture semantic information with distributed representations, ideally, the representations of NMT should embed ZP information by learning the alignments between bilingual pronouns from the training corpus. In practice, however, NMT models only manage to successfully translate some simple ZPs, but still fail when translating complex ones (e.g.,subject vs. object ZPs). 
 * Evaluation: general evaluation metrics for MT are not sensitive enough to capture translation errors caused by ZPs. 
We believe that it is the right time to take stock of what has been achieved in ZPT, so that researchers can get a bigger picture of where this line of research stands. In this paper, we present a survey of the major works on datasets, approaches and evaluation metrics that have been undertaken in ZPT. We first introduce the background of linguistic phenomenon and literature selection in Section. Section discusses the evolution of ZP-related tasks. Section summarizes the annotated datasets, which are significant to pushing the studies move forward. Furthermore, we investigated advanced approaches for improving ZPT models in Section. In addition to this, Section covers the evaluation methods that have been introduced to account for improvements in this field. We conclude by presenting avenues for future research in Section."
Exploring Better Text Image Translation with Multimodal Codebook,2305.17415v2,./img_ACL_2023/2305.17415v2.png,An example of text image translation. The Bounding box in red represents the text to be recognized. We can observe that the incorrect OCR result will negatively affect the subsequent translation.,"Text image translation (TIT) aims to translate the source texts embedded in the image to target translations, which has a wide range of applications and thus has important research value. However, current studies on TIT are confronted with two main bottlenecks: 1) this task lacks a publicly available TIT dataset, 2) dominant models are constructed in a cascaded manner, which tends to suffer from the error propagation of optical character recognition (OCR). In this work, we first annotate a Chinese-English TIT dataset named OCRMT30K, providing convenience for subsequent studies. Then, we propose a TIT model with a multimodal codebook, which is able to associate the image with relevant texts, providing useful supplementary information for translation. Moreover, we present a multi-stage training framework involving text machine translation, image-text alignment, and TIT tasks, which fully exploits additional bilingual texts, OCR dataset and our OCRMT30K dataset to train our model. Extensive experiments and in-depth analyses strongly demonstrate the effectiveness of our proposed model and training framework.","In recent years, multimodal machine translation (MMT) has achieved great progress and thus received increasing attention. Current studies on MMT mainly focus on the text machine translation with scene images. However, a more common requirement for MMT in real-world applications is text image translation (TIT), which aims to translate the source texts embedded in the image to target translations. Due to its wide applications, the industry has developed multiple services to support this task, such as Google Camera Translation. 
Current studies on TIT face two main bottlenecks. First, this task lacks a publicly available TIT dataset. Second, the common practice is to adopt a cascaded translation system, where the texts embedded in the input image are firstly recognized by an optical character recognition (OCR) model, and then the recognition results are fed into a text-only neural machine translation (NMT) model for translation. However, such a method tends to suffer from the problem of OCR error propagation, and thus often generates unsatisfactory translations. As shown in Figure, UTF8gbsn ""ÂØåÈî¶Ê∂àÈò≤"" (""fu jin xiao fang"") in the image is incorrectly recognized as UTF8gbsn ""ÂØåÈî¶Ê∂àÈò≥"" (""fu jin xiao yang""). Consequently, the text-only NMT model incorrectly translates it into ""Fujin Xiaoyang"". Furthermore, we use the commonly-used PaddleOCR to handle several OCR benchmark datasets. As reported in Table, we observe that the highest recognition accuracy at the image level is less than 67%and that at the sentence level is not higher than 81%. It can be said that OCR errors are very common, thus they have a serious negative impact on subsequent translation. 
In this paper, we first manually annotate a Chinese-English TIT dataset named OCRMT30K, providing convenience for subsequent studies. This dataset is developed based on five Chinese OCR datasets, including about 30,000 image-text pairs. 
Besides, we propose a TIT model with a multimodal codebook to alleviate the OCR error propagation problem. The basic intuition behind our model is that when humans observe the incorrectly recognized text in an image, they can still associate the image with relevant or correct texts, which can provide useful supplementary information for translation. Figure shows the basic architecture of our model, which mainly consists of four modules: 1) a text encoder that converts the input text into a hidden state sequence; 2) an image encoder encoding the input image as a visual vector sequence; 3) a multimodal codebook. This module can be described as a vocabulary comprising latent codes, each of which represents a cluster. It is trained to map the input images and ground-truth texts into the shared semantic space of latent codes. During inference, this module is fed with the input image and then outputs latent codes containing the text information related to ground-truth texts. 4) a text decoder that is fed with the combined representation of the recognized text and the outputted latent codes, and then generates the final translation. 
Moreover, we propose a multi-stage training framework for our TIT model, which can fully exploit additional bilingual texts and OCR data for model training. Specifically, our framework consists of four stages. First, we use a large-scale bilingual corpus to pretrain the text encoder and text decoder. Second, we pretrain the newly added multimodal codebook on a large-scale monolingual corpus. Third, we further introduce an image encoder that includes a pretrained vision Transformer with fixed parameters to extract visual features, and continue to train the multimodal codebook. Additionally, we introduce an image-text alignment task to enhance the ability of the multimodal codebook in associating images with related texts. Finally, we finetune the entire model on the OCRMT30K dataset. Particularly, we maintain the image-text alignment task at this stage to reduce the gap between the third and fourth training stages. 
Our main contributions are as follows: 
 
 * We release an OCRMT30K dataset, which is the first Chinese-English TIT dataset, prompting the subsequent studies. 
 * We present a TIT model with a multimodal codebook, which can leverage the input image to generate the information of relevant or correct texts, providing useful information for the subsequent translation. 
 * We propose a multi-stage training framework for our model, which effectively leverages additional bilingual texts and OCR data to enhance the model training. 
 * Extensive experiments and analyses demonstrate the effectiveness of our model and training framework."
Memory-efficient NLLB-200: Language-specific Expert Pruning of a Massively Multilingual Machine Translation Model,2212.09811v3,./img_ACL_2023/2212.09811v3.pdf,Average number of experts per layer after pruning 75%of experts with the global threshold algorithm (average activity threshold: 0.69). Pruning is done per language direction and the values are averaged over the 870 directions of the valid set.,"The recently released NLLB-200 is a set of multilingual Neural Machine Translation models that cover 202 languages. The largest model is based on a Mixture of Experts architecture and achieves SoTA results across many language pairs. It contains 54.5B parameters and requires at least four 32GB GPUs just for inference. In this work, we propose a pruning method that enables the removal of up to 80%of experts without further finetuning and with a negligible loss in translation quality, which makes it feasible to run the model on a single 32GB GPU. Further analysis suggests that our pruning metrics can identify language-specific experts.","The Transformer has become the dominant modeling paradigm in Natural Language Processing tasks. Many subsequent advances in the field came from increasing the computational budget, training data, and model size. Neural Machine Translation was not an exception, where massively multilingual NMT demonstrated promising results, while attempting to overcome the curse of multilinguality by scaling up model size. 
However, increasing the parameter size exacerbates the cost of training and hurts the memory footprint and inference latency. Sparsely-gated Mixture-of-Experts (MoE) models are an efficient alternative to dense models. For example, demonstrates that an MoE language model results in a 7x larger model compared to GPT-3, but requires only 30%of its energy for training and half of its FLOPs at inference. 
Mixture-of-Experts models are neural networks whose set of parameters is partitioned into experts. Contrary to dense models, where all network parameters are used for every input, an MoE model activates different parts of the network, the experts, depending on the input, which is typically done by a gating mechanism at the token level. MoE models are computationally efficient due to expert parallelism across a large number of GPUs, by having each GPU hold a subset of all experts and communicate with the other GPUs when it needs expert outputs for its local batch. 
In NLLB-200, a load balancing regularizer in the objective function promotes equal distribution of the tokens across experts. This encourages the model to use all the experts and ensures that all GPUs are used equally for the sake of computational efficiency. However, considering a large number of experts, it does not guarantee that all experts will be equally activated for a particular pair of languages at inference. It raises a research question: are there language-specific experts in multilingual MoE models? If this is the case, we may be able to prune such models without loss of translation quality for the language pairs of our interest. Reducing memory usage would be useful for a model like NLLB-200, which normally requires at least four 32GB GPUs at inference. 
In this work, we define metrics to assess the importance of each expert and prune the least important experts at inference. We aim to avoid fine-tuning because of its computational cost. In an ideal scenario, we would like to be able to identify the important experts in an MoE model so that practitioners can deploy large models, such as NLLB-200, on a single GPU. We summarize our main contributions as follows: 
 
 * We propose a pruning strategy that can remove 80%of experts in the NLLB-200 model without further finetuning and with a negligible loss in translation quality; 
 * We find that the decoder experts can be pruned more aggressively than the encoder experts; 
 * We show the emergence of language-specific experts in the NLLB-200 model; 
 * We demonstrate that the important language-specific experts in the decoder are shared between linguistically related languages; 
 * We release the ids of the pruned experts, along with other experts' gathered statistics so that anyone with a single 32GB GPU can use NLLB-200 at inference."
MultiTACRED: A Multilingual Version of the TAC Relation Extraction Dataset,2305.04582v2,./img_ACL_2023/2305.04582v2.pdf,"Example translations from English to German, Polish, Turkish and Chinese with XML markup for the head and tail entities to project relation argument annotations.","Relation extraction (RE) is a fundamental task in information extraction, whose extension to multilingual settings has been hindered by the lack of supervised resources comparable in size to large English datasets such as TACRED. To address this gap, we introduce the MultiTACRED dataset, covering 12 typologically diverse languages from 9 language families, which is created by machine-translating TACRED instances and automatically projecting their entity annotations. We analyze translation and annotation projection quality, identify error categories, and experimentally evaluate fine-tuned pretrained monoand multilingual language models in common transfer learning scenarios. Our analyses show that machine translation is a viable strategy to transfer RE instances, with native speakers judging more than 83%of the translated instances to be linguistically and semantically acceptable. We find monolingual RE model performance to be comparable to the English original for many of the target languages, and that multilingual models trained on a combination of English and target language data can outperform their monolingual counterparts. However, we also observe a variety of translation and annotation projection errors, both due to the MT systems and linguistic features of the target languages, such as pronoun-dropping, compounding and inflection, that degrade dataset quality and RE model performance.","Relation extraction (RE), defined as the task of identifying and classifying semantic relationships between entities from text (cf. Figure), is a fundamental task in information extraction. Extending RE to multilingual settings has recently received increased interest, both to address the urgent need for more inclusive NLP systems that cover more languages than just English, as well as to investigate language-specific phenomena and challenges relevant to this task. The main bottleneck for multilingual RE is the lack of supervised resources, comparable in size to large English datasets, as annotation for new languages is very costly. Most of the few existing multilingual RE datasets are distantly supervised, and hence suffer from noisy labels that may reduce the prediction quality of models. Available fully-supervised datasets are small, and cover either very few domain-specific relation types, or only a small set of languages. 
To address this gap, and to incentivize research on supervised multilingual RE, we introduce a multilingual version of one of the most prominent supervised RE datasets, TACRED. MultiTACRED is created by machine-translating TACRED instances and automatically projecting their entity annotations. Machine translation is a popular approach for generating data in cross-lingual learning. Although the quality of machine-translated data may be lower due to translation and alignment errors, it has been shown to be beneficial for classification and structured prediction tasks. 
The MultiTACRED dataset we present in this work covers 12 languages from 9 language families. We select typologically diverse languages which span a large set of linguistic phenomena such as compounding, inflection and pronoun-drop, and for which a monolingual pretrained language model is available. We automatically and manually analyze translation and annotation projection quality in all target languages, both in general terms and with respect to the RE task, and identify typical error categories for alignment and translation that may affect model performance. We find that overall translation quality is judged to be quite good with respect to the RE task, but that e.g.,pronoun-dropping, coordination and compounding may cause alignment and semantic errors that result in erroneous instances. In addition, we experimentally evaluate fine-tuned pretrained monoand multilingual language models (PLM) in common training scenarios, using source language (English), target language, or a mixture of both as training data. We also evaluate an English data fine-tuned model on back-translated test instances to estimate the effect of noise introduced by the MT system on model performance. Our results show that in-language training works well, given a suitable PLM. Cross-lingual zero-shot transfer is acceptable for languages well-represented in the multilingual PLM, and combining English and target language data for training considerably improves performance across the board. 
To summarize, our work aims to answer the following research questions: Can we reaffirm the usefulness of MT and cross-lingual annotation projection, in our study for creating large-scale, high quality multilingual datasets for RE? How do pretrained monoand multilingual encoders compare to each other, in within-language as well as cross-lingual evaluation scenarios? Answers to these questions can provide insights for understanding language-specific challenges in RE, and further research in cross-lingual representation and transfer learning. The contributions of this paper are: 
 
 * We introduce MultiTACRED, a translation of the widely used, large-scale TACRED dataset into 12 typologically diverse target languages: Arabic, German, Spanish, French, Finnish, Hindi, Hungarian, Japanese, Polish, Russian, Turkish, and Chinese. 
 * We present an evaluation of monolingual, cross-lingual, and multilingual models to evaluate target language performance for all 12 languages. 
 * We present insights into the quality of machine translation for RE, analyzing alignment as well as language-specific errors."
Towards Higher Pareto Frontier in Multilingual Machine Translation,2305.15718v1,./img_ACL_2023/2305.15718v1.pdf,"Multilingual performance frontier shifts outwards. X-axis and Y-axis indicate the performance of Low-Resource Languages and High-Resource Languages, respectively. Existing methods reflect a trade-off on the Pareto frontier (i.e., the gray curve). Our work aims to push the original Pareto frontier i.e., the blue dotted curve. To this effect, we ameliorate each individual model 's shortcoming while retaining their strengths, e.g., moving right the solution A to A' and moving up the solution B to B', via our Pareto Mutual Distillation.","Multilingual neural machine translation has witnessed remarkable progress in recent years. However, the long-tailed distribution of multilingual corpora poses a challenge of Pareto optimization, i.e., optimizing for some languages may come at the cost of degrading the performance of others. Existing balancing training strategies are equivalent to a series of Pareto optimal solutions, which trade off on a Pareto frontier. In this work, we propose a new training framework, Pareto Mutual Distillation (Pareto-MD), towards pushing the Pareto frontier outwards rather than making trade-offs. Specifically, Pareto-MD collaboratively trains two Pareto optimal solutions that favor different languages and allows them to learn from the strengths of each other via knowledge distillation. Furthermore, we introduce a novel strategy to enable stronger communication between Pareto optimal solutions and broaden the applicability of our approach. Experimental results on the widely-used WMT and TED datasets show that our method significantly pushes the Pareto frontier and outperforms baselines by up to +2.46 BLEU.","Multilingual neural machine translation (MNMT) is a popular paradigm that uses a unified model to handle the entire translation process for multiple language pairs. This paradigm is particularly effective at improving the performance of low-resource languages through transfer learning. Besides, MNMT is highly deployable since only one model is required. 
However, the severely imbalanced distribution of multilingual training data puts the MNMT in a situation of Pareto optimization (also known as multi-objective optimization). That is, when some languages are optimized, others degenerate. Existing methods can be considered a set of Pareto optimal solutions that trade off on a Pareto frontier, which focus on balancing the performance across different languages by adjusting the sampling distribution. The widely-used temperature-based sampling is typical evidence of the claim above, which uses a hyper-parameter to smooth the training distribution over all language pairs to enhance the representation of low-source Languages (LRLs) while sacrificing the which of High-Resource Languages (HRLs). Despite the emergence of several sophisticated dynamic sampling technologies designed to overcome the inflexibility of temperature-based sampling, their performance remains restricted to this Pareto frontier. 
In this work, we propose a novel training framework, named Pareto Mutual Distillation (Pareto-MD), to push the Pareto frontier of multilingual models. Specifically, Pareto-MD uses different training distributions that favor dissimilar subsets of languages to train two multilingual models simultaneously. These two models learn from each other at each training step with knowledge distillation. The underlying idea of Pareto-MD is to address shortcomings of individual Pareto optimal solutions via access to a better one in terms of that shortcoming, thereby raising the Pareto frontier, as Fig. depicts. To fully exploit the potential of our approach in multilingual settings, we further propose Automatic Pareto Mutual Distillation, which dynamically determines the contribution of distillation learning loss on each objective. These contributions, controlled by a set of distillation weights, adapt automatically to the evolving models, eliminating the need for manual hyper-parameter search. 
While our method applies essentially to any multi-objective optimization problem, we specifically demonstrate its benefit on multilingual machine translation. The experimental results on two widely-used datasets demonstrate the effectiveness of our method, which improves up to +2.46 BLEU, and the further analysis shows the Pareto frontier is pushed outwards visibly."
DiffusionNER: Boundary Diffusion for Named Entity Recognition,2305.13298v1,./img_ACL_2023/2305.13298v1.pdf,"Boundary diffusion in named entity recognition. The fixed forward diffusion process adds Gaussian noise to the entity boundaries at each timestep, and the noisy boundaries recover original state by denoising with the learnable reverse diffusion process. For inference, the reverse diffusion process generates entity boundaries and performs entity typing based on the noisy spans sampled from the Gaussian distribution.","In this paper, we propose DiffusionNER, which formulates the named entity recognition task as a boundary-denoising diffusion process and thus generates named entities from noisy spans. During training, DiffusionNER gradually adds noises to the gold entity boundaries by a fixed forward diffusion process and learns a reverse diffusion process to recover the entity boundaries. In inference, DiffusionNER first randomly samples some noisy spans from a standard Gaussian distribution and then generates the named entities by denoising them with the learned reverse diffusion process. The proposed boundary-denoising diffusion process allows progressive refinement and dynamic sampling of entities, empowering DiffusionNER with efficient and flexible entity generation capability. Experiments on multiple flat and nested NER datasets demonstrate that DiffusionNER achieves comparable or even better performance than previous state-of-the-art models.","Named Entity Recognition (NER) is a basic task of information extraction, which aims to locate entity mentions and label specific entity types such as person, location, and organization. It is fundamental to many structured information extraction tasks, such as relation extraction and event extraction. 
Most traditional methods formulate the NER task into a sequence labeling task by assigning a single label to each token. To accommodate the nested structure between entities, some methods further devise cascaded or stacked tagging strategies. Another class of methods treat NER as a classification task on text spans, and assign labels to word pairs or potential spans. In contrast to the above works, some pioneer works propose generative NER methods that formulate NER as a sequence generation task by translating structured entities into a linearized text sequence. However, due to the autoregressive manner, the generation-based methods suffer from inefficient decoding. In addition, the discrepancy between training and evaluation leads to exposure bias that impairs the model performance. 
We move to another powerful generative model for NER, namely the diffusion model. As a class of deep latent generative models, diffusion models have achieved impressive results on image, audio and text generation. The core idea of diffusion models is to systematically perturb the data through a forward diffusion process, and then recover the data by learning a reverse diffusion process. 
Inspired by this, we present DiffusionNER, a new generative framework for named entity recognition, which formulates NER as a denoising diffusion process on entity boundaries and generates entities from noisy spans. As shown in Figure, during training, we add Gaussian noise to the entity boundaries step by step in the forward diffusion process, and the noisy spans are progressively denoised by a reverse diffusion process to recover the original entity boundaries. The forward process is fixed and determined by the variance schedule of the Gaussian Markov chains, while the reverse process requires learning a denoising network that progressively refines the entity boundaries. For inference, we first sample noisy spans from a prior Gaussian distribution and then generate entity boundaries using the learned reverse diffusion process. 
Empowered by the diffusion model, DiffusionNER presents three advantages. First, the iterative denoising process of the diffusion model gives DiffusionNER the ability to progressively refine the entity boundaries, thus improve performance. Second, independent of the predefined number of noisy spans in the training stage, DiffusionNER can sample a different number of noisy spans to decode entities during evaluation. Such dynamic entity sampling makes more sense in real scenarios where the number of entities is arbitrary. Third, different from the autoregressive manner in generation-based methods, DiffusionNER can generate all entities in parallel within several denoising timesteps. In addition, the shared encoder across timesteps can further speed up inference. We will further analyze these advantages of DiffusionNER in. In summary, our main contributions are as follows: 
 
 * DiffusionNER is the first to use the diffusion model for NER, an extractive task on discrete text sequences. Our exploration provides a new perspective on diffusion models in natural language understanding tasks. 
 * DiffusionNER formulates named entity recognition as a boundary denoising diffusion process from the noisy spans. DiffusionNER is a novel generative NER method that generates entities by progressive boundary refinement over the noisy spans. 
 * We conduct experiments on both nested and flat NER to show the generality of DiffusionNER. Experimental results show that our model achieves better or competitive performance against the previous SOTA models."
Faithful Question Answering with Monte-Carlo Planning,2305.02556v1,./img_ACL_2023/2305.02556v1.pdf,"Given a question, Fameperforms reasoning through the iterative interaction of a controller with a reasoning environment. It produces the reasoning steps (in the form of an entailment tree) and the answer faithfully following from the steps. The entailment tree contains the basic fact (sent_*) and novel intermediate conclusions (int_*) connected by entailment steps.","Although large language models demonstrate remarkable question-answering performances, revealing the intermediate reasoning steps that the models faithfully follow remains challenging. In this paper, we propose Fame (FAithful question answering with MontE-carlo planning) to answer questions based on faithful reasoning steps. The reasoning steps are organized as a structured entailment tree, which shows how premises are used to produce intermediate conclusions that can prove the correctness of the answer. We formulate the task as a discrete decision-making problem and solve it through the interaction of a reasoning environment and a controller. The environment is modular and contains several basic task-oriented modules, while the controller proposes actions to assemble the modules. Since the search space could be large, we introduce a Monte-Carlo planning algorithm to do a look-ahead search and select actions that will eventually lead to high-quality steps. Fameachieves advanced performance on the standard benchmark. It can produce valid and faithful reasoning steps compared with large language models with a much smaller model size.","To tackle this issue, the recently proposed faithful question-answering (FQA) task asks the system to provide the reasoning steps that the answer faithfully follows, as demonstrated in Figure (a) and (c). The reasoning steps are organized as an entailment tree, where each non-leaf node indicates an intermediate entailment step. The provision of the faithful steps allows users to inspect and debug the system 's reasoning process, potentially enabling the construction of interactive and teachable QA systems. 
Existing FQA methods typically adopt a step-wise approach to generate an entailment step at a time. When determining which step to generate next, they produce several candidate steps and select one based on the validity of the step. However, they do not explicitly consider whether the selected step will ultimately result in a high-quality tree that supports the answer. For complex questions that require multiple reasoning steps, such a lack of foresight might lead to the irreversible miss of the optimal step in a huge search space. Furthermore, existing methods are based on either the model' s internal beliefs or a small number of given facts, which limits their ability to ground to the external world and update their known facts. 
In this paper, we propose Fame, a novel FQA method integrating Monte-Carlo planning. We formulate the task as a discrete decision-making problem and solve it through the interaction of a reasoning environment and a controller, as shown in Figure. The reasoning environment is modular. We decompose the reasoning into several basic task-oriented modules, such as a retriever for updating known facts and a single-step entailment module for combining several premises to obtain a novel intermediate conclusion. To assemble these modules, we leverage a controller (implemented with a generative language model) to observe the state of the environment and propose the next actions. The final answer is derived based on the validity and faithfulness of the generated entailment tree. 
To select actions foresightedly, we introduce a Monte-Carlo planning algorithm to do the look-ahead search. Specifically, we assign an action value to each candidate action, which is iteratively updated based on the quality of its successor states after the action is explicitly executed. With these values, we could make more informed decisions and select the actions that will eventually lead to a high-quality tree. In addition, we design a verifier-guided iterative training technique to train the controller. 
Experiments on the standard benchmark EntailmentBankQA show that Fameoutperforms previous best FQA methods by a large margin. Manual evaluation results demonstrate that Famecould produce valid and faithful reasoning steps compared with LLMs (i.e., GPT-3 and ChatGPT). Further ablation results illustrate the advantages of Monte-Carlo planning compared to other planning methods."
Unbalanced Optimal Transport for Unbalanced Word Alignment,2306.04116v1,./img_ACL_2023/2306.04116v1.pdf,Unbalanced monolingual word alignment matrix; frequent null alignment (enclosed in red boxes) and mapping beyond one-to-one (enclosed in orange dashed boxes) are primary challenges.,"Monolingual word alignment is crucial to model semantic interactions between sentences. In particular, null alignment, a phenomenon in which words have no corresponding counterparts, is pervasive and critical in handling semantically divergent sentences. Identification of null alignment is useful on its own to reason about the semantic similarity of sentences by indicating there exists information inequality. To achieve unbalanced word alignment that values both alignment and null alignment, this study shows that the family of optimal transport (OT), i.e., balanced, partial, and unbalanced OT, are natural and powerful approaches even without tailor-made techniques. Our extensive experiments covering unsupervised and supervised settings indicate that our generic OT-based alignment methods are competitive against the state-of-the-arts specially designed for word alignment, remarkably on challenging datasets with high null alignment frequencies.","Monolingual word alignment, which identifies semantically corresponding words in a sentence pair, has been actively studied as a crucial technique for modelling semantic relationships between sentences, such as for paraphrase identification, textual entailment recognition, and question answering. Its ability to declare redundant information in sentences is also useful for summarisation and sentence fusion. In addition, the alignment information is valuable for interpretability of model predictions and for realising interactive document exploration as well. 
fig: gold_alignment illustrates the challenges of monolingual word alignment. The first challenge is the null alignment, where words may not have corresponding counterparts, which causes alignment asymmetricity. Null alignment is prevalent in semantically divergent sentences; indeed, the null alignment ratio reaches 63.8%in entailment sentence pairs used in our experiments (shown in the third row of Table). Its identification explicitly declares semantic gaps between two sentences and helps reason about their semantic (dis) similarity. The second challenge is that alignment beyond one-to-one mapping needs to be addressed. These challenges constitute an unbalanced word alignment problem, where both word-by-word and null alignment should be fully identified. 
This study reveals that a family of optimal transport (OT) are suitable tools for unbalanced word alignment. Among the OT problems, balanced OT (BOT) should be the most prominent in natural language processing (NLP), which can handle many-to-many alignment. In contrast to BOT that is unable to deal with null alignment, partial OT (POT) and unbalanced OT (UOT) can handle the asymmetricity as desired in unbalanced word alignment, which has also attracted applications where null alignment is unignorable. 
This is the first study that connects these two paradigms of unbalanced word alignment and the family of OT problems that can naturally address null alignment as well as many-to-many alignment. We empirically (1) demonstrate that the OT-based methods are natural and sufficiently powerful approaches to unbalanced word alignment without tailor-made techniques and (2) deliver a comprehensive picture that unveils the characteristics of BOT, POT, and UOT on unbalanced word alignment with different null alignment ratios. We conduct extensive experiments using representative datasets for understanding OT-based alignment: effects of OT formalisation, regularisation, and a heuristic to sparsify alignments. Our primary findings can be summarised as follows. First, in unsupervised alignment, the best OT problem depends on null alignment ratios. Second, simple thresholding on regularised BOT can produce unbalanced alignment. Third, in supervised alignment, simple and generic OT-based alignment shows competitive performance to the state-of-the-art models specially designed for word alignment. 
The adoption of well-understood methods with a solid theory like OT is highly valuable in application development and scientific reproducibility. Furthermore, OT-based alignment performs superiorly or competitively to existing methods, despite being independent of tailor-made techniques. Our OT-based alignment methods are publicly available as a tool called OTAlign."
Guiding Computational Stance Detection with Expanded Stance Triangle Framework,2305.19845v1,./img_ACL_2023/2305.19845v1.png,The stance triangle framework proposed by. Vertices denote the three basic components. Edges denote expression act types.,"Stance detection determines whether the author of a piece of text is in favor of, against, or neutral towards a specified target, and can be used to gain valuable insights into social media. The ubiquitous indirect referral of targets makes this task challenging, as it requires computational solutions to model semantic features and infer the corresponding implications from a literal statement. Moreover, the limited amount of available training data leads to subpar performance in out-of-domain and cross-target scenarios, as data-driven approaches are prone to rely on superficial and domain-specific features. In this work, we decompose the stance detection task from a linguistic perspective, and investigate key components and inference paths in this task. The stance triangle is a generic linguistic framework previously proposed to describe the fundamental ways people express their stance. We further expand it by characterizing the relationship between explicit and implicit objects. We then use the framework to extend one single training corpus with additional annotation. Experimental results show that strategically-enriched data can significantly improve the performance on out-of-domain and cross-target evaluation.","Stance (and its variant stancetaking) is a concept defined as a linguistically articulated form of social action whose meaning is construed within language, interaction, and sociocultural value. Its subject can be the speaker in a conversation or the author of a social media post, and its object can be in the form of an entity, concept, idea, event, or claim. 
The stance detection task in natural language processing aims to predict the stance of a piece of text toward specified targets. Stance detection is commonly formulated as a classification problem, and is often applied to analyzing online user-generated content such as Twitter and Facebook posts. When given the text and one specified target (i.e., stance object), a classifier is used to predict a categorical label (e.g., Favor, Against, None). Along with social networking platforms 'growing impact on our lives, stance detection is crucial for various downstream tasks such as fact verification and rumor detection, with wide applications including analyzing user feedback and political opinions. For example, during the pandemic of COVID-19, it was essential to understand the public' s opinion on various initiatives and concerns, such as getting booster vaccinations and wearing facial masks. The insight from stance analysis could help public health organizations better estimate the expected efficacy of their mandates, as well as proactively detect pandemic fatigue before it leads to a serious resurgence of the virus. 
While state-of-the-art results have been achieved on text classification by adopting data-driven neural approaches, especially utilizing recent large-scale language backbones, stance detection remains challenging; there is a substantial gap between human and machine performance. One challenge comes from the ubiquitous indirect referral of targeted stance objects. When interacting socially online, people express their subjective attitude with brevity and variety: they often do not directly mention the final target, but mention its related entities, events, concepts, or claims. As examples shown in Table, unlike aspect-based sentiment analysis, where aspect terms are usually explicitly stated in sentences, targets specified for stance labeling can be flexibly assigned. For instance, in a tweet about COVID-19, while ""Dr. Fauci"" is not mentioned, one can infer that the user stands for him from the support of ""wearing a mask"" and ""science"". Therefore, target-aware context understanding requires capturing the relationship of explicitly-mentioned objects and various targets, but existing models lack such capability. 
Another challenge stems from limited annotated data for stance detection. When training on a corpus constructed with a small number of targets from a single domain, data-driven approaches cannot generalize well on out-of-domain samples and unseen targets. Meanwhile, due to low data diversity and the spurious correlation caused by single target labeling, models are prone to over-fit on superficial and biased features (e.g., sentiment-related lexicon). The strong baselines are observed to solely rely on the input text (e.g., tweets) but neglect the specified target, and fail to make correct predictions when we change the targeted object. As shown in Figure, the classifier always produces the same output Favor, even when irrelevant targets such as ""CD Disk"" are indicated. 
In this work, we investigate solutions for the aforementioned challenges from a linguistic perspective. The pragmatic and linguistics studies provide us with detailed theories of how humans perform stancetaking, and help us identify the key components and inference paths for stance analysis. The ""Stance Triangle"" is one of the most influential and generic linguistic frameworks. As shown in Figure, it presents three stancetaking acts: a subject (i.e., the stance holder) evaluates an object, positions themselves and others, and aligns with other subjects. While this model covers the important aspects of stancetaking, its broadness leaves the operationalization of stance in practical use cases under-specified. Regarding stance analysis of social networking platforms, modeling the implication of targets is important, but it is not well-formulated in the triangle framework. Therefore, we expand it by delineating the relationship between explicit and implicit objects, and outline two paths to complete the human-like inference. Aside from using the expanded framework for qualitative analysis, we further utilize it for strategic annotation enrichment, which shows strong potential to improve the robustness and generality of data-driven approaches. In summary, our contributions of this work are as follows: 
 
 * We make the first attempt to expand the linguistic framework ""stance triangle"" for improving computational stance detection, by characterizing the relationship and labels of explicit and implicit objects. 
 * We conduct qualitative analysis following the expanded framework on tweet stance detection, and outline the primary aspects and inference paths. 
 * We leverage the proposed framework to enrich the annotation of a single-domain corpus, and empirically demonstrate its effectiveness in improving the performance of out-of-domain and cross-target generalization."
Analyzing and Reducing the Performance Gap in Cross-Lingual Transfer with Fine-tuning Slow and Fast,2305.11449v1,./img_ACL_2023/2305.11449v1.png,"The performance gap P_≈ù-P_S/≈ù every hundred updates on the XNLI dataset. 'Original performance gap' means that we directly fine-tune the model, and 'Fine-tuning slow' / 'fine-tuning fast' / 'our method' means that we use the fine-tuning slow algorithm/fine-tuning fast algorithm/the combination of both algorithms respectively to fine-tune the model.","Existing research has shown that a multilingual pre-trained language model fine-tuned with one (source) language also performs well on downstream tasks for non-source languages, even though no fine-tuning is done on these languages. However, there is a clear gap between the performance of the source language and that of the non-source languages. This paper analyzes the fine-tuning process, discovers when the performance gap changes and identifies which network weights affect the overall performance most. Additionally, the paper seeks to answer to what extent the gap can be reduced by reducing forgetting. Based on the analysis results, a method named Fine-tuning slow and fast with four training policies is proposed to address these issues. Experimental results show the proposed method outperforms baselines by a clear margin.","Multilingual pre-trained language models (LMs), such as mBERT and XLM-R have shown strong Zero-Shot Cross-Lingual transfer capabilities. Such a model F is usually pre-trained with unlabeled corpora D in multiple languages S to enable the model to learn cross-lingual knowledge H_cross^pre. To adapt to a downstream task, the pre-trained LM F is typically fine-tuned with a supervised dataset D_≈ù of the downstream task T in one source language ≈ù‚àà S due to data scarcity in other languages. When the fine-tuned model F is applied to the test set of the same task in the source language ≈ù, it achieves strong performance P_≈ù. Interestingly, when F is applied to non-source languages, it can also achieve good performance. We denote the average performance on the test sets of other languages than ≈ù as P_S/≈ù. 
 However, the gap P_≈ù-P_S/≈ù is quite large (e.g., 13 percent for the XNLI dataset in Figure 1). One potential reason is that: during the fine-tuning of the model, the performance of non-source languages firstly increases with the performance of source language, then the arising of the performance of non-source languages becomes slower than that of the performance of source language as the forgetting of cross-lingual knowledge, resulting in a larger gap. Inspired by the study of catastrophic forgetting (CF) phenomenon in continual learning (CL), we introduce a classical concept in CL here to help solve our problem: the dilemma of plasticity vs. stability. 
Plasticity vs Stability. In CL, the learner needs to learn a sequence of different tasks incrementally. Plasticity means learning and performing well on the new task and stability means maintaining the learned knowledge of the previous tasks. The learner needs to find a balance between plasticity and stability because too much plasticity (e. g, changing the entire model drastically) causes serious CF of the learned knowledge, and too much stability (e.g.,freezing the whole model) makes the model can not learn new things. Fine-tuning a multi-lingual LM F using only the corpus of one source language also meets this balance dilemma. Thus, Fine-tuning LM F needs to protect the cross-lingual knowledge H_cross^pre (stability) and also learn the new task knowledge H_task^new via fine-tuning to adapt to the specific downstream task (plasticity). However, further analysis of the performance gap and the dilemma of plasticity and stability in cross-lingual fine-tuning is needed.
This paper further investigates three research questions: 1) When does the performance gap arise during fine-tuning using a labeled source language corpus? 2) Where is the most important part of the pre-trained model for achieving strong zero-shot cross-lingual performances? 3) To what extent can we reduce the performance gap by reducing the forgetting of H_cross^pre? Based on the experiments on three datasets of different downstream tasks, our analysis found that the performance gap arises significantly in the initial fine-tuning phase and increases slowly in the later phase (see Figure). Feed-forward weights in the bottom four layers are the key weights for the cross-lingual knowledge (See Figure and Table) and should be updated slowly to avoid forgetting H^pre_cross. Attention weights in the top two layers have the pre-training task (e.g., Masked-Language Modeling) knowledge H^pre_task and H^pre_task is useless for the downstream task. So these weights should be updated fast to encourage forgetting H^pre_task. We also find that protecting the cross-lingual knowledge by freezing the weights related to it can reduce the performance gap (enough stability) but cannot eliminate the gap completely (See Figure). That means only reducing the forgetting of H^pre_cross is not enough for solving the performance gap. 
Un-forgetting vs forgetting. Based on the above analysis, we propose a method called Fine-tuning slow and Fast algorithm to mitigate the forgetting of cross-lingual knowledge (stability) and also to selectively forget the knowledge related to the pre-training task (plasticity) to adapt F to the downstream task in fine-tuning F. Note that traditional techniques for solving the forgetting problem in continual learning are not applicable to our setting directly (see the reasons in Sec). 
The proposed method consists of four learning rate policies. Policies I and II (stability policies) are respectively designed to avoid forgetting of H_cross^pre in the first fine-tuning stage and to avoid the forgetting of H_cross^pre based on the tendency of the learning curve in the second fine-tuning stage. Policies III and IV (plasticity policies) are respectively designed to selectively forget the pre-training task knowledge in H_task^pre in the initial fine-tuning stage where the loss drops drastically and to further encourage forgetting of the pre-training task knowledge H_task^pre and the learning of H_task^new in the second fine-tuning stage. 
This paper's main contributions are as follows: (1) We analyze the performance gap in cross-lingual fine-tuning and answer to what extent we can reduce the performance gap by avoiding forgetting cross-lingual knowledge. (2) We propose a method consisting of four learning rate policies to reduce forgetting of cross-lingual knowledge (stability) and to encourage forgetting of pre-training task-related knowledge (plasticity). (3) We test our method in multiple datasets under zero and few-shot settings. Compared to the baseline, our method reduces the performance gap (Figure (XNLI) and Figure in Appendix A (MLQA and NER) ) and achieves better overall performance (Table) by protecting the cross-lingual knowledge and learning better task representation."
MM-SHAP: A Performance-agnostic Metric for Measuring Multimodal Contributions in Vision and Language Models & Tasks,2212.08158v3,./img_ACL_2023/2212.08158v3.png,"We display image-sentence alignment scores (ISA) and the textual degree T-SHAP that measures how much models focus on text rather than the image (with 100 - T-SHAP% the corresponding visual degree) for 3 VL models. Blue/red highlights on text tokens and image tokens (patches) contribute towards higher/lower ISA. Note: CLIP's ISA is an absolute score, while ALBEF and LXMERT predict ISA probabilities. See Section for more details on this figure; App. for more detailed analysis of this instance and more samples.","Vision and language models (VL) are known to exploit unrobust indicators in individual modalities (e.g., introduced by distributional biases) instead of focusing on relevant information in each modality. That a unimodal model achieves similar accuracy on a VL task to a multimodal one, indicates that so-called unimodal collapse occurred. However, accuracy-based tests fail to detect e.g., when the model prediction is wrong, while the model used relevant information from a modality. Instead, we propose MM-SHAP, a performance-agnostic multimodality score based on Shapley values that reliably quantifies in which proportions a multimodal model uses individual modalities. We apply MM-¬≠ SHAP in two ways: (1) to compare models for their average degree of multimodality, and (2) to measure for individual models the contribution of individual modalities for different tasks and datasets. Experiments with six VL models ‚Äì LXMERT, CLIP and four ALBEF variants ‚Äì on four VL tasks highlight that unimodal collapse can occur to different degrees and in different directions, contradicting the wide-spread assumption that unimodal collapse is one-sided. Based on our results, we recommend MM-SHAP for analysing multimodal tasks, to diagnose and guide progress towards multimodal integration. Code available at <https: //github. com/Heidelberg-NLP/MM-SHAP>.","Vision and language (VL) tasks are dominated by general-purpose pretrained transformer-based VL models. But we are only starting to understand why these multimodal (MM) models work so well, and how they utilise and fuse image and text modalities. Even worse, these highly parametrised neural VL models, pretrained on large amounts of data, tend to exploit artefacts and statistical correlations in the data, showing little to no evidence of detailed linguistic or visual understanding. Statistical biases towards indicators in one modality ‚Äì to the detriment of others ‚Äì can cause unimodal collapse, where seemingly MM models exploit one modality that exhibits biases, meaning that the MM system effectively reduces to a unimodal model ‚Äì e.g., if a model answers ""How many. . . ? "" questions with ""two"" ‚Äì the most frequent answer seen in training. Unimodal collapse is severe, as it leads to loss of system reliability. It also shows that multimodal fusion is far from being solved. Hence the importance of measuring mul¬≠ ti¬≠ mo¬≠ dal degree ‚Äì the degree to which modalities are used in model predictions ‚Äì with reliable metrics. 
To test for unimodal collapse, research has so far focused on performance tests: a VL model is evaluated on a MM task, but one modality crucial for solving it correctly is missing, corrupted or permuted. These tests are indicative of unimodal collapse, but we argue that they are not appropriate to reliably measure the contribution of each modality. Clearly, accuracy reflects whether a model prediction is (in) correct, but it may detect illicit cases where the model prediction is wrong, although it does use crucial indicators in a given modality. Conversely, a prediction might be correct, but may be derived from unrobust indicators. Fig. shows very different SHAP-based contribution patterns of image regions and text tokens leading to model responses of different image-sentence alignment (ISA) scores (e.g., ALBEF caption vs. foil), while yielding same ISA accuracy since both scores surpass the 0.5 classification threshold. 
As an alternative to accuracy-based methods, we propose MM-¬≠ SHAP, a performance-agnostic metric to quan¬≠ t¬≠ ify and interpret the contribution of individual modalities in VL models. MM-SHAP is based on Shapley values, a theoretically well-¬≠ founded interpretability method from cooperative game theory. We apply MM-SHAP to quantify the contribution of specific parts of the input towards model predictions. 
Our main contributions are: 
 * We propose MM-SHAP, a performance-agno¬≠ stic metric to measure the degree of contribution of each modality in VL (but not limited to V& L), to measure the degree to which individual modalities contribute to MM model predictions. We combine MM-SHAP with model accuracy to analyse the degree to which each modality contributes to model predictions. 
 * We use MM-SHAP to 1) compare models in terms of their reliance on different modalities, 2) compare the relevance of different modalities for a given task and dataset, and to 3) zoom in at sample-level to determine the contribution of each modality and each token in each modality for a model prediction (Fig. ). 
 * We conduct experiments with six VL models: LXMERT, CLIP and four ALBEF variants ‚Äì on four VL tasks: image-sentence alignment, VQA, GQA and on the more fine-grained VALSE* <g r a p h i c s> ‚óØ VL benchmark. 
 * We identify VL models that are balanced in their usage of two modalities (CLIP), models that show a higher visual degree (LXMERT) or a stronger textual degree (ALBEF). 
 * We show that 1) fine-tuning a model can affect its MM degree and that 2) current VL models do not all collapse towards the same modality, as reported in recent work, but that directions can differ from model to model."
FIREBALL: A Dataset of Dungeons and Dragons Actual-Play with Structured Game State Information,2305.01528v3,./img_ACL_2023/2305.01528v3.pdf,"Examples of our Utterance to Command task (top), which takes in an utterance and a game state to produce an Avrae command, and State to Narration task (bottom), which produces a narration given a dialogue history and game state information.","Dungeons & Dragons (D& D) is a tabletop roleplaying game with complex natural language interactions between players and hidden state information. Recent work has shown that large language models (LLMs) that have access to state information can generate higher quality game turns than LLMs that use dialog history alone. However, previous work used game state information that was heuristically created and was not a true gold standard game state. We present FIREBALL, a large dataset containing nearly 25,000 unique sessions from real D& D gameplay on Discord with true game state info. We recorded game play sessions of players who used the Avrae bot, which was developed to aid people in playing D& D online, capturing language, game commands and underlying game state information. We demonstrate that FIREBALL can improve natural language generation (NLG) by using Avrae state information, improving both automated metrics and human judgments of quality. Additionally, we show that LLMs can generate executable Avrae commands, particularly after finetuning.","Dungeons & Dragons (D& D) is a tabletop roleplaying game in which players assume the roles of characters in a fantasy adventure. Play is conducted primarily through natural language, with players roleplaying as their characters and describing their actions. Meanwhile another player, the Dungeon Master (DM), controls the fictional story world: setting obstacles, goals, and adventures, controlling monsters, and interpreting players 'actions in the context of the rules of the game. Although the DM makes a lot of the final decisions, the game is ultimately a collaborative storytelling experience. 
Due to its use of natural language as actions, each individual player must maintain a personal understanding of the game world which they build from conversational history and using Theory of Mind. The natural language action space also means that the AI needs the ability to adequately perform tasks such as language generation, language understanding, and planning. 
Although AI' s capabilities in this space are still nascent, have shown that D& D dialog can be improved by adding state information into the input of a large language model (LLM). However, the state information presented in that work was heuristically created using regular expressions and machine learning classifiers. Thus it cannot be considered true ground truth state information. Our work is unique because it provides true ground truth state information. 
We use this data for two tasks: Utterance to Command and State to Narration. In the first task, a model is given a game state and turn of the game (roleplay in natural language), and must predict the corresponding command that matches the intent of the roleplay. The second task is a constrained creative natural language generation task: given a state change resulting from a command execution, generate a narration that describes the results. Figure demonstrates both tasks. 
Our contributions are as follows: 
 
 * We present FIREBALL‚Äîa dataset of over 8M gameplay utterances, 2.1M commands, 1.2M gameplay states, and 160K unique actors (player & non-player characters). This is the first dataset of this size that includes detailed game state and character information for each scenario. 
 * We show that large language models such as GPT-3, can extract relevant information from natural language in order to produce commands that are capable of being run by the game environment. 
 * We demonstrate that LLMs, when finetuned on this dataset, can generate more grounded narrative text compared to language models tuned without game state information. 
By incorporating structured state information into language understanding and NLG tasks, we hope to help pave the way toward more ambitious creative generation goals, such as consistent long-form narrative generation and video games that can convert language input into discrete game actions or generate narrations and dialogues based on the game state."
A fine-grained comparison of pragmatic language understanding in humans and language models,2212.06801v2,./img_ACL_2023/2212.06801v2.pdf,Accuracy for each task. Error bars denote 95%CI. Dashed line indicates task-specific random baseline.,"Pragmatics and non-literal language understanding are essential to human communication, and present a long-standing challenge for artificial language models. We perform a fine-grained comparison of language models and humans on seven pragmatic phenomena, using zero-shot prompting on an expert-curated set of English materials. We ask whether models (1) select pragmatic interpretations of speaker utterances, (2) make similar error patterns as humans, and (3) use similar linguistic cues as humans to solve the tasks. We find that the largest models achieve high accuracy and match human error patterns: within incorrect responses, models favor literal interpretations over heuristic-based distractors. We also find preliminary evidence that models and humans are sensitive to similar linguistic cues. Our results suggest that pragmatic behaviors can emerge in models without explicitly constructed representations of mental states. However, models tend to struggle with phenomena relying on social expectation violations. footnote-1","Non-literal language understanding is an essential part of communication. For example, in everyday conversations, humans readily comprehend the non-literal meanings of metaphors (My new coworker is a block of ice), polite deceits (I love the gift), indirect requests (It 's a bit cold in this room), and irony (Classy pajamas, dude! ). These phenomena fall under the broad label of pragmatics, which encompasses the aspects of meaning that go beyond the literal semantics of what is said. 
A long-standing challenge for NLP is to build models that capture human pragmatic behaviors. The remarkable abilities of modern language models (LMs) have triggered a recent effort to investigate whether such models capture pragmatic meaning, both through philosophical arguments and empirical evaluations. However, prior empirical studies have primarily evaluated LMs based on a binary distinction between pragmatic and non-pragmatic responses, providing limited insights into models' weaknesses. A model could fail to reach the target pragmatic interpretation in multiple ways ‚Äì for example, by preferring a literal interpretation, or by preferring a non-literal interpretation that violates certain social norms. Understanding these error patterns can suggest specific directions for improving the models, and foreshadow where pragmatics might go awry in user-facing settings. 
From a cognitive perspective, understanding the pragmatic abilities of LMs could also offer insights into humans. Human pragmatic language comprehension involves a variety of mechanisms, such as basic language processing, knowledge of cultural and social norms, and reasoning about speakers 'mental states. However, it remains an open question when language understanding relies on explicit mentalizing ‚Äì which may be cognitively effortful ‚Äì versus lower-cost heuristics. Because LMs lack explicit, symbolic representations of mental states, they can serve as a tool for investigating whether pragmatic competence can arise without full-blown mentalizing. 
In this paper, we perform a fine-grained comparison of humans and LMs on pragmatic language understanding tasks. Adopting the approach of targeted linguistic evaluation, our analysis serves two goals: assessing the pragmatic capabilities of modern LMs, and revealing whether pragmatic behaviors emerge without explicitly constructed mental representations. Our test materials are a set of English multiple-choice questions curated by expert researchers, covering seven diverse pragmatic phenomena. We use zero-shot prompting to evaluate models with varying sizes and training objectives: GPT-2, Tk-Instruct, Flan-T5, and InstructGPT. 
Through model analyses and human experiments, we investigate the following questions: (1) Do models recover the hypothesized pragmatic interpretation of speaker utterances? (2) When models do not select the target response, what errors do they make ‚Äì and how do these error patterns compare to those of humans? (3) Do models and humans use similar cues to arrive at pragmatic interpretations? We find that Flan-T5 (XL) and text-davinci-002 achieve high accuracy and mirror the distribution of responses selected by humans. When these models are incorrect, they tend to select the incorrect literal (or straightforward) answer instead of distractors based on low-level heuristics. We also find preliminary evidence that models and humans are sensitive to similar linguistic cues. Our results suggest that some pragmatic behaviors emerge in models without explicitly constructed representations of agents' mental states. However, models perform poorly on humor, irony, and conversational maxims, suggesting a difficulty with social conventions and expectations."
Counterfactual Multihop QA: A Cause-Effect Approach for Reducing Disconnected Reasoning,2210.07138v1,./img_ACL_2023/2210.07138v1.pdf,"Illustration of disconnected reasoning in multi-hop QA, where red node denotes question Q, blue node is a supporting fact S, and orange and green nodes denote the remaining facts C. Deep gray nodes mean their variables are reference values instead of the given values, (e.g., S=s^* instead of S=s). (a): Causal graph of multi-hop QA model; (b): is a possible scenario of disconnected reasoning, which uses only one fact s to answer the question. (c): is another possibility of disconnected reasoning, e.g., the exclusive method to find whether s is a supporting fact by a process of elimination of other facts c. (d): is the true multi-hop reasoning. All facts s and c are taken into considered to produce the answer.","Multi-hop QA requires reasoning over multiple supporting facts to answer the question. However, the existing QA models always rely on shortcuts, e.g., providing the true answer by only one fact, rather than multi-hop reasoning, which is referred as disconnected reasoning problem. To alleviate this issue, we propose a novel counterfactual multihop QA, a causal-effect approach that enables to reduce the disconnected reasoning. It builds upon explicitly modeling of causality: 1) the direct causal effects of disconnected reasoning and 2) the causal effect of true multi-hop reasoning from the total causal effect. With the causal graph, a counterfactual inference is proposed to disentangle the disconnected reasoning from the total causal effect, which provides us a new perspective and technology to learn a QA model that exploits the true multi-hop reasoning instead of shortcuts. Extensive experiments have conducted on the benchmark HotpotQA dataset, which demonstrate that the proposed method can achieve notable improvement on reducing disconnected reasoning. For example, our method achieves 5.8%higher points of its Supp_s score on HotpotQA through true multihop reasoning. The code is available at supplementary material.","Multi-hop question answering (QA) requires the model to reason over multiple supporting facts to correctly answer a complex question. It is a challenging task, and many datasets, e.g., HotpotQA and approaches have been proposed for this reasoning task. 
One of the main problems of multihop QA models is disconnected reasoning, which allows the models to exploit the reasoning shortcuts instead of multi-hop reasoning to cheat and obtain the right answer. Taking Fig. as an example, to answer the question ""until when in the U. S. Senate"", we should consider two supporting facts to infer the answer ""Devorah Adler Director of Research for‚üπ Barack Obama served from 2005 to 2008‚üπ 2008"". However, one may also infer the correct answer by just utilizing the types of problems, e.g., we can find the corresponding fact ""from 2005 to 2008"" in the contexts without reasoning to answer this type of question ""until when"". 
One possible solution for reducing the disconnected reasoning is to strengthen the training dataset via extra annotations or adversarial examples, which make it cannot find the correct answers by only one supporting fact. For example, constructed the adversarial examples to generate better distractor facts. firstly defined a evaluate measure, DiRe in short, to measure how much the QA model can cheat via disconnected reasoning. Then, a transformed dataset is constructed to reduce disconnected reasoning. Besides, counterfactual intervention had also been explored to change the distribution of the training dataset. These methods improve the generalizability and interpretability of the multi-hop reasoning QA model via balancing the train data, which is noted as debiased training in QA model. However, when the existing approaches decrease the disconnected reasoning, the original performance also drops significantly. It is still challenging to reduce disconnected reasoning while maintaining the same accuracy on the original test set. 
Motivated by causal inference, we utilize the counterfactual reasoning to reduce the disconnected reasoning in multi-hop QA and also obtain the robust performance on the original dataset. We formalize a causal graph to reflect the causal relationships between question (Q), contexts and answer (Y). To evaluate the disconnected reasoning, contexts are further divided into two subsets: S is a supporting fact and C are the remaining supporting facts. Hence, we can formulate the disconnected reasoning as two natural direct causal effects of (Q, S) and (Q, C) on Y as shown in Fig. . With the proposed causal graph, we can relieve the disconnected reasoning by disentangling the two natural direct effects and the true multi-hop reasoning from the total causal effect. A novel counterfactual multihop QA is proposed to disentangle them from the total causal effect. We utilize the generated probing dataset proposed by and DiRe to measures how much the proposed multi-hop QA model can reduce the disconnected reasoning. Experiment results show that our approach can substantially decrease the disconnected reasoning while guarantee the strong performance on the original test set. The results indicate that the proposed approach can reduce the disconnected reasoning and improve the true multi-hop reasoning capability. 
The main contribution of this paper is threefold. Firstly, our counterfactual multi-hop QA model formulates disconnected reasoning as two direct causal effects on answer, which is a new perspective and technology to learn the true multi-hop reasoning. Secondly, our approach achieves notable improvement on reducing disconnected reasoning compared to various state-of-the-arts. Thirdly, our causal-effect approach is model-agnostic and can be used for reducing disconnected reasoning in many multi-hop QA architectures."
CAME: Confidence-guided Adaptive Memory Efficient Optimization,2307.02047v2,./img_ACL_2023/2307.02047v2.png,"Visualization of Non-negative Matrix Factorization (NMF). Generally, NMF reduces the memory requirements from O (nm) to O (n + m). In this paper, we focus on the special case of rank-1 factors.","Adaptive gradient methods, such as Adam and LAMB, have demonstrated excellent performance in the training of large language models. Nevertheless, the need for adaptivity requires maintaining second-moment estimates of the per-parameter gradients, which entails a high cost of extra memory overheads. To solve this problem, several memory-efficient optimizers (e.g., Adafactor) have been proposed to obtain a drastic reduction in auxiliary memory usage, but with a performance penalty. In this paper, we first study a confidence-guided strategy to reduce the instability of existing memory efficient optimizers. Based on this strategy, we propose CAME to simultaneously achieve two goals: fast convergence as in traditional adaptive methods, and low memory usage as in memory-efficient methods. Extensive experiments demonstrate the training stability and superior performance of CAME across various NLP tasks such as BERT and GPT-2 training. Notably, for BERT pre-training on the large batch size of 32,768, our proposed optimizer attains faster convergence and higher accuracy compared with the Adam optimizer. The implementation of CAME is publicly available.","Robust training of large language models (LLMs) often relies on adaptive gradient-based optimization methods. Through the use of cumulative second-order statistics, these methods adapt the per-parameter learning rate and demonstrate superior convergence speed during the training process of LLMs. However, the remarkable performance of adaptive methods incurs an extra cost of memory usage indeed. For example, Adam requires to preserve the first moment estimate and second raw moment estimate of each gradient in order to tune the learning rate for each parameter, which inevitably triples the memory usage concerning the optimizer states. Besides, with the growing size of the model, LLMs are becoming increasingly expensive in terms of memory, and the limitation of memory is gradually emerging as a main bottleneck for training LLMs. 
Many existing memory-efficient optimizers attempt to store second-order statistics with sublinear memory requirement while retaining the exceptional convergence property of adaptivity. Adafactor optimizer achieves remarkable memory cost reduction by applying the non-negative matrix factorization algorithm to factorize the accumulator matrix for squared gradients into two rank-1 factors as shown in Figure, where the memory requirement for the original matrix V decreases from O (nm) to O (n+m). Whereas, it is observed that Adafactor suffers a performance degradation in the training of large language models universally compared with conventional adaptive gradient-based optimization methods. The reason for this phenomenon is Adafactor inevitably introduces some errors that cause instability in training deep networks due to the operation of non-negative matrix factorization. 
In addition, in the case of large-batch training that aims to accelerate the training of deep neural networks, the memory consumption of each machine (GPU/TPU) is much higher than general batch size training, which further imposes a grave constraint on the performance of the trained model. In comparison to standard training tasks, large-batch training presents more challenges for optimizers. Empirically, when the mini-batch size increases after a certain point (e.g.,1024), the test accuracy of the converged solution decreases significantly compared with the baseline. To our knowledge, there is currently no work related to memory-efficient optimizers for large-batch training. 
Motivated by these challenges, we firstly study a confidence-guided strategy catered to alleviate the instability of Adafactor by calculating the confidence of the generated update at each training step. On the basis of the adaptation strategy, we propose a novel CAME optimizer that saves nearly the same memory footprint as existing memory-efficient optimizers while attaining faster convergence and superior generalization performance. To further assess the scalability of our proposed algorithm, we consider an additional challenging experiment - performing large-batch training on BERT using CAME optimizer. 
Contributions of our paper can be summarized in the following: 
 
 * Inspired by training instability of Adafactor, we explore a confidence-guided strategy centered on the existing error in the raw updates of Adafactor for parameters of large language models. 
 * In light of the dedicated strategy, we propose a novel optimization algorithm, CAME, for achieving faster convergence and less performance degradation catered at memory-efficient optimization. We further investigate the effect of the proposed memory-efficient optimization algorithm in large-batch training settings. 
 * We demonstrate the powerful performance of CAME with extensive NLP experiments: CAME shows faster convergence and better generalization capability than Adam in BERT pre-training task with two different batch sizes (32k and 8k); in the training of GPT-2 model and T5 model, CAME achieves fast convergence speed as Adam without degrading of performance. Notably, in the large-batch training of the BERT model, CAME obtains comparable validation accuracy with LAMB using around 15%less memory usage."
Early Discovery of Disappearing Entities in Microblogs,2210.07404v1,./img_ACL_2023/2210.07404v1.pdf,"Time-sensitive distant supervision: For the entities retrieved from a KB, disappearing contexts and other contexts are collected from microblogs by utilizing the year of entities' disappearance. We then train a sequence labeling model using the obtained contexts.","We make decisions by reacting to changes in the real world, in particular, the emergence and disappearance of impermanent entities such as events, restaurants, and services. Because we want to avoid missing out on opportunities or making fruitless actions after they have disappeared, it is important to know when entities disappear as early as possible. We thus tackle the task of detecting disappearing entities from microblogs, whose posts mention various entities, in a timely manner. The major challenge is detecting uncertain contexts of disappearing entities from noisy microblog posts. To collect these disappearing contexts, we design time-sensitive distant supervision, which utilizes entities from the knowledge base and time-series posts, for this task to build large-scale Twitter datasets for English and Japanese. To ensure robust detection in noisy environments, we refine pretrained word embeddings of the detection model on microblog streams of the target day. Experimental results on the Twitter datasets confirmed the effectiveness of the collected labeled data and refined word embeddings; more than 70%of the detected disappearing entities in Wikipedia are discovered earlier than the update on Wikipedia, and the average lead-time is over one month.","Our daily actions depend on the state of the real world and its changes, especially changes in the entities available to us. Among the various changes, the beginning of entities, or emerging entities such as new songs, movies, and events, are useful for understanding the trends in interests. At the same time, users need to be made aware of the end or disappearance of entities, such as closing stores or discontinuing services, as soon as possible so that they can avoid missing out on opportunities or taking fruitless actions after the entities are no longer available. It is also important to collect these entities to maintain knowledge bases (KBs) in which information about the entities is accumulated. Studies on discovering out-of-KB or emerging entities have been successful to an extent, as most of emerging entities have distinct names and can be characterized by mentions to their unseen names. In contrast, disappearing entities are not clearly characterized by their mentions, since they continue to be mentioned even after they disappear. 
Given such a situation, we take on the new task of discovering disappearing entities from microblogs where news and personal experiences are widely shared. To detect the entities' disappearance, we exploit the specific expressions that people use when mentioning disappearing entities in microblogs (Table). By capturing these contexts, we can discover a variety of entities in the early stage even before they disappear. To develop a dataset of disappearing entities and contexts, we use time-sensitive distant supervision, which collects specific contexts of entities by utilizing KB entities and timestamps of microblogs. Because this method requires the timing of the desired contexts, we extract the year of disappearance for each entity described in Wikipedia and incorporate it into the distant supervision. 
We train a named entity recognition (NER) model on the collected entities and contexts to discover disappearing entities. However, the NER model performs poorly for microblogs where posts are short and noisy, and the training data collected by the distant supervision contains noise, making it difficult to train a reliable model. We address this issue by considering that multiple posts are likely to mention the target entity when it disappears in the real world. Concretely, we utilize these posts to refine pretrained word embeddings and incorporate them into the NER model. This enables the model to consider the tokens that frequently appear among multiple posts and to recognize disappearing entities robustly. 
We built large-scale English and Japanese datasets from Twitter using the proposed time-sensitive distant supervision method. The experimental results demonstrated that the proposed method outperformed the baseline which simply collected the latest burst of posts about the disappearing entities as the disappearing contexts using time-sensitive distant supervision and used them to train the NER model. In addition, the evaluation of relative recall indicated that our method successfully found more than 70%of the target disappearing entities in Wikipedia. Except for entities like persons, whose Wikipedia articles were updated with little or no delay, our method detected entities such as services, facilities, and events on average more than 100 days earlier than the update of the disappearance in Wikipedia."
Lifting the Curse of Capacity Gap in Distilling Language Models,2305.12129v1,./img_ACL_2023/2305.12129v1.pdf,GLUE v. s. GFLOPs.,"Pretrained language models (LMs) have shown compelling performance on various downstream tasks, but unfortunately they require a tremendous amount of inference compute. Knowledge distillation finds a path to compress LMs to small ones with a teacher-student paradigm. However, when the capacity gap between the teacher and the student is large, a curse of capacity gap appears, invoking a deficiency in distilling LMs. While a few studies have been carried out to fill the gap, the curse is not yet well tackled. In this paper, we aim at lifting the curse of capacity gap via enlarging the capacity of the student without notably increasing the inference compute. Largely motivated by sparse activation regime of mixture of experts (MoE), we propose a mixture of minimal experts (MiniMoE), which imposes extra parameters to the student but introduces almost no additional inference compute. Experimental results on GLUE and CoNLL demonstrate the curse of capacity gap is lifted by the magic of MiniMoE to a large extent. MiniMoE also achieves the state-of-the-art performance at small FLOPs compared with a range of competitive baselines. With a compression rate as much as ‚àº50√ó, MiniMoE preserves ‚àº95%GLUE score of the teacher.","Pretrained language models (LMs) have become a popular choice for various downstream tasks, e.g., text classification, token classification, and question answering. Unfortunately, appealing performance comes with a huge cost of inference compute due to the scale of LMs. Knowledge distillation, as an alternative to model pruning and quantization, discovers a way to compress LMs with a teacher-student paradigm. 
However, in LM distillation, we recognize a curse of capacity gap as: ""Large teachers, poor students. "" The curse of capacity gap refers to a deficiency that a larger teacher might unexpectedly result in a poorer student especially when the capacity gap between the teacher and the student is large, as illustrated in Table. Notably, this is the first verification in LM distillation since previous studies recognize the curse in vision model distillation. Although a few studies have investigated to fill the gap, the curse is still not yet tackled. 
To the demand, we aim at lifting the curse of capacity gap via enlarging the capacity of the student without notably increasing the inference compute. We propose a mixture of minimal experts (MiniMoE), inspired by the intuition of sparse activation of mixture of experts (MoE). Thanks to that the activation process can be parallel on either single or multiple devices, MiniMoE on the one hand imposes extra parameters to the student, but on the other hand introduces negligibly additional inference compute brought by routing algorithm. To our best knowledge, this is the first work aiming at lifting the curse completely. 
Experiments are conducted on GLUE and CoNLL. The results exhibit that MiniMoE largely lifts the curse of the gap as in Table. MiniMoE also achieves state-of-the-art performance compared with a range of competitive baselines, as shown in Figure. With compression as much as ‚àº50√ó, MiniMoE preserves √ó95%GLUE score of the teacher. Thereby, we state that MiniMoE is a small yet nontrivial magic, making a great difference in lifting the curse."
Prompter: Zero-shot Adaptive Prefixes for Dialogue State Tracking Domain Adaptation,2306.04724v1,./img_ACL_2023/2306.04724v1.pdf,"Zero-shot domain adaptation. The model is trained on four source domains and tested on the train-booking domain without any supervised training. Bottom-left: T5 baseline predictions, Bottom-right: Prompter predictions. (Correct, incorrect) predictions are colored (green, red), respectively.","A challenge in the Dialogue State Tracking (DST) field is adapting models to new domains without using any supervised data ‚Äî zero-shot domain adaptation. Parameter-Efficient Transfer Learning (PETL) has the potential to address this problem due to its robustness. However, it has yet to be applied to the zero-shot scenarios, as it is not clear how to apply it unsupervisedly. 
Our method, Prompter, uses descriptions of target domain slots to generate dynamic prefixes that are concatenated to the key and values at each layer 's self-attention mechanism. This allows for the use of prefix-tuning in zero-shot. Prompter outperforms previous methods on both the MultiWOZ and SGD benchmarks. In generating prefixes, our analyses find that Prompter not only utilizes the semantics of slot descriptions but also how often the slots appear together in conversation. Moreover, Prompter' s gains are due to its improved ability to distinguish ""none"" -valued dialogue slots, compared against baselines.","Task-oriented dialogue (TOD) systems serve users through several tasks, such as booking a table in a restaurant or suggesting tourist attractions. One crucial component of these systems, Dialogue State Tracking (DST), is responsible for extracting users 'preferences (i.e.,slot-values) over key attributes (i.e.,slot-labels) of their service. 
DST has a significant role in TOD systems as it ensures that both the action taken in the back-end and the responses returned to the users are aligned with the preferences that the users indicate. 
A challenging task in this field is to adapt an existing DST model to a new domain it has not seen before without using any supervised data, i.e.,in the zero-shot scenario. This is important, as in many new scenarios, it is hard to collect data, let alone annotate it. Yet it is still an essential need for a TOD system to appropriately answer such queries in new contexts. The challenge arises from the differences in dialogue context, slot values, and slot labels among different domains. For example, a model could be trained on the' taxi-booking 'domain and thus capable of extracting the destination for a taxi; but when deployed to the' train-booking 'domain, the range of slot-values changes, resulting in a higher probability of a mistaken inference. We show an example (), where due to the superficial connections a baseline T5 model forms, it incorrectly predicts' Ashley Hotel 'as the train destination (bottom left). In many dialogue contexts, a large number of slots are unspecified. These are known as ""none"" -valued slots. In cases where the model is adapting to a new domain without any prior training, it often incorrectly predicts none values. This makes it even more important to address the problem of domain shift. 
 proposed to address this domain shift challenge via the language model' s intrinsic ability to reason over prompts. Specifically, they concatenate the description of each slot as a hard prompt into the dialogue context and then generate the answers using the T5 model. While it does well for a naive baseline, it makes mistakes due to its superficial understanding of slot labels. 
Meanwhile, another line of study has shown that Parameter-efficient Transfer Learning (PETL) methods are effective training methods to address domain shift. Due to the small number of parameters it introduces per task/instance, it overcomes overfitting in few-shot scenarios, outperforming earlier baselines. There have been various attempts to use these methods for DST tasks within a few-shot, continual learning setting. However, a significant barrier to adopting PETL is that such methods cannot be directly applied in zero-shot, as they all require some form of supervised training. 
In this study, we propose a new method to use prefix-tuning under a zero-shot scenario to benefit from the gains it brings for robustness, 
even without supervised data. Rather than fine-tuning the prefixes during training, we add a new mechanism into the T5 architecture called Prompter. Prompter simply takes the description of the slot and then generates the prefixes on the fly. We then append these prefixes at each layer of the encoder to represent the dialogue from the perspective of the subject slot label. This method makes minimal changes to LM parameters while generating unsupervised prefixes. This ensures both the preservation of general-purpose traits and extrapolation to new domains. 
We conduct experiments with the MultiWOZ 2.1 and SGD datasets. 
Prompter improves average JGA results across domains by 1.7 for MultiWOZ, and 9.1 points for the SGD dataset (considering 4 domains reported in prior studies) compared to the strongest baseline. This shows that PETL methods' robustness advantage is also favorable for unsupervised domain adaptation scenarios. To the best of our knowledge, these are the highest results achieved so far using a small language model. 
Through further analysis, we have discovered that Prompter not only considers the semantic similarities of slot descriptions but also the frequencies in which slots co-appear in the dialogue context. Furthermore, Prompter proves to be more effective in identifying slots that have no value within a conversation in comparison to previous methods."
Enhancing Dialogue Generation via Dynamic Graph Knowledge Aggregation,2306.16195v1,./img_ACL_2023/2306.16195v1.png,Conventional GNNs vs. Ours.,"Incorporating external graph knowledge into neural chatbot models has been proven effective for enhancing dialogue generation. However, in conventional graph neural networks (GNNs), message passing on a graph is independent from text, resulting in the graph representation hidden space differing from that of the text. This training regime of existing models therefore leads to a semantic gap between graph knowledge and text. In this study, we propose a novel framework for knowledge graph enhanced dialogue generation. We dynamically construct a multi-hop knowledge graph with pseudo nodes to involve the language model in feature aggregation within the graph at all steps. To avoid the semantic biases caused by learning on vanilla subgraphs, the proposed framework applies hierarchical graph attention to aggregate graph features on pseudo nodes and then attains a global feature. Therefore, the framework can better utilise the heterogeneous features from both the post and external graph knowledge. Extensive experiments demonstrate that our framework outperforms state-of-the-art (SOTA) baselines on dialogue generation. Further analysis also shows that our representation learning framework can fill the semantic gap by coagulating representations of both text and graph knowledge. Moreover, the language model also learns how to better select knowledge triples for a more informative response via exploiting subgraph patterns within our feature aggregation process. Our code and resources are available at <https: //github. com/tangg555/SaBART>.","Recent years have seen a surge of interest in developing chatbots with the facilitation of large-scale knowledge. As a highly expressive data format, Knowledge Graphs (e.g.,ConceptNet and DBpedia), which include world facts, are considered to be a key factor in building an effective dialogue generation system. In order to incorporate graph-structured knowledge, a range of Graph Neural Networks such as Graph Attention Networks (GATs) and Graph Convolutional Networks (GCNs) have been proposed to learn representations of the topological structure of the knowledge graph via message passing between entities. In open-domain dialogue generation, these GNNs are further embedded into generative frameworks to feed graph knowledge features into the language models (LMs). 
Despite prior success in leveraging graph knowledge with graph neural networks (GNN), current generative frameworks are still hindered by the representation gap in the hidden space between the LMs and GNNs, which poses significant challenges in exploiting graph knowledge in the subsequent text decoding process. As illustrated in, prior works using GNNs tend to fuse the graph features by transforming them into text form and then feeding them into the language model, which acts as a ""copy"" mechanism. In other words, these networks run as a pipeline where the graph knowledge is firstly transformed into additional text to avoid the problem of language model encoding brought about by the heterogeneous graph features. However, these separate encoding stages result in neural networks learning suboptimal representations of graph knowledge, which leads to information loss. With large-scale pretrained models such as GPT-2, BART and T5 being widely adopted in recent advances in dialogue generation, the drawbacks that arise from incompatibility between GNNs and LMs becomes a more severe problem, prohibiting chatbot systems from leveraging graph structured data effectively. 
In order to address the aforementioned challenges, we propose a novel representation learning framework to facilitate language understanding and generation, which permits effective incorporation of heterogeneous features via a dynamic graph knowledge aggregation mechanism. In contrast to existing works which incorporate graph knowledge with conventional GNNs (causing inadequacies in representation learning), we propose to involve language models in both text and graph knowledge incorporation at all steps via hierarchically aggregating knowledge on a dynamic pseudo graph. During the knowledge aggregation process, knowledge triples are reorganised as shown in (b), where pseudo nodes are created to learn conceptual representations from original knowledge triples. Conceptual semantics are forced to coagulate into pseudo nodes, and finally merge into a condensed feature vector to fill the semantic gap of the encoded text features. Our approach for incorporating text and graph knowledge features can be adapted to all language models with an encoder-decoder architecture. In this study, we choose BART, a SOTA language model for generation, as our language model in our experiments. This framework will hereinafter be referred to as SaBART (Subgraph-Aggregation BART). 
During subgraph knowledge aggregation, the language model is involved in learning three levels of features: (1) Subword-level, where conceptual embeddings are connected to entity mentions within text; (2) Knowledge-level, where original triples are transformed by language encoding; and (3) Semantic-level, where the context vector encoded from text is involved in knowledge aggregation. This implies that the neural networks are able to access both the text and graph features during representation learning. The text and graph unified encoding process also avoids the information loss caused by the representation shift in vanilla GNNs, thus improving efficiency and efficacy. Extensive experiments demonstrate that our proposed framework significantly outperforms current SOTA baselines in dialogue generation. We also conduct in-depth analysis into the underlying mechanism of why our proposed approach better incorporates heterogeneous features. Our contributions can be summarised as follows: 
 
 * We propose a novel representation learning framework where graph and text features can be effectively aggregated via hierarchical knowledge aggregation on a dynamically constructed pseudo graph; 
 * We conduct a comprehensive set of experiments to demonstrate the effectiveness of our proposed approach, where our framework achieves SOTA performance on the commonsense knowledge graph enhanced dialogue generation dataset; 
 * We conduct in-depth experiments to analyse the improvement of representation learning on both graph and text knowledge, and investigate the mechanism to address this representation gap problem of learning heterogeneous features."
Query Structure Modeling for Inductive Logical Reasoning Over Knowledge Graphs,2305.13585v1,./img_ACL_2023/2305.13585v1.pdf,Examples of inductive logical reasoning over KGs: testing queries contain unseen entities and relations (in red) during training. Each query is associated with an intrinsic logical structure and its natural language interpretation.,"Logical reasoning over incomplete knowledge graphs to answer complex logical queries is a challenging task. With the emergence of new entities and relations in constantly evolving KGs, inductive logical reasoning over KGs has become a crucial problem. However, previous PLMs-based methods struggle to model the logical structures of complex queries, which limits their ability to generalize within the same structure. In this paper, we propose a structure-modeled textual encoding framework for inductive logical reasoning over KGs. It encodes linearized query structures and entities using pre-trained language models to find answers. For structure modeling of complex queries, we design stepwise instructions that implicitly prompt PLMs on the execution order of geometric operations in each query. We further separately model different geometric operations (i.e., projection, intersection, and union) on the representation space using a pre-trained encoder with additional attention and maxout layers to enhance structured modeling. We conduct experiments on two inductive logical reasoning datasets and three transductive datasets. The results demonstrate the effectiveness of our method on logical reasoning over KGs in both inductive and transductive settings.","Logical reasoning over knowledge graphs (KGs) aims to answer complex logical queries given large-scale KGs. Recent years have witnessed increasing attention on logical reasoning over widely used KGs such as Freebase, Yago, NELL and Wikidata. With missing relations in the KG, it is challenging to deduce correct answers to complex queries by traversing the graph. Previous work primarily focuses on transductive logical reasoning where the training and testing are done on the same KG with the same group of entities. They typically rely on geometric embedding-based methods to map both entities and queries into a joint low-dimensional vector space. The goal is to push the embeddings of answer entities and queries to be close to each other, allowing answers to be predicted through embedding similarity even when the involved relation is absent. In contrast, the inductive setting of logical reasoning has been rarely studied which requires generalizing to unseen entities and relations or even new KGs. As real-world KGs are usually dynamic with emerging unseen entities and relations, it's significant to explore the inductive setting for complex query answering. 
Existing research on inductive logical reasoning mainly follows two directions. The first inherits embedding-based methods and incorporates type as additional information to improve inductive capability, which can not generalize to unseen types of entities and relations. The second direction leverages pre-trained language models (PLMs) to encode textual information of entities/relations for generalization to unseen elements. PLMs-based approaches provide more flexible solutions and generate better results. However, they only explore link prediction tasks of one-step reasoning, and simply linearize the triplet or subgraph into text sequence without modeling explicit reasoning structure. An example is shown in Figure. Two findings stand out. (1) The query q_1 and q_2 appear to be similar in format (both as a conjunction of three terms) but actually have different logical structures. PLMs-based methods that encode flattened queries can not model this structure information for correct logical reasoning. (2) Although queries q_1 and q_3 (also q_2 and q_4) contain different elements, they share the same logical structure. Motivated by these, we argue that structure modeling of different complex queries can further boost the generalization ability of logical reasoners. 
In this paper, we propose to model query structure for inductive logical reasoning over KGs. Specifically, we transform the query structure into a sequence using textual names of involved entities, relations, and logical operators. For complex query structures composed of multiple geometric operations over entities and relations, we introduce two measures to enable logical structure modeling during text encoding. First, we design stepwise instructions for different query types to indicate which operation in the query structure should be conducted at each step and feed them as the structural prompt to PLMs. Besides, we extend the pre-trained encoder with an additional attention layer and a maxout layer to respectively model different geometric operations including projection, intersection, and union on the representation space, to implicitly inject structured modeling into PLMs. Our proposed method is a generic inductive framework, which can be plugged into different PLMs for better performance. 
We conduct experiments on two datasets for inductive logical reasoning over KGs, FB15k-237-V2 and NELL-V3 as well as three transductive datasets, FB15k, FB15k-237, and NELL995. The results demonstrate that our method achieves strong inductive performance on unseen entities and relations, even across different KGs, without sacrificing logical reasoning capability and generalizability to new query structures."
DimonGen: Diversified Generative Commonsense Reasoning for Explaining Concept Relationships,2212.10545v2,./img_ACL_2023/2212.10545v2.pdf,An example of DimonGen. The input is a pair of concepts and the output is a set of sentences that capture different ways in which these concepts interact.,"In this paper, we propose DimonGen, which aims to generate diverse sentences describing concept relationships in various everyday scenarios. To support this, we first create a benchmark dataset for this task by adapting the existing CommonGen dataset. We then propose a two-stage model called MoREE to generate the target sentences. MoREE consists of a mixture of retrievers model that retrieves diverse context sentences related to the given concepts, and a mixture of generators model that generates diverse sentences based on the retrieved contexts. We conduct experiments on the DimonGen task and show that MoREE outperforms strong baselines in terms of both the quality and diversity of the generated sentences. Our results demonstrate that MoREE is able to generate diverse sentences that reflect different relationships between concepts, leading to a comprehensive understanding of concept relationships.","Concepts are mental representations of classes or categories of objects, events, or ideas, distinguished by shared characteristics that set them apart from other things. For instance, the concept of ""dog"" represents a class of animals that share characteristics such as being four-legged, having fur, and being domesticated. These concepts are crucial in helping us understand and communicate about the world around us. 
To fully grasp concepts, it is important to understand the relationships between them. Researchers have proposed using generated sentences as a means to model these relationships more effectively. For example, CommonGen aims to generate coherent sentences that describe everyday scenarios involving specific sets of common concepts, while Open Relation Modeling generates informative sentences that describe relationships between concepts/entities. 
However, in real-world scenarios, concepts often refer to broad classes, and their relationships can be complex. This can make it challenging to summarize these relationships through a single sentence. For example, ""dog"" and ""sheep"" are both animal concepts, but while ""dogs"" can herd ""sheep"", they can also attack them. A single sentence would not accurately convey this complexity, leading to an insufficient understanding. Additionally, this approach can also introduce bias, particularly when concepts are related to sensitive topics such as gender or race. For instance, the statement ""women are better suited for caregiving roles than men. "" is a biased statement. 
To mitigate the above issues, we propose a new task called DimonGen: Diversified Generative Commonsense Reasoning. The task involves generating diverse sentences that describe the relationships between two given concepts, such as the example shown in Fig. of the concept pair ""dog"" and ""sheep"". This helps build a comprehensive and diverse understanding of the relationships between the concepts in various everyday scenarios. 
DimonGen is a challenging task because it requires generating reasonable scenarios for a given pair of concepts without any context. This requires a deep understanding of relational and commonsense knowledge about the concepts. Additionally, the target outputs must reflect diverse relationships between the input concepts. Previous approaches to generating diverse content have used sampling from a designed vocabulary distribution or encoding inputs to various latent variables. However, these methods introduce diversity only at the generation stage which may not be suitable for the DimonGen task as it relies on the semantic information from the input contexts. 
To overcome the challenges, we propose MoREE: Mixture of Retrieval-Enhanced Experts, a two-stage method that utilizes external knowledge to generate diverse relationship sentences. In the first stage, MoREE retrieves diverse context sentences related to the given concepts using a mixture of retrievers model based on the Mixture of Experts (MoE) model. In the second stage, MoREE generates diverse relationship sentences conditioned on the retrieved contexts using a mixture of generators model. An Expectation-Maximization (EM) based matching algorithm is proposed to combine the two stages. By extracting diverse contexts from corporas before generation, MoREE aims to improve the diversity and quality of the generated relationship sentences. 
We build a benchmark dataset for DimonGen by adapting the existing CommonGen benchmark and conduct both quantitative and qualitative experiments on the dataset. The results indicate that our proposed MoREE model outperforms well-designed baselines in terms of both the quality and diversity of the generated sentences. For example, in the automatic evaluation, our method gains over 2%in the BLEU-4 score for quality and around 5%in Self-BLEU-4 for diversity. And in our human evaluation, the annotated score (up to 5) for quality increases from 3.77 to 4.21, and for diverse increases from 3.65 to 3.94. We also conduct detailed ablation studies and case studies to further verify the effectiveness of our proposed method. Overall, the results suggest that MoREE can generate diverse sentences that reflect relationships between concepts from multiple and varied perspectives."
Incorporating Attribution Importance for Improving Faithfulness Metrics,2305.10496v1,./img_ACL_2023/2305.10496v1.png,Hard and soft erasure criteria for comprehensiveness and sufficiency for two toy feature attribution (FA) methods A and B.,"Feature attribution methods (FAs) are popular approaches for providing insights into the model reasoning process of making predictions. The more faithful a FA is, the more accurately it reflects which parts of the input are more important for the prediction. Widely used faithfulness metrics, such as sufficiency and comprehensiveness use a hard erasure criterion, i.e.,entirely removing or retaining the top most important tokens ranked by a given FA and observing the changes in predictive likelihood. However, this hard criterion ignores the importance of each individual token, treating them all equally for computing sufficiency and comprehensiveness. In this paper, we propose a simple yet effective soft erasure criterion. Instead of entirely removing or retaining tokens from the input, we randomly mask parts of the token vector representations proportionately to their FA importance. Extensive experiments across various natural language processing tasks and different FAs show that our soft-sufficiency and soft-comprehensiveness metrics consistently prefer more faithful explanations compared to hard sufficiency and comprehensiveness.","Feature attribution methods (FAs) are popular post-hoc explanation methods that are applied after model training to assign an importance score to each token in the input. These scores indicate how much each token contributes to the model prediction. Typically, the top-k ranked tokens are then selected to form an explanation, i.e.,rationale. However, it is an important challenge to choose a FA for a natural language processing (NLP) task at hand since there is no single FA that is consistently more faithful. 
To assess whether a rationale extracted with a given FA is faithful, i.e.,actually reflects the true model reasoning, various faithfulness metrics have been proposed. Sufficiency and comprehensiveness, also referred to as fidelity metrics, are two widely used metrics which have been found to be effective in capturing rationale faithfulness. Both metrics use a hard erasure criterion for perturbing the input by entirely removing (i.e.,comprehensiveness) or retaining (i.e.,sufficiency) the rationale to observe changes in predictive likelihood. 
However, the hard erasure criterion ignores the different importance of each individual token, treating them all equally for computing sufficiency and comprehensiveness. Moreover, the hard-perturbed input is likely to fall out of the distribution the model is trained on, leading to inaccurate measurements of faithfulness. Figure shows an example of two toy FAs, A and B, identifying the same top two tokens (""like"", ""movie"") as a rationale for the prediction. Still, each of them assigns different importance scores to the two tokens resulting into different rankings. According to the hard erasure criterion, comprehensiveness and sufficiency will assign the same faithfulness score to the two rationales extracted by the two FAs. 
In this paper, we aim to improve sufficiency and comprehensiveness in capturing the faithfulness of a FA. We achieve this by replacing the hard token perturbation with a simple yet effective soft erasure criterion (see Figure for an intuitive example). Instead of entirely removing or retaining tokens from the input, we randomly mask parts of token vector representations proportionately to their FA importance. 
Our main contributions are as follows: 
 
 * We propose two new faithfulness metrics, soft-comprehensiveness and soft-sufficiency that rely on soft perturbations of the input. Our metrics are more robust to distribution shifts by avoiding entirely masking whole tokens; 
 * We demonstrate that our metrics are consistently more effective in terms of preferring more faithful rather than unfaithful (i.e.,random) FAs, compared to their ""hard"" counterparts across various NLP tasks and different FAs. 
 * We advocate for evaluating the faithfulness of FAs by taking into account the entire input rather than manually pre-defining rationale lengths."
Hidden Schema Networks,2207.03777v2,./img_ACL_2023/2207.03777v2.png,"Left: Hidden Schema Network model. Center: Decoder architecture as a modified GPT-2 with M layers and pseudo-self-attention mechanism to attend to the schema ùêû_j_1: j_L. This schema is defined as a sequence of L integers, each of which is represented as a one-hot vector. The ""c"" operations labels concatenation. Right: Encoder architecture as BERT, followed by a single Transformer block. In both center and right figure purple (pink) shaded blocks represent submodules with pretrained (randomly initialized) parameters.","Large, pretrained language models infer powerful representations that encode rich semantic and syntactic content, albeit implicitly. In this work we introduce a novel neural language model that enforces, via inductive biases, explicit relational structures which allow for compositionality onto the output representations of pretrained language models. Specifically, the model encodes sentences into sequences of symbols (composed representations), which correspond to the nodes visited by biased random walkers on a global latent graph, and infers the posterior distribution of the latter. We first demonstrate that the model is able to uncover ground-truth graphs from artificially generated datasets of random token sequences. Next, we leverage pretrained BERT and GPT-2 language models as encoder and decoder, respectively, to infer networks of symbols (schemata) from natural language datasets. Our experiments show that (i) the inferred symbols can be interpreted as encoding different aspects of language, as e.g.,topics or sentiments, and that (ii) GPT-like models can effectively be conditioned on symbolic representations. Finally, we explore training autoregressive, random walk ""reasoning"" models on schema networks inferred from commonsense knowledge databases, and using the sampled paths to enhance the performance of pretrained language models on commonsense If-Then reasoning tasks.","Much of the developmental and causal theories of human cognition are predicated on relational structures of knowledge that naturally exhibit compositionality. Semantic content is intrinsically relational, as one is only able to explain a given unit of knowledge ‚Äì such as a concept, word or perception ‚Äì insofar as there are other units of knowledge which relate to it. Thus we can partially construe a concept through its relationships to other concepts (like when we say ""a dog is an animal that barks""), just as we can partially construe it through its relationships to our perceptions (when we say ""that is a dog"", whilst pointing to a dog on the street) or the words we use (when we use the word dog to refer to the concept dog). Likewise, we can partially construe words not only through their relationships to concepts or percepts, but also through their relationships to other words, as words that occur in the same context tend to have similar meanings. Note that is precisely this contextual semantic word content what we explicit have access to, when processing our raw text datasets. On the other hand, generalization, reasoning and understanding seem to be inevitably tied to the compositional nature of knowledge. Indeed, the ability to compose a set of knowledge units (and their relations) into new, more complex relational units, which can be deployed to understand and reason about unseen data ‚Äì a feature usually referred to as combinatorial generalization ‚Äì is regarded as key to human-level intelligence. Relational structures allowing for compositionality thus seem to comprise not a sufficient, but a necessary attribute of any representation scheme that strives for the generalization power of human cognition. 
From the computational side, if one is to inform any modern language model with such structural characteristics, one will initially encounter the problem of finding suitable primitives or data structures. Distributed continuous word representations, for example, are routinely leveraged in many different downstream tasks. These representations are trained to encode average contextual semantics ‚Äì precisely the kind of semantic content typical of word co-occurrence relations we mentioned above ‚Äì into a semantic space, which allows meaning to change continuously within it. Yet, despite earlier attempts, it is unclear whether such representations can be meaningfully composed into representations of, say, unseen sentences and thus mimic the compositional character of natural language. More recently, contextualized continuous word representations inferred by deep learning architectures have shown spectacular results in many NLP tasks. Their success stems from those models' ability to infer flexible representations through, inter alia, raw, massive datasets, data-scalable attention mechanisms and minimal inductive biases. These representations are known to not only contain rich contextual word semantics, but also consistently encode sentence-level grammar, and the models from which they are obtained have been shown to implement different notions of compositionality too. Nevertheless, it is still unclear whether such representations can be composed into representations of novel sentences. In fact, most of their syntactic properties are implicit and therefore inferred only a posteriori, typically through probes which neither guarantee their presence, nor establish how they were obtained in the first place. 
Roughly following the program outlined by, we develop a novel, neural language model ‚Äì the Hidden Schema Network model (HSN) ‚Äì that enforces, via inductive biases, a discrete, relational structure for sentence representation which allows for compositionality onto the output representations of large, pretrained language models (LPLM). Using a variational autoencoder (VAE) framework, HSN leverages LPLMs to encode natural language sentences into sequences of symbols, which correspond to the nodes visited by biased random walkers on a global latent graph, while inferring the posterior distribution of the latter. 
In practice, translating the implicit knowledge encoded by LPLMs into explicit relational structures has some naturally appealing properties. For example, HSN can support symbolic reasoning via inference of missing semantic connections, through high-order paths along the inferred graphs. Likewise secondary, autoregressive ""reasoning"" models can be trained on the symbol sequences inferred by HSN from task-specific natural language sequences, like e.g.,question-answer pairs. The reasoning paths sampled from such autoregressive models could then be used to inform GPT-like models and improve their performance on natural language understanding tasks, with which they are known to struggle. Below we demonstrate that (i) HSN is able to uncover ground-truth graphs from artificially generated datasets of random token sequences, and that (ii) using pretrained BERT and GPT-2 language models as encoder and decoder, respectively, HSN is able to infer schema networks from natural language datasets, whose symbols encode different aspects of language (like e.g.,topics or sentiments). Finally, we also explore training secondary, autoregressive models on the schema networks inferred from commonsense knowledge databases, and using the sampled paths to enhance the performance of LPLM on commonsense If-Then reasoning tasks."
Ethical Considerations for Machine Translation of Indigenous Languages: Giving a Voice to the Speakers,2305.19474v1,./img_ACL_2023/2305.19474v1.pdf,Study performed on 22 participants that are members of Indigenous communities from the Americas.,"In recent years machine translation has become very successful for high-resource language pairs. This has also sparked new interest in research on the automatic translation of low-resource languages, including Indigenous languages. However, the latter are deeply related to the ethnic and cultural groups that speak (or used to speak) them. The data collection, modeling and deploying machine translation systems thus result in new ethical questions that must be addressed. Motivated by this, we first survey the existing literature on ethical considerations for the documentation, translation, and general natural language processing for Indigenous languages. Afterward, we conduct and analyze an interview study to shed light on the positions of community leaders, teachers, and language activists regarding ethical concerns for the automatic translation of their languages. Our results show that the inclusion, at different degrees, of native speakers and community members is vital to performing better and more ethical research on Indigenous languages.","With the advancement of data-driven machine translation (MT) systems, it has become possible to, with varying degrees of quality, to translate between any pair of languages. The only precondition is the availability of enough monolingual or parallel data. There are many advantages to having high-performing MT systems. For example, they increase access to information for speakers of indigenous languages and can assist revitalization efforts for these languages. 
Research on machine translation as well as natural language processing (NLP) more generally is moving towards low-resourced setups and multilingual models. Thus, the NLP community needs to open the discussion of repercussions and best practices for research on indigenous languages (that in most cases are also low-resourced) since non-artificial languages cannot exist without a community of people that use (or have traditionally used) them to communicate. 
Indigenous languages further differ from more widely used ones in a crucial way: they are commonly spoken by small communities, and many communities use their language (besides other features) as a delimiter to define their own identity, and have in many cases also a certain degree of endangerment. Furthermore, in some cases, highly sensitive information ‚Äì such as secret aspects of their religion ‚Äì has been encoded with the help of their language. This is why, in recent years, discussions around ethical approaches to studying endangered languages have been started. When we consider the past (and present) of some of the communities that speak these languages, we will find a colonial history, where research is not the exception. Therefore, it is possible to trespass on ethical limits when using typical NLP and data collection methodologies. 
In this work, we explore the basic concepts of ethics related to MT of endangered languages with a special focus on Indigenous communities, surveying previous work on the topic. To better understand the expectations and concerns related to the development of MT systems for Indigenous communities, we then conducted an interview study with 22 language activists, language teachers, and community leaders who are members of Indigenous communities from the Americas. Additionally, we also performed 1:1 dialogues with two study participants to deepen our understanding of the matter. The goal is to answer the following research questions: How do community members want to be involved in the MT process, and why? Are there sensible topics that are not ethical to translate, model, or collect data without the community's explicit permission? How can we collect data in an ethical way? 
Surprisingly, most survey participants positively view MT for their languages. However, they believe research on their languages should be done in close collaboration with community members. Open access to research discoveries and resources is also valued highly, as well as the high quality of the resulting translations. The personal interviews also confirmed this. Thus, our most important finding is that it is crucial to work closely with the communities to understand delicate ethical topics when developing MT systems for endangered languages. 
A Spanish translation of this paper is included in Appendix. This translation aims to share our findings with all study participants and their communities and facilitate access to a broader audience in the Americas."
Revisiting non-English Text Simplification: A Unified Multilingual Benchmark,2305.15678v1,./img_ACL_2023/2305.15678v1.pdf,Papers published each year with content related to text simplification and a specific language according to Google Scholar. The quantity of English text simplification work vastly exceeds all other languages.,"Recent advancements in high-quality, large-scale English resources have pushed the frontier of English Automatic Text Simplification (ATS) research. However, less work has been done on multilingual text simplification due to the lack of a diverse evaluation benchmark that covers complex-simple sentence pairs in many languages. This paper introduces the MultiSim benchmark, a collection of 27 resources in 12 distinct languages containing over 1.7 million complex-simple sentence pairs. This benchmark will encourage research in developing more effective multilingual text simplification models and evaluation metrics. Our experiments using MultiSim with pre-trained multilingual language models reveal exciting performance improvements from multilingual training in non-English settings. We observe strong performance from Russian in zero-shot cross-lingual transfer to low-resource languages. We further show that few-shot prompting with BLOOM-176b achieves comparable quality to reference simplifications outperforming fine-tuned models in most languages. We validate these findings through human evaluation.","Automatic text simplification (ATS) is the task of reducing the complexity of a text without changing its original content and meaning. ATS has many applications, from making a text easier to read for people with reading and cognitive disabilities and second language learners to reducing the complexity of medical texts for easier understanding by the general public. For better accessibility to diverse communities, this technology should be available without language barriers. 
Much of the recent success in English text simplification comes from large parallel corpora of texts with the same content written using both complicated and simple sentences. These resources enable the training of large language models for ATS in English. ATS research in other languages has received much less attention. Figure shows that the growth of English text simplification research outpaces progress in other languages. 
A diverse multilingual benchmark is essential for a more comprehensive evaluation of multilingual simplification methods, pre-trained models, and evaluation metrics. The lack of a multilingual benchmark that covers a set of high, medium, and low-resource languages belonging to different scripts and language families hinders advancement in multilingual ATS. In this paper, we address this gap in the field by introducing the MultiSim benchmark that covers 27 text simplification datasets (complex-simple pairs) in 12 different languages. MultiSim consists of a collection of datasets from the literature that we unify into a single format for easier accessibility to the research community. In summary, our main contributions are as follows: 
 
 * We present a comprehensive literature survey of all existing multilingual text simplification corpora, created via several methodologies categorized into four main approaches (). 
 * We release the MultiSim benchmark for multilingual text simplification, containing 1,749,056 simple-complex sentence pairs in 12 different languages. To our knowledge, this is the first multilingual benchmark for text simplification. (). 
 * We run various experiments using pre-trained multilingual language models and analyze their effectiveness in few-shot learning and cross-lingual transfer for challenging cases of low-resource languages or domain-specific simplification (). Our results highlight the benefits of domain and language script match for zero-shot transfer. We find that few-shot prompting large language models produces high-quality simplifications in both high and low-resource languages (). We validate these findings with human evaluation ()."
Actively Supervised Clustering for Open Relation Extraction,2306.04968v1,./img_ACL_2023/2306.04968v1.pdf,"Compared with the existing unsupervised two-stage methods, our method can provide explicit supervision for clustering by alternately performing clustering learning and relation labeling. Note that the human effort of the two settings is comparable.","Current clustering-based Open Relation Extraction (OpenRE) methods usually adopt a two-stage pipeline. The first stage simultaneously learns relation representations and assignments. The second stage manually labels several instances and thus names the relation for each cluster. However, unsupervised objectives struggle to optimize the model to derive accurate clustering assignments, and the number of clusters has to be supplied in advance. In this paper, we present a novel setting, named actively supervised clustering for OpenRE. Our insight lies in that clustering learning and relation labeling can be alternately performed, providing the necessary guidance for clustering without a significant increase in human effort. The key to the setting is selecting which instances to label. Instead of using classical active labeling strategies designed for fixed known classes, we propose a new strategy, which is applicable to dynamically discover clusters of unknown relations. Experimental results show that our method is able to discover almost all relational clusters in the data and improve the SOTA methods by 10.3%and 5.2%, on two datasets respectively.","Relation extraction (RE) aims to detect and extract the potential relation between the given entity pair in unstructured text. The extracted relation facts play a vital role in many downstream applications, such as knowledge base population, search engine, and question answering. To deal with the emerging unknown relational types in the real world, Open Relation Extraction (OpenRE) has been widely studied. 
The clustering-based unsupervised relation discovery is a classical paradigm for OpenRE. It can discover potential relations, by grouping several instances into relational clusters, and then manually labeling a few instances to name the relation of each cluster. Recently, introduced a deep clustering framework into OpenRE. They iteratively cluster the relation representations that are produced by large pretrained models and use the cluster assignments as pseudo-labels to refine the representations. Unfortunately, the above unsupervised methods struggle to learn good enough representations, and the cluster assignments are error-prone. When multiple relations are mixed in a cluster, it becomes difficult to name the cluster. Hence, instead of regarding OpenRE as a totally unsupervised task, researchers leverage the labeled data of predefined relations to provide explicit supervision signals for clustering learning, and achieve superior results. 
Different from the above two-stage methods, in this work, we present a new setting named actively supervised clustering for OpenRE (ASCORE). As shown in fig. , our insight lies in that clustering learning (i. e, deep clustering) and relation labeling can be alternately performed. In an iteration, a small number of key instances are selected for labeling. The unknown relations expressed by these instances are correspondingly discovered. More importantly, these labeled instances can provide explicit supervisory signals for clustering learning. The improved relation representations form a better cluster structure, which in turn is able to benefit the discovery of the neglected relations. Since potential relations are dynamically discovered in iterations, the number of clusters does not need to be provided in advance. 
Along with this setting, we design an active labeling strategy tailored for clustering. First, all instances are encoded to points in the representation space, where the clustering is performed. The goal of the strategy is to select the most informative points for labeling. Intuitively, two points that are far from each other in representation space usually express different relations. To discover as many relations as possible, we introduce a distance regularization to the strategy, so that diversified relation discovery can be facilitated. To prevent over-fitting caused by training with limited active labeled instances, all the selected key points are required to be the points of maximum local density. By doing so, a large number of high-quality pseudo-labels can be obtained, by assigning active labels to unlabeled data in a small neighborhood. To mitigate the error propagation issue, different loss functions are assigned to active labels and pseudo-labels with different reliability for clustering learning. Experimental results show that (1) the actively supervised method improves the SOTA two-stage methods by a large margin without a significant increase in human effort. (2) the proposed active strategy can discover more relational clusters, compared with the classical active strategy. 
To summarize, the main contributions of this work are as follows: (1) We present a new setting named actively supervised clustering for OpenRE, providing the necessary guidance for clustering without a significant increase in human effort. (2) Design of a new active labeling strategy tailored for clustering, that can effectively discover potential relational clusters in unlabeled data. (3) This method improves the SOTA two-stage methods by 10.3%and 5.2%on two well-known datasets, respectively."
ConvGQR: Generative Query Reformulation for Conversational Search,2305.15645v3,./img_ACL_2023/2305.15645v3.pdf,An example of conversational search session and the high-level comparison between the original method and our ConvGQR. The dashed box illustrates the potential connection (underline) between the relevant passage and expansion terms.,"In conversational search, the user's real search intent for the current conversation turn is dependent on the previous conversation history. It is challenging to determine a good search query from the whole conversation context. To avoid the expensive re-training of the query encoder, most existing methods try to learn a rewriting model to de-contextualize the current query by mimicking the manual query rewriting. However, manually rewritten queries are not always the best search queries. Thus, training a rewriting model on them would lead to sub-optimal queries. Another useful information to enhance the search query is the potential answer to the question. In this paper, we propose ConvGQR, a new framework to reformulate conversational queries based on generative pre-trained language models (PLMs), one for query rewriting and another for generating potential answers. By combining both, ConvGQR can produce better search queries. In addition, to relate query reformulation to the retrieval task, we propose a knowledge infusion mechanism to optimize both query reformulation and retrieval. Extensive experiments on four conversational search datasets demonstrate the effectiveness of ConvGQR.","Conversational search is a rapidly developing branch of information retrieval, which aims to satisfy complex information needs through multi-turn conversations. The main challenge is to determine users 'real search intents based on the interaction context and formulate good search queries accordingly. Existing methods can be roughly categorized into two groups. The first group directly uses the whole context as a query and trains a model to determine the relevance between the long context and passages. This approach requires additional training of retriever to take the long context as input, which is not always feasible. What is available in practice is a general retriever (e.g., ad-hoc search retriever) that uses a stand-alone query. The second group of approaches aims at producing a de-contextualized query using query reformulation techniques. Such a query can be submitted to any off-the-shelf retrievers. We focus on this second approach. 
Two types of query reformulation techniques have been widely studied in the literature, i.e., query rewriting and query expansion. The former trains a generative model to rewrite the current query to mimic the human-rewritten one, while the latter focuses on expanding the current query by relevant terms selected from the context. Although both approaches achieve promising results, they are all studied separately. Two important limitations are observed: (1) Query rewriting and query expansion can produce different effects. Query rewriting tends to deal with ambiguous queries and add missing tokens, while query expansion aims to add supplementary information to the query. Both effects are important for query reformulation. It is thus beneficial to use both of them. (2) Previous query rewriting models have been optimized to produce human-rewritten queries, independently from the passage ranking task. Even though human-rewritten queries usually perform better than the original queries, existing studies have shown that they may not be the best search queries alone. Therefore, it is useful to incorporate additional criteria directly related to ranking performance when reformulating a query. As shown in Fig. (left), although the human-rewritten query recovers the crucial missing information (i.e.,""goat"") from the context, it is still possible to further improve the search query. 
To tackle these problems, we propose ConvGQR, a new Generative Query Reformulation framework for Conversational search. It combines query rewriting with query expansion. The right side of Fig. illustrates the differences between ConvGQR and the existing query rewriting method. In addition to query rewriting based on human-rewritten queries, ConvGQR also learns to generate the potential answer of the query (e.g., the answer in the downstream question-answering task) and uses it for query expansion. This strategy is motivated by the fact that a passage containing the generated potential answer is more likely a relevant passage, because either the generated answer is the right answer, or it may co-occur with the right answer in the same passage. The final query reformulation model is trained by combining both query rewriting and query expansion criteria in the loss function. Moreover, the learning of both query rewriting and expansion are guided by the relevant passage information through our knowledge infusion mechanism to encourage query generation toward better search performance. We carry out extensive experiments on four conversational search datasets using both dense and sparse retrievers, and the results show that our method outperforms most existing query reformulation methods. Our further analysis confirms the complementary contributions of query rewriting and query expansion. 
Our contributions are summarized as follows: (1) We propose ConvGQR to integrate query rewriting and query expansion for conversational search. In particular, query expansion is performed by adding the generated potential answer by a generative PLM. This is a way to exploit PLM' s capability of capturing rich world knowledge. (2) We further design a knowledge infusion mechanism to optimize query reformulation with the guidance of passage retrieval. (3) We demonstrate the effectiveness of ConvGQR with two off-the-shelf retrievers (sparse and dense) on four datasets. Our analysis confirms the complementary effects of both components in conversational search."
NLPeer: A Unified Resource for the Computational Study of Peer Review,2211.06651v2,./img_ACL_2023/2211.06651v2.png,"NLPeer unites openly licensed datasets from different research communities, reviewing systems and time periods, including three previously unreleased text collections: ARR-22, COLING-20 and F1000-22.","Peer review constitutes a core component of scholarly publishing; yet it demands substantial expertise and training, and is susceptible to errors and biases. Various applications of NLP for peer reviewing assistance aim to support reviewers in this complex process, but the lack of clearly licensed datasets and multi-domain corpora prevent the systematic study of NLP for peer review. To remedy this, we introduce NLPeer ‚Äì the first ethically sourced multidomain corpus of more than 5k papers and 11k review reports from five different venues. In addition to the new datasets of paper drafts, camera-ready versions and peer reviews from the NLP community, we establish a unified data representation and augment previous peer review datasets to include parsed and structured paper representations, rich metadata and versioning information. We complement our resource with implementations and analysis of three reviewing assistance tasks, including a novel guided skimming task. Our work paves the path towards systematic, multi-faceted, evidence-based study of peer review in NLP and beyond. The data and code are publicly available.","Research publication is the primary unit of scientific communication. To ensure publication quality and to prioritise research outputs, most scientific communities rely on peer review ‚Äì a distributed procedure where independent referees determine if a manuscript adheres to the standards of the field. Despite its utility and wide application, peer review is an effortful activity that requires expertise and is prone to bias. An active line of research in NLP for peer review strive to address these challenges by supporting the underlying editorial process, decision making, review writing, and by studying review discourse. 
Despite the methodological advancements, several factors prevent NLP research for peer review at large. The computational study of peer review lacks a (1) solid data foundation: reviewing data is rarely public and comes with legal and ethical challenges; existing sources of peer reviewing data and the derivative datasets are not licensed, which legally prevents reuse and redistribution. Peer reviewing practices vary across research communities ‚Äì yet the vast majority of NLP research in peer review so far focused on a few machine learning conferences that make their data available through the OpenReview. net platform. A (2) multi-domain perspective on peer review is thus missing, and the transferability of findings between different communities and reviewing workflows remains unclear. Finally, a (3) unified data model for representing peer reviewing data is lacking: most existing datasets of peer reviews adhere to task-specific data models and formats, making it hard to develop and evaluate approaches for peer review support across datasets and domains. 
To address these issues, we introduce NLPeer. We apply a state-of-the-art workflow to gather ethically and legally compliant reviewing data from natural language processing (NLP) and computational linguistics (CL) communities. We complement it with multi-domain reviewing data from the F1000 Research platform and historical data from the openly licensed portion of the PeerRead corpus. The resulting resource (Figure) is the most comprehensive collection of clearly licensed, open peer reviewing datasets available to NLP to date. 
NLPeer includes peer reviews, paper drafts and revisions from diverse research fields and reviewing systems, over the time span from as early as 2012 until 2022. This ‚Äì for the first time ‚Äì enables systematic computational study of peer review across domains, communities, reviewing systems and time. The paper revisions make NLPeer well-suited for the study of collaborative text work. 
To facilitate the analysis, we unify the datasets under a common data model that preserves document structure, non-textual elements and is well suited for cross-document analysis. To explore the new possibilities opened by our resource, we conduct cross-domain experiments on review score prediction (to encourage consistent review scores), pragmatic labeling (to encourage balanced reviews) and the novel guided skimming for peer review task (to help guide review focus), along with easy-to-extend implementations. Our results indicate substantial variation in performance of NLP assistance between venues and research communities, point at synergies between different approaches to review structure analysis, and pave the path towards exploiting cross-document links between peer reviews and research papers for language model benchmarking. 
In summary, this work contributes (1) the first unified, openly licensed, multi-domain collection of datasets for the computational study of peer review, including (2) two novel datasets of peer reviews from the NLP and CL communities, and complemented by a (3) descriptive analysis of the resulting data and (4) extensive experiments in three applied NLP tasks for peer reviewing assistance."
Contextual Distortion Reveals Constituency: Masked Language Models are Implicit Parsers,2306.00645v1,./img_ACL_2023/2306.00645v1.pdf,"Example sentence and its constituency tree. We list perturbed sentences after substitution (1), decontextualization (2), and movement (3).","Recent advancements in pre-trained language models (PLMs) have demonstrated that these models possess some degree of syntactic awareness. To leverage this knowledge, we propose a novel chart-based method for extracting parse trees from masked language models (LMs) without the need to train separate parsers. Our method computes a score for each span based on the distortion of contextual representations resulting from linguistic perturbations. We design a set of perturbations motivated by the linguistic concept of constituency tests, and use these to score each span by aggregating the distortion scores. To produce a parse tree, we use chart parsing to find the tree with the minimum score. Our method consistently outperforms previous state-of-the-art methods on English with masked LMs, and also demonstrates superior performance in a multilingual setting, outperforming the state of the art in 6 out of 8 languages. Notably, although our method does not involve parameter updates or extensive hyperparameter search, its performance can even surpass some unsupervised parsing methods that require fine-tuning. Our analysis highlights that the distortion of contextual representation resulting from syntactic perturbation can serve as an effective indicator of constituency across languages.","Constituency parsing is a fundamental task in natural language processing (NLP) that involves uncovering the syntactic structure of a sentence by identifying the constituents it is composed of. While supervised constituency parsing methods necessitate the utilization of a labeled dataset containing sentences and their corresponding constituency parses, unsupervised methods for generating syntax trees emerge because manual annotation is labor-intensive and requires specialized linguistic knowledge. One line of work for unsupervised constituency parsing involves designing an objective function that enables the model to infer the hierarchical structure of language from the unannotated text. An alternative approach, known as Constituency Parse Extraction from Pre-trained Language Models (CPE-PLM), involves extracting parse trees from pre-trained language models without fine-tuning in an unsupervised manner. The main motivation for CPE-PLM is the assumption that pre-trained language models contain implicit syntactic knowledge learned during the pre-training stage. This knowledge can then be used to directly predict parse trees, eliminating the need for task-specific fine-tuning. While CPE-PLM systems have been shown to produce parse trees that resemble manually annotated ones, they have also been found to have lower performance than the first line of work. 
In this paper, we propose a simple yet effective CPE-PLM approach to bridge the performance gap between these two methods by input perturbations designed based on the intuition of constituency tests. Linguists use constituency tests to determine whether a span of words forms a constituent in a sentence. One common constituency test is the substitution test which replaces the span of words with a single pronoun (such as ""it"" or ""they"") and checks if the sentence is still grammatical. For example, in Figure, the span ""a film"" can be replaced with the pronoun ""it"", resulting in the sentence ""they watched it this afternoon, "" which is still grammatical. This suggests that ""a film"" is likely a constituent. Our goal in this work is to maximally explore the capabilities of PLMs to induce grammar by themselves. Specifically, we focus on masked LMs and leverage the inherent properties of the mask token prediction pre-training objective. The main idea is to make pre-trained language models think like linguists, such that with constituency tests, span-level scores reflecting the likelihood of a span being a constituent can be obtained. 
The evaluation of constituency tests traditionally relies on grammaticality judgments. trained a classifier that can make grammaticality decisions with external data. In contrast, our approach assesses the degree of alternation in contextual representations resulting from manipulations akin to those used in constituency tests. We hypothesize that, when the context of a span is manipulated, the contextual representations of constituents will exhibit minimal alteration compared to those of distituents (non-constituents). We refer to these manipulations as perturbations, as our method measures the sensitivity of the representations to these changes. We define three perturbations and for each perturbation, we alter the input sentence and compare the representations of the perturbed sentences to that of the original. The three perturbations on an example span are illustrated in Figure. By applying perturbations to each span of words within the input sentence, we generate scores indicating the likelihood of each span being a constituent. 
To evaluate the effectiveness of our approach, we compare it with existing methods for extracting parse trees from PLMs without fine-tuning (Section). Our model improves over the previously published best result by a large margin. In a multilingual setting, our model surpasses the previous state of the art in 6 out of 8 languages. Our model even outperforms some unsupervised parsing methods that require parameter updates, highlighting the effectiveness of our approach. 
Our main contributions can be summarized as follows: 
 
 * We propose a novel, simple and effective method for extracting constituency trees from masked LMs based on linguistic perturbations. 
 * We demonstrate that our proposed method achieves new state-of-the-art results under the no parameter update setting on the standard English dataset and 6 out of 8 languages from a multilingual dataset with a significantly smaller hyperparameter search space than previous methods. 
 * Our work identifies the crucial elements that benefit the overall performance gain and highlights the potential of utilizing perturbations on masked LMs for understanding the underlying structure of language."
COLA: Contextualized Commonsense Causal Reasoning from the Causal Inference Perspective,2305.05191v1,./img_ACL_2023/2305.05191v1.pdf,"An illustration of leveraging context to conduct commonsense causal reasoning. Both causes could be plausible under different contexts, while only the frequent one (i.e., Cause 1) is plausible without context.","Detecting commonsense causal relations (causation) between events has long been an essential yet challenging task. Given that events are complicated, an event may have different causes under various contexts. Thus, exploiting context plays an essential role in detecting causal relations. Meanwhile, previous works about commonsense causation only consider two events and ignore their context, simplifying the task formulation. This paper proposes a new task to detect commonsense causation between two events in an event sequence (i.e., context), called contextualized commonsense causal reasoning. We also design a zero-shot framework: COLA (Contextualized Commonsense Causality Reasoner) to solve the task from the causal inference perspective. This framework obtains rich incidental supervision from temporality and balances covariates from multiple timestamps to remove confounding effects. Our extensive experiments show that COLA can detect commonsense causality more accurately than baselines.","Commonsense Causal Reasoning (CCR) aims at identifying plausible causes and effects of events in natural language that are typically reasonable by an average person. To solve the task, existing efforts devoted by the community mainly rely on language models wholeheartedly with supervised learning approaches. Those ingenious engineering works have brought significant progress in recent years. However, recent studies found that pure engineering designs are inadequate to seize commonsense causation, as language models tend to reach higher scores by exploiting superficial artifacts in data. 
Recently, zhang2022rock first studied grasping commonsense causation from the causal inference perspective, by drawing analogies between observational studies and natural languages. The proposed framework ROCK achieves good potential for the zero-shot CCR task (e.g., COPA by). However, only focuses on the commonsense causation between a pair of events without specifying context. Given that events are complex, an event may have different causes under different contexts. Thus, it is necessary to utilize context when detecting commonsense causation, such as other events related to given ones. Missing a clear and specific context simplifies commonsense causal knowledge and hinders models from detecting commonsense causal relations more accurately. For example, as shown in, the frequent cause of ""Emma made a steak in the kitchen. "" is ""Emma felt hungry. "" However, the cause also could be ""Emma was doing her job"" if ""Emma is a chef at a steakhouse. "" Without the context of Emma 's job, models cannot distinguish those two causes and may return to the frequent one. 
To involve context when detecting commonsense causation, we propose a new task to detect causes between two events in an event sequence, called Contextualized Commonsense Causal Reasoning (Contextualized CCR). In this task, models are asked to detect commonsense causal relations between two given events enclosed in an event sequence. Other events in the event sequence can provide a clear and specific definition of the current context, helping models to capture commonsense causation more accurately. In fact, we find that contextualized CCR is a non-trivial task. Directly applying the framework ROCK cannot achieve competitive performance on the contextualized CCR since it cannot integrate context information. 
We propose the framework COLA, which incorporates contextual information from an event sequence, to solve the Contextualized CCR. Our framework adopts the potential-outcomes framework to estimate the causal estimand Œî defined as a type of ""average treatment effect"" (ATE), which measures the change in the likelihood of E_j' s occurrence when intervening E_i (denoted by E_i) as 
 Œî = ‚Ñô (E_i ‚â∫ E_j) - (E_i ‚â∫ E_j),
 where (¬∑) can be estimated with a pre-trained language model, such as a masked language model. The magnitude of average treatment effectinforms the strength of E_i's effect on E_j, and its sign indicates the direction of the effect. For instance, Œî‚âà 1 means E_j becomes more prone to occur due to the occurrences of E_i. In an ideal world (e.g., E_i and E_j on any study unit occur completely randomly), a plugging-in estimator in suffices for detecting commonsense causation. Nevertheless, spurious correlations introduced by pervasive confounding co-occurrences need to be eliminated for an unbiased estimation of the causal estimand. This can be done by balancing events that precede E_i, or covariates. To incorporate context, we design a mechanism to sample diversified covariates from multiple timestamps and use temporal propensity for balancing. 
We annotated commonsense causal relations between two events (~1.3k examples) within event sequences from ROCStories to benchmark our proposed contextualized CCR task. We conduct extensive experiments with multiple pre-trained language models, showing that COLAcan detect cause-and-effect relations more accurately than competitive baselines by a large margin. Our experiments also show that temporality is essential in our framework but not sufficient to detect commonsense causation without covariates being appropriately balanced."
MEMEX: Detecting Explanatory Evidence for Memes via Knowledge-Enriched Contextualization,2305.15913v2,./img_ACL_2023/2305.15913v2.pdf,"Examples of discarded meme types: (a) Text-only, (b) Code-mixed, (c) Image-only and (d) Cartoon.","Memes are a powerful tool for communication over social media. Their affinity for evolving across politics, history, and sociocultural phenomena makes them an ideal communication vehicle. To comprehend the subtle message conveyed within a meme, one must understand the background that facilitates its holistic assimilation. Besides digital archiving of memes and their metadata by a few websites like <knowyourmeme. com>, currently, there is no efficient way to deduce a meme 's context dynamically. In this work, we propose a novel task, MemeX ‚Äì given a meme and a related document, the aim is to mine the context that succinctly explains the background of the meme. At first, we develop MCC (Meme Context Corpus), a novel dataset for MemeX. Further, to benchmark MCC, we propose MIME (MultImodal Meme Explainer), a multimodal neural framework that uses common sense enriched meme representation and a layered approach to capture the cross-modal semantic dependencies between the meme and the context. MIME surpasses several unimodal and multimodal systems and yields an absolute improvement of ‚âà 4%F1-score over the best baseline. Lastly, we conduct detailed analyses of MIME' s performance, highlighting the aspects that could lead to optimal modeling of cross-modal contextual associations.","Social media has become a mainstream communication medium for the masses, redefining how we interact within society. The information shared on social media has diverse forms, like text, audio, and visual messages, or their combinations thereof. A meme is a typical example of such social media artifact that is usually disseminated with the flair of sarcasm or humor. While memes facilitate convenient means for propagating complex social, cultural, or political ideas via visual-linguistic semiotics, they often abstract away the contextual details that would typically be necessary for the uninitiated. Such contextual knowledge is critical for human understanding and computational analysis alike. We aim to address this requirement by contemplating solutions that facilitate the automated derivation of contextual evidence towards making memes more accessible. To this end, we formulate a novel task ‚Äì MemeX, which, given a meme and a related context, aims to detect the sentences from within the context that can potentially explain the meme. Table visually explains MemeX. Memes often camouflage their intended meaning, suggesting MemeX 's utility for a broader set of multimodal applications having visual-linguistic dissociation. Other use cases include context retrieval for various art forms, news images, abstract graphics for digital media marketing, etc. 
Table primarily showcases a meme' s figure (left) and an excerpt from the related context (right). This meme is about the revenge killing of an Ottoman Sultan, by the Janissaries (infantry units), in reaction to their disbanding, by the Sultan. The first line conveys the supporting evidence for the meme from the related context, emboldened and highlighted in Table. The aim is to model the required cross-modal association that facilitates the detection of such supporting pieces of evidence from a given related contextual document. 
The recent surge in the dissemination of memes has led to an evolving body of studies on meme analysis in which the primary focus has been on tasks, such as emotion analysis, visual-semantic role labeling, detection of phenomena like sarcasm, hate-speech, trolling and harmfulness. 
These studies indicate that off-the-shelf multimodal models, which perform well on several traditional visual-linguistic tasks, struggle when applied to memes. The primary reason behind this is the contextual dependency of memes for their accurate assimilation and analysis. Websites like <knowyourmeme. com> (KYM) facilitate important yet restricted information. MemeX requires the model to learn the cross-modal analogies shared by the contextual evidence and the meme at various levels of information abstraction, towards detecting the crucial explanatory evidence. The critical challenge is to represent the abstraction granularity aptly. Therefore, we formulate MemeX as an ""evidence detection"" task, which can help deduce pieces of contextual evidence that help bridge the abstraction gap. However, besides including image and text modality, there is a critical need to inject contextual signals that compensate for the constraints due to the visual-linguistic grounding offered by conventional approaches. 
Even with how effective and convenient memes are to design and disseminate over social media strategically, they are often hard to understand or are easily misinterpreted by the uninitiated, typically without the proper context. Thereby suggesting the importance of addressing a task like MemeX. Governments or organizations involved in content moderation over social media platforms could use such a utility, underlining the convenience that such a context deduction solution would bring about in assimilating harmful memes and thereby adjudicating their social implications in emergencies like elections or a pandemic. 
Motivated by this, we first curate MCC, a new dataset that captures various memes and related contextual documents. We also systematically experiment with various multimodal solutions to address MemeX, which culminates into a novel framework named MIME (MultImodal Meme Explainer). Our model primarily addresses the challenges posed by the knowledge gap and multimodal abstraction and delivers optimal detection of contextual evidence for a given pair of memes and related contexts. In doing so, MIME surpasses several competitive and conventional baselines. 
To summarize, we make the following main contributions. : 
 
 * A novel task, MemeX, aimed to identify explanatory evidence for memes from their related contexts. 
 * A novel dataset, MCC, containing 3400 memes and related context, along with gold-standard human annotated evidence sentence-subset. 
 * A novel method, MIME that uses common sense-enriched meme representation to identify evidence from the given context. 
 * Empirical analysis establishing MIME's superiority over various unimodal and multimodal baselines, adapted for the MemeX task."
Tackling Ambiguity with Images: Improved Multimodal Machine Translation and Contrastive Evaluation,2212.10140v2,./img_ACL_2023/2212.10140v2.pdf,Visual context resolving the ambiguity of English word glasses for English-to-French translation.,"One of the major challenges of machine translation (MT) is ambiguity, which can in some cases be resolved by accompanying context such as images. However, recent work in multimodal MT (MMT) has shown that obtaining improvements from images is challenging, limited not only by the difficulty of building effective cross-modal representations, but also by the lack of specific evaluation and training data. We present a new MMT approach based on a strong text-only MT model, which uses neural adapters, a novel guided self-attention mechanism and which is jointly trained on both visually-conditioned masking and MMT. We also introduce CoMMuTE, a Contrastive Multilingual Multimodal Translation Evaluation set of ambiguous sentences and their possible translations, accompanied by disambiguating images corresponding to each translation. Our approach obtains competitive results compared to strong text-only models on standard English‚ÜíFrench, English‚ÜíGerman and English‚ÜíCzech benchmarks and outperforms baselines and state-of-the-art MMT systems by a large margin on our contrastive test set. Our code and CoMMuTE are freely available.","Multimodal machine translation (MMT) typically refers to the use of additional non-textual data in text-based machine translation (MT). Here, we focus on the case where source texts are accompanied by images, the idea being to exploit visual data to improve the translation of ambiguous sentences. For example, in Figure, the English word glasses can either be translated as French verres 'drinking vessels' or lunettes 'spectacles', an ambiguity which is resolved using the image. 
A main research direction of MMT has been how to best exploit image representations and combine the image and text modalities. It has typically been difficult to surpass strong text-only baselines, the image modality often being ignored. A major issue holding back progress is that most current state-of-the-art MMT models are trained solely on the ‚àº30k examples of the Multi30k dataset, comprising image captions and their translations. This causes two issues: (i) the models do not exploit the large amount of text-only data available and therefore perform poorly in comparison to state-of-the-art text-only MT systems, and (ii) we show that very few examples require images to be correctly translated, which means that the datasets are ill-adapted to evaluating the use of the image modality. 
In this article, we aim to overcome these problems by proposing (i) a new MMT approach that is able to exploit (text-only) monolingual and parallel data as well as (multimodal) captioning data, and that reaches a good balance between maintaining high MT quality and effectively exploiting images, and (ii) a test set, CoMMuTE, containing contrastive evaluation pairs, where images provide the necessary context to disambiguate between multiple meanings of the same source sentence. 
Our suggested model is inspired by work on adapting frozen language models (LMs) to multimodal inputs; we propose to adapt a strong MT model to multimodal inputs with lightweight modules to exploit the large amount of textual data it was trained on. We also propose to better exploit the image by introducing guided self-attention and by combining the standard MMT objective with a visually-conditioned masked language modelling (VMLM) objective. Our model obtains competitive results compared to strong text-only baselines on standard En‚Üí {Fr, De, Cs} MMT benchmarks and outperforms them and state-of-the-art MMT models on our lexically ambiguous contrastive test set."
Cross-modal Attention Congruence Regularization for Vision-Language Relation Alignment,2212.10549v2,./img_ACL_2023/2212.10549v2.pdf,Global Alignment (GA) only aligns the entire image with the corresponding caption. Entity Alignment (EA) extracts entities from the image and caption for finer-grained alignment. Relation Alignment (RA) cross-modally aligns the intra-modal relations between entities in both the image and the text. We show RA is vital to improve compositional performance.,"Despite recent progress towards scaling up multimodal vision-language models, these models are still known to struggle on compositional generalization benchmarks such as Winoground. We find that a critical component lacking from current vision-language models is relation-level alignment: the ability to match directional semantic relations in text (e.g., 'mug in grass') with spatial relationships in the image (e.g., the position of the mug relative to the grass). To tackle this problem, we show that relation alignment can be enforced by encouraging the language attention from 'mug' to 'grass' (capturing the semantic relation 'in') to match the visual attention from the mug to the grass (capturing the corresponding physical relation). Tokens and their corresponding objects are softly identified using a weighted mean of cross-modal attention. We prove that this notion of soft cross-modal equivalence is equivalent to enforcing congruence between vision and language attention matrices under a 'change of basis' provided by the cross-modal attention matrix. Intuitively, our approach projects visual attention into the language attention space to calculate its divergence from the actual language attention, and vice versa. We apply our Cross-modal Attention Congruence Regularization (CACR) loss to fine-tune UNITER and improve its Winoground Group score by 5.75 points.","Compositionality is the ability to combine meanings of constituents according to structured rules. Recent work shows that Vision-Language Models (VLMs) fail to construct compositional representations and generally ignore syntactic & structural information. Winoground is a vision-language compositionality task that tests a VLM 's ability to match syntactic permutations of text with their visual interpretations, for example correctly matching ""grass in mug"" and ""mug in grass"" to their corresponding images. Winoground finds that all recent state-of-the-art VLMs perform below chance levels on this compositionality task. Contemporaneously, probe for structural knowledge in VLMs, finding that they encode significantly less linguistic syntax than Language Models (LMs) and virtually no visual structure. Recently, built a large dataset confirming that VLMs treat images as a' bag of objects 'and don' t adequately represent visuo-linguistic relations. 
Since models must determine whether the compositional structure of an image matches that of the caption, it 's important for the model to learn to cross-modally align intra-modal relations. That is, if the relation from' mug 'to' grass 'is' in-ness', the model should recognize when the equivalent physical relation holds between a mug and grass in the image, and representationally align these relations such that an image-text matching head may more easily determine whether the relations are cross-modally equivalent. In simpler terms, the compositional structure of input for each modality should be represented such that they can be cross-modally matched even for difficult examples like Winoground. 
Unfortunately, there has been less highly influential work on relation alignment between vision & language, and did not benchmark any such models. In this work, we begin exploration of these relation alignment approaches by tentatively grouping them into 3 categories: 
 
 * Structural Data: training a model on data that explicitly captures relational structure 
 * Structural Model: infusing an inductive bias into the architecture of the model that enables more compositional representations 
 * Structural Training: modifying the objective function or imposing a parameter constraint to encourage relation alignment 
Since Structural Data approaches require complex annotations and Structural Model approaches are often incompatible with large transformers, we identify Structural Training as a promising avenue for providing compositional inductive biases to VLMs due to their architecture-agnostic compatibility and computational scalability. 
In this work, we propose a Structural Training approach for relation alignment that uses the cross-modal attention matrix as a change of basis to the opposite modality, which we then compare to the original modality to calculate a divergence loss, effectively measuring cross-modal congruence between intra-modal attentions. 
We show how our approach, Cross-modal Attention Congruence Regularization (CACR), generalizes previous Structural Training work on cross-modal attention regularization (IAIS) by taking into account all possible entity alignments and computationally simplifying relation alignment. The CACR regularization term can easily be dropped into most transformer-based Vision-Language model objectives with no added data and minimal computational overhead, to encourage relation alignment during training. Finally, we show that CACR_base improves on IAIS_base‚Äîwhere IAIS_large holds the current state-of-the-art on Winoground."
Enhancing Personalized Dialogue Generation with Contrastive Latent Variables: Combining Sparse and Dense Persona,2305.11482v1,./img_ACL_2023/2305.11482v1.pdf,"The overview structure of the proposed model. Connections with dashed blue lines only appear during training, connections with dashed red lines only appear during inference, and connections with solid black lines indicate that they appear during both training and inference phases. The purple lines represent positive and negative example constructions in contrastive learning.","The personalized dialogue explores the consistent relationship between dialogue generation and personality. Existing personalized dialogue agents model persona profiles from three resources: sparse or dense persona descriptions and dialogue histories. However, sparse structured persona attributes are explicit but uninformative, dense persona texts contain rich persona descriptions with much noise, and dialogue history query is both noisy and uninformative for persona modeling. In this work, we combine the advantages of the three resources to obtain a richer and more accurate persona. We design a Contrastive Latent Variable-based model (CLV) that clusters the dense persona descriptions into sparse categories, which are combined with the history query to generate personalized responses. Experimental results on Chinese and English datasets demonstrate our model's superiority in personalization.","In order to develop personalized dialogue agents, current approaches enhance the personality of generated responses mainly utilizing three kinds of resources: (1) Defined sparse persona attributes; (2) Dense persona description texts; (3) Historical queries of current dialogue. Each of the three resources has its advantages and disadvantages. 
Sparse persona attributes (e.g., gender, age) are highly interpretable and have high information utilization, but the information is limited and cannot express complex persona features. Dense persona description text contains rich and flexible persona information but suffers from noisy expressions. Modeling personality directly from dialogue histories is free of additional persona information, but the persona information in history queries is both noisy and uninformative. 
To address these issues, in this paper, we improve personalized dialogue generation by combining the advantages of the three resources. We design a contrastive latent variable (CLV) -based model that clusters the dense persona descriptions into sparse categories, which are combined with the history query to generate personalized responses. Specifically, first, the dialog 's latest query and response together with dense persona description texts are encoded. Then the recognition distribution of query and response is jointly modeled with a pre-designed dual conditional variational autoencoder (CVAE). Simultaneously, the persona information is automatically separated into multiple parts to participate in the above process in parallel. These partitioned persona pieces of information are considered to hide different angles of portrayal. This process is also reinforced by contrastive learning. Next, a decider decides which category of persona information is used for persona modeling. Finally, a personalized generator combines the history query and additional persona information for response generation. Without explicit supervised signals, we design a pseudo-labeling and joint training method to train the decider. 
Our contributions are summarized as follows: (1) We design a framework named CLV based on contrastive latent variables to combine the advantages of three persona resources for personalized dialogue generation. The framework contains a self-separation algorithm and a decider, which are jointly trained to work in conjunction with each other. In this way, our work can both extract information more efficiently from the cluttered persona description text and not require persona information in the inference phase. (2) Under the designed CLV-based framework, we propose a self-separation algorithm to mine and categorize dense persona description text into sparse persona profiles. Furthermore, a decider is proposed to decide whether the dialogue should involve persona information and choose appropriate persona profiles among the persona profiles generated by the self-separation algorithm. This process helps to improve the consistency of personalized dialogue generation. (3) We conduct extensive experiments on the Chinese and English personalized dialogue datasets to demonstrate our model' s superiority. We also propose a refined evaluation framework for personalized dialogue generation, which considers the consistency, coherence, and diversity of dialogue generation at the same time."
Can LMs Learn New Entities from Descriptions? Challenges in Propagating Injected Knowledge,2305.01651v1,./img_ACL_2023/2305.01651v1.pdf,Knowledge editing tasks. We study a challenging entity knowledge propagation task where language models should make inferences after learning entities from their definitions. This differs from past knowledge editing which evaluates paraphrases of injected facts.,"Pre-trained language models (LMs) are used for knowledge intensive tasks like question answering, but their knowledge gets continuously outdated as the world changes. Prior work has studied targeted updates to LMs, injecting individual facts and evaluating whether the model learns these facts while not changing predictions on other contexts. We take a step forward and study LMs 'abilities to make inferences based on injected facts (or propagate those facts): for example, after learning that something is a TV show, does an LM predict that you can watch it? We study this with two cloze-style tasks: an existing dataset of real-world sentences about novel entities (ECBD) as well as a new controlled benchmark with manually designed templates requiring varying levels of inference about injected knowledge. Surprisingly, we find that existing methods for updating knowledge (gradient-based fine-tuning and modifications of this approach) show little propagation of injected knowledge. These methods improve performance on cloze instances only when there is lexical overlap between injected facts and target inferences. Yet, prepending entity definitions in an LM' s context improves performance across all settings, suggesting that there is substantial headroom for parameter-updating approaches for knowledge injection.","Pre-trained language models (LMs) acquire comprehensive real-world knowledge from massive amounts of pre-training data, allowing them to use this knowledge effectively in downstream tasks. However, without continual updating, the knowledge contained within these backend LMs will eventually become outdated. This temporal mismatch affects model performance on downstream tasks. As LMs become more widely deployed, their knowledge should be synced with the current state of the world while maintaining reasonable deployment costs. 
Prior work has investigated knowledge editing in pre-trained LMs, updating model parameters to alter outputs to match what users want. In these studies, the original fact and the altered fact are provided (e.g., changing ""X was born in Y. "" to ""X was born in Z. ""), and models are evaluated after a single update on each instance; see Figure for an example. These model editing methods successfully provide targeted updates, fixing incorrect or outdated individual facts. Yet, can LMs make inferences based on updated knowledge? Prior evaluation has largely focused on two aspects of knowledge editing, whether the edits were successfully injected and whether other irrelevant sentences were impacted, but do not capture whether the LMs now can reason based on the new fact that has been injected. 
We take a step further and evaluate whether LMs can propagate updated knowledge about new entities. We first inject definitions about the entity into LMs using various knowledge editing methods, then evaluate LMs 'performance on cloze tasks on a wide range of sentences about the entity (see Figure for an example). We refer to this task as entity knowledge propagation and introduce two cloze datasets to evaluate this challenging task.
Our first evaluation benchmark is the Entity Cloze By Date (ECBD) dataset, which presents novel entities tagged with origination dates (e.g., Hurricane Ian, 2022), their definition and probe sentences taken from their Wikipedia page. The task is to fill a masked span in probe sentences. Because Wikipedia contains a wide range of information, much of it not inferable from an entity' s definition, injecting entity knowledge via its definition has an unclear impact on the probe sentences; filling in the masked span is nontrivial even after the entity definition is provided. For more controlled study, we introduce a new benchmark (Entity Inferences) with manually designed probe sentences with multiple-choice answer options. Once one learns about the definition of an emerging entity, finding the correct answer for these probe sentences is easy. 
We find that existing parameter updating methods can handle simpler inferences in Entity Inferences, but fail to improve performances in ECBD, revealing a limitation in these methods. We further analyze the impact of fine-tuning. Distressingly, we find that simply prepending information in-context works very well, and matching the performance of this via parameter updates is challenging. A deeper analysis finds that model editing shows promising results only when the injected definition sentence and the cloze inference have lexical overlap. Our work establishes an evaluation paradigm and opens doors for work on editing methods that can propagate entity knowledge."
Explaining How Transformers Use Context to Build Predictions,2305.12535v1,./img_ACL_2023/2305.12535v1.pdf,"A Transformer Language Model, represented as modules writing into the residual stream.","Language Generation Models produce words based on the previous context. Although existing methods offer input attributions as explanations for a model 's prediction, it is still unclear how prior words affect the model' s decision throughout the layers. In this work, we leverage recent advances in explainability of the Transformer and present a procedure to analyze models for language generation. Using contrastive examples, we compare the alignment of our explanations with evidence of the linguistic phenomena, and show that our method consistently aligns better than gradient-based and perturbation-based baselines. Then, we investigate the role of MLPs inside the Transformer and show that they learn features that help the model predict words that are grammatically acceptable. Lastly, we apply our method to Neural Machine Translation models, and demonstrate that they generate human-like source-target alignments for building predictions.","Language Generation Models, like Transformer-based Language Models have recently revolutionized the field of Natural Language Processing (NLP). Despite this, there is still a gap in our understanding of how they are able to produce language that closely resembles that of humans. This means that we are unable to determine the cause of a model 's failure in specific instances, which can result in the generation of hallucinated content or toxic output. 
The majority of previous work in explainability of NLP model predictions has focused on analyzing them on downstream tasks, generally with a small output space, such as text classification or Natural Language Inference. This line of research includes a large body of work focusing on the analysis of the attention mechanism, and on applying gradient-based methods to obtain input attribution scores. 
Recently, several works have tackled the interpretability of Transformers on the Language Modeling task. studied the Transformer from the residual stream perspective, depicted in, where different components (MLPs, attention heads. . . ) read and write to subspaces of the residual stream. This approach has aided in explaining certain behaviours of language models, like induction heads, where attention heads search over the context for previous repetitions of the same token and copy the next token, or even specialized heads solving the Indirect Object Identification (IOI) task. Similarly, MLPs inside the Transformer have also been studied as elements writing into the residual stream. observed that MLP blocks can act as key-value memories, where values add to the residual, thus promoting the prediction of words that convey similar semantic meaning. 
Furthermore, the attention mechanism in the Transformer, composed of attention heads, an output weight matrix, and a layer normalization, can be decomposed into an interpretable operation, providing layer-wise explanations which have proven to be highly faithful. 
In this work, we propose explaining the predictions of Transformers language generators by combining the residual stream analysis perspective with the attention decomposition. Our approach measures the amount of logit (pre-activation of the softmax) added or subtracted by each token representation at each layer. We then track the logit contributions back to the model' s input by aggregating across layers (Logit explanation). Additionally, we consider the mixing of information in intermediate layers by using ALTI (ALTI-Logit explanation). 
To evaluate the proposed interpretability methods, we follow the recently introduced contrastive explanations framework, which aims to explain why the model predicted one token instead of a foil token, a priori explained by some linguistic phenomena evidence. Then, we analyze the role of MLPs and show that they aid the model in determining predictions that follow grammar rules. Finally, we demonstrate that NMT models generate human-like source-target alignments for building translations."
Clinical Note Owns its Hierarchy: Multi-Level Hypergraph Neural Networks for Patient-Level Representation Learning,2305.09756v1,./img_ACL_2023/2305.09756v1.pdf," (a) Examples of patient clinical notes with difficult contents (e.g.,jargons and abbreviations) and complex structures. Patient p_1 owns notes of radiology taxonomy (pink) and nursing taxonomy (blue). (b) Differences between existing hypergraphs and our proposed multi-level hypergraphs.","Leveraging knowledge from electronic health records (EHRs) to predict a patient's condition is essential to the effective delivery of appropriate care. Clinical notes of patient EHRs contain valuable information from healthcare professionals, but have been underused due to their difficult contents and complex hierarchies. Recently, hypergraph-based methods have been proposed for document classifications. Directly adopting existing hypergraph methods on clinical notes cannot sufficiently utilize the hierarchy information of the patient, which can degrade clinical semantic information by (1) frequent neutral words and (2) hierarchies with imbalanced distribution. Thus, we propose a taxonomy-aware multi-level hypergraph neural network (TM-HGNN), where multi-level hypergraphs assemble useful neutral words with rare keywords via note and taxonomy level hyperedges to retain the clinical semantic information. The constructed patient hypergraphs are fed into hierarchical message passing layers for learning more balanced multi-level knowledge at the note and taxonomy levels. We validate the effectiveness of TM-HGNN by conducting extensive experiments with MIMIC-III dataset on benchmark in-hospital-mortality prediction.","With improvement in healthcare technologies, electronic health records (EHRs) are being used to monitor intensive care units (ICUs) in hospitals. Since it is crucial to schedule appropriate treatments for patients in ICUs, there are many prognostic models that use EHRs to address related tasks, such as in-hospital mortality prediction. EHRs consist of three types of data; structured, semi-structured, and unstructured. Clinical notes, which are unstructured data, contain valuable comments or summary of the patient 's condition written by medical professionals (doctors, nurses, etc. ). However, compared to structured data, clinical notes have been underutilized in previous studies due to the difficult-to-understand contents and the complex hierarchies (Figure (a) ). Transformer-based methods like ClinicalBERT have been proposed to pre-train on large-scale corpus from similar domains, and fine-tune on the clinical notes through transfer learning. While Transformer-based methods can effectively detect distant words compared to other sequence-based methods like convolutional neural networks and recurrent neural networks, there are still limitations of increasing computational complexity for long clinical notes (Figure). 
Recently, with the remarkable success of the graph neural networks (GNNs), graph-based document classification methods have been proposed that can capture long range word dependencies and can be adapted to documents with different and irregular lengths. Some methods build word co-occurrence graphs by sliding fixed-size windows to model pairwise interactions between words. However, the density of the graph increases as the document becomes longer. Besides, there are also some methods apply hypergraph for document classification, which can alleviate the high density of the document graphs and extract high-order structural information of the documents. 
Adopting hypergraphs can reduce burden for managing long documents with irregular lengths, but additional issues remain when dealing with clinical notes: (1) Neutral words deteriorate clinical semantic information. In long clinical notes, there are many frequently written neutral words (e.g.,""rhythm"") that do not directly represent the patient' s condition. Most of the previous methods treat all words equally at the learning stage, which may result in dominance of frequent neutral words, and negligence of rare keywords that are directly related to the patient's condition. Meanwhile, the neutral word can occasionally augment information of rare keywords, depending on the intra-taxonomy context. Taxonomy represents the category of the clinical notes, where implicit semantic meaning of the words can differ. For example, ""rhythm"" occurred with ""fibrillation"" in ECG taxonomy can represent serious cardiac disorder of a patient, but when ""rhythm"" is written with ""benadryl"" in Nursing taxonomy, it can hardly represent the serious condition. Therefore, assembling intra-taxonomy related words can leverage ""useful"" neutral words with rare keywords to jointly augment the clinical semantic information, which implies the necessity of introducing taxonomy-level hyperedges. (2) Imbalanced distribution of multi-level hyperedges. There are a small number of taxonomies compared to notes for each patient. As a result, when taxonomy-level and note-level information are learned simultaneously, note-level information can obscure taxonomy-level information. To learn more balanced multi-level information of the clinical notes, an effective way for learning the multi-level hypergraphs with imbalanced distributed hyperedges is required. 
 To address the above issues, we propose TM-HGNN (Taxonomy-aware Multi-level HyperGraph Neural Networks), which can effectively and efficiently utilize the multi-level high-order semantic information for patient representation learning. Specifically, we adopt patient-level hypergraphs to manage highly unstructured and long clinical notes and define multi-level hyperedges, i.e., note-level and taxonomy-level hyperedges. Moreover, we conduct the hierarchical message passing from note-level to taxonomy-level hyperedges using edge-masking. To hierarchically learn word embeddings without mixture of information between note and taxonomy, note and taxonomy hyperedges are disconnected. Note-level word embeddings are learned only with intra-note local information. The following taxonomy-level propagation introduce clinical semantic information by assembling the intra-taxonomy words and separating inter-taxonomy words for better patient-level representation learning. The contributions of this article can be summarized as follows (Figure): 
 
 * To address issue 1, we construct multi-level hypergraphs for patient-level representation learning, which can assemble ""useful"" neutral word with rare keyword via note and taxonomy level hyperedges to retain the clinical semantic information. 
 * To address issue 2, we propose hierarchical message passing layers for the constructed graphs with imbalanced hyperedges, which can learn more balanced multi-level knowledge for patient-level representation learning. 
 * We conduct experiments with MIMIC-III clinical notes on benchmark in-hospital-mortality task. The experimental results demonstrate the effectiveness of our approach."
Incorporating Distributions of Discourse Structure for Long Document Abstractive Summarization,2305.16784v2,./img_ACL_2023/2305.16784v2.pdf,"An example of RST tree: [Rhetorical structure theory (RST) is a theory of text organization. ] ^EDU1 [Although the RST structure is difficult to annotate, ] ^EDU2 [there are still many scholars who have studied it. ] ^EDU3","For text summarization, the role of discourse structure is pivotal in discerning the core content of a text. Regrettably, prior studies on incorporating Rhetorical Structure Theory (RST) into transformer-based summarization models only consider the nuclearity annotation, thereby overlooking the variety of discourse relation types. This paper introduces the 'RSTformer', a novel summarization model that comprehensively incorporates both the types and uncertainty of rhetorical relations. Our RST-attention mechanism, rooted in document-level rhetorical structure, is an extension of the recently devised Longformer framework. Through rigorous evaluation, the model proposed herein exhibits significant superiority over state-of-the-art models, as evidenced by its notable performance on several automatic metrics and human evaluation.","For writing a good summary of a long document, it is of paramount importance to discern the salient information within the text and to comprehend the intricate interconnections among its various components. Contemporary leading-edge systems for abstractive (long) text summarization employ Transformer encoder-decoder architecture. These sequence-to-sequence (seq2seq) models first transform the source document into a high-dimensional content representation and then decode the predicted summary conditioned on the representation. It has been demonstrated in the past that such an architecture does a poor job of digging high-level discourse structure during the encoding phase. However, discourse structure is very important for deciding what to include vs. not to include in the summary. Given that previous work has indicated that the performance of neural language models can be enhanced through the incorporation of latent structure information, we will here explore the integration of discourse relation structure into the Longformer model; this architecture has been shown to be particularly suitable for encoding long input texts. 
Rhetorical Structure Theory (RST) serves as a discourse framework designed to articulate the interrelationships among sentences at the document level. This framework distinguishes a plethora of coherence relations delineating the manner in which two text segments are interconnected (e.g., one segment might give a reason for a claim made in another segment, or alternatively, two segments may contrast with one another). Moreover, RST distinguishes between paratactic relations, where both segments carry equivalent discourse importance, and hypotactic relations, which classify the segment of greater centrality to the overarching discourse structure as the 'nucleus' and the less central one as the 'satellite'. Figure shows a simple example of an RST tree. In this instance, EDU1 serves as the nucleus of the elaboration relation, whereas the combination of EDUs 2 and 3 constitutes the satellite of said relation. Furthermore, we can see that EDU3 assumes a more central role within the concession relation, hence it is marked as its nucleus, while EDU2 holds less important: if EDU2 was left out, the elaboration relation between EDUs 1 and 3 would still hold, but if EDU3 was removed, an elaboration relation between EDU1 and EDU2 would not hold, and the coherence would be lost. As has been recognized early on, this discourse information can be effectively used in summarization tasks. 
While there have been some previous attempts at integrating discourse structure into neural text summarization models, as seen in, these approaches do not utilize relation labels and solely consider the 1-best RST tree obtained from preprocessing of a discourse parser. We argue that this leads to two significant issues: Firstly, information pertaining to relation type is overlooked, despite its known relevance to the summarization task. Secondly, there may be benefits in considering distributions over coherence relation labels, rather than limiting analysis to the 1-best results. One reason is that external discourse parsers are known to perform poorly on out-of-domain data, and may hence propagate errors into the summarization model. There is a subsequent risk that these errors will be incrementally amplified during back-propagation, thus potentially impairing the model 's performance. A second reason is that there might inherently be several coherence relations holding at the same time, which might be beneficial to represent through the distributions of the discourse structure. Hence, we posit that the output of the RST parser holds greater significance when it not only provides the model with the n-best results but also conveys the remaining uncertainty associated with them. 
In the remainder of the paper, we explore whether incorporating the labeled discourse relation structure with uncertainty, which can be understood as the distributions of discourse structure, into the attention mechanism can effectively augment the performance of neural summarization models. Our main contributions are as follows: 
 
 * We represent a generic approach for infusing labeled discourse relations with uncertainty into the encoder' s self-attention layer of Longformer, wherein the self-attention heads are made to specialize in specific discourse categories. Additionally, our modules are orthogonal to the choice of the underlying encoder-decoder Transformer-based architecture, thereby enabling them to be seamlessly incorporated into other advanced models. 
 * We provide empirical evidence supporting the notion that conveying uncertainty and introducing labeled discourse relations to the Transformer are complementary actions, both significantly contributing to the enhancement of the final performance. Our model also surpasses current state-of-the-art models across multiple evaluation metrics. 
 * Quantitative and qualitative analyses show that our model exceeds the baseline model in both novel word generation and factual consistency checking. Furthermore, our model comes closer to human answers in terms of sentence alignment and overall generation quality."
Evaluating Open-Domain Question Answering in the Era of Large Language Models,2305.06984v3,./img_ACL_2023/2305.06984v3.pdf,Examples of failures in open-domain QA evaluation. Top: Jicheng is a credible answer although not present in the list of gold answers. Existing automated evaluation mechanisms fail to identify it as correct. Bottom: A seemingly correct but unattributable answer from InstructGPT for which automatic evaluation goes astray.,"Lexical matching remains the de facto evaluation method for open-domain question answering (QA). Unfortunately, lexical matching fails completely when a plausible candidate answer does not appear in the list of gold answers, which is increasingly the case as we shift from extractive to generative models. The recent success of large language models (LLMs) for QA aggravates lexical matching failures since candidate answers become longer, thereby making matching with the gold answers even more challenging. Without accurate evaluation, the true progress in open-domain QA remains unknown. In this paper, we conduct a thorough analysis of various open-domain QA models, including LLMs, by manually evaluating their answers on a subset of NQ-open, a popular benchmark. Our assessments reveal that while the true performance of all models is significantly underestimated, the performance of the InstructGPT (zero-shot) LLM increases by nearly +60%, making it on par with existing top models, and the InstructGPT (few-shot) model actually achieves a new state-of-the-art on NQ-open. We also find that more than 50%of lexical matching failures are attributed to semantically equivalent answers. We further demonstrate that regex matching ranks QA models consistent with human judgments, although still suffering from unnecessary strictness. Finally, we demonstrate that automated evaluation models are a reasonable surrogate for lexical matching in some circumstances, but not for long-form answers generated by LLMs. The automated models struggle in detecting hallucinations in LLM answers and are thus unable to evaluate LLMs. At this time, there appears to be no substitute for human evaluation.","Reliable benchmarks have been a bedrock to measuring progress in open-domain QA, the task of answering information-seeking questions over a massive text corpus. In recent years, we have seen great strides in open-domain QA by novel models (; inter alia) that continue to raise state-of-the-art on well-established benchmarks such as Natural Questions-open. The standard procedure for evaluating open-domain QA models, borrowed from reading comprehension, is to perform lexical matching between gold answers provided in the benchmark and models' predictions. However, as the performance of open-domain QA approaches that of humans, these classic evaluation methods begin to fail. Such failures largely stem from the incomplete list of gold answers that do not fully cover all plausible answers. For example, in Figure, ""Jicheng"" is a correct answer to what was the city of Beijing previously known as? while not annotated as a gold answer in Natural Questions-open (NQ-open; ). 
With the recent success of generative QA systems in the open-domain setting, it becomes harder for lexical matching to recognize correct answers, and in turn for us, to recognize performance differences between models. The problem is exacerbated by a tendency of Large Language Models (LLM) -based systems (; inter alia) to occasionally hallucinate plausible but incorrect answers. For instance, in Figure, InstructGPT generates ""Jack Nicholson"" in great details to answer who won the oscar for best actor in 1975? but although looks natural, the answer is not factually correct (he won in 1976). Therefore, human confirmation of answer correctness demands additional effort and care due to the ability of LLMs to formulate these answers as complete and seemingly authoritative. 
While it might be assumed that improved performance under lexical matching would reflect improved performance in an absolute sense, even if some correct answers are missed, we show this assumption does not hold. For this purpose, we manually re-evaluate several open-domain QA models on a random subset of NQ-open, an established benchmark. Not only is true performance substantially underestimated by this benchmark, but the relative performance of the models alters after re-evaluation: InstructGPT (zero-shot) achieves an accuracy of 12.6%on our NQ-open subset, but our human judgment reveals its true performance to be 71.4%, a nearly +60%improvement. Our linguistic analysis of the failure cases of lexical matching, an extension of a similar study by, shows that the mismatches are mostly linguistically shallow and could be captured by simple patterns, such as regular expressions. 
In contrast, automated evaluation mechanisms such as BEM based on semantic matching between the gold answers and generated answers produce a relative performance that is mostly consistent with human evaluation, although the absolute improvements are lower. However, long-form answers, generated by LLMs, introduce a new challenge that did not occur on prior models; they are prone to carry unattributable information. Automated evaluation models often deem the hallucinated responses correct, which is why, InstructGPT (zero-shot) is overestimated under these models, compared to human judgment. 
We repeated this experiment with the 20-year-old CuratedTREC dataset that provides its gold answers in the form of regular expressions. We observe that the relative performance of models remains mostly consistent under all three evaluation mechanisms, i.e., regular expressions, human evaluation, and semantic matching, with only slight differences in absolute performance. However, the ranking discrepancy still persists between the two LLMs, i.e., InstructGPT (zero-shot) and InstructGPT (few-shot). Also, only under human judgment does the absolute performance of LLMs exceed that of the heavily engineered statistical NLP systems from 20 years ago on this collection. Until recently, the best of these classical systems has been substantially superior to even the best of the modern neural models. In light of our observations, we highlight that while semantic matching against exact answers would have been sufficient for QA evaluation prior to LLMs, they cannot accurately evaluate LLMs."
DITTO: Data-efficient and Fair Targeted Subset Selection for ASR Accent Adaptation,2110.04908v4,./img_ACL_2023/2110.04908v4.pdf,ASR Accent Adaptation using.,"State-of-the-art Automatic Speech Recognition (ASR) systems are known to exhibit disparate performance on varying speech accents. To improve performance on a specific target accent, a commonly adopted solution is to finetune the ASR model using accent-specific labeled speech. However, acquiring large amounts of labeled speech for specific target accents is challenging. Choosing an informative subset of speech samples that are most representative of the target accents becomes important for effective ASR finetuning. To address this problem, we propose (Data-efficient and faIr Targeted subseT selectiOn) that uses Submodular Mutual Information () functions as acquisition functions to find the most informative set of utterances matching a target accent within a fixed budget. An important feature of is that it supports fair targeting for multiple accents, it can automatically select representative data points from multiple accents when the ASR model needs to perform well on more than one accent. We show that is 3-5 times more label-efficient than other speech selection methods on the Indic-TTS and L2 datasets.","State-of-the-art speech recognition systems have seen tremendous progress in the last few years, with end-to-end architectures becoming a default modeling choice. While end-to-end models yield impressive Word Error Rates (WERs) and work well for certain user populations, they severely underperform when confronted with out-of-domain test utterances in target accents that are unseen or rarely seen during training. 
A common solution to address such mismatched settings is to adapt a well-trained, speaker-independent ASR model with a small amount of accent-specific target data to adapt models to the target setting. While these works propose different fine-tuning schedules that would be most beneficial given the limited amount of target data, the question of which utterances should be chosen in order to be transcribed and further used for fine-tuning has received far less attention. This is extremely important, since procuring and labeling accent-specific data is challenging and expensive. Awasthi et. al. present a method to select sentences within a fixed budget that are most likely to induce ASR errors to record accented audio on, resulting in higher-quality personalized ASR models for target accents compared to random selection. However, they assume access to a small seed set of labeled utterances from the target speaker. We address a more realistic setting wherein we have access only to a limited number of unlabeled utterances from the target domain, and without access to accented speakers to read out the selected texts. 
 ¬ß. ¬ß Our Contributions
In this work, we propose a data-efficient and fair targeted subset selection approach that makes use of a suite of submodular mutual information () functions (originally defined in). For a specific target accent, we are given access to a small number (20 in our experiments) of unlabeled speech utterances, called the target (or query) set. We aim at identifying the most informative subset of speech utterances from a large unlabeled pool of diverse accents that best matches the target set. We procure the best matching subset by maximizing an function instantiated using pairwise similarities between speech representations. We find to be an effective targeted subset selection technique for adapting ASR models in accents at multiple granularities - within Indian accents and accents around the world. uses a limited transcription budget, i.e., just around 20-35%of that of random. Furthermore, we show that can fairly select subsets that can cover multiple target accents using a facility location based function."
CoLaDa: A Collaborative Label Denoising Framework for Cross-lingual Named Entity Recognition,2305.14913v1,./img_ACL_2023/2305.14913v1.pdf,Comparison between previous methods (a/b/c) and our CoLaDa at the i-th iteration (d) CoLaDa starts at ‚Ñ≥_tgt^0 and performs denoising iteratively. ùíü_src: Source-language labeled data. ùíü_trans: Translation data. ùíü_tgt: Target-language unlabeled data with pseudo-labels generated by NER models. ‚Ñ≥_src/trans/tgt: NER model learned on ùíü_src/trans/tgt.,"Cross-lingual named entity recognition (NER) aims to train an NER system that generalizes well to a target language by leveraging labeled data in a given source language. Previous work alleviates the data scarcity problem by translating source-language labeled data or performing knowledge distillation on target-language unlabeled data. However, these methods may suffer from label noise due to the automatic labeling process. In this paper, we propose CoLaDa, a Collaborative Label Denoising Framework, to address this problem. Specifically, we first explore a model-collaboration-based denoising scheme that enables models trained on different data sources to collaboratively denoise pseudo labels used by each other. We then present an instance-collaboration-based strategy that considers the label consistency of each token's neighborhood in the representation space for denoising. Experiments on different benchmark datasets show that the proposed CoLaDa achieves superior results compared to previous methods, especially when generalizing to distant languages.","The named entity recognition (NER) task aims to locate and classify entity spans in a given text into predefined entity types. It is widely used for many downstream applications, such as relation extraction and question answering. Deep neural networks have made significant progress on this task leveraging large-scale human-annotated data for training. However, fine-grained token-level annotation makes it costly to collect enough high-quality labeled data, especially for low-resource languages. Such scenarios motivate the research on zero-shot cross-lingual NER, which attempts to leverage labeled data in a rich-resource source language to solve the NER task in a target language without annotated data. 
Recent attempts at cross-lingual NER can be roughly categorized from two aspects: learning language-independent features via feature alignment and learning language-specific features from automatically labeled target-language data. Despite bringing great success to cross-lingual NER, the former line of research misses exploiting language-specific features and thus shows substandard performance, especially when transferring to distant languages, e.g., from English to Arabic. Hence, a series of studies focuses on the latter category, which typically creates pseudo-labeled target-language data and uses it to perform conventional supervised learning or teacher-student learning. For example, as shown in Fig (a), earlier studies, such as TMP, first translate labeled data in the source language and then perform label projection. Recently, several approaches have utilized a weak model, which could be an NER model either trained on the source language 's labeled data as in TSL, or further finetuned on the generated translation data as in UniTrans, to annotate the unlabeled target-language data for improvement, as shown in Fig (b) and Fig (c). 
Unfortunately, these methods inevitably suffer from the label noise induced by inaccurate translation and label projection, or the weak model' s limited capability. Although some methods are proposed to mitigate the label noise problem by additionally training an instance selector or designing heuristic rules for data selection, they independently manipulate either the translation data (ùíü_trans) or the target-language data (ùíü_tgt) pseudo-labeled by NER models trained in the source language. Hence, all these methods ignore the complementary characteristics between both for denoising. Particularly, from the text view, ùíü_tgt is collected from a natural text distribution of the target-language data, while ùíü_trans can be regarded as a way of data augmentation to provide more lexicon variants. From the labeling function view, labels of ùíü_trans are obtained via the label projection algorithm, which have little association with those of ùíü_tgt generated by NER models. 
With such consideration, we propose a model-collaboration-based denoising scheme, which incorporates models trained on both data sources to mutually denoise the pseudo-labels of both data sources in an iterative way. As shown in Fig (d), we first leverage ‚Ñ≥_tgt trained on the pseudo-labeled target-language data ùíü_tgt to denoise the translation data annotated by label projection. In this way, the learned model ‚Ñ≥_trans will be less affected by noise in the translation data. We then employ the improved ‚Ñ≥_trans to re-label the target-language unlabeled data ùíü_tgt. It is expected that there is less noise in the relabeled data, and thus we can produce a more powerful ‚Ñ≥_tgt. We perform this procedure for several iterations, so that all the involved data sources and models can be improved in an upward spiral. 
 Moreover, borrowing the idea from anomaly detection that a given data point 's neighborhood information can be used to measure its anomalism, here we find that the similar tokens in the feature space can also collaborate for denoising. Previous studies have shown that instances with the same label are more likely to locate close to each other in the representation space. Our intuition is that, if a token' s label conflicts a lot with labels of other tokens in its neighborhood, then this label is probably noisy. Therefore, we further propose an instance-collaboration-based denoising strategy to explore the neighborhood structure of each token for denoising, as shown in Figure. Specifically, we utilize the label consistency of each token's neighborhood in the representation space to re-weight the soft-labeled examples in knowledge distillation. 
We integrate the instance-collaboration-based denoising strategy into the model-collaboration-based denoising scheme and propose a Collaborative Label Denoising framework, i.e., CoLaDa, for cross-lingual NER. We conduct extensive experiments on two popular benchmarks covering six languages for evaluation. Experimental results show that our method outperforms existing state-of-the-art methods. Qualitative and quantitative analyses further demonstrate the effectiveness of our framework in reducing the data noise."
Language model acceptability judgements are not always robust to context,2212.08979v1,./img_ACL_2023/2212.08979v1.pdf,"We measure the impact of different contexts on the performance of an LM on linguistic acceptability tasks by prefixing sentences (here, sourced from subject-verb agreement challenge sets) from a diverse collection of sources. Each block represents a sentence. Red striped blocks are unacceptable sentences within a given task, while green solid ones are acceptable. We also vary the diversity of prefixes by sampling them from tasks/datasets different from the test domain (indicated by shape).","Targeted syntactic evaluations of language models ask whether models show stable preferences for syntactically acceptable content over minimal-pair unacceptable inputs. Most targeted syntactic evaluation datasets ask models to make these judgements with just a single context-free sentence as input. This does not match language models 'training regime, in which input sentences are always highly contextualized by the surrounding corpus. This mismatch raises an important question: how robust are models' syntactic judgements in different contexts? In this paper, we investigate the stability of language models 'performance on targeted syntactic evaluations as we vary properties of the input context: the length of the context, the types of syntactic phenomena it contains, and whether or not there are violations of grammaticality. We find that model judgements are generally robust when placed in randomly sampled linguistic contexts. However, they are substantially unstable for contexts containing syntactic structures matching those in the critical test content. Among all tested models (GPT-2 and five variants of OPT), we significantly improve models' judgements by providing contexts with matching syntactic structures, and conversely significantly worsen them using unacceptable contexts with matching but violated syntactic structures. This effect is amplified by the length of the context, except for unrelated inputs. We show that these changes in model performance are not explainable by simple features matching the context and the test inputs, such as lexical overlap and dependency overlap. This sensitivity to highly specific syntactic features of the context can only be explained by the models' implicit in-context learning abilities.","The unprecedented progress in the development of neural large language models has been accompanied by a comparable proliferation of methods that aim to better understand and characterize models 'linguistic capacities. Of the many methods for this, the minimal-pair paradigm (mpp), which is methodologically standard in linguistics, has emerged as a popular approach to evaluate models' knowledge of linguistic phenomena in an unsupervised manner. Under the mpp, models are presented with datasets containing pairs of minimally differing text sequences‚Äîusually differing in word order or in a few tokens‚Äîone of which is deemed by humans to be acceptable and the other unacceptable. Drawing on the LLMs 'trained ability to produce probabilities over token sequences, we can evaluate them according to the mpp by testing whether models assign relatively greater probability to the acceptable sequence. 
Studies that employ mpp datasets generally compare the probability of two stand-alone text sequences without any explicit linguistic context (or, the probability of two words that are part of some stand-alone sentence). However, this is not a naturalistic or realistic approach: utterances usually occur in some linguistic context, where the context itself could affect linguistic preferences. The syntactic priming literature investigates the effect of linguistic contexts to some extent, but mostly in a constrained setting with only one or a small number of context sentences. The interaction of context with minimal pair accuracies remains underexplored for multi-sentence contexts, despite the fact that multi-sentence inputs are more likely for many NLP tasks‚Äîespecially with the rise of prompting and in-context learning. Furthermore, Transformer-based language models are typically trained on large sequences, where masked tokens are predicted given a completely full context window, consisting of many sentences. It is unclear how to evaluate mpp by utilizing this context window, given recent research that has raised questions about the sentence representations acquired in long-form input. 
We evaluate the sensitivity of LLMs' acceptability preferences in a more realistic evaluation setting, with one or more additional sentences in the input context. We focus on LLM sensitivity to three particular features of the context: (1) the length of the input sequence, (2) the similarity of the context to the minimal pair being judged, and (3) whether the prefix context contains ungrammatical language. illustrates our method at a high level: For a given mpp dataset, we generate new, minimal pair test examples for a given syntactic phenomenon by artificially simulating a long context window. Specifically, we prepend the given test example pair with context constructed from sentences drawn from Wikipedia (unrelated context), and compare it with contexts constructed with minimal-pair sentences from the same (in-domain) or different (out-of-domain) syntactic phenomena in the mpp dataset. 
We find that the model 's judgements are highly robust to the presence of unrelated, out-of-domain Wikipedia sentences in the context, regardless of the size of the context. However, we observe strong sensitivity to in-domain context manipulations. As the context length increases, acceptable, grammatical in-domain contexts improve the models' judgements significantly. Conversely, we observe a strong negative effect of exposing the model to longer and longer ungrammatical or unacceptable context: models' judgements degrade drastically, performing far below chance. This sensitivity is specific to the particular type of syntactic structural similarity of the context: we do not see the same degree of improvement/degradation in prediction behavior for contexts consisting of out-of-domain sentences of valid or violated syntactic structures. 
To better understand our results, we performed several exploratory analyses. We explored several linguistic features (lexical overlap, dependency overlap) to explain whether syntactic similarity can explain our results () and found that the trends cannot be explained only by these low-level overlap features. We also investigated model calibrations when subjected to prefixed stimuli using perplexity margins, to explain the changes in accuracy with different types of prefixes (Appendix). We observe that perplexity margins drastically reduce as the context length increases, which offers insights into why acceptability judgement capability of the model improves/degrades with the choice of prefix. Our results, therefore, can only be explained by the presence of implicit, instruction-free in-context learning ability of the model, and invite further scrutiny and investigation to long-form sentence understanding capabilities of LLMs."
Morphological Inflection: A Reality Check,2305.15637v1,./img_ACL_2023/2305.15637v1.png,"The four logically possible train-eval overlap types if evaluation data consists of (lemma, feature set) pairs: both, featsOnly, lemmaOnly, neither, as well as featsAttested= both ‚à™ featsOnly and featsNovel= lemmaOnly ‚à™ neither.","Morphological inflection is a popular task in sub-word NLP with both practical and cognitive applications. For years now, state-of-the-art systems have reported high, but also highly variable, performance across data sets and languages. We investigate the causes of this high performance and high variability; we find several aspects of data set creation and evaluation which systematically inflate performance and obfuscate differences between languages. To improve generalizability and reliability of results, we propose new data sampling and evaluation strategies that better reflect likely use-cases. Using these new strategies, we make new observations on the generalization abilities of current inflection systems.","footnote-1Morphological inflection is a task with wide-reaching applications in NLP, linguistics, and cognitive science. As the reverse of lemmatization, it is a critical part of natural language generation, particularly for languages with elaborate morphological systems. Since morphological inflection is a particular type of well-defined regular string-to-string mapping problem, it is also useful for testing the properties of different neural network architectures. Within cognitive science and linguistics, computational models of inflection have a long history in arbitrating between competing theories of morphological representation and acquisition, and inflection is often a focus of computational typology. 
However, despite the task 's popularity, standard evaluation practices have significant weaknesses. We discuss three aspects of these practices which hamper investigators' ability to derive informative conclusions. (1) Uniform sampling, which creates unnatural train-test splits, (2) Evaluation of single data splits, which yields unstable model rankings, and (3) uncontrolled overlaps between train and test data components, which obscure diagnostic information about systems 'ability to perform morphological generalizations. 
 ¬ß. ¬ß Practice 1: Uniform Sampling
Training and evaluation sets have been (with some exceptions) sampled uniformly by type from a corpus such as those available in the UniMorph Database. While practical to implement for corpora that lack frequency information, uniform sampling is also unrealistic because morphological forms exhibit a highly skewed Zipfian distribution in any large text. Thus, uniform sampling creates an unnatural bias towards low-frequency types. Since high frequency is correlated with irregularity across many but not all languages, this creates a bias towards more regular and reliable training items. 
We provide two alternatives for producing realistic or challenging data sets: (1) a frequency-weighted sampling strategy to achieve a more realistic distribution of out-of-vocabulary (OOV) lemmas and inflectional categories and better match practical use-cases or input during child language acquisition, and (2) a sampling strategy that explicitly balances OOV lemmas and inflectional categories in order to directly evaluate models' generalization ability along these dimensions. 
 ¬ß. ¬ß Practice 2: Single Data Splits
The current practice in inflection evaluation, employed, for example, in the SIGMORPHON, CoNLL-SIGMORPHON and SIGMORPHON-UniMorph shared tasks in recent years, examines different models with one particular data set that is considered representative of the language or the inflection task at hand. This data set, and therefore all evaluation, usually consists of one pre-defined train- (dev-) test split. 
However, this method is problematic because it implicitly assumes that the results from a single split are informative and generalizable. In reality, this assumption is untenable, particularly when facing severe data limitation, as is the case for the majority of languages in the world (cf. ): In UniMorph 4, for example, data set size varies significantly across languages, with the smallest, Manx (Celtic, IE), containing only one lemma with 14 inflected forms, and the largest, Czech (Slavic, IE) containing approximately 800,000 lemmas with 50.3 million forms. If the performance on a single split is not necessarily representative, then the original model ranking derived from the one particular data split might also not generalize well. 
The concerns outlined above were demonstrated in, which investigated model generalizability in low-resource morphological segmentation. Using data from 11 languages, they provided evidence that: (1) there are major differences in the numerical performance and rankings of each evaluated model type when using different splits from the same data set, and (2) even within a single split, large performance variability can arise for each model type when it is trained using different random seeds. These findings illustrate that common methods of model evaluation can lead to largely coincidental conclusions. We extend this approach to morphological inflection by applying multiple data splits, and evaluating variability between splits. 
 ¬ß. ¬ß Practice 3: Uncontrolled Overlaps
The typical morphological inflection task paradigm presents (lemma, inflected form, feature set) triples during training and asks a system to predict inflected forms from (lemma, feature set) pairs during evaluation. Note that since the lemma and feature set can be combined independently, it is possible for either lemmas or feature sets that appeared during training to reappear during test without any individual triple violating train-on-test. Test pairs with OOV lemmas or feature sets require a system to generalize along different morphological dimensions. Performance is likely related to the relative rates of OOV lemmas and feature sets in the evaluation split, yet existing sampling strategies generally leave these variables uncontrolled. 
We observe that uncontrolled OOV rates vary dramatically between different sampled data splits, and that uncontrolled sampling biases test sets towards ""easier"" items with in-vocabulary lemmas and feature sets. To remedy this, we argue that performance should be reported independently for items with each lemma/feature set overlap type regardless of sampling strategy. Furthermore, if a project's research goal is to evaluate the generalization ability of a model, lemma/feature set overlap-aware sampling should be used to ensure that a sufficient number of test items of each overlap type are present."
miCSE: Mutual Information Contrastive Learning for Low-shot Sentence Embeddings,2211.04928v2,./img_ACL_2023/2211.04928v2.pdf,"Schematic illustration of AMI pipeline: A) Starting from an input sentence, two views are created by drop-out augmentation (indicated with red and blue). Each view produces a different attention tensor. B) The attention tensor is sliced into tiles, and sampling is then conducted on aligned tiles. High correlation across attention-aligned tiles allows sampling without a significant shift in the attention distribution at a modest accuracy compromise. C) Subsequently, assuming a log-normal distribution of the attention tensor, the joint distribution is computed, and mutual information is maximized.","This paper presents miCSE, a mutual information-based contrastive learning framework that significantly advances the state-of-the-art in few-shot sentence embedding. The proposed approach imposes alignment between the attention pattern of different views during contrastive learning. Learning sentence embeddings with miCSE entails enforcing the structural consistency across augmented views for every sentence, making contrastive self-supervised learning more sample efficient. As a result, the proposed approach shows strong performance in the few-shot learning domain. While it achieves superior results compared to state-of-the-art methods on multiple benchmarks in few-shot learning, it is comparable in the full-shot scenario. This study opens up avenues for efficient self-supervised learning methods that are more robust than current contrastive methods for sentence embedding.","Measuring sentence similarity has been challenging due to the ambiguity and variability of linguistic expressions. The community's strong interest in the topic can be attributed to its applicability in numerous language processing applications, such as sentiment analysis, information retrieval, and semantic search. Language models perform well on these tasks but typically require fine-tuning on the downstream task and corpora. In terms of sentence embeddings, contrastive learning schemes have already been adopted successfully. The idea of contrastive learning is that positive and negative pairs are generated given a batch of samples. Whereas the positive pairs are obtained via augmentation, negative pairs are often created by random collation of sentences. Following the construction of pairs, contrastive learning forces the network to learn feature representations by pushing apart different samples (negative pairs) or pulling together similar ones (positive pairs). While some methods seek to optimize for selecting ""hard"" negative for negative pair generation, others investigated better augmentation techniques for positive pair creation. In this regard, many methods have been proposed to create augmentations to boost representation learning. Standard approaches for the augmentation aim at input data level (a. k. a discrete augmentation), which comprises word level operations such as swapping, insertion, deletion, and substitution. In contrast to that, continuous augmentation operates at the representation level, comprising approaches like interpolation or ""mixup"" on the embedding space. Most recently, augmentation was also proposed in a more continuous fashion operating in a parameter level via simple techniques such as drop-out or random span masking. The intuition is that ""drop-out"" acts as minimal data augmentation, providing an expressive semantic variation. However, it will likely affect structural alignment across views. Since positive pairs are constructed from identical sentences, we hypothesize that the structural dependency over the views should be preserved by utilizing drop-out noise. Building on this idea, we maximize the structural dependence by enforcing distributional similarity over the attention values across the augmentation views. To this end, we employ maximization of the mutual information (MI) on the attention tensors of the positive pairs. However, since attention tensors can be very high-dimensional, computing MI can quickly become a significant burden if not intractable. This paper proposes a simple solution to alleviate the computational burden of MI computation, which can be deployed efficiently. Similar to, we adopt the Log-Normal distribution to model attention. Empirical evidence confirms this model as a good fit while facilitating the optimization objective to be defined in closed form. In this case, mutual information can be provably reformulated as a function of correlation, allowing native GPU implementation. As discussed above, the proposed approach builds upon the contrastive learning paradigm known to suffer from model collapse. This issue becomes even more problematic when enforcing MI on the attention level, as it tightens the positive pairs via regularizing the attention. Therefore the selection of negative pairs becomes more critical in our setup. To this end, we utilize momentum contrastive learning to generate harder negatives. A ""tighter"" binding on positive pairs and repulsion on ""harder"" negative pairs empowers the proposed contrastive objective, yielding more powerful representations. 
Combining ideas from momentum contrastive learning and attention regularization, we propose miCSE, a conceptually simple yet empirically powerful method for sentence embedding, with the goal of integrating semantic and structural information of a sentence in an information-theoretic and Transformer-specific manner. We conjecture the relation between attention maps and a form of syntax to be the main driver behind the success of our approach. We speculate that our proposed method injects structural information into the model as an inductive bias, facilitating representation learning with fewer samples. The adopted structural inductive biases provide a ""syntactic"" prior as an implicit form of supervision during training, which promotes few-shot learning capabilities in neural language models. To validate this, we introduced a low-shot setup for training sentence embeddings. In this benchmark, we finetune the language model only with a small number of training samples. Note that this is a very challenging setup. The inherent difficulty can be attributed to the need to mitigate the domain shift in the low-shot self-supervised learning scheme. We emphasize the importance of this task, as in many real-world applications, only small datasets are often available. Such cases include NLP for low-resource languages or expert-produced texts (e.g., medical records by doctors), personalized LM for social media analysis (e.g., personalized hate speed recognition on Twitter), etc. Our proposed method significantly improves over the state-of-the-art in the low-shot sentence embedding benchmark. This is the first work that explores how to combine semantic and structural information through attention regularization and empirically demonstrates this benefit for low-shot sentence embeddings. 
Previous works: Recently, VaSCL, ConSERT, PCL and proposed contrastive representation learning with diverse augmentation strategies on positive pair. However, we proposed a principled approach for enforcing alignment in positive pairs at contrastive learning without discretely augmenting the data. Similar to us, ESimCSE and MoCoSE proposed to exploit a momentum contrastive learning model with negative sample queue for sentence embedding to boost uniformity of the representations. However, unlike us, they do not enforce any further tightening objective on the positive pairs nor consider few-shot learning. Very recently, authors in InforMin-CL and InfoCSE proposed information minimization-based contrastive learning. Specifically, the authors propose to minimize the information entropy between positive embeddings generated by drop-out augmentation. Our model differs from this paper and the method in, which focuses on using mutual information for self-supervised learning. A key difference compared to these methods is that they estimate MI directly on the representation space. In contrast, our method computes the MI on attention. Other related work include. 
The contributions of the proposed work are: First, we propose to inject structural information into language models by adding an attention-level objective. Second, we introduce Attention Mutual Information (AMI), a sample-efficient self-supervised contrastive learning. Third, we introduce low-shot learning for sentence embedding. We show that our method performs comparably to the state-of-the-art in the full-shot scenario and significantly better in few-shot learning."
Forgotten Knowledge: Examining the Citational Amnesia in NLP,2305.18554v2,./img_ACL_2023/2305.18554v2.png,Average number of unique references in an AA paper published in different years.,"Citing papers is the primary method through which modern scientific writing discusses and builds on past work. Collectively, citing a diverse set of papers (in time and area of study) is an indicator of how widely the community is reading. Yet there is little work looking at broad temporal patterns of citation. This work, systematically and empirically examines: How far back in time do we tend to go to cite papers? How has that changed over time, and what factors correlate with this citational attention/amnesia? We chose NLP as our domain of interest, and analyzed ‚àº71.5K papers to show and quantify several key trends in citation. Notably, ‚àº62%of cited papers are from the immediate five years prior to publication, whereas only ‚àº17%are more than ten years old. Furthermore, we show that the median age and age diversity of cited papers was steadily increasing from 1990 to 2014, but since then the trend has reversed, and current NLP papers have an all-time low temporal citation diversity. Finally, we show that unlike the 1990s, the highly cited papers in the last decade were also papers with the least citation diversity; likely contributing to the intense (and arguably harmful) recency focus. Code, data, and a demo are available at the project homepage.","Study the past if you would define the future.
 ‚Äî Confucius
 The goal of scientific research is to create a better future for humanity. To do this we innovate on ideas and knowledge from the past. Thus, a central characteristic of the scientific method and modern scientific writing is to discuss other work: to build on ideas, to critique or reject earlier conclusions, to borrow ideas from other fields, and to situate the proposed work. Even when proposing something that others might consider dramatically novel, it is widely believed that these new ideas have been made possible because of a number of older ideas. Citation (referring to another paper in a prescribed format) is the primary mechanism to point the reader to these prior pieces of work and also to assign credit for shaping current work. Thus, we argue that examining citation patterns across time can lead to crucial insights into what we value, what we have forgotten, and what we should do in the future. 
Of particular interest is the extent to which good older work is being forgotten ‚Äî citational amnesia. More specifically, for this paper, we define citational amnesia as shown below: 
 Citational Amnesia: the tendency to not cite enough relevant good work from the past (more than a few years old). 
 We cannot directly measure citational amnesia empirically because determining ""enough"", ""relevance"", and ""good"" require expert researcher judgment. However, what we can measure is the collective tendency of a field to cite older work. Such an empirical finding enables reflection on citational amnesia. A dramatic drop in our tendency to cite older work should give us cause to ponder whether we are putting enough effort to read older papers (and stand on the proverbial shoulders of giants). 
Note that we are not saying that old work should be cited simply because it exists. We are saying that we should consciously reflect on the diversity of the papers we explore when conducting research. Diversity can take many forms, including reading relevant papers from diverse fields, by authors from diverse regions, and relevant papers published from various time periods ‚Äî the focus of this paper. Exploring a diverse set of papers allows us to benefit from important and diverse research perspectives. Looking at older literature makes us privy to broader trends, and informs us in ways that are beneficial well beyond the immediate work. 
Historically, citational amnesia was impacted by various factors around access and invention. For example, the invention of the printing press in the year 1440 allowed a much larger number of people to access scientific writing. The era of the internet and digitization of scientific literature that began in the 1990s also greatly increased the ease with which one could access past work. However, other factors such as the birth of paradigm-changing technologies may also impact citation patterns; ushering in a trend of citing very new work or citing work from previously ignored fields of work. Such dramatic changes are largely seen as beneficial; however, strong tailwinds may also lead to a myopic focus on recent papers and those from only some areas, at the expense of benefiting from a wide array of work. 
We choose as our domain of interest, papers on Natural Language Processing (NLP), specifically those in the ACL Anthology. This choice is motivated by the fact that NLP (and other related fields of Artificial Intelligence) are in a period of dramatic change: There are notable and frequent gains on benchmark datasets; NLP technology is becoming increasingly ubiquitous in society; and new sub-fields of NLP such as Computational Social Science, Ethics and NLP, and Sustainable NLP are emerging at an accelerated rate. The incredibly short research-to-production cycle and move-fast-and-break-things attitude in NLP (and Machine Learning more broadly) has also led to considerable adverse outcomes for various sections of society, especially those with the least power. Thus reading and citing more broadly is especially important now. 
In this work, we compiled a temporal citation network of 71.5K NLP papers that were published between 1990 and 2021, along with their meta-information such as the number of citations they received in each of the years since they were published ‚Äî the Age of Citations (AoC) dataset. We use AoC to answer a series of specific research questions on what we value, what we have forgotten, what factors are associated with this citational attention/amnesia, what are the citation patterns of different types of papers, and how these citation patterns have changed over time. Finally, we show that many of the highly cited papers from the past decade have very low temporal citation diversity; and because of their wide reach, may have contributed to the intense recency focus in NLP. All of the data and code associated with the project will be made freely available on the project homepage."
Multilingual LLMs are Better Cross-lingual In-context Learners with Alignment,2305.05940v3,./img_ACL_2023/2305.05940v3.pdf,"Working example of different ICL prompts explored in this work. In example # 1, randomly selecting the prompt examples fails as it prompts irrelevant contradictions, whereas semantic alignment succeeds as it makes the context with similar reviews. In example # 2, semantic alignment fails; it extracts demonstrations about 'multiple pieces', but these are not helpful for the LLM, whereas a simple task aligner works. In the last example, it is a combination of semantic and task alignments that works.","In-context learning (ICL) unfolds as large language models become capable of inferring test labels conditioned on a few labeled samples without any gradient update. ICL-enabled large language models provide a promising step forward toward bypassing recurrent annotation costs in a low-resource setting. Yet, only a handful of past studies have explored ICL in a cross-lingual setting, in which the need for transferring label-knowledge from a high-resource language to a low-resource one is immensely crucial. To bridge the gap, we provide the first in-depth analysis of ICL for cross-lingual text classification. We find that the prevalent mode of selecting random input-label pairs to construct the prompt-context is severely limited in the case of cross-lingual ICL, primarily due to the lack of alignment in the input as well as the output spaces. To mitigate this, we propose a novel prompt construction strategy ‚Äì Cross-lingual In-context Source-Target Alignment (X-InSTA). With an injected coherence in the semantics of the input examples and a task-based alignment across the source and target languages, X-InSTA is able to outperform random prompt selection by a large margin across three different tasks using 44 different cross-lingual pairs. footnote-1","The emergence of large-scale, pretrained, Transformer-based language models (LLMs) has marked the commencement of an avant-garde era in NLP. Departing from the traditional methods of neural language learning with temporally separated training-testing phases for downstream tasks, 
 pretrained LLMs have shown the ability to infer labels from test inputs conditioned on the training data within a single pass. This is known as In-context learning ‚Äì an LLM is prompted with a few input-output pairs from the training data (commonly referred to as demonstrations) followed by the test input; for generative tasks (summarization, text-to-code, chain-of-thought reasoning, etc. ) the LLM is then required to produce an output; for classification tasks, the probabilities of the next tokens predicted by the LLM are mapped to the label space. All of this is done without updating the parameters of the LLM. In-context learning is particularly promising for two different aspects. Firstly, it reduces the need for task-specific training data, and thus, the cost of human annotation. Secondly, while the LLM was trained in a compute-intensive environment, the removal of the need for task-specific gradient-based weight updates can significantly reduce the carbon footprint of automated NLP/NLU since the inference-time compute-necessity is orders of magnitude smaller than that of the training/finetuning phases. Multiple recent advancements have been proposed to optimize the ICL ability of the LLMs. 
Challenges in cross-lingual ICL: Given that there is an order-of-magnitude discrepancy in the availability of annotated data in a high-resource language vs. a low-resource one, the ability to learn from the high-resource source context to solve tasks in low-resource targets sounds enticing. Yet, the application of ICL in a cross-lingual setting remains largely unexplored. Previous attempts at multilingual ICL use randomly selected input-label pairs to construct the prompt-context. This limits the ability of an LLM to infer from the context. As suggested, ICL emerges as the ability to infer target labels from the pretraining distribution conditioned upon the context; each input-label pair in the prompt-context are, in turn, sampled from the prompt token distribution. Theoretically, the expected prediction error decreases as the number of examples in the prompt increases. However, such infinitely long prompts are practically infeasible to attain. imposed that a distinguishability of the prompt-concept, shared across the prompt-examples, from all other possible concepts is essential for an optimal predictor. A random sampling of prompt examples is unlikely to construct a prompt with distinguishable concepts. Furthermore, given (x_i, y_i) and (x_i+1, y_i+1) as two consecutive input-label pairs in the prompt-context, the transition probability from y_i to x_i+1 is a low-probability one under the pretraining distribution. The transition becomes even more improbable if we are to simply append a test example to the prompt-context of a different language. Consider the following example of ICL prompting for cross-lingual sentiment classification: 
 1. 
 That movie was good. 
 Positive
 2. 
 Depression is the new pandemic. 
 Negative
 3. 
 Ella lo est√° haciendo bien?
 
 The text segments are concatenated from left-to-right and top-to-bottom; therefore, two English input-label pairs are followed by a Spanish test input. There are irremovable, token-level low-probability transitions from the labels to the next input sentences. On top of this, we have three completely unrelated sentences juxtaposed together with an abrupt change in language. Intuitively, it is less likely for an LLM to be able to map the third input to its correct label, positiva (positive in Spanish) following the very much convoluted patterns presented in English. 
Proposed approach: We seek to develop prompt-design strategies for ICL in a cross-lingual setting that can overcome the foregoing challenges. A two-way alignment of the source and target examples is proposed. We start with injecting semantic coherence into the prompt-context by selecting similar examples; this aligns the labeled demonstrations as well as the test inputs to share a set of common concepts. Next, we seek to enforce an alignment of task-level signals across languages. We introduce manually-designed task-specific mappings from the source language to the target language, thereby providing the LLM with a 'natural' transition from the former to the latter. Together, these two approaches constitute our proposed prompts-selection strategy, X-InSTA (Cross-lingual In-context Source-Target Alignment, see Figure for working examples). X-InSTA shows a staggering 18%relative improvement over random prompt selection averaged across three different text classification tasks in multiple different languages with English being the source language. Careful perturbations to these alignment methods disclose the importance of label space structure induced by LLMs for cross-lingual ICL. 
Our contributions are summarized below: 
 
 1. We propose X-InSTA, a novel method of aligning prompt examples in a cross-lingual scenario. To the best of our knowledge, this is the first attempt to push prompt design techniques for ICL in cross-lingual settings beyond the trivial strategy of random example selection. 
 2. We present the first, in-depth analysis of the role of semantic similarity between prompt examples for cross-lingual ICL. 
 3. A novel concept of task-based prompt alignment is presented. We show its efficacy with 44 different source-target language pairs and empirically relate this to the underlying structures of multilingual representations of the LLM."
To Copy Rather Than Memorize: A Vertical Learning Paradigm for Knowledge Graph Completion,2305.14126v1,./img_ACL_2023/2305.14126v1.pdf,Learning paradigm of embedding models.,"Embedding models have shown great power in knowledge graph completion (KGC) task. By learning structural constraints for each training triple, these methods implicitly memorize intrinsic relation rules to infer missing links. However, this paper points out that the multi-hop relation rules are hard to be reliably memorized due to the inherent deficiencies of such implicit memorization strategy, making embedding models underperform in predicting links between distant entity pairs. To alleviate this problem, we present Vertical Learning Paradigm (VLP), which extends embedding models by allowing to explicitly copy target information from related factual triples for more accurate prediction. Rather than solely relying on the implicit memory, VLP directly provides additional cues to improve the generalization ability of embedding models, especially making the distant link prediction significantly easier. Moreover, we also propose a novel relative distance based negative sampling technique (ReD) for more effective optimization. Experiments demonstrate the validity and generality of our proposals on two standard benchmarks. Our code is available at <https: //github. com/rui9812/VLP>.","Knowledge graphs (KGs) structurally represent human knowledge as a collection of factual triples. Each triple (h, r, t) represents that there is a relation r between head entity h and tail entity t. With the massive human knowledge, KGs facilitate a myriad of downstream applications. However, real-world KGs such as Freebase are far from complete. This motivates substantial research on the knowledge graph completion (KGC) task, i.e., automatically inferring missing triples. 
As an effective solution for KGC, embedding model learns representations of entities and relations with pre-designed relation operations. For example, TransE represents relations as translations between head and tail entities. RESCAL, DistMult and ComplEx model the three-way interactions in each triple. RotatE, QuatE and DualE represent relations as rotations in different dimensions. Rot-Pro further introduces the orthogonal projection for each relation. 
Essentially, embedding models learn structural constraints for every factual triple during the training period. For example, for each training triple (h, r, t), TransE constrains that the head embedding h plus the relation embedding r equals the tail embedding t. Such single-triple constraints empower embedding models to implicitly perceive (i.e., memorize) the high-order entity connections and intrinsic relation rules. As shown in Figure, by imposing the structural constraints (e.g., ùê°+ùê´=ùê≠ in TransE) on the five training triples, embedding models can memorize the entity connection (x, r_1 r_2, z) and the relation rule r_1 r_2‚Üí r. In this way, the missing link (x, r, z) can be inferred at test time without any explicit prompt. We refer to this single-triple learning paradigm as Horizontal Learning Paradigm (HLP), since the relation rules are implicitly induced by the horizontal paths between head and tail entities. 
However, this paper shows that the HLP-based embedding models are hard to reliably memorize the multi-hop relation rules, which is attributed to inevitable single-triple bias and high-demanding memory capacity. The unreliable multi-hop relation rules in the implicit memory cannot serve as rational basis for prediction, leading to the inferior performance of embedding models in predicting links between distant entity pairs. This brings us a question: is there a general paradigm for embedding models to alleviate this problem of HLP and achieve superior performance? 
We give an affirmative answer by presenting Vertical Learning Paradigm (VLP), which endows embedding models with the ability to explicitly consult related factual triples (i.e., vertical references) for more accurate prediction. Specifically, to answer (h, r, ? ), VLP first selects N relevant reference queries in the training graph, and then treats their ground-truth entities as the reference answers for embedding models to jointly predict the target t. This learning process can be viewed as an explicit copy strategy, which is different from the implicit memorization strategy of HLP, making it significantly easier to predict distant links. Moreover, to effectively optimize the models, we further propose a novel Relative Distance based negative sampling technique (ReD), which can generate more informative negative samples and reduce the toxicity of false negative samples. Note that VLP and ReD are both general techniques and can be widely applied to various embedding models. Our contributions are summarized as follows: 
 
 * We show that existing embedding models underperform in predicting links between distant entity pairs, since they are hard to reliably memorize the multi-hop relation rules. 
 * We present a novel learning paradigm named VLP, which can empower embedding models to leverage explicit references as cues for more accurate prediction. 
 * We further propose a new relative distance based negative sampling technique named ReD for more effective optimization. 
 * We conduct in-depth experiments on two standard benchmarks, demonstrating the validity and generality of the proposed techniques."
Parallel Context Windows for Large Language Models,2212.10947v3,./img_ACL_2023/2212.10947v3.pdf,"In-context learning (ICL) accuracy against n-shot training examples for the BANKING77 intent classification dataset using the model Jurassic-1-Grande (17B). The blue line shows the improvement in performance as the context window is filled with examples; the orange line shows how our Parallel Context Windows method, which adds up to four times more training examples, provides a significant boost in performance. The error bars represent the standard deviation across multiple runs, as explained in Section.","When applied to processing long text, Large Language Models (LLMs) are limited by their context window. Existing efforts to address this limitation involve training specialized architectures, and cannot be easily applied to off-the-shelf LLMs. We present Parallel Context Windows (PCW), a method that alleviates the context window restriction for any off-the-shelf LLM without further training. The key to the approach is to carve a long context into chunks (""windows""), restrict the attention mechanism to apply only within each window, and re-use the positional embeddings across the windows. Our main results test the PCW approach on in-context learning with models that range in size between 750 million and 178 billion parameters, and show substantial improvements for tasks with diverse input and output spaces. We show additional benefits in other settings where long context windows may be beneficial: multi-hop questions and retrieval-augmented question answering with multiple retrieved documents. Our results highlight Parallel Context Windows as a promising method for applying off-the-shelf LLMs in a range of settings that require long text sequences. We make our code publicly available at <https: //github. com/ai21labs/parallel-context-windows>.","A key parameter of a Large Language Model (LLM) is its context window, the number of text tokens it can process in a forward pass. Current LLM architectures limit the context window size‚Äîtypically up to several thousand tokens‚Äî because the global nature of the attention mechanism imposes computational costs quadratic in context length. This presents an obstacle to use cases where the LLM needs to process a lot of text, e.g., tackling tasks that require long inputs, considering large sets of retrieved documents for open-book question answering, or performing in-context learning when the desired input-output relationship cannot be adequately characterized within the context window. 
Previous work has addressed such obstacles by training dedicated architectures, e.g., training sparse attention mechanisms for long inputs and Fusion-in-Decoder readers for retrieved documents. However, these architectures are often tailored to specific use cases, and they are often constrained in terms of their size as a trade-off, in order to facilitate long text consumption. It remains an open problem to find an effective way to allow off-the-shelf LLMs to process text longer than its original context window, without dedicated training. 
In this paper, we introduce Parallel Context Windows (PCW), illustrated in Figure, a new approach for addressing this problem in any decoder-based LLM, and show its efficacy in several setups. PCW involves splitting long text into multiple parallel contexts, each equally accessible during output generation. Doing so consists of two simple post-hoc modifications to a pretrained LLM, neither of which requires any further training: (1) using sparse masking to allow each context window to attend only to itself, while still allowing the generated text to attend to all contexts simultaneously; and (2) reusing the model 's positional embeddings within each parallel context window, sidestepping the problem of extrapolating positional embeddings and signaling to the model that each window is equally ""close"" to the generated tokens. 
We conducted an in-depth investigation of the extent to which Parallel Context Windows can improve LLMs' ability to perform in-context learning: when a pretrained LLM is given an input sequence of concatenated ""training"" input‚Äìoutput pairs representing a task, followed by a single ""test"" input, it is able to supply the corresponding test output with high accuracy. Crucially, in the setting of in-context learning, the context window limitation inherently caps the number of training examples that can be inserted before the test example. This significantly limits the applicability of in-context learning for tasks with long or highly diverse inputs or outputs. 
We focus on these types of tasks, showing that Parallel Context Windows significantly aid in-context learning of two task families that tend to suffer from low in-context learning performance: classification tasks that have many classes and extractive question answering tasks. We experiment with GPT2 models having between 750M and 1.5B parameters, LLaMA models having between 7B and 65B parameters, and Jurassic-1 models having between 7B and 178B parameters. Notably, using 3 Parallel Context Windows for classification tasks with more than 6 classes results in average performance gains of 6.7 and 7.1 points for LLaMA models 32.5B and 65B, respectively, and 7.4,8.2, and 8.7 gains for Jurassic-1 models 7B, 17B, and 178B, respectively. (see example in Figure). Our results show that Parallel Context Windows broadens the scope of tasks that can be learned via the popular setup of in-context learning, to tasks that require more training examples than permitted in current context sizes. 
We further explore the applicability of PCW to two other settings that may benefit from the integration of several documents. One is multi-hop question answering, where the different pieces of information are shown in different windows. We show that in some cases parallel reading is beneficial, through a test case on the HotpotQA benchmark. The other setting is retrieval-augmented question answering, where we show that reading several retrieved documents in parallel is advantageous, through a test case on the Natural Questions benchmark. 
Overall, we provide clear evidence that, without any further training, Parallel Context Windows can make a large amount of text accessible to an off-the-shelf LLM during decoding. We thus see promise in further investigation of Parallel Context Windows for applying off-the-shelf LLMs in other applications that require such capabilities, such as tackling tasks with long inputs."
LAMBADA: Backward Chaining for Automated Reasoning in Natural Language,2212.13894v2,./img_ACL_2023/2212.13894v2.pdf,The search trace of Lambada on an example from the ParaRules subset of ProofWriter (the Sign Agreement and failed Fact Check modules are omitted for brevity).,"Remarkable progress has been made on automated reasoning with natural text, by using Language Models (LMs) and methods such as Chain-of-Thought and Selection-Inference. These techniques search for proofs in the forward direction from axioms to the conclusion, which suffers from a combinatorial explosion of the search space, and thus high failure rates for problems requiring longer chains of reasoning. The classical automated reasoning literature has shown that reasoning in the backward direction (i.e.,from the intended conclusion to supporting axioms) is significantly more efficient at proof-finding. Importing this intuition into the LM setting, we develop a Backward Chaining algorithm, called Lambada, that decomposes reasoning into four sub-modules. These sub-modules are simply implemented by few-shot prompted LM inference. We show that Lambada achieves sizable accuracy boosts over state-of-the-art forward reasoning methods on challenging logical reasoning datasets, particularly when deep and accurate proof chains are required.","Automated reasoning, the ability to draw valid conclusions from explicitly provided knowledge, has been a fundamental goal for AI since its early days. Furthermore, logical reasoning, especially reasoning with unstructured, natural text is an important building block for automated knowledge discovery and holds the key for future advances across various scientific domains. While in recent years tremendous progress has been made towards natural language understanding thanks to pretrained language models (LMs), the performance of these models for logical reasoning still lags behind compared to the advancements in other areas such as reading comprehension and question-answering. 
While many problems benefit from LM scaling, scaling has been observed to provide limited benefit for solving complex reasoning problems. For example, observed that for the Gopher family of LMs, the benefit of scaling for logic-based tasks is significantly worse than for other language tasks. Moreover, while finetuning initially seemed to enable logical reasoning in LMs, further exploration revealed that finetuned LMs mostly exploit spurious correlations (e.g., the correlation between the number of rules and the label) as opposed to learning to reason. Recently, prompting strategies such as Chain-of-Thought and Scratchpad have contributed to improving performance of LMs on reasoning tasks, although they have been also shown to struggle with proof planning for more complex logical reasoning problems. 
One solution to the aforementioned problems is to integrate the strength and reliability of classical AI models in logical reasoning with LMs. In the literature, there are two major approaches to logical reasoning: 
 * Forward Chaining (FC) where one starts from the facts and rules (""theory""), and iterates between making new inferences and adding them to the theory until the goal statement can be proved or disproved, 
 * Backward Chaining (BC) where one starts from the goal and uses the rules to recursively decompose it into sub-goals until the sub-goals can be proved or disproved based on the theory. 
 Previous approaches to reasoning with LMs mostly incorporate elements of FC into LMs. FC requires selecting a subset of facts and rules from the entire set, which might be difficult for an LM as it requires a combinatorial search over a large space. Moreover, deciding when to halt and declare failure to prove is challenging in FC, as also noted by, sometimes requiring specialized modules trained on intermediate labels. Indeed, the classical automated reasoning literature is heavily weighted towards BC or goal-directed strategies for proof-finding. 
In this paper, we show experimentally that BC is better suited for text-based deductive logical reasoning, as it does not require a combinatorial search for subset selection and there are more natural halting criteria for it. We develop a hybrid LAnguage Model augmented BAckwarD chAining technique (Lambada), where BC drives the high-level proof planning, and the LM performs the textual understanding and individual reasoning steps. We conduct experiments with challenging datasets for LM reasoning containing examples expressed in naturalistic text. The datasets contain proof chains of up to 5 hops in depth, and examples where the goal can neither be proved nor disproved from the provided theory. We show that Lambada achieves substantially higher deductive accuracy, and is considerably more likely to generate valid reasoning chains compared to other techniques which find correct conclusions with spurious proof traces, while also being more query efficient than other LM-based modular reasoning approaches. Our results strongly indicate that future work on reasoning with LMs should incorporate backward chaining or goal-directed planning strategies."
BERM: Training the Balanced and Extractable Representation for Matching to Improve Generalization Ability of Dense Retrieval,2305.11052v1,./img_ACL_2023/2305.11052v1.pdf,Idea of our method. R1: semantic unit balance. R2: essential matching unit extractability.,"Dense retrieval has shown promise in the first-stage retrieval process when trained on in-domain labeled datasets. However, previous studies have found that dense retrieval is hard to generalize to unseen domains due to its weak modeling of domain-invariant and interpretable feature (i.e., matching signal between two texts, which is the essence of information retrieval). In this paper, we propose a novel method to improve the generalization of dense retrieval via capturing matching signal called BERM. Fully fine-grained expression and query-oriented saliency are two properties of the matching signal. Thus, in BERM, a single passage is segmented into multiple units and two unit-level requirements are proposed for representation as the constraint in training to obtain the effective matching signal. One is semantic unit balance and the other is essential matching unit extractability. Unit-level view and balanced semantics make representation express the text in a fine-grained manner. Essential matching unit extractability makes passage representation sensitive to the given query to extract the pure matching information from the passage containing complex context. Experiments on BEIR show that our method can be effectively combined with different dense retrieval training methods (vanilla, hard negatives mining and knowledge distillation) to improve its generalization ability without any additional inference overhead and target domain data.","Dense retrieval encodes the texts to dense embeddings and efficiently gets the target texts via approximate nearest neighbor search. Compared with the traditional word-to-word exact matching methods such as BM25, dense retrieval can capture the relevance at the semantic level of two texts. Because of the excellent performance in efficiency and effectiveness, dense retrieval has been widely used in first-stage retrieval that efficiently recalls candidate documents from the large corpus. 
However, recent studies show that the excellent performance of dense retrieval relies on the training on large in-domain datasets. When the trained dense retrieval models are applied to the domains that are inconsistent with the training datasets (i.e., zero-shot setting), the performance of the models drops seriously. The poor generalization limits the application scenarios of dense retrieval because it is common that not enough training samples can be obtained in some domains such as medicine, biology and law that have restrictions on data privacy or require professional knowledge to annotate. 
In this work, we point out that according to out-of-domain generalization learning theory, making the model capture domain-invariant feature (i.e., essence of tasks) is effective in improving generalization ability. As for dense retrieval, matching signal between query and passage is the important domain-invariant feature and reflects the essence of information retrieval (IR). For example, MoDIR shows that representation from the interaction-based cross-encoder (more fine-grained description for matching) is much more domain-invariant than it from dense retrieval. Match-Prompt, NIR-Prompt and MatchPyramid point out the positive significance of matching signals for various IR tasks. The challenge of making dense retrieval model learn to capture matching signal is that in many IR tasks such as open-domain question answering and document retrieval, the content that matches the query is usually only a unit of the text. The description of matching signal needs to distinguish the matching and not matching information in the text and estimate the overall relevance. This requires the retrieval model to be able to evenly express each unit in the text and dynamically extract matching units through the interaction of the two text representations. However, the requirement on efficiency in first-stage retrieval makes dense retrieval only estimate relevance via vector similarity such as dot product and cosine. Previous training methods based on this architecture lack the above capability because of the coarse-grained training objective and interaction. 
In this paper, we propose a novel method called BERM to capture the matching signal between query and passage, which is the domain-invariant feature, to improve the generalization ability of dense retrieval during the training on the single source domain without using the target domain data and other additional modules. First, we introduce a novel concept in dense retrieval, the matching representation. Matching representation is determined by the text representations (output of text encoder) of query and passage, which can reflect the matching information of query and passage. We propose that in the training of dense retrieval models, in addition to using contrastive loss to optimize the text representation, the information of the matching representation can be used as a constraint to assist the optimization. Based on this, we divide the single passage into multiple units (each sentence is a unit) and propose two requirements on the generalizable dense retrieval models as the constraint in training (shown in Figure). One is semantic unit balance of text representation (R1). The other is essential matching unit extractability of matching representation (R2). These two requirements can be integrated into different dense retrieval training methods and address the challenge mentioned above. R1 means the semantics of units in a passage are implicitly aggregated to its text representation and the text representation should evenly and comprehensively express the semantics of each unit. R2 means that the combination of text representations of query and passage (i.e., matching representation) should extract the information of the matching (i. e, the text chunk in the passage that matches the query and we call it essential matching unit) while reducing the overfitting of domain biases. This reflects the ability of the dense retrieval model to determine and score the information that really matches the query in a passage containing complex context, which is the essence of the dense retrieval and domain-invariant. R1 and R2 achieve that on the premise that the text representation expresses each unit in a balanced manner, to make essential matching units for different queries be extracted, the semantics of units tend to be orthogonal to each other. In this way, in dot product between representations of query and passage, the semantics of essential matching unit are preserved, while the other units are masked, which is suitable for matching. 
Experiments on the standard zero-shot retrieval benchmark (BEIR) show that our method can be effectively combined with different dense retrieval training methods (vanilla, hard negatives mining, and knowledge distillation) to improve the generalization ability without any additional modules, inference overhead, and target domain data. Even in domain adaptation, our method is also effective and performs better than baselines."
Multiview Identifiers Enhanced Generative Retrieval,2305.16675v1,./img_ACL_2023/2305.16675v1.pdf,"An example of multiview identifiers for a passage. Corresponding to the query ""Who is the singer of does he love you? "", the semantic-related identifiers are highlighted in red.","Instead of simply matching a query to pre-existing passages, generative retrieval generates identifier strings of passages as the retrieval target. At a cost, the identifier must be distinctive enough to represent a passage. Current approaches use either a numeric ID or a text piece (such as a title or substrings) as the identifier. However, these identifiers cannot cover a passage's content well. As such, we are motivated to propose a new type of identifier, synthetic identifiers, that are generated based on the content of a passage and could integrate contextualized information that text pieces lack. Furthermore, we simultaneously consider multiview identifiers, including synthetic identifiers, titles, and substrings. These views of identifiers complement each other and facilitate the holistic ranking of passages from multiple perspectives. We conduct a series of experiments on three public datasets, and the results indicate that our proposed approach performs the best in generative retrieval, demonstrating its effectiveness and robustness. The code is released at <https: //github. com/liyongqi67/MINDER>.","Text retrieval is a fundamental task in information retrieval and plays a vital role in various language systems, including search ranking and open-domain question answering. In recent years, the dual-encoder approach, which encodes queries/passages into vectors and matches them via the dot-product operation, has been the de-facto implementation. However, this approach is limited by the embedding space bottleneck and missing fine-grained interaction. 
 An emerging alternative to the dual-encoder approach is generative retrieval. Generative retrieval utilizes autoregressive language models to generate identifier strings of passages, such as titles of Wikipedia pages, as an intermediate target for retrieval. The predicted identifiers are then mapped as ranked passages in a one-to-one correspondence. Employing identifiers, rather than generating passages directly, could reduce useless information in a passage and makes it easier for the model to memorize and learn. At a cost, the identifier must be distinctive enough to represent a passage. Therefore, high-quality identifiers have been the secret to effective generative retrieval. 
Previous studies have explored several types of identifiers, such as titles of documents, numeric IDs, and distinctive substrings. However, these identifiers are still limited: numeric IDs require extra memory steps and are ineffective in the large-scale corpus, while titles and substrings are only pieces of passages and thus lack contextualized information. More importantly, a passage should answer potential queries from different views, but one type of identifier only represents a passage from one perspective. 
In this work, we argue that generative retrieval could be improved in the following ways: (1) Synthetic identifiers. To address the limitations of titles and substrings in providing contextual information, we propose to create synthetic identifiers that are generated based on a passage 's content. In practice, we find the pseudo-queries, that are generated upon multiple segments of a passage, could serve as effective synthetic identifiers. For example, as shown in Figure, the pseudo-query ""What is the first song in the album Greatest Hits Volume Two about? "" spans multiple sentences in the passage. Once a query could be rephrased into a potentially-asked pseudo-query, the target passage could be effectively retrieved. (2) Multiview identifiers. We believe that a single type of identifier is not sufficient to effectively represent a passage. Using multiple types of identifiers, such as titles, substrings, and synthetic identifiers, can provide complementary information from different views. (i) One type of identifier, like the title, may be unavailable in some scenarios. In this case, synthetic identifiers could alternatively work. (ii) Different views of identifiers are better suited for different types of queries. Titles could respond to general queries, while substrings are more effective for detailed ones. And the synthetic identifiers could cover some complex and difficult queries that require multiple segments. (iii) For one specific query, passages could be scored and ranked holistically from different views. 
Based on the above insights, we propose the Multiview Identifiers eNhanceD gEnerative Retrieval approach, MINDER, as illustrated in Figure. To represent a passage, we assign three views of identifiers: the title, substring, and synthetic identifiers (pseudo-queries). MINDER takes a query text and an identifier prefix indicating the type of identifier to be generated as input, and produces the corresponding identifier text as output. Passages are ranked based on their coverage with the predicted three views of identifiers. We evaluate MINDER on three public datasets, and the experimental results show MINDER achieves the best performance among the current generative retrieval methods. 
The key contributions are summarized: 
 
 -0em 
 * We are the first to propose synthetic identifiers (generated based on the passage' s content) to integrate contextualized information. In practice, we find pseudo-queries could serve as effective synthetic identifiers. 
 * This is the first work that considers multiple views of identifiers simultaneously. Passages could be ranked holistically from different perspectives. 
 * Our approach achieves state-of-the-art performance in generative retrieval on three widely-used datasets."
Prompting Language Models for Linguistic Structure,2211.07830v2,./img_ACL_2023/2211.07830v2.pdf,Sequence tagging via structured prompting. Each predicted label is appended to the context along with the next word to iteratively tag the full sentence.,"Although pretrained language models (PLMs) can be prompted to perform a wide range of language tasks, it remains an open question how much this ability comes from generalizable linguistic understanding versus surface-level lexical patterns. To test this, we present a structured prompting approach for linguistic structured prediction tasks, allowing us to perform zeroand few-shot sequence tagging with autoregressive PLMs. We evaluate this approach on part-of-speech tagging, named entity recognition, and sentence chunking, demonstrating strong few-shot performance in all cases. We also find that while PLMs contain significant prior knowledge of task labels due to task leakage into the pretraining corpus, structured prompting can also retrieve linguistic structure with arbitrary labels. These findings indicate that the in-context learning ability and linguistic knowledge of PLMs generalizes beyond memorization of their training data.","The rapid increase in the scale of pretrained language models (PLMs) has led to a new paradigm of NLP modeling: in-context learning, or prompting. In this setting, the model is used to perform a task directly via the predictions of the LM head without additional finetuning on the target task, often with a few demonstrations of the desired behavior provided within the input. This setup has led to impressive few-shot performance on various tasks ranging from classification to summarization and generation. 
Due to their broad success on tasks requiring language understanding, we hypothesize that these models also contain significant linguistic knowledge. However, we are not aware of existing prompting methods that can directly test this hypothesis on autoregressive PLMs. Behavioral analysis of PLMs uses methods similar to prompting to measure knowledge stored in language models, but this technique is difficult to generalize to tasks that predict more complex structures. Additionally, current approaches for applying PLMs to linguistic structured prediction tasks finetune on the downstream task, which confounds measuring underlying model knowledge. 
We propose a new approach, structured prompting, that iteratively prompts autoregressive PLMs to probe for wordand span-level linguistics framed as sequence tagging tasks (Section). At timestep t, a label for the t-th word in the sequence is decoded from the LM; the model prediction is then fed back into the model along with the next word to progress to timestep t+1. We evaluate our approach on three sequence tagging tasks: POS tagging, sentence chunking, and NER. Our experiments show that PLMs can perform effective few-shot sequence tagging in the structured prompting setup, and that performance increases with the demonstration set size and model size, consistent with other prompting methods (Section). 
We further analyze structured prompting by examining how the model generalizes to various representations for labels (Section) as well as by analyzing the presence of task data in the pretraining corpus and how this affects model performance (Section). These experiments show that structured prompting can recover linguistic information from the model without using standard task labels, indicating that PLMs contain this knowledge in a general manner beyond memorization of the task from pretraining data. Interestingly, while PLMs perform best with meaningful labels (such as original task labels or full class names in English), the model can also in-context learn from arbitrary labels. Additionally, the model exhibits strong prior knowledge of the task labels' mapping onto the underlying classes, likely due to the prevalence of task data in the pretraining corpus. 
The contributions of this work are therefore threefold: (1) we introduce a new paradigm, structured prompting, that probes PLMs for sequence knowledge without further training, (2) we find that this approach recovers linguistic structure from PLMs in a few-shot manner, and (3) we present an analysis to quantify the effect of label form and pretraining data on in-context learning performance. Overall, our findings provide insight into both the linguistic generalizations learned by PLMs and how in-context learning works in general."
RE-Matching: A Fine-Grained Semantic Matching Method for Zero-Shot Relation Extraction,2306.04954v1,./img_ACL_2023/2306.04954v1.pdf,"An example of the matching pattern of relational data. The input sentence contains a given entity pair, which should match the corresponding hypernyms (usually the entity type). The context describes the relations between entities, containing relation-irrelevant redundant information, which should be ignored when matching.","Semantic matching is a mainstream paradigm of zero-shot relation extraction, which matches a given input with a corresponding label description. The entities in the input should exactly match their hypernyms in the description, while the irrelevant contexts should be ignored when matching. However, general matching methods lack explicit modeling of the above matching pattern. In this work, we propose a fine-grained semantic matching method tailored for zero-shot relation extraction. Following the above matching pattern, we decompose the sentence-level similarity score into entity and context matching scores. Due to the lack of explicit annotations of the redundant components, we design a feature distillation module to adaptively identify the relation-irrelevant features and reduce their negative impact on context matching. Experimental results show that our method achieves higher matching F_1 score and has an inference speed 10 times faster, when compared with the state-of-the-art methods.","Relation extraction (RE) is a fundamental task of natural language processing (NLP), which aims to extract the relations between entities in unstructured text. Benefitting from high-quality labeled data, neural relation extraction has achieved superior performance. However, it is expensive and even impractical to endlessly label data for a fast-growing number of new relations. 
 In order to deal with the emerging new relations that lack labeled data, zero-shot relation extraction (ZeroRE) has recently attracted more attention. frame the ZeroRE as a slot-filling task solved in a question-answering way. Each relation is associated with a few question templates. However, the templates are expensive and time-consuming to build. simplify the templates to readily available relational descriptions and reformulate ZeroRE as a semantic matching task. Recently, pretrained model based ZeroRE methods have achieved great success. Siamese scheme and full encoding scheme are two mainstream methods for matching semantics. The siamese scheme separately encodes the input and description. Therefore, the encoded representations of descriptions can be both stored and reused for each input, resulting in a fast inference. However, insufficient interaction during encoding also limits the matching performance. By contrast, the full encoding scheme performs self-attention over the pair to enrich interaction, although the performance increase comes with a computational overhead. (For m inputs and n descriptions, the siamese scheme requires m+n encodings, while the number is m√ó n for full encoding). An approach that combines the advantages of both can be attractive. 
Unlike ordinary sentence pairs, relational data has a unique matching pattern, which is not explicitly considered by general matching methods. As shown in fig. , the entities in the input should exactly match their hypernyms in the description (e.g., Apple and organization). Meanwhile, not all contextual words contribute equally to the relation semantics. For example, the clause ""is a great company"" is only used to modify Apple instead of expressing the relationship between Apple and California. Such redundant components should be ignored when matching. Due to the lack of explicit annotations to the redundant components, it is non-trivial for the model to learn to identify them. 
In this work, we propose a fine-grained semantic matching method that improves both the accuracy and speed over the current state-of-the-art. Specifically, we decouple encoding and matching into two modules. While the encoding module follows a siamese scheme for efficiency, the matching module is responsible for the fine-grained interaction. Following the matching pattern of relational data, the sentence-level similarity score is decomposed into two: entity matching and context matching scores. To deal with the redundant components without explicit annotations, we design a feature distillation module. Context features that maximize a classification loss are identified as relation-irrelevant features. Then, the context representations are projected into the orthogonal space of the features to improve context matching. Experimental results show that this method outperforms state-of-the-art (SOTA) methods for ZeroRE, in terms of both accuracy and speed. Our codes are publicly available.
The main contributions are three-fold: (1) We propose a fine-grained semantic matching method for ZeroRE, which explicitly models the matching pattern of relational data; (2) We propose a context distillation method, which can reduce the negative impact of irrelevant components on context matching; (3) Experimental results show that our method achieves SOTA matching F_1 score together with an inference speed 10 times faster."
Annotating and Detecting Fine-grained Factual Errors for Dialogue Summarization,2305.16548v1,./img_ACL_2023/2305.16548v1.pdf,Example summaries that are factually consistent and inconsistent with a source dialogue.,"A series of datasets and models have been proposed for summaries generated for well-formatted documents such as news articles. Dialogue summaries, however, have been under explored. In this paper, we present the first dataset with fine-grained factual error annotations named DiaSumFact. We define fine-grained factual error detection as a sentence-level multi-label classification problem, and we evaluate two state-of-the-art (SOTA) models on our dataset. Both models yield sub-optimal results, with a macro-averaged F1 score of around 0.25 over 6 error classes. We further propose an unsupervised model EnDeRanker via candidate ranking using pretrained encoder-decoder models. Our model performs on par with the SOTA models while requiring fewer resources. These observations confirm the challenges in detecting factual errors from dialogue summaries, which call for further studies, for which our dataset and results offer a solid foundation.","Factual inconsistency in abstractive summarization ‚Äî a phenomenon where model-generated summaries contain facts that are inconsistent with the source document ‚Äî is a widely known problem and has been studied extensively in the document summarization community. An example is shown in Figure, where the source document is a dialogue ‚Äî the type of documents that this paper focuses on. 
Existing work covers topics on factual inconsistency including error typology and factuality annotations of state-of-the-art neural summarization models, automatic factual error detectors, methods to correct factual errors in summaries and methods to produce factually more consistent summaries. Almost all of these works focus on news summarization based on two datasets: CNN/DailyMail and XSum. 
Dialogue summarization (cf Figure), which aims to produce a condensed version of a dialogue while maintaining its salient information, is equally important due to its application to summarizing meeting transcripts, daily conversations, customer service dialogues and medical dialogues. However, factual consistency in dialogue summarization is under explored as there are currently no benchmark datasets that contain fine-grained error categories. This paper aims to fill in this gap. 
To investigate factual consistency in dialogue summarization, we release DiaSumFactwith fine-grained sentence-level annotations regarding factual consistency for 475 model summaries (1,340 sentences) from six neural dialogue summarization models on two popular datasets: SAMSum and QMSum. We adopt a two-dimensional typology that considers the semantic roles and verifiability of error spans separately. 
We formulate factual error detection as a sentence-level multi-label classification task and use DiaSumFactto evaluate two state-of-the-art factual error detection models designed for document summarization. As there are no existing error detection model for fine-grained error categories, we adapt the two binary classification models to fit to our task. Empirical results show that they don't work well on the task, indicating its difficulty and the domain gap between document summarization and dialogue summarization. 
We then propose two models: BertMultiand EnDeRanker. BertMultiis a multi-class classification model trained on synthetic data, which is created by corrupting sentences from reference summaries. EnDeRankeris a simple unsupervised model that can leverage any pretrained encoder-decoder model to detect factual errors. Given a model-generated summary sentence containing a span of interest for error detection, EnDeRankercomputes log likelihood scores for the sentence and its variants containing replacement spans fetched from the source dialogue. The scores are computed as BARTScore, which will be explained in. We compare the scores of the sentences to determine if the span of interest and hence the summary sentence contains a factual error. We run experiments with T5, BART and PEGASUS, fine-tuned either on news summarization or dialogue summarization, as the encoder-decoder for EnDeRanker. The results show that BertMultiand EnDeRankerperforms on par with the adapted state-of-the-art models in terms of macro-averaged F1. 
Motivated by the strong complementarity between models, we further present two ensemble models combining the four models above. The results, while exceeding those of the individual models, are still far from indicating a practical model for factual error detection over dialogue summaries. This calls for further studies, for which our dataset and results form a solid foundation. 
To summarise, this paper makes the following contributions: 
 
 * We annotate and present DiaSumFact, the first dataset with fine-grained sentence-level factual errors for dialogue summarization, providing rich annotation including error classes, erroneous spans and explanation. 
 * We investigate the effectiveness of adapting state-of-the-art factual error detection models for document summarization on model-generated dialogue summaries, demonstrating the difficulty of the task. 
 * We propose BertMulti, a weakly-supervised multi-class classifier and EnDeRanker, an unsupervised factual error detector that requires no human labeled data for training and can leverage existing pre-trained encoder-decoder models. Both models perform on par with adapted SOTA factual error detection models for document summarization. 
 * Our experiments and analyses reveal the strengths and weaknesses of different factual error detection models, and point out future directions to improve them."
Deep Model Compression Also Helps Models Capture Ambiguity,2306.07061v1,./img_ACL_2023/2306.07061v1.png,Visualization of feature distributions from RoBERTa-base encoder layers using t-SNE,"Natural language understanding (NLU) tasks face a non-trivial amount of ambiguous samples where veracity of their labels is debatable among annotators. NLU models should thus account for such ambiguity, but they approximate the human opinion distributions quite poorly and tend to produce over-confident predictions. To address this problem, we must consider how to exactly capture the degree of relationship between each sample and its candidate classes. In this work, we propose a novel method with deep model compression and show how such relationship can be accounted for. We see that more reasonably represented relationships can be discovered in the lower layers and that validation accuracies are converging at these layers, which naturally leads to layer pruning. We also see that distilling the relationship knowledge from a lower layer helps models produce better distribution. Experimental results demonstrate that our method makes substantial improvement on quantifying ambiguity without gold distribution labels. As positive side-effects, our method is found to reduce the model size significantly and improve latency, both attractive aspects of NLU products.","Datasets constructed for natural language understanding (NLU) tasks, such as natural language inference (NLI) and text emotion analysis, contain a large amount of ambiguous samples. As exemplified in Table, each ambiguous sample is too debatable to be assigned a single gold label. Recent work has revealed that these disagreements among annotators are not annotation noise, which could have simply been resolved by aggregating more annotations, but rather a reproducible signal. This suggests that NLU models should predict not only majority labels, but also label distributions that respect such ambiguity. 
Since Transformer-based pre-trained language models (PLMs) have become popular for NLU tasks, the accuracies of various NLU models have been substantially improved. Nevertheless, they are still not good at approximating the human opinion distributions, or label distributions drawn from a larger number of annotators, and their predictions tend to be over-confident. If NLU products frequently produce over-confident predictions for ambiguous samples, it is not likely that they would be reliable for users who have different opinions. 
As an attempt to address this problem, previous work has demonstrated that label smoothing helps make the prediction distributions close to human opinion distributions, simply addressing the issue of over-confidence. However, this does not explicitly address how to exactly capture the degree of relationship between each sample and its candidate classes (i.e., how to estimate p (y=c|x) for each sample x). Some researchers have tried to use empirically-gold label distributions for directly learning the relationship, but these approaches require significant additional annotation costs. 
In this paper, we propose a novel method that employs compression techniques for deep learning models, namely layer pruning and knowledge distillation (KD), and show how these compression techniques help models capture such a degree of relationship. We first observe that hidden states in lower layers more accurately encode the information about the sample-classes relationship, and that validation accuracies from internal classifiers inserted between adjacent layers are converging. This indicates that pruning a part of higher layers can make the models well represent the relationship information with their prediction distribution, while retaining the accuracy. We also observe that transferring the distribution knowledge that represents more accurate information about the relationship from a lower layer into the final classifier at the top of the pruned network can help the models produce better distribution. 
Experimental results demonstrate that our method significantly outperforms existing ones that do not use additional distribution datasets. Without using such additional resources, our method also outperforms, or is comparable with, those that do use these resources over NLI benchmarks. Moreover, since our method uses compression techniques for deep learning models, this also reduces the model size significantly and improves latency as well. Both are attractive aspects of NLU products because they lead to consequent reduction in the cloud cost or to deployment on cheaper on-devices. 
Deep model compression aims at eliminating redundant components of pre-trained deep learning models (via pruning or low-rank factorization) to improve latency and reduce the model size. At the same time, maintaining the performance of the original model (via KD) is essential. While the goal of compression itself is not directly relevant to capturing ambiguity, we demonstrate that compression methods can also be used for accurately capturing ambiguity and suggest that such an approach presents another novel research direction for this task."
Soft Language Clustering for Multilingual Model Pre-training,2306.07610v1,./img_ACL_2023/2306.07610v1.pdf,"Model architecture of XLM-P. For each input, we look up the prompts in the key-value prompt pool { (k_j, P_j) }, where P_j ‚àà‚Ñù^L_p √ó D and L_p is the number of vectors for each prompt. The retrieved prompts are prepended to the input embeddings E, and serve as soft categorization information across languages.","Multilingual pre-trained language models have demonstrated impressive (zero-shot) cross-lingual transfer abilities, however, their performance is hindered when the target language has distant typology from source languages or when pre-training data is limited in size. In this paper, we propose XLM-P, which contextually retrieves prompts as flexible guidance for encoding instances conditionally. Our XLM-P enables (1) lightweight modeling of language-invariant and language-specific knowledge across languages, and (2) easy integration with other multilingual pre-training methods. On the tasks of XTREME including text classification, sequence labeling, question answering, and sentence retrieval, both baseand large-size language models pre-trained with our proposed method exhibit consistent performance improvement. Furthermore, it provides substantial advantages for low-resource languages in unsupervised sentence retrieval and for target languages that differ greatly from the source language in cross-lingual transfer.","Multilingual pre-trained language models (mPLMs) such as mBERT, mBART, XLM-R and mT5 have lately produced notable advancements in a number of downstream NLP tasks. In particular, the use of mPLMs significantly enhances few-shot fine-tuning and makes possible efficient zero-shot cross-lingual transfer. Essentially, an ideal mPLM should satisfy two properties: alignment between language pairs, which has been widely studied in the literature; and a good trade-off between high-resource and low-resource languages, which remains largely unexplored despite the success of mPLMs. 
In this paper, we focus on the second property, specially the potential for model performance to suffer when a large number of languages are added. This can occur due to restricted model capacity or computational limitations, resulting in underrepresented languages being allocated less capacity. Furthermore, the model 's coverage of world' s languages remains inadequate, limiting the range of language technology applications it can support. A typical solution for the coverage-performance trade-off in multilingual learning is to assign additional model parameters to specific languages, such as language identity embeddings, adaptors, and language-aware layers. However, it is impractical for multilingual pre-training to maintain a separate component for each language, which can lead to more complicated and challenging optimization, especially for low-resource languages. 
We propose to approach the above language-aware components from a different perspective. In linguistic typology, some patterns such as nominative-accusative alignment have broad global distributions, whereas others like morphology are more specific and detailed. To take advantage of this, we introduce XLM-P, which uses a set of compact embeddings to represent soft clustering of the language patterns beyond language identity. We refer these embeddings as prompts, due to their similarity to prompt tuning. Concretely, we build a key-value prompt pool and use the attention mechanism to look up the prompts for each input. The retrieved prompts are then prepended to the input embeddings, and serve as categorization information to adapt the model weights conditionally. This allows for more efficient and effective multilingual learning by leveraging the patterns and similarities across languages rather than maintaining separate components for each language. 
We evaluate the proposed XLM-P on Cross-Lingual Natural Language Understanding tasks and Cross-Lingual Sentence Retrieval tasks of the XTREME benchmark, and the consistent improvement in performance demonstrates its effectiveness. In addition, we conduct empirical analyses to investigate the underlying reasons of the improvement of XLM-P. The advantages of XLM-P can be summed up as follows: 
 
 * The prompt pool and instance-wise prompt retrieval are lightweight and only result in 0.35%and 0.23%increase in parameters for the base and large models, respectively. When fine-tuning on downstream tasks, the prompt module can be easily added or removed as needed. 
 * Our XLM-P divides the prompts into general and specific ones without any explicit supervision. The dynamically retrieved instance-wise prompts tame the sentence encoding, thus enhancing the capability of multilingual pre-trained models. 
 * The prompt module is model-agnostic and can be outfitted with the other frameworks (e.g., encoder-decoder style PLMs) and multilingual pre-training objectives (e.g., contrastive learning used in this paper). 
 Overall, XLM-P is a versatile and efficient approach for improving multilingual pre-training."
Large Language Models Meet NL2Code: A Survey,2212.09420v2,./img_ACL_2023/2212.09420v2.pdf,"A simple example of the NL2Codetask. The code blocks marked in grey, green, and yellow represent the natural language problem description, the predicted code solution, and the test cases, respectively.","The task of generating code from a natural language description, or NL2Code, is considered a pressing and significant challenge in code intelligence. Thanks to the rapid development of pre-training techniques, surging large language models are being proposed for code, sparking the advances in NL2Code. To facilitate further research and applications in this field, in this paper, we present a comprehensive survey of 27 existing large language models for NL2Code, and also review benchmarks and metrics. We provide an intuitive comparison of all existing models on the HumanEval benchmark. Through in-depth observation and analysis, we provide some insights and conclude that the key factors contributing to the success of large language models for NL2Codeare ""Large Size, Premium Data, Expert Tuning"". In addition, we discuss challenges and opportunities regarding the gap between models and humans. We also create a website <https: //nl2code. github. io> to track the latest progress through crowd-sourcing. To the best of our knowledge, this is the first survey of large language models for NL2Code, and we believe it will contribute to the ongoing development of the field.","Is it possible for novice programmers, even those without any programming experience, to create software simply by describing their requirements in natural language? This is a long-standing fascinating question, which poses challenges to research areas like software engineering, programming language, and artificial intelligence. Realizing this scenario would have an unprecedented impact on our lives, education, economy, and labour market, as it would change the centralized software development and operation paradigm. Due to its promising and intriguing future, natural-language-to-code (NL2Code) has been proposed as a research task that has attracted widespread interest in both academia and industry, with the goal of generating code from natural language descriptions. 
Early studies on NL2Codewere mainly based on heuristic rules or expert systems, such as probabilistic grammar-based methods and those focusing on domain-specific languages, which are inflexible and not scalable. Other studies utilized static language models, like n-gram and Hidden Markov, which have sparse vector representations and cannot model long-term dependencies. Subsequently, neural networks, including CNN, RNN, and LSTM, were employed to model the relationship between NL and code. In 2017, the Transformer model was introduced for machine translation and later applied to the NL2Codetask. However, these deep learning models require a significant amount of labelled pairs of NL and code for training, and have limited capabilities for the NL2Codetask. 
Recently, a growing number of large language models (LLMs) with Transformer architecture have been trained on large-scale unlabelled code corpus. These models have the ability to generate code in a zero-shot manner and have achieved impressive results in the NL2Codetask. As a milestone, Codex has shown that an LLM with 12 billion parameters is able to solve 72.31%of challenging Python programming problems created by humans. More encouragingly, Codex has been used to power a commercial product and improve coding efficiency in practice. Following Codex's success, various LLMs for the NL2Codetask have emerged, with model sizes ranging from millions to billions of parameters. Examples include AlphaCode, which aims to solve competitive-level programming problems, and InCoder, which supports filling code in arbitrary positions using bidirectional contexts. Other models such as CodeGen, PaLM-Coder, PanGu-Coder, CodeGeeX, and SantaCoder have also gained great attention. As the model size increases, LLMs have been shown to exhibit some emergent capabilities such as human-like programming and debugging. 
Large language models have kindled hope for the NL2Codetask due to their impressive power and potential value. Despite the significant progress, there are still numerous challenges and opportunities, calling for more advanced and innovative future work. Currently, considering the variety of techniques and applications, there is a growing need for a comprehensive survey to provide a systematic overview of this field and identify critical challenges. To this end, in this paper, we carefully investigate 27 advanced LLMs for NL2Code (), and also review benchmarks and metrics (). We conduct an intuitive comparison of all the existing LLMs on the HumanEval benchmark, perform a thorough analysis, and eventually attribute the success of these LLMs to ""Large Size, Premium Data, Expert Tuning"" (). This means large model and data size, high-quality training data and expert hyper-parameter tuning. We also discuss the challenges and opportunities regarding the ability gap between LLMs and Humans (). In addition, we have built a website <https: //nl2code. github. io> to keep track of the latest progress and support crowd-sourcing updates. To the best of our knowledge, this is the first survey of LLMs for NL2Code, and we hope it will contribute to the ongoing development of this exciting field."
When Does Aggregating Multiple Skills with Multi-Task Learning Work? A Case Study in Financial NLP,2305.14007v1,./img_ACL_2023/2305.14007v1.png,An illustration of MTL system with shared encoder and task-specific prediction headers.,"Multi-task learning (MTL) aims at achieving a better model by leveraging data and knowledge from multiple tasks. However, MTL does not always work ‚Äì sometimes negative transfer occurs between tasks, especially when aggregating loosely related skills, leaving it an open question when MTL works. Previous studies show that MTL performance can be improved by algorithmic tricks. However, what tasks and skills should be included is less well explored. In this work, we conduct a case study in Financial NLP where multiple datasets exist for skills relevant to the domain, such as numeric reasoning and sentiment analysis. Due to the task difficulty and data scarcity in the Financial NLP domain, we explore when aggregating such diverse skills from multiple datasets with MTL can work. Our findings suggest that the key to MTL success lies in skill diversity, relatedness between tasks, and choice of aggregation size and shared capacity. Specifically, MTL works well when tasks are diverse but related, and when the size of the task aggregation and the shared capacity of the model are balanced to avoid overwhelming certain tasks.","Multi-task learning (MTL) is a machine learning paradigm where multiple learning tasks are optimized simultaneously, exploiting commonalities and differences across them. MTL is expected to outperform single-task learning (STL) as it utilizes more training data and enables inter-task knowledge sharing. However, MTL may also bring about multi-task conflict and negative transfer. Empirically, in many MTL systems, only a small portion of tasks benefit from MT joint training while others suffer from negative transfer. Therefore, it is still an open question when MTL will work. 
MTL systems have two components: MTL algorithms and the tasks included for aggregation. Recent progress in MTL has shown that appropriate MTL algorithms (e.g., architecture and optimization) can mitigate negative transfers (, inter alia). However, it is still unclear when MTL works from the perspective of the relations between tasks and skills to be aggregated for better performance in a practical setting. 
To understand this, we conduct a practical case study on Financial NLP. We choose Financial NLP mainly because (1) Financial NLP tasks are hard: GPT-3 does not perform well on financial tasks (see), though it is a good zero/few-shot learner in general domains; and (2) Financial NLP datasets typically address different skills (e.g., quantitative reasoning, and sentiment analysis), and have a limited data size (, inter alia). Therefore, it is promising to aggregate Financial NLP tasks using MTL, which not only compiles and augments the small datasets, but also benefits the difficult tasks through relevant information transfer and comprehensive reasoning. However, no previous work explores the benefits of aggregating Financial NLP resources using MTL. Particularly, we explore the following hypotheses about when MTL works: 
 * When various skills are included: Intuitively, positive transfers are likely to happen among tasks regarding the same skill. However, diversified skills might benefit the MTL system through implicit data augmentation, attention focusing, and feature eavesdropping. Our empirical results also show that skill diversity benefits MTL. 
 * When the aggregated tasks are well related: We find that the close relation (measured qualitatively and quantitatively) among Financial NLP tasks explains why diversified skills help each other, and contributes to the success of MTL. 
 * When the aggregation size matches shared capacity: Too many objectives may exhaust the MTL shared capacity and cause interference among tasks. We find that excessive aggregation size in a limited capacity model restricts the performance of some tasks. Thus aggregation size should be appropriate for the shared capacity. 
To facilitate exploration of H1 and H2, we survey existing Financial NLP resources and propose FinDATA (Financial Data And Tasks Aggregation), a collection of Financial NLP tasks covering various financial text understanding skills. To check H3, we propose SPAL-FinBERT (Shared Parallel Attention Layer with FinBERT), an MTL architecture based on pre-trained FinBERT, but is highly parameter-efficient ‚Äì with 99.8%fewer trainable parameters but outperforming the vanilla FinBERT MTL on several tasks. Our contributions include 
 * We conduct a case study on Financial NLP to explore what properties of task aggregation lead to the success of MTL. 
 * We survey and aggregate several existing Financial NLP tasks and datasets, illustrating that MTL can be a cheap and efficient improvement for Financial NLP performance. 
 * We propose SPAL-FinBERT, a parameter-efficient MTL architecture with good performance. This model may also have broader use cases in other settings."
Linguistic representations for fewer-shot relation extraction across domains,2307.03823v1,./img_ACL_2023/2307.03823v1.pdf,Model architecture. Yellow tokens denote BERT special tokens. Dotted lines indicate using BERT embeddings to seed the graph for the R-GCN.,"Recent work has demonstrated the positive impact of incorporating linguistic representations as additional context and scaffolding on the in-domain performance of several NLP tasks. We extend this work by exploring the impact of linguistic representations on cross-domain performance in a few-shot transfer setting. An important question is whether linguistic representations enhance generalizability by providing features that function as cross-domain pivots. We focus on the task of relation extraction on three datasets of procedural text in two domains, cooking and materials science. Our approach augments a popular transformer-based architecture by alternately incorporating syntactic and semantic graphs constructed by freely available off-the-shelf tools. We examine their utility for enhancing generalization, and investigate whether earlier findings, e.g.,that semantic representations can be more helpful than syntactic ones, extend to relation extraction in multiple domains. We find that while the inclusion of these graphs results in significantly higher performance in few-shot transfer, both types of graph exhibit roughly equivalent utility.","In many specialized domains, such as healthcare or finance, one of the principal limitations for the implementation of machine learning based NLP methods is the availability of high quality data, which tends to be both time-consuming and expensive to acquire. While pre-trained language models allow impressive performance gains across a number of tasks, those gains frequently fail to generalize to specialized domains. Robust representations that allow models to both take advantage of pretrained models and generalize across domains are therefore highly desirable. 
Recent works such as have demonstrated the significant potential of using human-annotated linguistic information as scaffolding for learning language models. Other works such as and use automatically generated semantic annotations. These works depend on the idea that the structure that the linguistic frameworks provide allows models to better learn salient features of the input. In addition, however, linguistic structures offer abstraction over the variation of natural language, providing representations that might express meaning in domain-general ways. We therefore extend earlier work to investigate whether including linguistic representations encourages learning domain-agnostic representations of relations such that models can generalize better in a few-shot setting, i.e.,learning from less high-quality data in new domains. We focus on the case of automatically generated linguistic annotations, to evaluate the impact they can have on downstream tasks without expensive human annotation of parse data. 
We use two linguistic formalisms, to evaluate and compare their impact: dependency parses, and abstract meaning representations (AMR). AMR seeks to represent meaning at the level of a sentence in the form of a rooted, directed graph. AMR is based on Propbank, and factors out syntactic transformations due to verb alternations, passivization, and relativization, leading to a less sparse expression of textual variance. Dependency parsing, by contrast, remains at a low level of abstraction, with structures that do not nest outside of the words in the original text. 
We study the problem of relation extraction on procedural text. We use procedural text because of what we term their implicit schemas. Across domains, our datasets describe the process of combining ingredients under certain conditions in a sequential fashion to produce a desired product. These range from preparing a cooking recipe to synthesizing a chemical compound to extracting materials from ores. As a result, the relations that we derive from each dataset share a loose semantic correspondence, both to each other and to basic semantic relations such as verb arguments and locations. For example, the actions ""boil"" and ""heat"" in ""Boil the mixture in a medium saucepan"" and ""Heat the solvent in the crucible"" are similar. 
We hypothesize that the underlying semantics of all of these datasets are similar enough that models should be able to better generalize across domains from the explicit inclusion of syntactic and semantic structural features. We use three datasets across two domains: recipes for cooking, and materials science synthesis procedures. Each of these datasets defines the task of generating a comprehensive, descriptive graph representation of a procedure. We simplify this task into relation extraction in order to better compare the impact of different linguistic formalisms. 
We augment a popular transformer-based relation extraction baseline with features derived from AMR and dependency parses and investigate their impact in a few-shot setting both in-domain and across domains. Experiments show that both AMR parses and dependencies significantly enhance model performance in few-shot settings but that the benefit disappears when models are trained on more data. We additionally find that while cross-domain transfer can degrade the performance of purely text-based models, models that incorporate linguistic graphs provide gains that are robust to those effects. 
We make our code available with our submission."
Easy Guided Decoding in Providing Suggestions for Interactive Machine Translation,2211.07093v2,./img_ACL_2023/2211.07093v2.pdf,Examples of Translation Suggestions in computer-aided translation. Highlighted are spans with incorrect words selected by humans. The TS system automatically generates alternatives for the spans to obtain final translation results.,"Machine translation technology has made great progress in recent years, but it cannot guarantee error-free results. Human translators perform post-editing on machine translations to correct errors in the scene of computer-aided translation. In favor of expediting the post-editing process, many works have investigated machine translation in interactive modes, in which machines can automatically refine the rest of translations constrained by human's edits. Translation Suggestion (TS), as an interactive mode to assist human translators, requires machines to generate alternatives for specific incorrect words or phrases selected by human translators. In this paper, we utilize the parameterized objective function of neural machine translation (NMT) and propose a novel constrained decoding algorithm, namely Prefix-Suffix Guided Decoding (PSGD), to deal with the TS problem without additional training. Compared to the state-of-the-art lexically constrained decoding method, PSGD improves translation quality by an average of","The emergence of machine translation technology assists human translation to improve translation efficiency. Even though there is a quality gap between the outputs of machine translation (MT) systems and manual translations by professional translators, MT can still practically reduce time in comparison with translating from scratch. Later, the advances at the sequence-to-sequence model further made a breakthrough in translation technology, inspiring the industry to transform human translation into computer-aided translation (CAT) to a great extent. CAT usually relies on an MT engine and a platform with a user-friendly interface, with which humans perform post-editing (PE) on machine translations to achieve final results with quality standards. 
In the past, the post-editing process was typically static and machines would no longer respond to humans 'modifications once humans started post-editing. Recent works investigate interactive protocols and algorithms so that humans and machines can collaborate and machines automatically refine the translations according to the human' s edits. One promising mode is Translation Suggestion (TS) proposed by as a pioneer, which requires the machine to provide alternatives for specific spans of incorrect words or phrases selected by humans, namely making suggestions given prefix and suffix constraints. In practical applications as shown in Figure, it usually happens when human translators would like to edit part of the MT output. It can be easily implemented with a user interface if machines correctly provide suggestions for the selected incorrect spans. has proven the significance of TS in post-editing in terms of resolving two pitfalls of earlier works. The importance has also been recognized by the Conference of Machine Translation (WMT), and they released the Naive Translation Suggestion shared task in WMT 2022. 
One of the solutions to TS can be training a supervised model with TS annotated data. trained such an end-to-end Transformer-like model as a benchmark system. applied model fine-tuning with TS data augmentation on pre-trained NMT models. However, supervised learning, which relies on a large amount of labeled data, is too heavy to be easily adjusted to other domains. In addition, due to the complicated post-editing process, it is expensive to obtain such limited annotated data. 
Our idea is to investigate inference algorithms given prefix and suffix constraints. We tested the state-of-the-art lexically constrained decoding algorithm on the WeTS dataset, and found that omissions frequently occur in suggestion generations. There are two reasons behind: (1) the division of beams in the dynamic beam allocation narrows the search space so that there would not be enough candidates that match constraints to be picked. This is more likely to happen when constraints are much longer than the average length of the span selected, such as in TS applications; (2) the beam search stops when the probability of eos, the special token for the end of a sentence, appears largest in the softmax distribution, but the probability of the entire sentence generation has not been considered. In terms of efficiency, this decoding algorithm contains unnecessary calculations since it always generates suggestions step by step from the beginning of the translation to the end of the sentence, including prefix and suffix constraints. 
In this paper, we propose a neat prefix-suffix guided decoding (PSGD) algorithm for the TS task. There are three main contributions: (1) PSGD emphasizes the probability of the entire generation including prefix and suffix constraints, but only decodes for the incorrect span rather than the whole translation sentence, which improves both suggestion quality and time efficiency. (2) PSGD theoretically avoids dividing beams as in so that the original beam search space can be used to improve the quality of generated translation suggestions. (3) PSGD does not require any additional training/fine-tuning on the original NMT model, which means it can be applied to any auto-regressive machine translation system with flexibility. 
Our experimental observations show that PSGD significantly outperforms the state-of-the-art lexically constrained decoding method by an average increase of 
 10.87BLEU and8.62BLEU on the benchmark datasets WeTS and WMT 2022 Naive Translation Suggestion datasets (WMT22-TS), respectively. Experimental results also demonstrate PSGD's superiority in overall time efficiency by a63.4%time reduction. In addition, on both the WeTS and WMT22-TS datasets, PSGD is superior over other supervised learning systems trained with TS annotated data."
HAHE: Hierarchical Attention for Hyper-Relational Knowledge Graphs in Global and Local Level,2305.06588v2,./img_ACL_2023/2305.06588v2.pdf,An example of hyper-relational fact structure.,"Link Prediction on Hyper-relational Knowledge Graphs (HKG) is a worthwhile endeavor. HKG consists of hyper-relational facts (H-Facts), composed of a main triple and several auxiliary attribute-value qualifiers, which can effectively represent factually comprehensive information. The internal structure of HKG can be represented as a hypergraph-based representation globally and a semantic sequence-based representation locally. However, existing research seldom simultaneously models the graphical and sequential structure of HKGs, limiting HKGs' representation. To overcome this limitation, we propose a novel Hierarchical Attention model for HKG Embedding (HAHE), including global-level and local-level attention. The global-level attention can model the graphical structure of HKG using hypergraph dual-attention layers, while the local-level attention can learn the sequential structure inside H-Facts via heterogeneous self-attention layers. Experiment results indicate that HAHE achieves state-of-the-art performance in link prediction tasks on HKG standard datasets. In addition, HAHE addresses the issue of HKG multi-position prediction for the first time, increasing the applicability of the HKG link prediction task. Our code is publicly available.","Knowledge graphs (KGs) are semantic networks that define entity relationships. Early KG research use binary relationships, often expressed as a triple-based fact (subject, relation, object). Yet, n-ary relational facts (containing more than two entities) are abundant in real-world KGs like Freebase and Wikidata. represent an n-ary relational fact as a hyper-relational fact (H-Fact) consisting of a main triple (s, r, o) and several auxiliary attribute-value qualifiers { (a_i: v_i) }, and KGs composed of H-Facts are called hyper-relational knowledge graphs (HKGs). 
As shown in Figure, an H-fact can describe a real-world fact. Unlike traditional triple-based facts, H-Facts do not just raise the number of entities in facts from two to n. It structurally and effectively represents the n-ary relational facts prevalent in reality. Globally, it extends ordinary graph structure to hypergraph structure. Locally, it defines five heterogeneous roles of s, r, o, a, v within facts to capture the semantic information of the fact 'Barack Obama held position as US president', as illustrated in Figure. 
Recent research has demonstrated various embedding strategies for hyper-relational representations. However, current approaches only consider global hypergraph structures or local semantic sequence structures. For instance, StarE employs the information transfer function of graph neural networks (GNN) to unidirectionally pass auxiliary key-value pair information into the main triples 'relations, thereby capturing the graph structure but insufficiently between multiple entities and relations within the H-facts. In contrast, GRAN initially incorporates the Transformer encoder into the HKG embedding, capturing the fully connected semantic information locally inside H-facts, while disregarding the global structure. Consequently, representing the global and local structure of HKG simultaneously with hierarchical attention becomes a promising research direction, but an inadequate representation of HKG structure constrains HKG embeddings. 
To overcome this limitation, we propose a novel Hierarchical Attention model for HKG Embedding (HAHE) that incorporates global-level and local-level attention. We update the global node embeddings using the HKG hypergraph structure. However, by complete connectivity, the previous hypergraph attention network just converts all hypergraph nodes into a regular graph and then utilizes the GAT layer for node embedding updates, rendering it unable to distinguish which nodes comprise a hyperedge. Consequently, we design hypergraph dual-attention layers to aggregate node embedding information into hyper-edge embedding through the attention mechanism. After obtaining the hyper-edge embedding, we update the node embedding by feeding it back to the node through the attention mechanism. In this way, nodes are allowed to learn more distant information from the whole HKG. This hypergraph dual-attention method significantly enhances learning capacity. It then transfers the updated node information to the local level' s attention. Inspired by GRAN's heterogeneous attention, we define five types of nodes and fourteen types of edges in a single H-Fact and develop heterogeneous self-attention layers with both node-bias and edge-bias attention to learn the semantic content of H-Facts. The last step is to output the link prediction findings using an MLP-based decoding process for one-position or multi-position link prediciton tasks on HKGs. 
Experiments on link prediction were performed on three HKG standard datasets, JF17K, Wikipeople, and WD50K. The state-of-the-art results indicate that HAHE is effective in the link prediction task. In addition, adequate ablation experiments were designed to highlight the importance of global and local focus, and HAHE is also used for the HKG multi-position prediction task, i.e., predicting two or more entities or relations simultaneously in a single H-fact, hence increasing the applicability of the HKG link prediction task. Ultimately, we make our code publicly available and discuss the limitations and future work of HKG embedding representation."
MidMed: Towards Mixed-Type Dialogues for Medical Consultation,2306.02923v2,./img_ACL_2023/2306.02923v2.pdf,An example of MidMed.,"Most medical dialogue systems assume that patients have clear goals (medicine querying, surgical operation querying, etc. ) before medical consultation. However, in many real scenarios, due to the lack of medical knowledge, it is usually difficult for patients to determine clear goals with all necessary slots. In this paper, we identify this challenge as how to construct medical consultation dialogue systems to help patients clarify their goals. To mitigate this challenge, we propose a novel task and create a human-to-human mixed-type medical consultation dialogue corpus, termed MidMed, covering five dialogue types: task-oriented dialogue for diagnosis, recommendation, knowledge-grounded dialogue, QA, and chitchat. MidMed covers four departments (otorhinolaryngology, ophthalmology, skin, and digestive system), with 8,175 dialogues. Furthermore, we build baselines on MidMed and propose an instruction-guiding medical dialogue generation framework, termed InsMed, to address this task. Experimental results show the effectiveness of InsMed.","Current medical dialogue systems mainly focus on diagnosis by obtaining symptoms and then making diagnosis automatically. These dialogue systems have shown significant potential and alluring technological value to simplify diagnostic procedures. Previous works assume that patients have explicit goals (medicine querying, surgical operation querying, etc. ), and perform in the way of task-oriented dialogue to accomplish patients 'goals. 
However, explicit patient goals are usually unavailable in real-world scenarios. For example, a patient wants to consult about his itchy skin but lacks medical knowledge. Thus, it is difficult for the patient to decide which slots (e.g.,medicine or a surgical operation) are needed. To figure out explicit patient goals, medical consultation services are needed, which provide advice of treatment, medicine, food, etc. , as shown in Figure. However, those medical consultation services are under explored in previous works. 
To facilitate the study of medical consultation, we construct a new human-to-human mixed-type dialogue dataset for medical consultation (MidMed), covering five dialogue types: task-oriented dialogue for diagnosis, knowledge-grounded dialogue, QA, recommendation, and chitchat. MidMed is constructed by revising dialogues of MedDialog (a human-to-human medical diagnosis dialogue dataset). As shown in Figure, a patient queries about ""sweaty hands"", and has no explicit goal for medicine or a surgical operation. In the scenario, the doctor first collects the symptoms and makes a diagnosis. To help clarify the patient' s goal, the doctor further recommends medicine and food, replies for foods to avoid, and gives emotional comfort. Through the consultation, the patient determines to apply ""dexamethasone cream"" and have more ""tomatoes"". Finally, MidMed is obtained, containing 8,175 dialogues and 98,000 utterances, with at least three dialogue types in each dialogue. 
To promote research on medical consultation dialogue systems, we conduct benchmarking experiments on MidMed for end-to-end dialogue generation. Furthermore, to generate informative and relevant responses with dialogue topic sequences, inspired by, we present an instruction-guiding medical dialogue generation framework (InsMed) to handle mixed-type dialogues. InsMed is composed of a dialogue topic selection, a reference knowledge selection, and an instruction-based response generation module. Specifically, the topic selection module and the reference knowledge selection module are designed to pick suitable dialogue topics and reference knowledge for generating responses, respectively. Then, dialogue topics and reference knowledge are converted to instructions in natural language with well-designed templates. For example, an instruction is ""In the next utterance, the doctor will recommend a diet. The recommended diet is fruits and vegetables"". These instructions are concatenated with dialogue context as the input to generation models. 
This work makes the following contributions: 
 
 * We identify a new challenge, that is, in many real-world scenarios, it is usually difficult for patients to have clear goals before medical consultations. 
 * To mitigate this challenge, we propose a novel task, medical consultation over mixed-type dialogue, and collect a new Chinese human-to-human mixed-type dialogue dataset, in which each session has rich variability of dialogue types with natural topic transitions. 
 * We build baselines on MidMed and propose an instruction-guiding response generation framework InsMed to address this task. Experimental results show the effectiveness of InsMed."
CASE: Aligning Coarse-to-Fine Cognition and Affection for Empathetic Response Generation,2208.08845v2,./img_ACL_2023/2208.08845v2.pdf,"Examples from the EmpatheticDialogues dataset. The alignment of cognition and affection (i.e., emotional state and emotional reaction) leads to highly empathetic and informative expression in responses.","Empathetic conversation is psychologically supposed to be the result of conscious alignment and interaction between the cognition and affection of empathy. However, existing empathetic dialogue models usually consider only the affective aspect or treat cognition and affection in isolation, which limits the capability of empathetic response generation. In this work, we propose the CASE model for empathetic dialogue generation. It first builds upon a commonsense cognition graph and an emotional concept graph and then aligns the user's cognition and affection at both the coarse-grained and fine-grained levels. Through automatic and manual evaluation, we demonstrate that CASE outperforms state-of-the-art baselines of empathetic dialogues and can generate more empathetic and informative responses.","Human empathetic conversations allow both parties to understand each other 's experiences and feelings, which is crucial for establishing seamless relationships and is also integral to building a trustful conversational AI. 
In social psychology, empathy consists of two aspects: cognition and affection. The cognitive aspect corresponding to the understanding of the user' s situation and experiences. The affective aspect requires the comprehension of the user 's emotional state and his/her potential emotional reaction. Although existing work of empathetic dialogue involves both aspects of empathy, there are still issues that need to be addressed. First, most work considers only the affective aspect, like detecting the user' s emotional state to enhance empathy expression. Second, although recent work explored both roles of cognition and affection in empathy expression, they usually treat cognition and affection in isolation without considering their relationship. 
However, human empathetic responses often result from conscious alignment and interaction between cognition and affection of empathy. For one thing, the user 's overall emotional state manifested in the context suggests the user' s attitude toward current situation (i.e., cognition). Thus, for the listener, aligning the user 's expressed cognition to the proper emotional state is essential for an appropriate empathetic response. As in case-1 of Figure, the alignment of cognition (i.e., intent ""to go to the beach"") with different emotional states (i.e., ""excited"" vs. ""disappointed"") produces different appropriate empathetic expressions (i.e., ""love"" and ""which beach are you going to go to"" vs. ""hate"" and ""waiting for the beach""), respectively. For another, the user' s situation drives the listener to infer the deeper specific cognitions and associate them with the underlying emotional reactions. In this way, the listener can produce a more actively empathetic response instead of only understanding and repeating the user 's expressed cognition. As in case-2 of Figure, building association between inferred cognitions and emotional reactions, i.e., ""to give up"" and ""frustrated"" vs. ""to try harder"" and ""hopeful"", yields cognitively distinct but highly empathetic responses, i.e., response-2a vs. response-2b. The two cases highlight the necessity of aligning cognition and affection on both overall and specific (i.e., coarse and fine-grained) level for empathy modeling in response generation. 
To this end, we align Cognition and Affection for reSponding Empathetically (CASE) on coarse and fine-grained levels by fusing sentence-level commonsense knowledge from COMET and word-level concept knowledge from ConceptNet. Commonsense knowledge infers the user' s situation as cognition and infers emotional reactions to the situation, which are implied in the dialogue. Concept knowledge serves to extract the emotional state manifested in the dialogue. For encoding the two types of knowledge, we first construct commonsense cognition graph and emotional concept graph, where the initial independent representation of cognition and emotional concept is carefully adjusted by dialogue context adopting graph transformers. Then, we design a two-level strategy to align cognition and affection using mutual information maximization (MIM) (Appendix). The coarse-grained level considers overall cognition and affection manifested in the dialogue context to align contextual cognition and contextual emotional state, which are extracted with a knowledge discernment mechanism. The fine-grained level builds the fine-grained association between cognition and affection implied in the dialogue to align each specific cognition and corresponding emotional reaction. Further, an empathy-aware decoder is devised for generating empathetic expressions. 
Our contributions are summarized as follows: (1) We devise a unified framework to model the interaction between cognition and affection for integrated empathetic response generation. (2) We construct two heterogeneous graphs involving commonsense and concept knowledge to aid in the modeling of cognition and affection. (3) We propose a two-level strategy to align coarse-grained and fine-grained cognition and affection adopting mutual information maximization. (4) Extensive experiments demonstrate the superior of CASE in automatic and manual evaluation."
ArgU: A Controllable Factual Argument Generator,2305.05334v1,./img_ACL_2023/2305.05334v1.png,Generating stance and argument scheme controlled factual arguments using ArgU.,"Effective argumentation is essential towards a purposeful conversation with a satisfactory outcome. For example, persuading someone to reconsider smoking might involve empathetic, well founded arguments based on facts and expert opinions about its ill-effects and the consequences on one 's family. However, the automatic generation of high-quality factual arguments can be challenging. Addressing existing controllability issues can make the recent advances in computational models for argument generation a potential solution. In this paper, we introduce ArgU: a neural argument generator capable of producing factual arguments from input facts and real-world concepts that can be explicitly controlled for stance and argument structure using Walton' s argument scheme-based control codes. Unfortunately, computational argument generation is a relatively new field and lacks datasets conducive to training. Hence, we have compiled and released an annotated corpora of 69,428 arguments spanning six topics and six argument schemes, making it the largest publicly available corpus for identifying argument schemes; the paper details our annotation and dataset creation framework. We further experiment with an argument generation strategy that establishes an inference strategy by generating an ""argument template"" before actual argument generation. Our results demonstrate that it is possible to automatically generate diverse arguments exhibiting different inference patterns for the same set of facts by using control codes based on argument schemes and stance.","Although arguing is an innate human quality, formulating convincing arguments is an art. A successful narrative aiming to persuade someone should be rhetorically appealing, trustworthy, factually correct, and logically consistent, which makes formulating good arguments challenging. Incorporating neural language models, the relatively new field of computational argument generation has shown promise in assisting with argument synthesis. Argument generators like Project Debater have successfully formulated convincing arguments across different domains including legal, politics, education, etc. , and can potentially find new argumentative connections. However, lacking explicit control mechanisms, neural argument generators often render illogical and inappropriate arguments, reducing their trustworthiness and applicability for practical use. Furthermore, training such models requires a considerable amount of quality data, which is hard to collect and annotate. Hence, we propose ArgU, a controllable neural argument generator trained on a curated and quality-controlled corpus of annotated argument texts from abortion, minimum wage, nuclear energy, gun control, the death penalty and school uniform. 
ArgU strives to enable effective, scalable and appealing argument generation. As depicted in Figure, it takes as input worldly knowledge and concepts as variables and coherently combines them to generate an argument that exhibits the desired pro/con stance and inference structure. Using control codes to regulate argument stance and reasoning, ArgU generates a variety of argument texts for the same set of facts, thus providing diverse response options. Internally ArgU implements a 2-step generation process, where it first generates an ""argument template"", which depicts the structure of the final argument based on the control codes, and finally yields the argument text by modifying the template to include the augmented input fact variables. We ground our work on prominent theoretical foundations, where the inference structure-based control codes derive from six Walton's argument schemes: ""Means for Goal"", ""Goal from Means"", ""From Consequence"", ""Source Knowledge"", ""Source Authority"", and ""Rule or Principle"". 
Since human annotation is expensive and time-consuming, we devise a multi-phased annotation framework for systematically leveraging human and automatic annotation mechanisms to yield a curated dataset of 69,428 examples for controllable argument synthesis. We release our curated corpus to facilitate further research; an example constitutes an argument text, a set of real-world concepts and knowledge from which the argument derives, and the stance and argument scheme of the text. We further detail and analyze our annotation framework and share variants of topic-independent computational models for automatically annotating factual spans from argument text and identifying the asserted argument schemes. We summarize our contributions below: 
 
 * We propose an argument generator that methodically generates factual arguments following a specified stance and argument scheme (Sec. ). 
 * We share a quality-controlled annotated dataset conducive to training such generators. To our knowledge, this is the largest available corpora that identify argument schemes from argument text (Sec. ). 
 * We share our annotation framework and release domain-independent computational models that automatically identify factual spans and argument schemes from argument text from any topic (Sec. )."
Attributable and Scalable Opinion Summarization,2305.11603v1,./img_ACL_2023/2305.11603v1.pdf,"Hercules is trained to encode sentences from reviews as paths through a hierarchical discrete latent space (top). At inference time, we encode all sentences from the input reviews, and identify frequent paths or subpaths to use for the summary (bottom). The consensus opinion from the three example inputs is that the food is good, so the subpath shown in red is repeated; decoding it should result in an output like ""Good food"".","We propose a method for unsupervised opinion summarization that encodes sentences from customer reviews into a hierarchical discrete latent space, then identifies common opinions based on the frequency of their encodings. We are able to generate both abstractive summaries by decoding these frequent encodings, and extractive summaries by selecting the sentences assigned to the same frequent encodings. Our method is attributable, because the model identifies sentences used to generate the summary as part of the summarization process. It scales easily to many hundreds of input reviews, because aggregation is performed in the latent space rather than over long sequences of tokens. We also demonstrate that our appraoch enables a degree of control, generating aspect-specific summaries by restricting the model to parts of the encoding space that correspond to desired aspects (e.g., location or food). Automatic and human evaluation on two datasets from different domains demonstrates that our method generates summaries that are more informative than prior work and better grounded in the input reviews.","Online review websites are a useful resource when choosing which hotel to visit or which product to buy, but it is impractical for a user to read hundreds of reviews. There has been significant interest in methods for automatically generating summaries or meta-reviews that aggregate the diverse opinions contained in a set of customer reviews about an entity (e.g., a product, hotel or restaurant) into a single summary. 
Early work on opinion summarization extracted reviewers' sentiment about specific features or selected salient sentences from reviews based on centrality, while more recent methods based on neural models have used sentence selection in learned feature spaces or abstractive summarizers that generate novel output. 
Following, we define opinion summarization, or review aggregation, as the task of generating a textual summary that reflects frequent or popular opinions expressed in a large number of reviews about an entity. Systems are extractive if they select sentences or spans from the input reviews to use as the summary, or abstractive if they generate novel output. Review aggregation is challenging for a number of reasons. Firstly, it is difficult to acquire or create reference summaries, so models are almost always trained without access to gold standard references. Secondly, popular entities may have hundreds of reviews, which can cause computational difficulties if the approach is not scalable. Finally, good summaries should be abstractive and not contain unnecessary detail, but should also not hallucinate false information. Ideally, a summarization system should be attributable, offering some evidence to justify its output. 
Previous work has either been exclusively extractive (which is inherently attributable and often scalable but leads to unnecessarily specific summaries) or exclusively abstractive. We propose a hybrid method, that produces abstractive summaries accompanied by references to input sentences which act as evidence for each output sentence, allowing us to verify which parts of the input reviews were used to produce the output. Depicted in, we first learn to encode natural language sentences from reviews as paths through a hierarchical discrete latent space. Then, given multiple review sentences about a specific entity, we identify common subpaths that are shared among many inputs, and decode them back to natural language, yielding the output summary. The sentences whose encodings contain the selected subpaths (shown in red in) act as evidence for that generated sentence. 
Our approach, Hercules, is unsupervised and does not need reference summaries during training, instead relying on properties of the encoding space induced by the model. Since the aggregation process occurs in encoding space rather than over long sequences of tokens, Hercules is highly scalable. Generated summaries are accompanied by supporting evidence from input reviews, making Hercules attributable. It also offers a degree of controllability: we can generate summaries that focus on a specific aspect of an entity (e.g., location) or sentiment by restricting aggregation to subpaths that correlate with the desired property. 
Our contributions are as follows: 
 
 * We propose a method for representing natural language sentences as paths through a hierarchical discrete latent space (). 
 * We exploit the properties of the learned hierarchy to identify common opinions from input reviews, and generate abstractive summaries alongside extractive evidence sets (). 
 * We conduct extensive experiments on two English datasets covering different domains, and show that our method outperforms previous state-of-the-art approaches, while offering the additional advantages of attributability and scalability (Sections and)."
Targeted Data Generation: Finding and Fixing Model Weaknesses,2305.17804v1,./img_ACL_2023/2305.17804v1.pdf,"Illustration of the Targeted Data Generation (TDG) pipeline. In the automatic subgroup discovery stage, TDG identifies challenging clusters that can benefit from additional data while minimizing potential negative impacts on performance in other regions (i.e., high generalization (GC) and low interference (IC), as defined in). In the subgroup augmentation with LLM stage, TDG utilizes GPT-3 to generate additional examples for identified challenging clusters.","Even when aggregate accuracy is high, state-of-the-art NLP models often fail systematically on specific subgroups of data, resulting in unfair outcomes and eroding user trust. Additional data collection may not help in addressing these weaknesses, as such challenging subgroups may be unknown to users, and underrepresented in the existing and new data. We propose Targeted Data Generation (TDG), a framework that automatically identifies challenging subgroups, and generates new data for those subgroups using large language models (LLMs) with a human in the loop. TDG estimates the expected benefit and potential harm of data augmentation for each subgroup, and selects the ones most likely to improve within-group performance without hurting overall performance. In our experiments, TDG significantly improves the accuracy on challenging subgroups for state-of-the-art sentiment analysis and natural language inference models, while also improving overall test accuracy.","Despite very high accuracy, state-of-the-art NLP models still exhibit systematic failures on specific subgroups of data. For example, found that a 95%-accurate sentiment analysis model did much worse on club reviews (90%) and movie theater reviews (85%), while notes how a commercial chatbot avoids any engagement on topics that even mention Islam or the middle east. The existence of these challenging subgroups can lead to unfair outcomes, erode user trust, and ultimately limit deployment of models, even when aggregate accuracy is very high. 
One possible solution is to collect or generate more data. However, the additional data may still under-sample from specific challenging subgroups, even if data collection is adversarial, especially when subgroups are not immediately obvious or salient to humans. Therefore it helps little in addressing these weaknesses. Tools for discovering challenging subgroups still require human creativity and effort. show that experts are able to improve existing subgroups via careful data augmentation with large language models (LLMs), but finding such challenging subgroups still requires human ingenuity. Perhaps more importantly, they find that naively augmenting certain subgroups can drastically hurt other subgroups and overall performance. Hence, the challenge is not only to find challenging subgroups, but also to determine which subgroups are amenable to data augmentation, and how to augment them effectively. 
In this work, we propose Targeted Data Generation (TDG), a framework to automatically identify challenging subgroups that can benefit from more data, and then generate that data with LLMs (). Given a target model, TDG clusters validation data into potential challenging subgroups. We then use held-out data to estimate how much each subgroup would benefit from more data, and how much additional data would hurt performance in other regions. Finally, having identified challenging subgroups amenable to data augmentation, we use GPT-3 coupled with local subgroup models to generate new data, so as to improve subgroup performance while remaining faithful to the original data distribution. 
We evaluate TDG on three tasks: sentiment analysis (SST), paraphrase detection (QQP), and natural language inference (MNLI). We evaluate various clustering techniques, and find that clustering based on the target model's own representation yields the clusters most amenable to data augmentation (with the exception of QQP, where our analysis indicates label noise would make data augmentation ineffective). Finally, augmenting these clusters with GPT-3 results in significant improvements on correspondent test clusters, and also small improvements on overall accuracy."
End-to-end Knowledge Retrieval with Multi-modal Queries,2306.00424v1,./img_ACL_2023/2306.00424v1.pdf,"An illustration of the multimodal retrieval task from the ReMuQ dataset. The image shows the Empire State Building and the question asks if it is the tallest building in ""the city"". Neither the image nor the question explicitly mentions that ""the city"" is New York. The challenge therefore is to use the cues in the image and question to retrieve relevant information and answer the question. In this illustration we show the retrieved knowledge using only the image (K1), only the question (K2), or both image and question (K3). Only K3 can be used to answer the question correctly.","We investigate knowledge retrieval with multi-modal queries, i.e.,queries containing information split across image and text inputs, a challenging task that differs from previous work on cross-modal retrieval. We curate a new dataset called ReMuQ for benchmarking progress on this task. ReMuQ requires a system to retrieve knowledge from a large corpus by integrating contents from both text and image queries. We introduce a retriever model ""ReViz"" that can directly process input text and images to retrieve relevant knowledge in an end-to-end fashion without being dependent on intermediate modules such as object detectors or caption generators. We introduce a new pretraining task that is effective for learning knowledge retrieval with multimodal queries and also improves performance on downstream tasks. We demonstrate superior performance in retrieval on two datasets (ReMuQ and OK-VQA) under zero-shot settings as well as further improvements when finetuned on these datasets.","Humans recall, retrieve, and communicate information using many indirect hints and cues. For instance, if we want to explain the concept of a ""leopard"" but have forgotten the name, we can relate the concept to a picture of a tiger and say ""it is an animal that looks like this, but has spots instead of stripes"". Similarly, when children learn to draw a new shape like an oval, teachers often prompt them by showing a circle, but saying ""make the circle stretched-out"". This method of learning new concepts from visual aids and language descriptions is a common way of reinforcing existing knowledge and allowing learners to explore and retrieve new concepts. 
We propose a task for vision-language models to retrieve knowledge with multi-modal queries, i.e.,queries in which hints about the information to be retrieved are split across image and text inputs. Figure contains an example of this task, where the image shows the Empire State Building in New York City. If we retrieve knowledge using only the image, is it likely that the retrieved information (K1) will be related to the Empire State Building. However, K1 is insufficient to answer the question. On the other hand, if we retrieve knowledge using only the question, then the information retrieved (K2) is likely to be related to the tallest building in all cities (and not restricted to New York City). K2 by itself is also insufficient to answer the question. This example shows that the combined query containing both image and text (question) is necessary for retrieving relevant knowledge (K3). 
We introduce a new benchmark and dataset called ReMuQ (Retrieval with Multimodal Queries) to train and evaluate models to retrieve the answer from a corpus given multimodal (vision + language) queries. To create multimodal queries, we start with the WebQA dataset as a source ‚Äì WebQA contains images annotated with questions and answers. We select questions from WebQA where the answer includes both an image and text. We then remove any image information from text and combine the image and the augmented text to form a new multimodal query. We also construct a large retrieval corpus consisting of answer options of all questions as the source of knowledge for this task. 
This task requires integrating the contents from both modalities and retrieve knowledge ‚Äì in this paper we denote such a system as a ""VL-Retriever"". Existing VL-Retrievers typically follow a two-step process to retrieve knowledge: (1) converting the image into captions or keywords, appending them to the text query, and (2) using a text-retriever system to retrieve the knowledge. However, this approach can result in a loss of important information from the image, such as context and background. Additionally, using a caption generation model trained on a particular domain does not transfer well to other domains in real-world applications. 
To address these issues, we propose an end-to-end VL-Retriever that has the potential to leverage the entire image, rather than just object categories, keywords, and captions. We call this model ReViz, a retriever model for ""Reading and Vizualizing"" the query. As part of ReViz, we use a vision transformer-based model, ViLT, to directly encode the image from raw pixels with context inputs, and we employ BERT as the knowledge encoder to represent the long, free-form text as a knowledge embedding. ReViz differs from previous retrieval models in two main ways. First, it does not require an extra cross-modal translator (e.g., a captioning model) or object detector to represent the images. Second, its end-to-end design allows for the flexible retraining of each submodule of the model, which can mitigate potential issues caused by domain gaps. 
Unlike neural text-retrievers, the query and knowledge encoders in ReViz are of different types of modality (i.e.,multimodal transformer and language transformer). The different semantic spaces of the query and knowledge embeddings make alignment between them difficult. To address this, we propose a novel multimodal retrieval pretraining task. To create training data, we construct triplets of (input-image, input-text, output-knowledge) from the WiT dataset which contains encyclopedia-type knowledge from Wikipedia. We process the data such that the input image and text have mutually exclusive information. 
Our contributions and findings are listed below. 
 
 * We introduce a new dataset ReMuQ to facilitate research on retrieval with multimodal queries. 
 * We propose an end-to-end VL-Retriever, ReViz, that directly acquires knowledge given multimodal query. ReViz is not dependent on any cross-modal translator, such as an image captioning model or an object detector. 
 * We pretrain ReViz on a novel multimodal retrieval pretraining task, VL-ICT. We observe that with the proposed pretraining on the WiT dataset, our VL-Retriever is a powerful zero-shot multimodal retriever that surpasses existing single-modal knowledge retrieval methods."
VendorLink: An NLP approach for Identifying & Linking Vendor Migrants & Potential Aliases on Darknet Markets,2305.02763v1,./img_ACL_2023/2305.02763v1.pdf," (i) Closed-Set Vendor Verification Task: A supervised pre-training task that performs classification using a BERT-cased classifier in a closed-set environment to verify unique vendor migrants across existing markets, (ii) Open-Set Vendor Identification Task: A text-similarity task in an open-set environment that utilizes style representations from the established BERT-cased classifier to verify known vendors and identify potential-aliases, (iii) Low-Resource Market Adaptation Task: A knowledge-transfer task in a closed-set environment to adapt new market knowledge and verify migrants across Low-Resource (LR) emerging markets.","The anonymity on the Darknet allows vendors to stay undetected by using multiple vendor aliases or frequently migrating between markets. Consequently, illegal markets and their connections are challenging to uncover on the Darknet. To identify relationships between illegal markets and their vendors, we propose VendorLink, an NLP-based approach that examines writing patterns to verify, identify, and link unique vendor accounts across text advertisements (ads) on seven public Darknet markets. In contrast to existing literature, VendorLink utilizes the strength of supervised pre-training to perform closed-set vendor verification, open-set vendor identification, and low-resource market adaption tasks. Through VendorLink, we uncover (i) 15 migrants and 71 potential aliases in the Alphabay-Dreams-Silk dataset, (ii) 17 migrants and 3 potential aliases in the Valhalla-Berlusconi dataset, and (iii) 75 migrants and 10 potential aliases in the Traderoute-Agora dataset. Altogether, our approach can help Law Enforcement Agencies (LEA) make more informed decisions by verifying and identifying migrating vendors and their potential aliases on existing and Low-Resource (LR) emerging Darknet markets.","Conventional search engines index surface-web websites that only constitute 4%of the entire internet. The remaining comprises 90%Deep Web (not indexed) and 6%Darknet, which uses advanced anonymity enhancing protocols. While the former serves legitimate purposes requiring anonymity, the latter is also used for illegal activities such as financial fraud, child exploitation, and trading of illicit weapons, prohibited drugs, and chemicals. 
Given the Darknet 's scope, size, and anonymity, it is difficult for LEA to uncover connections between illegal marketplaces. While manual detection of such connections is a time-consuming and resource-extensive process, the recent success of online scrapers and monitoring systems has enabled researchers and LEA to analyze and automatically identify Darknet contents. This research proposes a vendor verification and identification approach to help LEA make better decisions by linking vendors, offloading manual labor, and generating similarity-based analyses. In contrast to the existing Darknet literature, VendorLink, as illustrated in Figure, emphasizes the following contributions to the problem of verifying and identifying vendors on Darknet markets: (i) Closed-Set Vendor Verification Task: Due to limited resources, LEA prioritizes investigating Darknet vendors based on the size and nature of their trade. Thus, Darknet vendors often distribute their business across multiple markets to stay undetected. Likewise, some vendors relocate and resume their business in other markets after a market seizes. We refer to these migrating vendors as migrants for brevity. Unfortunately, this movement prevents LEA from correctly estimating the size of a vendor' s operations. To aid LEA, we perform supervised pre-training by conducting multiclass classification in a closed-set environment to analyze different writing styles in text ads and classify vendor migrants to unique vendor accounts across three Darknet markets. Moreover, researchers have observed a significant difference in language structure between Darknet and Surface net websites. Since most contextualized models are trained on surface web data, the supervised pre-training step allows our model to adapt to the Darknet market domain knowledge. (ii) Open-set Vendor Identification Task: Darknet vendors often create aliases and work in groups to distribute their products across multiple markets, allowing them to expand their business without being detected by LEA. Moreover, given the scope and anonymity of the Darknet, manually linking these profiles is infeasible. Hundreds of new markets and vendors emerge daily on the Darknet. While the existing literature has established impressive performance on the vendor verification task, any trained classifier will fail during inference to encounter unknown vendors from emerging markets in real-to-close-world scenarios. Therefore, in this research, we use the style representations from the pre-trained classifier to compute the cosine similarity between the text ads to verify existing vendors and identify potential aliases and unknown vendors in an open-set environment. (iii) Low-Resource Market Adaptation task: While research has demonstrated impressive performance for the Darknet 's vendor verification task, high computational and storage requirements pose a significant challenge to LEA. Furthermore, with the exponential growth of Darknet markets and vendors with new content every year, there is a dire need for systems that can verify existing vendors from a known database and simultaneously adapt to new market knowledge from emerging vendors and markets. After all, not all LEA have the resources to train computationally expensive models from scratch. Therefore, this experiment investigates our classifier' s capability to benefit transfer learning in a low-resource setting for adapting new market knowledge and performing closed-set vendor verification on emerging (upcoming) vendors and markets. Finally, we evaluate the influence of knowledge transfer on our trained low-resource model against the zero-shot and transformers-based baselines."
Modeling User Satisfaction Dynamics in Dialogue via Hawkes Process,2305.12594v1,./img_ACL_2023/2305.12594v1.pdf,An example dialogue showing the dynamics of user satisfaction across different interaction turns.,"Dialogue systems have received increasing attention while automatically evaluating their performance remains challenging. User satisfaction estimation (USE) has been proposed as an alternative. It assumes that the performance of a dialogue system can be measured by user satisfaction and uses an estimator to simulate users. The effectiveness of USE depends heavily on the estimator. Existing estimators independently predict user satisfaction at each turn and ignore satisfaction dynamics across turns within a dialogue. In order to fully simulate users, it is crucial to take satisfaction dynamics into account. To fill this gap, we propose a new estimator ASAP (sAtisfaction eStimation via HAwkes Process) that treats user satisfaction across turns as an event sequence and employs a Hawkes process to effectively model the dynamics in this sequence. Experimental results on four benchmark dialogue datasets demonstrate that ASAP can substantially outperform state-of-the-art baseline estimators.","Dialogue systems are playing an increasingly important role in our daily lives. They can serve as intelligent assistants to help users accomplish tasks and answer questions or as social companion bots to converse with users for entertainment. In recent years, the research and development of dialogue systems has made remarkable progress. However, due to the complexity of human communication, the latest dialogue systems may still fail to understand users 'intents and generate inappropriate responses. These deficiencies pose huge challenges to deploying dialogue systems to real-life applications, especially high-stakes ones such as finance and health. In light of this, it is crucial to evaluate the performance of dialogue systems adequately in their development phase. 
Generally speaking, there are two types of evaluation methods, human evaluation and automatic evaluation. Human evaluation is fairly effective, but costly and hard to scale up. By contrast, automatic evaluation is more scalable. However, due to the ambiguity of what constitutes a high-quality dialogue, there are currently no universally accepted evaluation metrics. Existing commonly used metrics such as BLEU usually do not agree with human judgment. Nonetheless, user satisfaction estimation (USE) has been proposed as an alternative. USE assumes that the performance of a dialogue system can be approximated by the satisfaction of its users and simulates users' satisfaction with an estimator. In this regard, USE performs automatic evaluation and is thus scalable. 
Aside from helping developers find the defects of a dialogue system, USE also makes it possible to carry out timely human intervention for dissatisfied users and continuously optimize the system from human feedback. In essence, USE is a multi-class classification problem and the goal is to predict user satisfaction at each turn. Take the dialogue shown in Figure as an example, where user satisfaction is measured on a three-point scale. At the first two turns, the system responds appropriately. However, at the third turn, even though the response seems to be reasonable, the system asks for information that the user has already provided at the first turn, which may lead to dissatisfaction. 
As a model-based metric, the evaluation quality of USE relies heavily on the satisfaction estimator used. In order to train a robust estimator, different approaches have been proposed. Despite the effectiveness of these approaches, they estimate user satisfaction at each turn independently and ignore the dynamics of user satisfaction across turns within a dialogue. Given that a user's satisfaction is not only related to the current dialogue context, but may also be related to the satisfaction states at previous turns, we argue that modeling user satisfaction dynamics is valuable for training a more powerful estimator. 
To achieve this, we propose ASAP (sAtisfaction eStimation via HAwkes Process), a novel approach that leverages Hawkes process to capture the dynamics of user satisfaction. Hawkes process is a self-exciting point process and it has been widely adopted to model sequential data such as financial transactions and healthcare records. In particular, we make the following contributions: 
 
 * We first propose a base estimator to predict user satisfaction based solely on the dialogue context. We then incorporate a Hawkes process module to model user satisfaction dynamics by treating the satisfaction scores across turns within a dialogue as an event sequence. 
 * We propose a discrete version of the continuous Hawkes process to adapt it to the USE task and implement this module with a Transformer architecture. 
 * We conduct extensive experiments on four dialogue datasets. The results show that ASAP substantially outperforms baseline methods."
FormNetV2: Multimodal Graph Contrastive Learning for Form Document Information Extraction,2305.02549v2,./img_ACL_2023/2305.02549v2.pdf,"Graph of a sample region from a form. Token bounding boxes are identified, and from them the graph is constructed. Nodes are labeled and the graph structure is shown abstracted away from its content.","The recent advent of self-supervised pre-training techniques has led to a surge in the use of multimodal learning in form document understanding. However, existing approaches that extend the mask language modeling to other modalities require careful multi-task tuning, complex reconstruction target designs, or additional pre-training data. In FormNetV2, we introduce a centralized multimodal graph contrastive learning strategy to unify self-supervised pre-training for all modalities in one loss. The graph contrastive objective maximizes the agreement of multimodal representations, providing a natural interplay for all modalities without special customization. In addition, we extract image features within the bounding box that joins a pair of tokens connected by a graph edge, capturing more targeted visual cues without loading a sophisticated and separately pre-trained image embedder. FormNetV2 establishes new state-of-the-art performance on FUNSD, CORD, SROIE and Payment benchmarks with a more compact model size.","Automated information extraction is essential for many practical applications, with form-like documents posing unique challenges compared to article-like documents, which have led to an abundance of recent research in the area. In particular, form-like documents often have complex layouts that contain structured objects like tables, columns, and fillable regions. Layout-aware language modeling has been critical for many successes. 
To further boost the performance, many recent approaches adopt multiple modalities. Specifically, the image modality adds more structural information and visual cues to the existing layout and text modalities. They therefore extend the masked language modeling (MLM) from text to masked image modeling (MIM) for image and text-image alignment (TIA) for cross-modal learning. The alignment objective may also help to prime the layout modality, though it does not directly involve text layouts or document structures. 
In this work, we propose FormNetV2, a multimodal transformer model for form information extraction. Unlike existing works ‚Äì which may use the whole image as one representation, or image patches, or image features of token bounding boxes ‚Äì we propose using image features extracted from the region bounded by a pair of tokens connected in the constructed graph. This allows us to capture a richer and more targeted visual component of the intraand inter-entity information. Furthermore, instead of using multiple self-supervised objectives for each individual modality, we introduce graph contrastive learning to learn multimodal embeddings jointly. These two additions to FormNetV1 enable the graph convolutions to produce better super-tokens, resulting in both improved performance and a smaller model size. 
In experiments, FormNetV2 outperforms its predecessor FormNetV1 as well as the existing multimodal approaches on four standard benchmarks. In particular, compared with FormNetV1, FormNetV2 outperforms it by a large margin on FUNSD (86.35 v. s. 84.69) and Payment (94.90 v. s. 92.19); compared with DocFormer, FormNetV2 outperforms it on FUNSD and CORD with nearly 2.5x less number of parameters."
Backpack Language Models,2305.16765v1,./img_ACL_2023/2305.16765v1.pdf,"Transformers are monolithic functions of sequences. In Backpacks, the output is a weighted sum of non-contextual, learned word aspects.","We present Backpacks: a new neural architecture that marries strong modeling performance with an interface for interpretability and control. Backpacks learn multiple non-contextual sense vectors for each word in a vocabulary, and represent a word in a sequence as a context-dependent, non-negative linear combination of sense vectors in this sequence. We find that, after training, sense vectors specialize, each encoding a different aspect of a word. We can interpret a sense vector by inspecting its (non-contextual, linear) projection onto the output space, and intervene on these interpretable hooks to change the model 's behavior in predictable ways. We train a 170M-parameter Backpacklanguage model on OpenWebText, matching the loss of a GPT-2 small (124M-parameter) Transformer. On lexical similarity evaluations, we find that Backpacksense vectors outperform even a 6B-parameter Transformer LM' s word embeddings. Finally, we present simple algorithms that intervene on sense vectors to perform controllable text generation and debiasing. For example, we can edit the sense vocabulary to tend more towards a topic, or localize a source of gender bias to a sense vector and globally suppress that sense.","Consider the prefix The CEO believes that _ _ _, and the problem of debiasing a neural language model 's distribution over he/she. Intuitively, the bias for he originates in the word CEO, because replacing CEO with nurse flips the observed bias. A successful intervention to debias CEO must reliably apply in all contexts in which the word CEO appears; ideally we would want to make a non-contextual change to the model that has predictable effects in all contexts. In general, in all aspects of interpretability and control, it is desirable to make interventions with a tractable interface (e.g., non-contextual representations) that apply globally. 
Such interventions are difficult in Transformer models because their contextual representations are monolithic functions of their input. Almost any intervention on the model has complex, non-linear effects that depend on context. We would instead like models that enable precise, rich interventions that apply predictably in all contexts, and are still expressive, so they are a viable alternative to Transformers. 
We address these challenges with a new neural architecture, the Backpack, for which predictions are log-linear combinations of non-contextual representations. We represent each word in a vocabulary as a set of non-contextual sense vectors that represent distinct learned aspects of the word. For example, sense vectors for the word ""science"" could encode types of science, connections to technology, notions of science being ""settled, "" or different aspects of the scientific process (replication or experiment) (Table). Sense vectors do not learn classic word sense, but more general aspects of a word' s potential roles in different contexts; in fact, they can be seen as a multi-vector generalization of classic word vectors. 
To make interventions on sense vectors behave predictably in different contexts, a Backpackrepresents each word in a sequence as a linear combination of the sense vectors for all words in the sequence. The expressivity of a Backpackcomes from the network that computes the weights of the linear combination as a function of the whole sequence; for example, in all our experiments we use a Transformer for this. Since sense vectors are softly selected depending on the context, they can specialize; each sense can learn to be predictively useful in only some contexts. The log-linear contribution of senses to predictions then implies that the interventions on sense vectors we demonstrate in Section apply identically (up to a non-negative scalar weight) regardless of context. 
Our experiments demonstrate the expressivity of Backpacklanguage models, and the promise of interventions on sense vectors for interpretability and control. In Section we train Backpacklanguage models on 50B tokens (5 epochs) of OpenWebText; a Backpackwith 124M parameters in the contextual network (and 46M parameters for sense vectors) achieves the perplexity of a 124M-parameter Transformer; thus one pays for more interpretability with a larger model size. In Section, we show that sense vectors specialize to encode rich notions of word meaning. Quantitatively, on four lexical similarity datasets (e.g., SimLex999), sense vectors of a 170M parameter Backpackoutperform word embeddings of the 6B-parameter GPT-J-6B Transformer, and approach the performance of state-of-the-art specialized methods for this task. Finally, in Section we show that sense vectors offer a control mechanism for Backpacklanguage models. For example, stereotypically gendered profession words (e.g., ""CEO"" or ""nurse"") tend to learn a sense vector associated with this gender bias; by downscaling this sense vector, we greatly reduce disparity in contextual predictions in a limited setting."
Limitations of Language Models in Arithmetic and Symbolic Induction,2208.05051v1,./img_ACL_2023/2208.05051v1.png,Examples of addition: the baseline setting (top) and Scratchpad with intermediate computation steps (bottom). A similar method with more detailed demonstration is introduced in.,"Recent work has shown that large pretrained Language Models (LMs) can not only perform remarkably well on a range of Natural Language Processing (NLP) tasks but also start improving on reasoning tasks such as arithmetic induction, symbolic manipulation, and commonsense reasoning with increasing size of models. However, it is still unclear what the underlying capabilities of these LMs are. Surprisingly, we find that these models have limitations on certain basic symbolic manipulation tasks such as copy, reverse, and addition. When the total number of symbols or repeating symbols increases, the model performance drops quickly. We investigate the potential causes behind this phenomenon and examine a set of possible methods, including explicit positional markers, fine-grained computation steps, and LMs with callable programs. Experimental results show that none of these techniques can solve the simplest addition induction problem completely. In the end, we introduce LMs with tutor, which demonstrates every single step of teaching. LMs with tutor is able to deliver 100%accuracy in situations of OOD and repeating symbols, shedding new insights on the boundary of large LMs in induction.","Transformer-based large pretrained Language Models, such as GPT3 and T5, have been widely used as few-shot learners in many NLP tasks. Recent work even finds these models can achieve state-of-the-art performance in arithmetic and symbolic reasoning. Although these models exhibit surprisingly impressive capabilities in complex arithmetic reasoning tasks, such as MultiArith and GSM8k, it has also been pointed out that they tend to make certain calculation errors and perform significantly worse when the number of math operations increases in equations. find that GPT3 displays strong proficiency in 2-digit arithmetic addition, but struggles in arithmetic addition on numbers with more than three digits. also observe that the fine-tuned T5 model can not correctly add or subtract arbitrarily long numbers. Larger models might perform better on the testing data, but worse on numbers that are longer than the training data (out-of-distribution, OOD). However, even with the largest T5 model they experimented, the out-of-distribution (OOD) accuracy is not as high as the in-distribution accuracy, and increasing the training data does not improve OOD generalization beyond a critical amount. 
The horizontal axis is the number of digits and the vertical axis is the accuracy. The prompts for GPT3 consist of 4 examples. The T5 models are trained on 1-5 digits of up to 2,000 examples and each training example consists of random numbers in the format of 2 4 1. In-dist: in-distribution. Out-of-dist. : out-of-distribution (OOD). In-distribution refers to training on up to k-digit numbers and testing on up to k-digit numbers while out-ofdistribution refers to training on up to k-digit numbers and testing on numbers with more digits. Œ± indicates the repetition level of the examples. An example x_1‚ãØ x_n with n digits are sampled with the next digit probability p (x_i+1|x_i) = Œ±, when x_i+1=x_i; otherwise, (1-Œ±) /9. Larger Œ± indicates a higher repetition level. Figure shows two possible addition exemplars for LMs. Addition can be considered as a basic arithmetic operation and a simple symbolic manipulation task. The scratchpad version gives more details on how humans do basic arithmetic. show that with more fine-grained demonstrations, the accuracy of addition can be improved dramatically with fine-tuning. Yet, it still can not achieve 100%on OOD data, even with thousands of training data points provided. Figure shows the performance of GPT-3 and T5 on addition using the scratchpad version of training data. The problem becomes more severe when there are repeating digits in the addition operands. 
As the performance drops with repeating digits, we suspect that LMs might not handle the repeating symbols well. Figure illustrates the performance of GPT-3 and T5 on the copy task, one of the simplest symbolic manipulation operations. GPT-3 and T5 still can not perform well on OOD. We further do a preliminary experiment where a T5 model is fine-tuned using the data containing repeating numbers of up to 80 digits, T5 still can not achieve 100%in-distribution accuracy on long repeating digits. The results indicate that there are two problems intervening: Transformers are not good at handling repeating symbols and OOD generalization. The repeating symbols can also be a problem even for in-distribution data. 
Why do large pretrained LMs that can do complex language generation fail on a simple symbolic manipulation task? 
In this paper, we investigate the potential causes behind this phenomenon. We believe that overcoming the aforementioned limitations is of critical importance for the future application of Transformer-based LMs to reasoning-intensive tasks. What are the necessary steps to take to significantly improve or even approach 100%accuracy on these simple but fundamentally important induction tasks? We examine a set of possible mitigation solutions including fine-grained computation steps, positional markers, and LMs with callable programs. 
Since incorporating computation steps improves the OOD generalization in arithmetic addition, one possible direction is to provide more fine-grained computation steps in the fine-tuning data or the few-shot prompt. However, it may not be sufficient to alleviate the problem of repeating numbers. When a human does addition, the position of each digit is used to differentiate the repeating digits. However, the self-attention mechanism in the Transformer may not tell which ""1"" is referred to in the input. This prompts us to explore using positional markers to differentiate the important tokens. Using these two methods to augment the reasoning process, we find that the performance of pretrained LMs still can not reach satisfying results. Then we resort to a method where the copy operation is implemented as a primitive function and explore whether the LM can further boost its performance. 
We experiment with three symbolic manipulation tasks: copying, reversing, and addition. Experimental results show that although generalization in these symbolic manipulation tasks is straightforward for humans, it is still challenging for LMs, and none of these mitigation methods fully solves the problems. In the end, we introduce LMs with tutor which demonstrates every single step of teaching, pinpointing where these digits come from. LMs with tutor is able to deliver 100%accuracy in situations of OOD and repeated symbols. In this design, LMs are used to generate actions that mimic operations in multiple tape Turing machines, rather than the intermediate results. These actions generate the intermediate results on tapes. We hope this could shed light on the capability of Transformer-based LMs in addition to providing large training datasets or scaling up the size of these models. 
To conclude, our main contributions are: 
 * We identify a set of simple symbolic manipulation tasks and uncover the limitations of the LMs in arithmetic and symbolic induction. 
 * We examine a set of potential techniques including positional markers, fine-grained computation steps, and LMs with callable programs. Though these techniques could mitigate the limitations of the LMs, none of them can completely solve the generalization problem. 
 * Finally, we demonstrate that LMs with tutor is able to deliver 100%accuracy in situations of OOD and repeated symbols. Our analysis could inspire new thoughts to overcome the limitation of LMs in symbolic manipulation."
How poor is the stimulus? Evaluating hierarchical generalization in neural networks trained on child-directed speech,2301.11462v2,./img_ACL_2023/2301.11462v2.pdf,"The question types that models prefer when offered a choice between 6 questions. These 6 questions are formed by modifying a declarative with a relative clause on the subject according to 'prepose' and 'delete' rules. The correct category is Prepose Main, Delete Main. Within each architecture, the proportions across all 6 question types necessarily sum to 1. Each bar shows the average across 10 model re-runs, with single-standard-deviation error bars.","When acquiring syntax, children consistently choose hierarchical rules over competing non-hierarchical possibilities. Is this preference due to a learning bias for hierarchical structure, or due to more general biases that interact with hierarchical cues in children 's linguistic input? We explore these possibilities by training LSTMs and Transformers‚Äîtwo types of neural networks without a hierarchical bias‚Äîon data similar in quantity and content to children' s linguistic input: text from the CHILDES corpus. We then evaluate what these models have learned about English yes/no questions, a phenomenon for which hierarchical structure is crucial. We find that, though they perform well at capturing the surface statistics of child-directed speech (as measured by perplexity), both model types generalize in a way more consistent with an incorrect linear rule than the correct hierarchical rule. These results suggest that human-like generalization from text alone requires stronger biases than the general sequence-processing biases of standard neural network architectures.","Syntax is driven by hierarchical structure, yet we typically encounter sentences as linear sequences of words. How do children come to recognize the hierarchical nature of the languages they acquire? Some argue that humans must have a hierarchical inductive bias‚Äîan innate predisposition for hierarchical structure. An alternative view is that no such bias is necessary: there may be clear evidence for hierarchical structure in children 's input, so that children would choose hierarchical rules even without a hierarchical bias. 
At first blush, recent work in natural language processing (NLP) may seem to indicate that no hierarchical bias is necessary. Neural networks trained on naturally-occurring text perform impressively on syntactic evaluations even though they have no explicit syntactic structure built into them. However, these results do not provide strong evidence about the learning biases required to learn language from the data available to humans because these models receive very different training data than humans do. First, NLP models are typically trained on far more data than children receive, so models have more opportunities to encounter rare syntactic structures. Second, most training sets in NLP are built from Internet text (e.g., Wikipedia), which differs qualitatively from the utterances that children typically hear; e.g., sentences in Wikipedia are on average 25 words long, compared to 5 words for sentences in the North American English subset of the CHILDES corpus of child-directed speech. 
In this work, to evaluate if neural networks without a hierarchical bias generalize like children do, we train models on text comparable to the sentences in children' s linguistic input: English data from CHILDES. We then analyze what they have learned about the relationship between declarative sentences, such as, and their corresponding yes/no questions, such as: . . Those are your checkers. . Ã± Are those your checkers? 
Crucially, nearly all naturally-occurring yes/no questions are consistent with two rules: one based on hierarchical structure, and one based on linear order: , . HierarchicalQ: The auxiliary at the start of a yes/no question corresponds to the main auxiliary of the corresponding declarative. . LinearQ: The auxiliary at the start of a yes/no question corresponds to the first auxiliary of the corresponding declarative. 
Despite the scarcity of evidence disambiguating these rules, children reliably favor HierarchicalQ, albeit with occasional errors consistent with LinearQ. Yes/no questions thus are a prime candidate for an aspect of English syntax for which human-like generalization requires a hierarchical bias. We evaluate yes/no question performance in LSTMs and Transformers, two neural-network architectures that have no inherent hierarchical inductive bias. These architectures employ different computational mechanisms, so consistent results across both would indicate that our results are not due to idiosyncrasies of one particular architecture. 
To investigate if models generalize more consistently with the hierarchical or linear rule, we evaluate them on cases where the rules make different predictions, such as: under HierarchicalQ, the question that corresponds to is, whereas under LinearQ it is. . . The boy who has talked can read. . Ã± Can the boy who has talked read? . Ãß *Has the boy who talked can read? 
We find that across several ways of framing the learning task, models fail to learn HierarchicalQ. Instead, they generalize in ways that depend on linear order and on the identities of specific words. These results suggest that children's training data, if taken to be words alone, may not contain enough hierarchical cues to encourage hierarchical generalization in a learner without a hierarchical bias. Thus, explaining human acquisition of syntax may require postulating that humans have stronger inductive biases than those of LSTMs and Transformers, or that information other than word sequences plays a crucial role."
Open Set Relation Extraction via Unknown-Aware Training,2306.04950v1,./img_ACL_2023/2306.04950v1.pdf,"The decision boundary optimized only on the known relations cannot cope with an open set setting, in which the input may come from the relations unobserved in training. We target at regularizing the decision boundary by synthesizing difficult negative instances.","The existing supervised relation extraction methods have achieved impressive performance in a closed-set setting, where the relations during both training and testing remain the same. In a more realistic open-set setting, unknown relations may appear in the test set. Due to the lack of supervision signals from unknown relations, a well-performing closed-set relation extractor can still confidently misclassify them into known relations. In this paper, we propose an unknown-aware training method, regularizing the model by dynamically synthesizing negative instances. To facilitate a compact decision boundary, ""difficult"" negative instances are necessary. Inspired by text adversarial attacks, we adaptively apply small but critical perturbations to original training instances and thus synthesizing negative instances that are more likely to be mistaken by the model as known relations. Experimental results show that this method achieves SOTA unknown relation detection without compromising the classification of known relations.","Relation extraction (RE) is an important basic task in the field of natural language processing, aiming to extract the relation between entity pairs from unstructured text. The extracted relation facts have a great practical interest to various downstream applications, such as dialog system, knowledge graph, web search, among others. 
Many efforts have been devoted to improving the quality of extracted relation facts. Conventional supervised relation extraction is oriented to known relations with pre-specified schema. Hence, the paradigm follows a closed-set setting, meaning that during both training and testing the relations remain the same. Nowadays, neural RE methods have achieved remarkable success within this setting; and in contrast, open relation extraction (OpenRE) is focused on discovering constantly emerging unknown relations. Common practices include directly tagging the relational phrases that link entity pairs, and clustering instances with the same relation. However, relation extraction in real applications follows an open-set setting, meaning that both known and unknown relations are mixed within testing data. This requires that a model can not only distinguish among the known relations, but also filter the instances that express unknown relations. The ability to filter these instances is also called none-of-the-above (NOTA) detection. 
Unfortunately, a well-performing closed-set model can still confidently make arbitrarily wrong predictions when exposed to unknown test data. As shown in fig. (a), the decision boundary is optimized only on the known relational data (white points), leading to a three-way partition of the whole space. Consequently, the unknown relational data (black points), especially those far from the decision boundary, will be confidently classified into one of the known relations. By contrast, a more compact decision boundary (as shown in fig. (b) ) is desirable for NOTA detection. However, the compact decision boundary requires ""difficult"" negative data (red points in fig. (b) ) to be used, so strong supervision signals can be provided. It is important to note that synthesizing such negative data is a non-trivial task. 
In this work, we propose an unknown-aware training method, which simultaneously optimizes known relation classification and NOTA detection. To effectively regularize the classification, we iteratively generate negative instances and optimize a NOTA detection score. During the testing phase, instances with low scores are considered as NOTA and filtered out. The key of the method is to synthesize ""difficult"" negative instances. Inspired by text adversarial attacks, we achieve the goal by substituting a small number of critical tokens in original training instances. This would erase the original relational semantics and the model is not aware of it. By using gradient-based token attribution and linguistic rules, key tokens that express the target relation are found. Then, the tokens are substituted by misleading normal tokens that would cause the greatest increase of NOTA detection score, thus misleading negative instances, which are more likely to be mistaken by the model as known relations, are synthesized. Human evaluation shows that almost all the synthesized negative instances do not express any known relations. Experimental results show that the proposed method learns more compact decision boundary and achieve state-of-the-art NOTA detection performance. Our codes are publicly available at Github. 
The contributions are threefold: (1) we propose a new unknown-aware training method for more realistic open-set relation extraction. The method achieves state-of-the-art NOTA detection, without compromising the classification of known relations; (2) the negative instances are more challenging to the model, when compared to the mainstream synthesis method (e.g., generative adversarial network (GAN) -based method); (3) the comprehensive evaluation and analysis facilitate future research on the pressing but underexplored task."
Unsupervised Extractive Summarization of Emotion Triggers,2306.01444v1,./img_ACL_2023/2306.01444v1.pdf,An example post from CovidET-EXT annotated with emotion triggers. The highlighted sentences represent triggers of the tagged emotions.,"Understanding what leads to emotions during large-scale crises is important as it can provide groundings for expressed emotions and subsequently improve the understanding of ongoing disasters. Recent approaches trained supervised models to both detect emotions and explain emotion triggers (events and appraisals) via abstractive summarization. However, obtaining timely and qualitative abstractive summaries is expensive and extremely time-consuming, requiring highly-trained expert annotators. In time-sensitive, high-stake contexts, this can block necessary responses. We instead pursue unsupervised systems that extract triggers from text. First, we introduce CovidET-EXT, augmenting 's abstractive dataset (in the context of the COVID-19 crisis) with extractive triggers. Second, we develop new unsupervised learning models that can jointly detect emotions and summarize their triggers. Our best approach, entitled Emotion-Aware Pagerank, incorporates emotion information from external sources combined with a language understanding module, and outperforms strong baselines. We release our data and code at <https: //github. com/tsosea2/CovidET-EXT>.","Language plays a central role in social, clinical, and cognitive psychology, and social media presents a gold mine for such analysis: people turn to social media to share experiences around challenges in their personal lives and seek diagnosis, treatment, and emotional support for their conditions. During crises, such as natural disasters or global pandemics, large-scale analysis of language on social media ‚Äî both how people feel and what 's going on in their lives to lead to these feelings ‚Äî can have a profound impact on improving mental health solutions as well as helping policymakers take better-informed decisions during a crisis. 
Recent work taps into this broad challenge by jointly detecting emotions and generating a natural language description about what triggers them (triggers include both objective events and subjective appraisals of those events). Trigger explanation is formulated as a supervised, abstractive summarization task that is emotion-specific. Unlike generic summarization however, due to the high cognitive load to provide judgments for each emotion, obtaining human-written summaries for this task is time-consuming and requires significant annotator training. This results in small, domain-specific datasets that are difficult to scale ‚Äî especially in the face of new crisis events where the timing of such analysis is often pivotal. 
This work instead takes a fully unsupervised approach such that we do not rely on any labeled data, thus becoming agnostic to distributional shifts in domain or types of crisis, and robust for time-critical events. We posit that emotion triggers can be summarized effectively in an extractive manner where unsupervised methods are well-suited; we thus tackle the challenge of simultaneous emotion prediction and trigger extraction. 
For this new task, we first introduce CovidET-EXT, augmenting' s CovidET with manually annotated extractive summaries corresponding to each of their abstractive summaries. The result is a dataset of 
 1,883Reddit posts about the COVID-19 pandemic, manually annotated with7fine-grained emotions (from CovidET) and their corresponding extractive triggers (Figure). For every emotion present in a post, our annotators highlight sentences that summarize the emotion triggers, resulting in6,741extractive summaries in total. Qualitative analyses of the dataset indicate good agreement among the annotators, and follow-up human validations of the annotations also reveal high correctness. CovidET-EXT provides an ideal test bed to facilitate the development of extractive (supervised or unsupervised) techniques for the tasks of emotion detection and trigger summarization in crisis contexts. 
 
 =-1 We propose Emotion-Aware PageRank (EAP), a novel, fully unsupervised, graph-based approach for extractive emotion trigger summarization from text. The core of our method is to decompose the traditional PageRank ranking algorithm into multiple biased PageRanks, one for each emotion. To bias our model towards various emotions, our approach harnesses lexical information from emotion lexicons. Critically, unlike previous graph-based unsupervised approaches, which represent the text as a bag-of-words or word embeddings, EAP incorporates a language understanding module leveraging large language models to ensure that the summaries for an emotion are coherent in the context of that emotion. Results on our CovidET-EXT indicate the effectiveness of our EAP, which significantly pushes the Rouge-L score of our summaries by an average of2.7%over strong baselines. 
 
 Our contributions are as follows: 1) We introduce CovidET-EXT, a manually annotated benchmark dataset for the task of emotion detection and trigger summarization. 2) We propose Emotion-Aware PageRank, a variation of PageRank that combines a language understanding module and external emotion knowledge to generate emotion-specific extractive summaries. 3) We carry out a comprehensive set of experiments using numerous baselines to evaluate the performance on CovidET-EXT and show that our proposed EAP significantly outperforms strong baselines."
Pre-training Multi-party Dialogue Models with Latent Discourse Inference,2305.15175v1,./img_ACL_2023/2305.15175v1.pdf,The overview of our pre-training process. The left part shows the turn-level Expectation-Maximization process while the right part illustrates the discourse-level Variational Inference enhancement.,"Multi-party dialogues are more difficult for models to understand than one-to-one two-party dialogues, since they involve multiple interlocutors, resulting in interweaving reply-to relations and information flows. To step over these obstacles, an effective way is to pre-train a model that understands the discourse structure of multi-party dialogues, namely, to whom each utterance is replying. However, due to the lack of explicitly annotated discourse labels in multi-party dialogue corpora, previous works fail to scale up the pre-training process by putting aside the unlabeled multi-party conversational data for nothing. To fully utilize the unlabeled data, we propose to treat the discourse structures as latent variables, then jointly infer them and pre-train the discourse-aware model by unsupervised latent variable inference methods. Experiments on multiple downstream tasks show that our pre-trained model outperforms strong baselines by large margins and achieves state-of-the-art (SOTA) results, justifying the effectiveness of our method. The official implementation of this paper is available at <https: //github. com/EricLee8/MPD_EMVI>.","Dialogue system is an important area that has been studied for a long time in natural language processing field. Different from plain texts, dialogues are harder for models to understand since they are full of informal, colloquial expressions, and many ellipses. Among them, multi-party dialogues are even more complex since they involve multiple interlocutors, resulting in interweaving reply-to relations and information flows. Specifically, in multi-party dialogues, the current utterance can be a reply to any preceding utterance in the dialogue history, forming complex discourse structures. 
Intuitively, it is important for models to perceive the discourse structures, or in other words, to whom each utterance is replying, when comprehending multi-party dialogues. This intuition is in line with the process we humans participate in multi-party dialogues: we first read or listen to the dialogue history, knowing who speaks what to whom, then choose an utterance as the addressee, and finally utter a response. Literature has also justified that incorporating the discourse knowledge into models is beneficial for better understanding multi-party dialogues. Unfortunately, the process of choosing addressees is a naturally unobservable action, resulting in a large amount of multi-party conversational data without addressee labels. In this work, we focus on leveraging the unlabeled data to pre-train a model for multi-party dialogue understanding. 
To utilize the discourse structure, previous works seek help from human laborers to annotate the addressee labels on small datasets, where they either explicitly model the discourse structure using Graph Neural Networks or multi-task learning, or attempt to pre-train a model using objectives that are related to addressees by supervised learning. These works heavily rely on annotated addressee labels, which are rare in practice since the annotation process requires large amounts of human resources. As a result, they fail to be practical in real-world applications and are hard to scale up by utilizing more unlabeled multi-party conversational data. 
To make full use of the unlabeled corpora, a natural idea is to treat the unobservable discourse structure (reply-to relations) as latent variables, then adopt latent variable models to jointly infer them and optimize the discourse-aware models. However, it is not that simple when it comes to practice. For the Expectation-Maximization (EM) algorithm, the posterior distribution of the reply-to relations is intractable since it requires a square-level time complexity. If we turn to Variational Inference (VI) for help, the choice of the categorical prior distribution of the reply-to relations becomes troublesome: naive assumptions such as uniform distributions are too weak to make the training process converge. 
To step over the above obstacles, we subtly combine the single-turn EM algorithm and multi-turn VI into a two-stage pre-training strategy. In the first stage, we adopt the EM algorithm to jointly model the context-response matching objective and single-turn addressee inference, which requires only a linear time complexity and can preliminarily guide the model to a relatively good converging point with utterance-level knowledge. In the second stage, we extend the latent variables from single-turn addressees to multi-turn reply-to relations and optimize the model via both the EM algorithm and VI framework, where the prior distribution of the reply-to relations is no longer troublesome since it can be derived exactly from the E-steps. This stage further enhances the model with discourse-level knowledge and guides it converge to a better point. 
To sum up, the contributions of this work are: 
 
 * We successfully scale up the pre-training for multi-party dialogue understanding by leveraging the huge amounts of multi-party conversational corpora without addressee labels, while previous methods fail to work on these corpora. 
 * We subtly combine the single-turn EM algorithm and multi-turn VI framework in a two-stage pre-training process, which equips the model with knowledge of different granularities and makes it converge to an ideal point. 
 * The pre-trained model serves as a powerful encoder for multi-party dialogues and outperforms strong baselines by large margins, achieving SOTA results on multiple downstream tasks."
Multi-Source Test-Time Adaptation as Dueling Bandits for Extractive Question Answering,2306.06779v1,./img_ACL_2023/2306.06779v1.pdf,"The illustration of multi-source test-time adaptation from user feedback studied in this work. Each model is trained from a distinct source domain. With unlabeled test data, models are adapted online from user feedback.","In this work, we study multi-source test-time model adaptation from user feedback, where","Large language models (LLMs) can be fine-tuned or prompted with texts to achieve good performance in NLP tasks. However, because of the unexpected distribution shift at test time, the effectiveness of LLMs can degenerate. They may also generate outputs that are untrustworthy or toxic and fail to meet user expectations. One critical issue that we need to address is to improve the generalization ability of LLMs. Recent research on test-time adaptation (TTA) suggests a possible way to do this, by continually updating the deployed model with target data from an arbitrary test distribution. 
Interacting with users is important during test-time adaptation. First, user feedback allows the model to better align with humans. Users can directly teach the model to learn by interaction so that the model can be better trained to follow human instructions and reduce the generation of toxic and harmful content. Besides, obtaining feedback from users can also reduce the cost of data annotation by experts, and the collected data will be more in line with the distribution of the users, which makes the adaptation more economical and effective. 
Leveraging multiple learned models of tasks is also important for TTA. As in previous work, utilizing multiple known tasks helps the model better learn new tasks (or distributions), such as meta-learning and multi-source domain adaptation. To take advantage of known tasks, compared to reusing task data, directly using their learned models has gained popularity recently, which is much cheaper for online adaptation and has better data privacy protection. Recent work on lightweight tuning empowers LLMs to store knowledge of a large number of tasks cheaply. Platforms like Huggingface also allow users to share locally trained models, promoting a large amount of knowledge stored as models in the cloud. So, it has become more critical for TTA to adapt from multiple learned models of tasks."
Decoupling Pseudo Label Disambiguation and Representation Learning for Generalized Intent Discovery,2305.17699v1,./img_ACL_2023/2305.17699v1.pdf,The illustration of GID task.,"Generalized intent discovery aims to extend a closed-set in-domain intent classifier to an open-world intent set including in-domain and out-of-domain intents. The key challenges lie in pseudo label disambiguation and representation learning. Previous methods suffer from a coupling of pseudo label disambiguation and representation learning, that is, the reliability of pseudo labels relies on representation learning, and representation learning is restricted by pseudo labels in turn. In this paper, we propose a decoupled prototype learning framework (DPL) to decouple pseudo label disambiguation and representation learning. Specifically, we firstly introduce prototypical contrastive representation learning (PCL) to get discriminative representations. And then we adopt a prototype-based label disambiguation method (PLD) to obtain pseudo labels. We theoretically prove that PCL and PLD work in a collaborative fashion and facilitate pseudo label disambiguation. Experiments and analysis on three benchmark datasets show the effectiveness of our method.","Intent classification (IC) is an important component of task-oriented dialogue (TOD) systems. Traditional intent classification models are based on a closed-set hypothesis. That is, they rely on a pre-defined intent set provided by domain experts and can only recognize limited in-domain (IND) intent categories. However, users may input out-of-domain (OOD) queries in the real open world. OOD intent detection identifies whether a user query falls outside the range of pre-defined IND intent set. Further, OOD intent discovery task (also known as new intent discovery) groups unlabeled OOD intents into different clusters. However, all these work cannot expand the recognition scope of the existing IND intent classifier incrementally. 
To solve the problem, proposes the Generalized Intent Discovery (GID) task, which aims to simultaneously classify a set of labeled IND intents while discovering and recognizing new unlabeled OOD types incrementally. As shown in Fig, GID extends a closed-set IND classifier to an open-world intent set including IND and OOD intents and enables the dialogue system to continuously learn from the open world. Previous GID methods can be generally classified into two types: pipeline and end-to-end. The former firstly performs intent clustering and obtains pseudo OOD labels using K-means or DeepAligned, and then mixes labeled IND data with pseudo-labeled OOD data to jointly learn a new classifier. However, pipeline-based methods separate the intent clustering stage from the joint classification stage and these pseudo OOD labels obtained in the intent clustering stage may induce severe noise to the joint classification. In addition, the deep semantic interaction between the labeled IND intents and the unlabeled OOD data is not fully considered in the intent clustering stage. To alleviate these problems, proposes an end-to-end (E2E) framework. It mixes labeled IND data with unlabeled OOD data in the training process and simultaneously learns pseudo OOD cluster assignments and classifies IND& OOD classes via self-labeling. 
E2E framework achieves state-of-the-art results in most scenarios, but there are still two key challenges: (1) Pseudo Label Disambiguation. In the GID task, the performance of the joint classifier depends on pseudo labels of unlabeled OOD data, so we need to improve the reliability of pseudo labels during the training process, which is called ""pseudo label disambiguation"". (2) Representation Learning. We hope to form a clear cluster boundary for different IND and OOD intent types, which also benefits pseudo label disambiguation. As shown in Fig (a), the state-of-the-art E2E method adopts a self-labeling strategy for pseudo label disambiguation and representation learning. Firstly, it obtains the pseudo label of an OOD query by its augmented view in a swapped prediction way for pseudo label disambiguation. Next, it uses the pseudo labels as supervised signals and adopts a cross-entropy classification loss for representation learning. Therefore, pseudo label disambiguation and representation learning are coupled, which has led to a non-trivial dilemma: the inaccurate pseudo labels will limit the quality of representation learning, and poor representation quality will in turn prevent effective pseudo label disambiguation. We also find that the coupling of pseudo label disambiguation and representation learning leads to slow convergence of the model (see Section). 
To solve this problem, we propose a novel Decoupled Prototype Learning framework (DPL) for generalized intent discovery, which aims to decouple pseudo label disambiguation and representation learning. Different from the previous E2E method, DPL consists of two complementary components: prototypical contrastive representation learning (PCL) to get good intent representations and prototype-based label disambiguation (PLD) to obtain high-quality pseudo labels, as shown in Fig (b). In our framework, PCL and PLD work together to realize the decoupling of pseudo label disambiguation and representation learning. Specifically, we firstly employ the output probability distribution of the joint classifier to align samples and corresponding prototypes and perform prototypical contrastive representation learning. We aim to pull together similar samples to the same prototype and obtain discriminative intent representations. Secondly, based on the embeddings and class prototypes learned by PCL, we introduce a prototype-based label disambiguation, which gradually updates pseudo labels based on the class prototypes closest to the samples. Finally, we use these pseudo labels to train a joint classifier. We leave the details in the following Section. In addition, we theoretically explain that prototypical contrastive representation learning gets closely aligned representations for examples from the same classes and facilitates pseudo label disambiguation (Section). We also perform exhaustive experiments and qualitative analysis to demonstrate that our DPL framework can obtain more reliable pseudo labels and learn better representations in Section. 
Our contributions are three-fold: (1) We propose a novel decoupled prototype learning (DPL) framework for generalized intent discovery to better decouple pseudo label disambiguation and representation learning. (2) We give a theoretical interpretation of prototypical contrastive representation learning to show that it gets better representations to help pseudo label disambiguation. (3) Experiments and analysis on three benchmark datasets demonstrate the effectiveness of our method for generalized intent discovery."
DecompEval: Evaluating Generated Texts as Unsupervised Decomposed Question Answering,2307.06869v1,./img_ACL_2023/2307.06869v1.pdf,"The overview of DecompEval. We take the evaluation of coherence in dialogue generation as an example. Left: The input of evaluation is formulated as an instruction-style question, which contains an instruction, a tuple of evaluation inputs, and a yes/no question about the quality of generated responses. Medium: The instruction-style question is decomposed into subquestions according to sentences. At each step, the instruction-tuned PLM generates an answer to the current subquestion based on the input prompt. Then, the answer becomes the constituent of the input prompt at the next step. Right: The instruction-tuned PLM recomposes all the subquestions with their answers to answer the original question and acquire the evaluation result.","Existing evaluation metrics for natural language generation (NLG) tasks face the challenges on generalization ability and interpretability. Specifically, most of the well-performed metrics are required to train on evaluation datasets of specific NLG tasks and evaluation dimensions, which may cause over-fitting to task-specific datasets. Furthermore, existing metrics only provide an evaluation score for each dimension without revealing the evidence to interpret how this score is obtained. To deal with these challenges, we propose a simple yet effective metric called DecompEval. This metric formulates NLG evaluation as an instruction-style question answering task and utilizes instruction-tuned pre-trained language models (PLMs) without training on evaluation datasets, aiming to enhance the generalization ability. To make the evaluation process more interpretable, we decompose our devised instruction-style question about the quality of generated texts into the subquestions that measure the quality of each sentence. The subquestions with their answers generated by PLMs are then recomposed as evidence to obtain the evaluation result. Experimental results show that DecompEval achieves state-of-the-art performance in untrained metrics for evaluating text summarization and dialogue generation, which also exhibits strong dimension-level / task-level generalization ability and interpretability.","Recently, pre-trained language models (PLMs) such as GPT, BART, and T5 have achieved promising performance in natural language generation (NLG) tasks, such as text summarization and dialogue generation. As the quality of generated texts gradually approaches that of human-written texts, there is an increasing demand for automatic evaluation metrics of generated texts. 
However, existing evaluation metrics are still struggling to measure the quality of generated texts accurately. Traditional metrics such as BLEU, METEOR, and ROUGE rely on n-gram overlap between generated texts and reference texts, which fail to detect the issues in the content of generated texts. Recent works resort to model-based evaluation metrics to compute the similarity between generated texts and reference texts based on contextual representations from pre-trained models or adopt the score of language modeling / masked language modeling for evaluation. Other works choose to train evaluation models on the evaluation datasets to fit human scores or distinguish human-written texts from negative samples, aiming to obtain higher correlations with human judgments in various evaluation dimensions (such as coherence and consistency) of specific datasets. 
We argue that there are two main challenges in building an evaluation metric for text generation: 1) Generalization Ability: Most of the existing metrics that have high correlations with human judgments on evaluation datasets are directly trained on the corresponding datasets. This may result in over-fitting to task-specific data and harm their generalization ability to other NLG tasks and dimensions. 2) Interpretability: Although recently proposed evaluation metrics can measure the quality of generated texts from multiple dimensions, they only provide an evaluation score for each dimension without giving evidence to interpret how they predict this score. 
To deal with these challenges, we propose a simple yet effective evaluation metric called DecompEval. Firstly, to improve the generalization ability, we formulate NLG evaluation as an instruction-style question answering (QA) task, and utilize instruction-tuned pre-trained language models to solve this task without training on task-specific data. The instruction-style question consists of an instruction, the input of NLG evaluation, and a yes/no question, e.g., ""Answer the following yes/no question. . . Is this a coherent response given the dialogue history? "" for the evaluation of coherence in dialogue generation, where the specific evaluation input is omitted. Secondly, we propose a question decomposition strategy to make the evaluation process more interpretable, instead of directly making instruction-tuned PLMs answer the original question. This strategy decomposes the question into the subquestions which sequentially evaluate the corresponding dimension of each sentence in the generated texts. Then, we recompose these subquestions with their answers generated by the PLM as evidence to make the PLM answer the original question, which is used to compute the final evaluation result. The evidence can promote the understanding of the evaluation process by indicating the potential problematic sentences that affect the evaluation score. 
Our main contributions are as follows: 
 
 * We propose an evaluation metric called DecompEval, which formulates NLG evaluation as an instruction-style QA task, and solves it with instruction-tuned PLMs via question decomposition. 
 * We conduct experiments on the benchmark datasets for evaluating text summarization and dialogue generation. Experimental results show that DecompEval can achieve state-of-the-art performance in untrained metrics. 
 * We empirically show that DecompEval can generalize to other evaluation dimensions and tasks (such as data-to-text generation) better than all the baselines, while improving the interpretability via decomposed subquestions with their answers."
Towards Better Entity Linking with Multi-View Enhanced Distillation,2305.17371v1,./img_ACL_2023/2305.17371v1.pdf,"The illustration of two types of entities. Mentions in contexts are in bold, key information in entities is highlighted in color. The information in the first type of entity is relatively consistent and can be matched with a corresponding mention. In contrast, the second type of entity contains diverse and sparsely distributed information, can match with divergent mentions.","Dense retrieval is widely used for entity linking to retrieve entities from large-scale knowledge bases. Mainstream techniques are based on a dual-encoder framework, which encodes mentions and entities independently and calculates their relevances via rough interaction metrics, resulting in difficulty in explicitly modeling multiple mention-relevant parts within entities to match divergent mentions. Aiming at learning entity representations that can match divergent mentions, this paper proposes a Multi-View Enhanced Distillation (MVD) framework, which can effectively transfer knowledge of multiple fine-grained and mention-relevant parts within entities from cross-encoders to dual-encoders. Each entity is split into multiple views to avoid irrelevant information being over-squashed into the mention-relevant view. We further design cross-alignment and self-alignment mechanisms for this framework to facilitate fine-grained knowledge distillation from the teacher model to the student model. Meanwhile, we reserve a global-view that embeds the entity as a whole to prevent dispersal of uniform information. Experiments show our method achieves state-of-the-art performance on several entity linking benchmarks.","Entity Linking (EL) serves as a fundamental task in Natural Language Processing (NLP), connecting mentions within unstructured contexts to their corresponding entities in a Knowledge Base (KB). EL usually provides the entity-related data foundation for various tasks, such as KBQA, Knowledge-based Language Models and Information Retrieval. Most EL systems consist of two stages: entity retrieval (candidate generation), which retrieves a small set of candidate entities corresponding to mentions from a large-scale KB with low latency, and entity ranking (entity disambiguation), which ranks those candidates using a more accurate model to select the best match as the target entity. This paper focuses on the entity retrieval task, which poses a significant challenge due to the need to retrieve targets from a large-scale KB. Moreover, the performance of entity retrieval is crucial for EL systems, as any recall errors in the initial stage can have a significant impact on the performance of the latter ranking stage. 
Recent advancements in pre-trained language models (PLMs) have led to the widespread use of dense retrieval technology for large-scale entity retrieval. This approach typically adopts a dual-encoder architecture that embeds the textual content of mentions and entities independently into fixed-dimensional vectors to calculate their relevance scores using a lightweight interaction metric (e.g., dot-product). This allows for pre-computing the entity embeddings, enabling entities to be retrieved through various fast nearest neighbor search techniques. 
The primary challenge in modeling relevance between an entity and its corresponding mentions lies in explicitly capturing the mention-relevant parts within the entity. By analyzing the diversity of intra-information within the textual contents of entities, we identify two distinct types of entities, as illustrated in Figure. Entities with uniform information can be effectively represented by the dual-encoder; however, due to its single-vector representation and coarse-grained interaction metric, this framework may struggle with entities containing divergent and sparsely distributed information. To alleviate the issue, existing methods construct multi-vector entity representations from different perspectives. Despite these efforts, all these methods rely on coarse-grained entity-level labels for training and lack the necessary supervised signals to select the most relevant representation for a specific mention from multiple entity vectors. As a result, their capability to effectively capture multiple fine-grained aspects of an entity and accurately match mentions with varying contexts is significantly hampered, ultimately leading to suboptimal performance in dense entity retrieval. 
In order to obtain fine-grained entity representations capable of matching divergent mentions, we propose a novel Multi-View Enhanced Distillation (MVD) framework. MVD effectively transfers knowledge of multiple fine-grained and mention-relevant parts within entities from cross-encoders to dual-encoders. By jointly encoding the entity and its corresponding mentions, cross-encoders enable the explicit capture of mention-relevant components within the entity, thereby facilitating the learning of fine-grained elements of the entity through more accurate soft-labels. To achieve this, our framework constructs the same multi-view representation for both modules by splitting the textual information of entities into multiple fine-grained views. This approach prevents irrelevant information from being over-squashed into the mention-relevant view, which is selected based on the results of cross-encoders. 
We further design cross-alignment and self-alignment mechanisms for our framework to separately align the original entity-level and fine-grained view-level scoring distributions, thereby facilitating fine-grained knowledge transfer from the teacher model to the student model. Motivated by prior works, MVD jointly optimizes both modules and employs an effective hard negative mining technique to facilitate transferring of hard-to-train knowledge in distillation. Meanwhile, we reserve a global-view that embeds the entity as a whole to prevent dispersal of uniform information and better represent the first type of entities in Figure. 
Through extensive experiments on several entity linking benchmarks, including ZESHEL, AIDA-B, MSNBC, and WNED-CWEB, our method demonstrates superior performance over existing approaches. The results highlight the effectiveness of MVD in capturing fine-grained entity representations and matching divergent mentions, which significantly improves entity retrieval performance and facilitates overall EL performance by retrieving high-quality candidates for the ranking stage."
Automated Metrics for Medical Multi-Document Summarization Disagree with Human Evaluations,2305.13693v1,./img_ACL_2023/2305.13693v1.pdf,"Spearman correlations between rankings produced by human-assessed quality facets (F1-F4), automated metrics (M1-M7), and combined pairwise system rankings (PW-combined) on the Cochrane MSLR dataset. Rankings from automated metrics are highly correlated as a group except for PIO-Overlap (A). PIO-Overlap rankings are strongly correlated with rankings from human-assessed facets, especially PIO agreement (B). Metrics most strongly associated with PW-Combined rankings are Delta-EI and PIO-Overlap (C). Rankings from commonly reported automated metrics like ROUGE and BERTScore are not correlated or anti-correlated with human-assessed system rankings (D).","Evaluating multi-document summarization (MDS) quality is difficult. This is especially true in the case of MDS for biomedical literature reviews, where models must synthesize contradicting evidence reported across different documents. Prior work has shown that rather than performing the task, models may exploit shortcuts that are difficult to detect using standard","Multi-document summarization (MDS) requires models to summarize key points across a set of related documents. Variants of this task have drawn significant attention in recent years, with the introduction of datasets in domains like newswire, Wikipedia, science, medical literature reviews, and law; and substantial methodological work to design model architectures tailored to this task. 
In this work, we focus on MDS for literature reviews (MSLR), a challenging variant of the task in which one attempts to synthesize all evidence on a given topic. When manually performed, such reviews usually take teams of experts many months to complete. Good review summaries aggregate the results of different studies into a coherent passage, while the evidence presented in the input studies will often be in conflict, complicating the synthesis task. 
Evaluating conditional text generation models is notoriously difficult, impeding progress in the field. Prior work on summarization evaluation has proposed various lexical and modeling-based approaches to assess generation quality, but these metrics predominately use correlation with human-assessed quality facets over relatively small numbers of examples to demonstrate utility. This limitation of current metric evaluation implies that existing automated measures may not generalize well. Further, evaluation in the multi-document setting adds additional complexity, e.g., prior work has shown that MDS models may sometimes exploit shortcuts that do not reflect as detectable changes in automated metrics. 
To address these challenges, we collect human annotations to evaluate current models and to support automated metrics development for the medical MDS task. We construct a dataset of such evaluations using public submissions from the 2022 MSLR shared task on literature review MDS. Selecting top-performing models, we label the summary quality of a sample of these models' outputs on the Cochrane subtask. As part of our analysis, we compare system rankings produced by automated metrics and human evaluations. Strikingly, our results highlight consistent and significant disagreements between automated metrics and humans, motivating the need for better automated evaluation metrics in this domain. 
We contribute the following: 
 
 * A dataset of summaries and quality annotations on participant submissions to the MSLR shared task. We include human annotations for 6 models on 8 individual quality facets () and pairwise preferences provided by five raters (). 
 * An analysis of lexical features among inputs, generated, and target summaries (), showing a large amount of undesirable copying behavior. 
 * An analysis of correlations between automated evaluation metrics and human-assessed quality (), and the differences in system rankings produced by automated metrics versus human evaluation (). We propose several novel evaluation metrics based on desired features of MSLR summaries (). We find that system rankings derived from commonly reported automated metrics are not correlated or even anti-correlated with rankings produced by human assessments of quality, though some of the metrics we propose demonstrate promise in capturing certain quality facets."
SimOAP: Improve Coherence and Consistency in Persona-based Dialogue Generation via Over-sampling and Post-evaluation,2305.11130v2,./img_ACL_2023/2305.11130v2.pdf,"We use a dialogue model to generate 10,000 responses each for 100 utterances in PersonaChat and use perplexity (PPL) to rerank them. The response is good when the TF-IDF similarity between the response and ground truth is above 0.25 and the result of the natural language inference model between the response and persona information is entailment. PPL in brackets is the average value of all responses in each rank.","Language models trained on large-scale corpora can generate remarkably fluent results in open-domain dialogue. However, for the persona-based dialogue generation task, consistency and coherence are also key factors, which are great challenges for language models. Existing works mainly focus on valuable data filtering, model structure modifying, or objective function designing, while their improvements are limited and hard to generalize to all types of pre-trained language models. However, we find that language models can produce consistent and coherent responses if we consider enough generations. Thus, the problems lay in large-scale response generation and target response selection. In this work, a simple but effective two-stage SimOAP strategy is proposed, i.e., over-sampling and post-evaluation. The over-sampling stage takes large-scale responses from existing trained models efficiently via off-the-shelf distilling and compressing methods, and the post-evaluation stage selects a good response based on multiple well-designed evaluation metrics from large-scale candidates. Experimental results show that the proposed plug-in SimOAP strategy improves the backbone models and outperforms the baseline strategies in both automatic and human evaluations.","Open-domain dialogue systems need to give appropriate responses based on history utterances. An ideal open-domain dialogue system should generate consistent, coherent, and diverse responses. Part of the existing open-domain dialogue generation work focuses on improving the diversity of responses, while avoiding generating generic responses and achieving good results. How to improve the consistency of dialogue generation is also an urgent problem to be solved. In addition, there is still the problem of poor coherence in dialogue generation. 
To improve the consistency and coherence of dialogue generation, the existing works mainly improve from three aspects: valuable data construction and filtering, model structure modifying and objective function designing. However, the problem of poor consistency and coherence is still a tough challenge, especially in persona-based dialogue generation. Because multiple constraints need to be satisfied simultaneously, part of which cannot be directly optimized, and part of the constraints conflict with each other, such as the conflict between the fluency of responses and the consistency of persona information. In addition, the above methods need to retrain the model and can only adapt to the part of the existing dialogue models. For example, carefully design the objective function and scale model sizes from 117M to 8.3B parameters, which brings a lot of training costs. Fortunately, we find that the existing dialogue models actually have strong capabilities that can generate consistent and coherent responses, and we just need to find ways to release their capabilities. 
First, we take a deep look at the characteristics of dialogue models, which believe that the response with the highest probability is the best. However, we wonder whether the high-probability responses generated by dialogue models are necessarily better than the low-probability responses. Based on the statistics in Figure, when the generation probability of responses decreases, the ratio of good responses increases first and then decreases. It shows that the ratio of good responses among low-probability responses is higher than that of high-probability responses, which is counter-intuitive. This is most likely because dialogue models use PPL as an optimization goal, but it is inconsistent with the requirements of coherence and consistency. To verify whether the good response with high TF-IDF similarity and high probability of entailment is indeed superior to the response directly generated by the model, we use the human evaluation for experimental validation. As shown in Table, such responses are better than those directly generated by the model. Therefore, it only needs to sample large-scale diverse responses from existing dialogue models and then select good responses. 
Inspired by the aforementioned motivations, We propose a simple two-stage method: over-sampling and post-evaluation (SimOAP) to improve the coherence and consistency in persona-based dialogue. There are two challenges in our work. The large-scale sampling will bring additional time cost, how to accelerate the model is a challenge. Large-scale sampling can produce good responses, how to pick good responses from them is another challenge. We address the above two challenges using oversampling and post-evaluation, respectively. In the over-sampling stage, SimOAP uses existing dialogue models for large-scale sampling, and the distilled or compressed models are used to reduce the additional time cost. In the post-evaluation stage, the TF-IDF algorithm and natural language inference (NLI) are used for coherence and consistency evaluation, respectively. 
To verify the effectiveness of our method, we conduct experiments on a persona-based dialogue dataset Personachat. Automatic evaluations and human evaluations show that our method effectively boosts the performance of dialogue models and outperforms two baselines on most metrics. In addition, applying our method to small models can also achieve a better performance than using large models directly. 
Our contributions in this paper are three folds: 
 
 * We verify that the high-probability responses generated by dialogue models are not necessarily better than the low-probability responses. That is, dialogue models can generate good responses, but they are not selected. 
 * We propose a simple two-stage method: over-sampling and post-evaluation to improve the coherence and consistency in persona-based dialogue generation and it is model-agnostic. 
 * We conduct comprehensive experiments on a persona-based dialogue dataset. Automatic evaluations and human evaluations show that our method improves the backbone models and outperforms the baselines."
NatLogAttack: A Framework for Attacking Natural Language Inference Models with Natural Logic,2307.02849v2,./img_ACL_2023/2307.02849v2.png,Overview of NatLogAttack generation and attacking process.,"Reasoning has been a central topic in artificial intelligence from the beginning. The recent progress made on distributed representation and neural networks continues to improve the state-of-the-art performance of natural language inference. However, it remains an open question whether the models perform real reasoning to reach their conclusions or rely on spurious correlations. Adversarial attacks have proven to be an important tool to help evaluate the Achilles 'heel of the victim models. In this study, we explore the fundamental problem of developing attack models based on logic formalism. We propose NatLogAttack to perform systematic attacks centring around natural logic, a classical logic formalism that is traceable back to Aristotle' s syllogism and has been closely developed for natural language inference. The proposed framework renders both label-preserving and label-flipping attacks. We show that compared to the existing attack models, NatLogAttack generates better adversarial examples with fewer visits to the victim models. The victim models are found to be more vulnerable under the label-flipping setting. NatLogAttack provides a tool to probe the existing and future NLI models' capacity from a key viewpoint and we hope more logic-based attacks will be further explored for understanding the desired property of reasoning.","While deep neural networks have achieved the state-of-the-art performance on a wide range of tasks, the models are often vulnerable and easily deceived by imposing perturbations to the original input, which seriously hurts the accountability of the systems. In depth, this pertains to model robustness, capacity, and the development of models with more advanced intelligence. 
Natural language inference (NLI), also known as textual entailment, is a fundamental problem that models the inferential relationships between a premise and hypothesis sentence. The models built on distributed representation have significantly improved the performance on different benchmarks. However, it is still highly desirable to conduct research to probe if the models possess the desired reasoning ability rather than rely on spurious correlation to reach their conclusions. 
Adversarial attacks have proven to be an important tool to reveal the Achilles 'heel of victim models. Specifically for natural language inference, the logic relations are easily broken if an attack model does not properly generate the adversarial examples following the logic relations and related semantics. Therefore, unlike other textual attack tasks such as those relying on semantic similarity and relatedness, it is more challenging to create effective attacks here. 
In this study, we explore the basic problem of developing adversarial attacks based on logic formalism, with the aim to probe victim models for the desired reasoning capability. Specifically, we propose NatLogAttack, in which the adversarial attacks are generated based on natural logic, a classical logic formalism with a long history that has been closely developed with natural language inference. From a general perspective, natural language inference provides an appropriate setup for probing the development of distributed representation and the models based on that. A robust solution for the task requires manipulation of discrete operations and adversarial attacks can help understand whether and how the required symbols and inference steps emerge from the data and the learned distributed representation. Our work has also been inspired by recent research on exploring the complementary strengths of neural networks and symbolic models. 
Our research contributes to the development of logic-based adversarial attacks for natural language understanding. Specifically, we propose a novel attack framework, NatLogAttack, based on natural logic for natural language inference. Our experiments with both human and automatic evaluation show that the proposed model outperforms the state-of-the-art attack methods. Compared to the existing attack models, NatLogAttack generates better adversarial examples with fewer visits to the victim models. In addition to the commonly used attack setting where the labels of generated examples remain the same as the original pairs, we also propose to construct label-flipping attacks. The victim models are found to be more vulnerable in this setup and NatLogAttack succeeds in deceiving them with much smaller numbers of queries. NatLogAttack provides a systematic approach to probing the existing and future NLI models' capacity from a basic viewpoint that has a traceable history, by combining it with the recent development of attacking models. The proposed framework is constrained by the natural logic formalism and we hope more logic-based attacks will be further explored for understanding the desired property of natural language reasoning."
Improved Instruction Ordering in Recipe-Grounded Conversation,2305.17280v1,./img_ACL_2023/2305.17280v1.pdf,A conversation snippet of the cooking instructional dialogue task with colorgood and colorbad system responses and the corresponding error type of each bad response.,"In this paper, we study the task of instructional dialogue and focus on the cooking domain. Analyzing the generated output of the GPT-J model, we reveal that the primary challenge for a recipe-grounded dialog system is how to provide the instructions in the correct order. We hypothesize that this is due to the model's lack of understanding of user intent and inability to track the instruction state (i.e., which step was last instructed). Therefore, we propose to explore two auxiliary subtasks, namely User Intent Detection and Instruction State Tracking, to support Response Generation with improved instruction grounding. Experimenting with our newly collected dataset, ChattyChef, shows that incorporating user intent and instruction state information helps the response generation model mitigate the incorrect order issue. Furthermore, to investigate whether ChatGPT has completely solved this task, we analyze its outputs and find that it also makes mistakes (10.7%of the responses), about half of which are out-of-order instructions. We will release ChattyChef to facilitate further research in this area at: <https: //github. com/octaviaguo/ChattyChef>.","Historically, work on conversational agents has mostly fallen into one of two categories: open-domain chatbots or goal-directed dialogue systems within narrow domains. However, recent advances in large language models have paved the way for the exploration of dialog agents that can engage in conversations with users to accomplish open-ended objectives, such as learning about a new topic, interpreting bureaucratic policies to answer questions, or negotiating within strategy games. 
In this paper, we explore the task of Recipe-Grounded Conversation, where the dialogue agent is expected to converse with a user to walk him/her through the cooking procedure of a recipe, while answering any questions that might arise along the way (see examples in Figure). Although many types of dialogue tasks have been proposed and explored, very little prior work has focused on providing instructions to a user to complete a task. In contrast to other dialogue tasks, such as document-grounded conversation, accurately tracking the conversation state is more crucial in recipe-grounded dialogue. This is because the agent needs to know which step in the recipe the user is currently working on in order to answer questions, such as: what is the next step? 
To investigate what challenges may arise in recipe-grounded conversation, we have collected a dataset by crowdsourcing (see). As an initial baseline model, we used this data to fine-tune GPT-J following a similar protocol to. Specifically, the conversation history, the grounded recipe, and the gold system response are concatenated as a long input sequence to the model (see for details). We show that fine-tuning GPT-J on a few recipe-grounded conversations works surprisingly well, however, the model makes a significant number of mistakes based on our manual inspection over 10 conversations (131 generated responses) of the fine-tuned model (Table). Examples of each type of error are presented in Figure. Notably, the most prevalent type of errors is presenting information from the recipe to the user in the wrong order. We thus focus on tackling this most common error in our work. We hypothesize two potential causes: (1) GPT-J struggles to understand the user 's intent, and (2) GPT-J has difficulty tracking the current state throughout the conversation. Both are crucial in many scenarios, for example, when the user asks for more information about the current instruction, the system should not skip ahead to a later step of the recipe. Based on these hypotheses, we experiment with two supplemental tasks to improve instruction ordering: User Intent Detection () and Instruction State Tracking (). 
The goal of Intent Detection is to classify the user' s current intent within a fixed set of possibilities (e.g., ask for the next instruction or ask for details about ingredients). Because the set of intents varies across domains, we take a few-shot transfer learning approach to leverage existing dialogue intent datasets, such as MultiWOZ 2.2 and Schema Guided Dialogue. We show that incorporating natural language descriptions of intents can enable more effective transfer. For example, F1 score for detecting 19 different user intents in ChattyChef increases from 32.0 to 65.1 when transferring from MultiWOZ (). In addition to Intent Detection, we also explore a simple yet effective method for Instruction State Tracking. State tracking aims to identify which recipe step the user is currently working on. We show that based on unigram F1 overlap, despite the approach's simplicity, we are able to identify the most relevant recipe step at each turn of the conversation with nearly 80%accuracy. 
The information from these two subtasks is then used to support Response Generation to improve instruction ordering (). Specifically, instead of feeding the whole recipe into the generation model, we leverage the instruction state to select only the most relevant knowledge. To incorporate user intents, we enrich the input prompt to the model with natural language descriptions of the predicted intent. Experiments show that even though intent and instruction state predictions are not perfect, including this information in the Response Generation model helps mitigate the wrong-order issue. We release <g r a p h i c s> ChattyChef, a new dataset of cooking dialogues, to support future work on instruction-grounded conversational agents."
Adaptive and Personalized Exercise Generation for Online Language Learning,2306.02457v1,./img_ACL_2023/2306.02457v1.png,We first assess student knowledge states from their learning history and then generate exercises based on estimated states and instructor control of desired properties including domain knowledge (vocabulary) and difficulty levels (expected error numbers).,"Adaptive learning aims to provide customized educational activities (e.g., exercises) to address individual learning needs. However, manual construction and delivery of such activities is a laborious process. Thus, in this paper, we study a novel task of adaptive and personalized exercise generation for online language learning. To this end, we combine a knowledge tracing model that estimates each student 's evolving knowledge states from their learning history and a controlled text generation model that generates exercise sentences based on the student' s current estimated knowledge state and instructor requirements of desired properties (e.g., domain knowledge and difficulty). We train and evaluate our model on real-world learner interaction data from Duolingo and demonstrate that LMs guided by student states can generate superior exercises. Then, we discuss the potential use of our model in educational applications using various simulations. These simulations show that our model can adapt to students' individual abilities and can facilitate their learning efficiency by personalizing learning sequences.","Adaptive learning technologies which continuously monitor student progress to dynamically adjust the level or type of learning materials based on the individual 's abilities are quite popular. Empirical studies have shown various benefits of adaptive learning, such as improved student learning outcomes, lower dropout rates, and increased instructor satisfaction. Despite their effectiveness, designing adaptive systems is challenging as it usually involves planning a series of exercises that is personalized and adaptive to each student, which requires diverse exercise planning as well as an understanding of the student learning process. 
On the other hand, powered by advances in neural NLP, works have been done for automatically generating text-based exercises or questions for educational purposes in second language learning, mathematics, and computer science. Nevertheless, how to apply these approaches in adaptive systems remains an open question. First, existing methods largely rely on pre-defined question templates or specified information sources (e.g., a passage), thereby resulting in limited knowledge coverage and low question difficulty control, and as a consequence, do not meet each student' s individual and nuanced learning needs. Besides, they are usually designed to generate standalone exercises, whereas adaptive learning systems usually require a continuous supply of exercises. Another related line of research studies exercise recommendation to customize learning content based on individual capabilities and goals. However, these systems are limited by the diversity of the exercise pool.
To address the above limitations, we study the task of exercise generation in the context of adaptive learning, where we hypothesize that a student 's dynamic knowledge state holds the key to generating adaptive and personalized exercises. Specifically, we ground our study in the domain of language learning to create exercise sentences for translation, of which Figure illustrates the overall process. We start with an assumption about the dynamics between exercise difficulty, vocabulary, and a student' s knowledge state (). Then, we propose an approach () that marries knowledge tracing (KT; ), a technique for estimating students 'mastery states of knowledge components from their learning history, with a controlled text generation model that generates the next exercise based on instructor requirements, such as specified domain knowledge and target difficulty. We further explore various strategies to adapt the generation of exercises based on students' changing knowledge states. In doing this, our model not only supports personalized generation where the instructor (or the system) can express some desired properties of the generated exercises but is also adaptive to each student's learning progress. 
We conduct extensive experiments on real-world student learning data from Duolingo, a popular online language learning platform that offers structured and individualized learning content. Our results () show that pre-trained LMs can help KT assess student language knowledge while student states estimated by KT can guide LMs to generate adaptive and personalized exercises. We further discuss the potential use of our model in educational applications with simulations. The simulations show that our model can dynamically adjust exercise difficulty to match individual learning progress and facilitate their learning efficiency by customizing exercise sequences."
Why Did the Chicken Cross the Road? Rephrasing and Analyzing Ambiguous Questions in VQA,2211.07516v2,./img_ACL_2023/2211.07516v2.pdf,"An ambiguous visual question from our dataset. Answers are grouped by the underlying question they answer, and the question is rephrased for each group. Answers within a group do not necessarily match, but do answer the same question.","Natural language is ambiguous. Resolving ambiguous questions is key to successfully answering them. Focusing on questions about images, we create a dataset of ambiguous examples. We annotate these, grouping answers by the underlying question they address and rephrasing the question for each group to reduce ambiguity. Our analysis reveals a linguistically-aligned ontology of reasons for ambiguity in visual questions. We then develop an English question-generation model which we demonstrate via automatic and human evaluation produces less ambiguous questions. We further show that the question generation objective we use allows the model to integrate answer group information without any direct supervision.","The ability to ask questions allows people to efficiently fill knowledge gaps and convey requests; this makes questions a natural interface for interacting with digital agents. Visual question answering (VQA) models more specifically seek to answer questions about images, which can be useful in a variety of settings, such as assistive tech. A number of datasets have been proposed for training VQA models, including VQAv2, VizWiz, and GQA. Such datasets are not only useful for training ‚Äì they represent the aggregate judgements of speakers on a variety of factors, including ambiguity. 
Ambiguity is a core feature of natural language, and can exist at all levels of linguistic analysis. In the context of data annotation, ambiguity often leads to disagreement between annotators. Given that the data resulting from crowdsourced annotation projects is typically used in a categorical fashion to train and evaluate models, annotator disagreements are problematic. Past work has often looked at detecting and resolving disagreements from the perspective of trust, where some annotators are assumed to be more or less trustworthy. However, in the case of ambiguity, an annotator's honest effort might still lead to disagreement; in such cases, collecting more annotations can fail to establish a consensus. This differs from disagreements arising as a result of mistakes and cheating, where gathering more annotations would effectively outvote low-quality annotations. Ambiguity in the context of questions presents a particularly rich problem: firstly, from a formal point of view, question semantics are an area of active development and debate; this makes empirical accounts of questions particularly useful. Secondly, questions are increasingly relevant to natural language processing (NLP) research. Many NLP tasks are cast as question-answering (QA), including a growing number of tasks which can be cast as few-shot QA. Against this backdrop, we aim to document and describe ambiguities in VQA as well as to introduce a model for resolving them. 
Our main contributions are: 
 * We examine how ambiguity appears in the VQAv2 data by constructing a dataset of 1,820 annotated visual image-question-answer triples. For each question, we ask annotators to re-group answers according to the underlying question they answer, and to rewrite questions to unambiguously correspond to that group. An example from our dataset can be seen in. For the ambiguous VQA question given at the top, annotators group existing answers into two topical groups (species and color). Then, for each group, annotators rewrite the original question such that it could be answered by answers from the corresponding group, but not from other groups. 
 * We create an ontology of causes for linguistic ambiguity based on the PropBank ontology, and annotate our data with these causes. 
 * We develop a visual question generation model which learns to rewrite questions; we validate this model with the re-grouped answers and re-written questions from our dataset. Our model can be used to cluster answers into their groups without any supervision for answer groups."
BIC: Twitter Bot Detection with Text-Graph Interaction and Semantic Consistency,2208.08320v2,./img_ACL_2023/2208.08320v2.png," (a) Different types of combining modalities. Previous methods adopt text modality and graph modality alone, or just shallow combine them. There is a need for an interactive method that interacts and exchanges information across the modalities. (b) Genuine users and Twitter bots have different patterns of semantic consistency. Tweets in red are abnormal and these example tweets show semantic inconsistency.","Twitter bots are automatic programs operated by malicious actors to manipulate public opinion and spread misinformation. Research efforts have been made to automatically identify bots based on texts and networks on social media. Existing methods only leverage texts or networks alone, and while few works explored the shallow combination of the two modalities, we hypothesize that the interaction and information exchange between texts and graphs could be crucial for holistically evaluating bot activities on social media. In addition, according to a recent survey, Twitter bots are constantly evolving while advanced bots steal genuine users' tweets and dilute their malicious content to evade detection. This results in greater inconsistency across the timeline of novel Twitter bots, which warrants more attention. In light of these challenges, we propose BIC, a Twitter Bot detection framework with text-graph Interaction and semantic Consistency. Specifically, in addition to separately modeling the two modalities on social media, BIC employs a text-graph interaction module to enable information exchange across modalities in the learning process. In addition, given the stealing behavior of novel Twitter bots, BIC proposes to model semantic consistency in tweets based on attention weights while using it to augment the decision process. Extensive experiments demonstrate that BIC consistently outperforms state-of-the-art baselines on two widely adopted datasets. Further analyses reveal that text-graph interactions and modeling semantic consistency are essential improvements and help combat bot evolution.","Twitter bots are controlled by automated programs and manipulated to pursue malicious goals such as advocating for extremism and producing spam. Bots are also involved in spreading misinformation during the pandemic. Since Twitter bots pose threat to online society, many efforts have been devoted to detecting bots. 
The majority of the existing approaches are text-based and graph-based. The text-based methods analyze the content to detect Twitter bots by natural language processing techniques. adopted recurrent neural networks to extract textual information. utilized the pre-trained language model BERT to help detect bots. The graph-based methods model the Twittersphere as graphs and adopt geometric neural networks or concepts of network dynamics to identify bots. constructed a heterogeneous graph and leveraged the different relation information. exploited the ego-graph of Twitter users and proposed a histogram and customized backward operator. 
However, existing methods are faced with two challenges. On the one hand, these methods only adopt texts or graphs alone, and only a few works shallowly combine the two modalities as Figure (a) shows. The text-based model can not get the graph modality information while the graph-based model can not get the text modality information. We hypothesize that it is wise to interact and exchange information between texts and graphs to evaluate bot activities. On the other hand, pointed out that Twitter bots are constantly evolving. Advanced bots steal genuine users 'tweets and dilute their malicious content to evade detection, which results in greater inconsistency across the timeline of advanced bots as Figure (b) illustrates. Previous methods can not capture this characteristic. Namely, there is an urgent need for a method that can identify advanced bots. 
In inspire of these challenges, we propose a framework BIC (Twitter Bot Detection with Text-Graph Interaction and Semantic Consistency). BIC separately models the two modalities, text and graph, in social media. A text module is adopted to encode the textual information and a graph module to encode graph information. BIC employs a text-graph interaction module to enable different modality information exchange across modalities in the learning process. To capture the inconsistency of advanced bots, BIC leverages a semantic consistency module, which employs the attention weights and a sample pooling function. Our main contributions are summarized as follows: 
 
 * We propose to interact and exchange information across text and graph modalities to help detect bots. We find that capturing novel bots' inconsistency can increase detection performance. 
 * We propose a novel Twitter bot detection model, BIC. It is an end-to-end model and contains a text-graph interaction module to exchange modality information and a semantic consistency module to capture the inconsistency of advanced bots. 
 * We conduct extensive experiments to evaluate BIC and state-of-the-art models on two widely used datasets. Results illustrate that BIC outperforms all baseline methods. Further analyses reveal the effectiveness of the text-graph interaction module and semantic consistency module."
Plug-and-Play Knowledge Injection for Pre-trained Language Models,2305.17691v2,./img_ACL_2023/2305.17691v2.pdf,Effect of dropout rates on the performance of general map-tuning.,,^* Equal contribution^‚Ä† Corresponding authors
SWiPE: A Dataset for Document-Level Simplification of Wikipedia Pages,2305.19204v1,./img_ACL_2023/2305.19204v1.pdf,"Sample from SWiPE, a Wikipedia-based dataset for document-level simplification. Many edits in SWiPE require document-level context.","Text simplification research has mostly focused on sentence-level simplification, even though many desirable edits‚Äîsuch as adding relevant background information or reordering content‚Äîmay require document-level context. Prior work has also predominantly framed simplification as a single-step, input-to-output task, only implicitly modeling the fine-grained, span-level edits that elucidate the simplification process. To address both gaps, we introduce the SWiPE dataset, which reconstructs the document-level editing process from English Wikipedia (EW) articles to paired Simple Wikipedia (SEW) articles. In contrast to prior work, SWiPE leverages the entire revision history when pairing pages in order to better identify simplification edits. We work with Wikipedia editors to annotate 5,000 EW-SEW document pairs, labeling more than 40,000 edits with proposed 19 categories. To scale our efforts, we propose several models to automatically label edits, achieving an F-1 score of up to 70.6, indicating that this is a tractable but challenging NLU task. Finally, we categorize the edits produced by several simplification models and find that SWiPE-trained models generate more complex edits while reducing unwanted edits.","Text simplification (TS) aims to make complex documents accessible to larger audiences by lowering the barrier of reading for children, non-native speakers, and novice readers in technical domains. TS has primarily been approached in a sentence-level sequence-to-sequence (seq2seq) manner, following the methodology of mature NLG tasks such as machine translation. Prior work framed at the sentence level has focused on simplification edits that occur within sentence units, such as lexical replacements and sentence splitting. Yet, many simplification operations, such as background elaborations or content reordering require document-level context. 
A major roadblock to advances in document-level simplification has been the lack of large-scale and high-quality datasets. The two most popular sources of data for the English language are either the news-based Newsela which is not available publicly or the combination of English Wikipedia (EW) and Simple English Wikipedia (SEW), which is large-scale but requires non-trivial processing to align Wikipedia articles with their simplified versions. The alignment task has predominantly been framed as finding pairs of semantically similar sentences within the latest revisions of EW and SEW pages. 
Our first contribution is to adapt the Wikipedia content alignment task to document-level granularity. We explore the entire revision history of Wikipedia pages and match individual revisions of SEW pages with best-aligned EW revisions, rather than rely on the most recent revisions which might yield factually misaligned pairs due to outdated information. By applying our alignment method to the entire revision history of SEW ‚Äì and processing two orders of magnitude more content ‚Äì we create the SWiPE dataset, a high-quality and large-scale document-level simplification dataset. SWiPE consists of 145,161 document pairs, which we processed into an alignment sequence composed of three operations: unchanged text, insertion, and deletion. Figure provides an illustrative alignment sequence of a SWiPE sample. 
Our second contribution is a comprehensive analysis of edits that occur in SWiPE. We propose a 19-category edit taxonomy based on prior work and expanded for document-level edits. The categories are organized into four coarse-grained classes representing simplification objectives: Lexical, Syntactic, Semantic, and Discourse-level edits. We collaborate with active SEW editors to annotate 5,000+ alignment sequences of SWiPE. The collected annotations of around 40,000 edits reveal that all four edit classes are prevalent in SWiPE (each occurs in at least 40%of annotated documents). Document-level context is required for at least 43%of edits, and diverse edits often co-occur within documents, as SEW editors combine editing strategies when producing SEW pages. 
Our third contribution is to propose models that can automatically identify edit categories and models that generate document-level simplified text. For the task of edit identification, our best model achieves a categorization F-1 score of 70.6, leaving room for future improvement. When analyzing simplification models based on the edits they produce, we find that SWiPE-trained models can produce more complex edits than prior work while generating fewer undesirable edits that potentially introduce factually incorrect content. We release the SWiPE data, the models, and experimental code publicly."
Rethinking Masked Language Modeling for Chinese Spelling Correction,2305.17721v1,./img_ACL_2023/2305.17721v1.pdf,Mistakes made by regularly fine-tuned BERT.,"In this paper, we study Chinese Spelling Correction (CSC) as a joint decision made by two separate models: a language model and an error model. Through empirical analysis, we find that fine-tuning BERT tends to over-fit the error model while under-fit the language model, resulting in poor generalization to out-of-distribution error patterns. Given that BERT is the backbone of most CSC models, this phenomenon has a significant negative impact. To address this issue, we are releasing a multi-domain benchmark LEMON, with higher quality and diversity than existing benchmarks, to allow a comprehensive assessment of the open domain generalization of CSC models. Then, we demonstrate that a very simple strategy ‚Äì randomly masking 20%non-error tokens from the input sequence during fine-tuning ‚Äì is sufficient for learning a much better language model without sacrificing the error model. This technique can be applied to any model architecture and achieves new state-of-the-art results on SIGHAN, ECSpell, and LEMON.","Chinese Spelling Correction (CSC) is a crucial task in natural language processing (NLP) behind many downstream applications, e. g, web search, named entity recognition, optical character recognition. It aims to detect and correct the potential spelling errors in a sentence. BERT and its enhanced variants have achieved state-of-the-art results in the current CSC community (name a few). 
From a high-level perspective, CSC requires a language model and an error model working collaboratively to make a decision. Suppose that the input sentence contains 
 ncharactersX = (x_1, . . . , x_n). The model predicts the corrected character at each positionY = (y_1, . . . , y_n). At each positioni, letx_-iindicate the characters at all other positions, then by Bayes Rule, we have: 
 P (y_i|X) ‚àùP (y_i|x_-i) _language model¬∑P (x_i|y_i, x_-i) _error model
 where the language model decides the distribution of the charactery_igiven the context, while the error model represents the distribution of the potential misspelled characterx_igiven the context and its correct form (see Appendix for the derivation). According to the BERT architecture, these two models are jointly trained and evaluated. However, their respective performances have not been throughout studied by previous work. 
 
 UTF8gkai In this paper, we make a key observation that BERT-based CSC models typically over-fit the error model, yet under-fit the language model, because the error model is much easier to memorize compared to the language model. As a result, the model generalizes very poor to unseen edit pairs (x_i, y_i) and fails to exploit the contextx_-i. We illustrate this fact in Figure. Here, the model has been exposed to edit pairs ""ÁîüÁ°¨‚ÜíÂ£∞Èü≥"" (correct stiff to sound) and ""ÁîüÈü≥‚ÜíÂ£∞Èü≥"" (correct raw to sound) during training. During testing, the model fails to detect an unseen edit pair ""Â£∞ÂΩ±‚ÜíÂ£∞Èü≥"" (correct shadow to sound) and meanwhile over-corrects ""ÁîüÁ°¨‚ÜíÂ£∞Èü≥"" (correct stiff to sound). This is due to the fact that the model naively memorizes the training edit pairs, failing to identify if they fit the broader context. We will present qualitative analysis of this phenomenon in later sections. 
 
 The consequence of a sub-optimal or under-fit language model is that the model struggles to generalize to new contexts and new domains. SIGHAN is the current most widely-used benchmark in CSC, but it is limited in two ways: (1) a narrow sentence corpus sourced exclusively from the Chinese essays by foreign speakers; (2) a low diversity of edit pairs (i.e.,370 edit pairs in its test set). As a result, it does not pose enough challenge to the model's generalization ability. To this end, we present LEMON, a new benchmark that is a large-scalemulti-domain dataset with natural spelling errors, which spans 7 domains and contains over 22,000 examples with 7,627 distinct edit pairs collected from real human daily writing. It provides a comprehensive evaluation of CSC models in real-world scenarios. 
 
 Based on LEMON and other public benchmarks, we demonstrate that a very simple method can effectively enhance language modeling without causing adverse effect to error modeling, thus significantly improves CSC model performances. The method is to randomly mask 20%of the non-error tokens from the input sentence during fine-tuning (this is different from masking 15%tokens during pre-training in BERT). If
x_iis masked, it forces the model to predicty_igivenx_-iwithout any clue aboutx_i, equivalent to trainingP (y_i|x_-i). This masked-fine-tuning (Masked-FT) technique is unlike other data augmentation methods based on homophone substitution, random substitution or confusion sets, in that it does not impose any assumption about human errors. As a result, it enables learning a completely unbiased error model from real human data. This property let Masked-FT achieve new state-of-the-art across CSC benchmarks. 
We also show that Masked-FT is effective in domain transfer. Suppose that there is an annotated parallel corpus for a certain domain, and we want to transfer the model of such a domain to a new domain where only monolingual (i.e.,unannotated) corpus is available. We propose to train the model with the parallel data along with a masked language modeling (MLM) loss from the monolingual corpus. The idea behind is to transfer the language model to the new domain while preserving the error model that is learned through the parallel data. Empirical results demonstrate that this way of using monolingual data produces a better model than data synthesis methods based on confusion sets. 
Our contributions are summarized as follows. (1) We perform empirical analysis showing that BERT-based CSC models learn a sub-optimal language model, resulting in a bad performance on out-of-distribution edit pairs. (2) We release a large-scale and multi-domain benchmark for CSC, which is more challenging than existing ones. (3) We demonstrate that a simple masked-fine-tuning strategy significantly enhance language modeling without hurting error modeling, leading to new state-of-the-art results across benchmarks."
f-Divergence Minimization for Sequence-Level Knowledge Distillation,2307.15190v1,./img_ACL_2023/2307.15190v1.pdf,Typical phenomena of (a) KL distillation and (b) reverse KL distillation. Gray curve: teacher distribution. Green curve: student distribution.,"Knowledge distillation (KD) is the process of transferring knowledge from a large model to a small one. It has gained increasing attention in the natural language processing community, driven by the demands of compressing ever-growing language models. In this work, we propose an f-distill framework, which formulates sequence-level knowledge distillation as minimizing a generalized","In ACL '23, pages 10817‚Äì10834, with additional notes from the conference. fancy 
Increasingly large language models have continued to achieve state-of-the-art performance across various natural language generation tasks, such as data-to-text generation, summarization, and dialogue generation. However, super-large language models are inaccessible to most users and researchers due to their prohibitively large model size, emphasizing the importance of high-performing, parameter-efficient small neural models. 
A widely used approach to training small models is knowledge distillation, where the small model (known as the student) learns the knowledge from a much larger model (known as the teacher). KD has shown great success in helping smaller models achieve competitive performance across a wide range of applications. 
Existing KD approaches can be categorized into two main branches: representation matching and distribution matching. The former aims to imitate the teacher' s real-valued intermediate-layer representations, say, with mean squared error. Our work focuses on the latter, distribution matching, where the student model learns the teacher 's predictive distribution. minimize the cross-entropy loss against the teacher-predicted soft labels, which is equivalent to minimizing the Kullback‚ÄìLeibler (KL) divergence between the teacher and student. propose SeqKD, arguing that KL divergence should be minimized at the sequence level for language models. However, such an approach tends to learn an overly smooth student distribution to cover the entire support of the teacher distribution due to the asymmetric nature of the KL divergence. This is often known as the mode-averaging problem (Figure a). 
 propose ENGINE, a non-autoregressive translation model that minimizes the energy function defined by the teacher' s output distribution. It can be shown that their objective is related to minimizing the reverse KL between the teacher and student (see Section). This, on the other hand, results in the mode-collapsing problem, where the student model is overly concentrated on certain high-probability regions of the teacher distribution (Figure b). 
In this paper, we address knowledge distillation for text generation tasks, and propose f-distill, a unified framework that formulates sequence-level knowledge distillation as minimizing 
 f-divergence functions. Existing SeqKD and ENGINE methods are approximations of KL and reverse KL distillations under the f-distill framework. Further, our formulation naturally leads to Jensen‚ÄìShannon (JS) divergence and total variation distance (TVD) distillations, where the divergence measures are symmetric in teacher and student distributions. This forces the student to learn the teacher's distribution better, alleviating mode averaging and collapsing problems. 
 
 We further develop efficient algorithms for our f-distill approach. First, we show that sequence-level f-divergence can be decomposed step by step either exactly or as an upper bound. Second, we propose to sample from the teacher model in an offline manner, mitigating the additional training cost of symmetric divergence measures (namely, JS and TVD). 
 
 We evaluated our approach on four datasets: DART for data-to-text generation, XSum for summarization, WMT16 EN-RO for machine translation, and Commonsense Dialogue. Experiments show that our proposed f-distill variants consistently outperform existing distribution-matching KD methods, allowing f-distill to achieve an add-on performance improvement when combined with representation-matching KD methods. Further, results show that our symmetric distilling losses outperform asymmetric ones, confirming that extreme mode averaging or collapsing is not ideal. 
 
 To sum up, our contributions are three-fold: 
 * We propose f-distill, a novel distilling framework that generalizes KL distillation and balances mode averaging and collapsing; 
 * We derive step-wise decomposition and propose an offline sampling method to efficiently compute sequence-levelf-divergences; and 
 * We provide detailed experimental analysis across four text generation datasets to show the effectiveness of our approach."
Supervised Adversarial Contrastive Learning for Emotion Recognition in Conversations,2306.01505v2,./img_ACL_2023/2306.01505v2.pdf,"Comparison of different training objectives on a two-class case. CE and CE+SCL mean cross-entropy and soft supervised contrastive learning, respectively.","Extracting generalized and robust representations is a major challenge in emotion recognition in conversations (ERC). To address this, we propose a supervised adversarial contrastive learning (SACL) framework for learning class-spread structured representations in a supervised manner. SACL applies contrast-aware adversarial training to generate worst-case samples and uses joint class-spread contrastive learning to extract structured representations. It can effectively utilize label-level feature consistency and retain fine-grained intra-class features. To avoid the negative impact of adversarial perturbations on context-dependent data, we design a contextual adversarial training (CAT) strategy to learn more diverse features from context and enhance the model's context robustness. Under the framework with CAT, we develop a sequence-based SACL-LSTM to learn label-consistent and context-robust features for ERC. Experiments on three datasets show that SACL-LSTM achieves state-of-the-art performance on ERC. Extended experiments prove the effectiveness of SACL and CAT.","Emotion recognition in conversations (ERC) aims to detect emotions expressed by speakers during a conversation. The task is a crucial topic for developing empathetic machines. Existing works mainly focus on context modeling and emotion representation learning to recognize emotions. However, these methods have limitations in discovering the intrinsic structure of data relevant to emotion labels, and struggle to extract generalized and robust representations, resulting in mediocre recognition performance. 
In the field of representation learning, label-based contrastive learning techniques are used to learn a generalized representation by capturing similarities between examples within a class and contrasting them with examples from other classes. Since similar emotions often have similar context and overlapping feature spaces, these techniques that directly compress the feature space of each class are likely to hurt the fine-grained features of each emotion, thus limiting the ability of generalization. 
To address these, we propose a supervised adversarial contrastive learning (SACL) framework to learn class-spread structured representations in a supervised manner. SACL applies contrast-aware adversarial training to generate worst-case samples and uses a joint class-spread contrastive learning objective on both original and adversarial samples. It can effectively utilize label-level feature consistency and retain fine-grained intra-class features. 
Specifically, we adopt soft SCL on original samples to obtain contrast-aware adversarial perturbations. Then, we put perturbations on the hidden layers to generate hard positive examples with a min-max training recipe. These generated samples can spread out the representation space for each class and confuse robust-less networks. After that, we utilize a new soft SCL on obtained adversarial samples to maximize the consistency of class-spread representations with the same label. Under the joint objective on both original and adversarial samples, the network can effectively learn label-consistent features and achieve better generalization. 
In context-dependent dialogue scenarios, directly generating adversarial samples interferes with the correlation between utterances, which is detrimental to context understanding. To avoid this, we design a contextual adversarial training (CAT) strategy to adaptively generate context-level worst-case samples and extract more diverse features from context. This strategy applies adversarial perturbations to the context-aware network structure in a multi-channel way, instead of directly putting perturbations on context-free layers in a single-channel way. After introducing CAT, SACL can further learn more diverse features and smooth representation spaces from context-dependent inputs, as well as enhance the model 's context robustness. 
Under SACL framework, we design a sequence-based method SACL-LSTM to recognize emotion in the conversation. It consists of a dual long short-term memory (Dual-LSTM) module and an emotion classifier. Dual-LSTM is a modified version of the contextual perception module, which can effectively capture contextual features from a dialogue. With the guidance of SACL, the model can learn label-consistent and context-robust emotional features for the ERC task. 
We conduct experiments on three public benchmark datasets. Results consistently demonstrate that our SACL-LSTM significantly outperforms other state-of-the-art methods on the ERC task, showing the effectiveness and superiority of our method. Moreover, extensive experiments prove that our SACL framework can capture better structured and robust representations for classification. 
The main contributions are as follows: 1) We propose a supervised adversarial contrastive learning (SACL) framework to extract class-spread structured representations for classification. It can effectively utilize label-level feature consistency and retain fine-grained intra-class features. 2) We design a contextual adversarial training (CAT) strategy to learn more diverse features from context-dependent inputs and enhancing the model' s context robustness. 3) We develop a sequence-based method SACL-LSTM under the framework to learn label-consistent and context-robust emotional features for ERC. 4) Experiments on three benchmark datasets show that SACL-LSTM significantly outperforms other state-of-the-art methods, and prove the effectiveness of the SACL framework."
Semantic Structure Enhanced Event Causality Identification,2305.12792v1,./img_ACL_2023/2305.12792v1.pdf,"An example of the ECI task, as well as the semantic graph and semantic structures corresponding to the unstructured text. The orange nodes denote events.","Event Causality Identification (ECI) aims to identify causal relations between events in unstructured texts. This is a very challenging task, because causal relations are usually expressed by implicit associations between events. Existing methods usually capture such associations by directly modeling the texts with pre-trained language models, which underestimate two kinds of semantic structures vital to the ECI task, namely, event-centric structure and event-associated structure. The former includes important semantic elements related to the events to describe them more precisely, while the latter contains semantic paths between two events to provide possible supports for ECI. In this paper, we study the implicit associations between events by modeling the above explicit semantic structures, and propose a Semantic Structure Integration model (SemSIn). It utilizes a GNN-based event aggregator to integrate the event-centric structure information, and employs an LSTM-based path aggregator to capture the event-associated structure information between two events. Experimental results on three widely used datasets show that SemSIn achieves significant improvements over baseline methods.",
Combo of Thinking and Observing for Outside-Knowledge VQA,2305.06407v1,./img_ACL_2023/2305.06407v1.pdf,"Comparison with previous paradigms. Orange lines indicate processes involving cross-modality space. (a) The conventional VQA paradigm fuses image and question text into the cross-modality space, and then predicts answers in a close-set classification manner. (b) Language-centric paradigm applies captioning and tagging tools to describe the visual context, and abandons the visual features to convert the VQA task into an open-ended generative QA task. (c) The proposed paradigm intends to constrain the cross-modality space into the same space as natural-language space so that models can directly decode both text and multimodal embeddings.","Outside-knowledge visual question answering is a challenging task that requires both the acquisition and the use of open-ended real-world knowledge. Some existing solutions draw external knowledge into the cross-modality space which overlooks the much vaster textual knowledge in natural-language space, while others transform the image into a text that further fuses with the textual knowledge into the natural-language space and completely abandons the use of visual features. In this paper, we are inspired to constrain the cross-modality space into the same space of natural-language space which makes the visual features preserved directly, and the model still benefits from the vast knowledge in natural-language space. To this end, we propose a novel framework consisting of a multimodal encoder, a textual encoder and an answer decoder. Such structure allows us to introduce more types of knowledge including explicit and implicit multimodal and textual knowledge. Extensive experiments validate the superiority of the proposed method which outperforms the state-of-the-art by 6.17%accuracy. We also conduct comprehensive ablations of each component, and systematically study the roles of varying types of knowledge. Codes and knowledge data can be found at <https: //github. com/PhoebusSi/Thinking-while-Observing>.","Conventional visual question answering (VQA) tasks require models to answer questions based on image content. Such tasks have been thoroughly studied on conventional VQA datasets VQAv2. However, real-world questions often rely on a certain amount of knowledge beyond images. Therefore, Knowledge Base Question Answering (KB-VQA) tasks always require models to answer questions by referring to the corresponding knowledge facts in a specific pre-defined knowledge base. Yet any pre-defined knowledge base is far from covering real-world knowledge. Recently, the outside-knowledge visual question answering (OK-VQA) task has been proposed and provides the most open VQA setting. That is, any knowledge resource can be used to answer its challenging and diverse questions. 
Most previous work on OK-VQA follows conventional VQA paradigm (as shown in Figure (a) ) based on visual-language pre-trained (VLP) models, and injects knowledge into the same cross-modality space afterward. However, knowledge in cross-modality space is much less than that in natural-language space. This paradigm excels at visual understanding, but refers to little knowledge, like a human who focuses on observing but does not think enough. 
To take the advantage of the vast knowledge in natural-language space, state-of-the-art methods on OK-VQA follow language-centric paradigm (as shown in Figure (b) ) based on pre-trained language models (PLMs). However, although more knowledge can be introduced, the paradigm is counter-intuitive because many visual details are lost when converting an image into text. Therefore, it is like a human who starts thinking after brief observing. 
For a human, a feasible solution to OK-VQA is combo Thinking while Observing. To this end, we propose TwO, which is a framework consisting of a multimodal encoder, a textual encoder and an answer decoder. As shown in Figure (c), the multimodal encoder directly encodes the visual features and acts as the role of observer, while the textual encoder encodes a range of knowledge resources and acts as the role of thinker. Finally, the answer decoder decodes the latent embeddings from both encoders to generate the final answer. In addition, a pre-training stage is added to help constrain the output of both encoders to the same latent space. 
Previous methods have thoroughly studied explicit textual knowledge such as Wikipedia, as well as implicit textual knowledge in GPT-3. However, the discussion of multimodal knowledge, which further utilizes visual features, is still in its infancy in OK-VQA. In this paper, we accumulate explicit multimodal knowledge during pre-training on VQAv2. Besides, inspired by prompting GPT-3 for implicit textual knowledge, we use prompt to bring in implicit multimodal knowledge stored in the unifying VLP model OFA. Moreover, we refine a taxonomy of existing methods by knowledge (refer to Figure) where our method is the first to bring in all types of knowledge. 
To summarize, our contributions are as follows: (1) We propose a simple and effective paradigm that combines the advantages of both conventional VQA and language-centric paradigms. (2) Our method can deal with more comprehensive types of knowledge, and is the first to bring in implicit multimodal knowledge through a prompt-learning fashion. In addition, we empirically analyze the roles of different types of knowledge. (3) Experimental results show the effectiveness of our method, which establishes a new SoTA accuracy on OK-VQA with a 6.17%gain."
Your spouse needs professional help: Determining the Contextual Appropriateness of Messages through Modeling Social Relationships,2307.02763v1,./img_ACL_2023/2307.02763v1.pdf,The same message can be appropriate or not depending on the social context in which it is said.,"Understanding interpersonal communication requires, in part, understanding the social context and norms in which a message is said. However, current methods for identifying offensive content in such communication largely operate independent of context, with only a few approaches considering community norms or prior conversation as context. Here, we introduce a new approach to identifying inappropriate communication by explicitly modeling the social relationship between the individuals. We introduce a new dataset of contextually-situated judgments of appropriateness and show that large language models can readily incorporate relationship information to accurately identify appropriateness in a given context. Using data from online conversations and movie dialogues, we provide insight into how the relationships themselves function as implicit norms and quantify the degree to which context-sensitivity is needed in different conversation settings. Further, we also demonstrate that contextual-appropriateness judgments are predictive of other social factors expressed in language such as condescension and politeness.","These authors contributed equally to this work ‚ãÜThese authors contributed equally to this work footnote 
Interpersonal communication relies on shared expectations of the norms of communication. Some of these norms are widely shared across social contexts, e.g., racial epithets are taboo, enabling NLP models to readily identify certain forms of offensive language. Yet, not all norms are widely shared; the same message said in two different social contexts may have different levels of acceptability (Figure). While NLP has recognized the role of social context as important, few works have directly incorporated this context into modeling whether messages violate social norms. Here, we explicitly model relationships as the social context in which a message is said in order to assess whether the message is appropriate. 
NLP models have grown more sophisticated in modeling the social norms needed to identify offensive content. Prior work has shown the benefits of modeling context, such as the demographics of annotators and readers and the online community in which a message is said. However, these works overlook normative expectations within people 's relationships. 
In this paper, we introduce a new dataset of over 12,236 instances labeled for whether the message was appropriate in a given relationship context. Using this data, we show that computation models can accurately identify the contextual appropriateness of a message, with the best-performing model attaining a 0.70 Binary F1. Analyzing the judgments of this classifier reveals the structure of the shared norms between relationships. Through examining a large corpus of relationship-labeled conversations, we find that roughly 19%of appropriate messages could be perceived as inappropriate in another context, highlighting the need for models that explicitly incorporate relationships. Finally, we show that our model' s relationship-appropriate judgments provide useful features for identifying subtly offensive language, such as condescension."
How Do In-Context Examples Affect Compositional Generalization?,2305.04835v3,./img_ACL_2023/2305.04835v3.pdf,Test compositional generalization under in-context learning. This case belongs to Phrase Recombination in CoFe. The phrases modify the objects in examples but are recombined with subject in test input.,"Compositional generalization‚Äîunderstanding unseen combinations of seen primitives‚Äîis an essential reasoning capability in human intelligence. The AI community mainly studies this capability by fine-tuning neural networks on lots of training samples, while it is still unclear whether and how in-context learning‚Äîthe prevailing few-shot paradigm based on large language models‚Äîexhibits compositional generalization. In this paper, we present CoFe, a test suite to investigate in-context compositional generalization. We find that the compositional generalization performance can be easily affected by the selection of in-context examples, thus raising the research question what the key factors are to make good in-context examples for compositional generalization. We study three potential factors: similarity, diversity and complexity. Our systematic experiments indicate that in-context examples should be structurally similar to the test case, diverse from each other, and individually simple. Furthermore, two strong limitations are observed: in-context compositional generalization on fictional words is much weaker than that on commonly used ones; it is still critical that the in-context examples should cover required linguistic structures, even though the backbone model has been pre-trained on large corpus. We hope our analysis would facilitate the understanding and utilization of in-context learning paradigm.","Compositional generalization is an essential capability of human intelligence. It means to understanding and producing novel expressions by recombining known components in language. Taking examples in Figure, after learning the combination ""baby in a room"", human intelligence can easily generalize to ""Jackson in a room"". On exploring this human-like capability in deep learning models, several benchmarks such as SCAN, CFQ and COGS have been proposed based on semantic parsing tasks. In these benchmarks, the training set cover all the primitives while lacking certain combinations, and the test set focuses on these missing combinations. By fine-tuning generic neural models on these benchmarks, much work reported that these models exhibit poor compositional generalization. 
Recently, in-context learning with large language models exhibits impressive performance on various tasks. By conditioning on few-shot in-context examples, the pre-trained language model, with extremely large model size and pre-trained corpus, can perform downstream tasks without any update on pre-trained parameters. 
Behind the impressive performance of in-context learning, we are curious whether this prevailing paradigm can take a step towards compositional generalization. To investigate this, we first take an initial exploration: for each test case in COGS, we select in-context examples from its training set and ensure that all primitives in each test case are covered by the equipped in-context examples. Our initial exploration suggests that compositional generalization can be easily affected by in-context examples: with only covering primitives, davinci 175B lags behind fine-tuned GPT2-Large with 24.2%accuracy (similar to the observation in); with also covering some local structures (inspired by), davinci outperforms fine-tuned GPT2-Large with 3.9%accuracy. Based on these initial observations, we raise and investigate the question: How do in-context examples affect compositional generalization? 
We construct the test suite CoFe (based on COGS) to facilitate our systematic investigation. Taking the coverage of primitives as a basic principle in CoFe, we further define and inject three factors in selecting in-context examples: similarity, diversity, and complexity. Similarity is considered as the matching of hidden structures behind concrete expressions. Diversity reflects whether the context presents repeated patterns or not. Complexity portrays the amount of information contained in each example. By controlling these factors in constructing CoFe, we can systematically investigate how would in-context examples influence the performance on compositional generalization. 
Our experiments demonstrate that all three factors matter for in-context compositional generalization. We leverage six large language models in GPT series: davinci, code-cushman-001, code-cushman-002, text-davinci-002, text-chat-davinci-002, and code-davinci-002. The observations are consistent across models: to better perform compositional generalization, all backbone models prefer in-context examples with higher structural similarity to the test case, higher diversity among different examples, and lower complexity in each individual example. Furthermore, beyond the influence from these factors, in-context compositional generalization still faces two challenges. One is that in-context learning has difficulty recombining fictional words (e.g., random tokens) rather than commonly used ones. The other one is that in-context examples are still required to cover the linguistic structures in NL expressions, even though the backbone model has been pre-trained on large corpus. 
Our contributions are three-fold: 1) to answer the research question posed, we investigate three factors in selecting in-context examples and draw consistent conclusions across models; 2) we construct CoFe to conduct our systematic investigation, and will release it to facilitate further exploration of in-context compositional generalization; 3) we also point out two remaining challenges that in-context learning still struggles to handle. We hope our analysis would provide insights on how to select proper in-context examples, and to shed light on the future research of in-context compositional generalization. CoFe is publicly available at <https: //github. com/microsoft/ContextualSP/tree/master/cofe>."
How to Plant Trees in Language Models: Data and Architectural Effects on the Emergence of Syntactic Inductive Biases,2305.19905v1,./img_ACL_2023/2305.19905v1.png,"The syntactic transformations paradigm. A pre-trained model is fine-tuned on examples that are consistent with both syntactic (hierarchical) and positional/word order (linear) explanations. Then, it is evaluated on examples where only a model with a syntactic inductive bias will produce the correct output. We investigate which components of pre-training induce hierarchical inductive biases. Adapted from and.","Accurate syntactic representations are essential for robust generalization in natural language. Recent work has found that pre-training can teach language models to rely on hierarchical syntactic features‚Äîas opposed to incorrect linear features‚Äîwhen performing tasks after fine-tuning. We test what aspects of pre-training are important for endowing encoder-decoder Transformers with an inductive bias that favors hierarchical syntactic generalizations. We focus on architectural features (depth, width, and number of parameters), as well as the genre and size of the pre-training corpus, diagnosing inductive biases using two syntactic transformation tasks: question formation and passivization, both in English. We find that the number of parameters alone does not explain hierarchical generalization: model depth plays greater role than model width. We also find that pre-training on simpler language, such as child-directed speech, induces a hierarchical bias using an order-of-magnitude less data than pre-training on more typical datasets based on web text or Wikipedia; this suggests that in cognitively plausible language acquisition settings, neural language models may be more data-efficient than previously thought.","Accurate syntactic representations are necessary for robust generalization to new natural language inputs and for the generation of correct outputs. Consider the problem of identifying the subject of ""said"" in the following sentence: . Can you repeat what the senator next to the cats said? 
Typical language models (LMs), which receive linear sequences of words as input, could conceivably rely on a linear or positional feature that usually, but does not always, identifies the correct subject of a verb. An LM could learn, for example, that the first noun in the sentence is always the subject. This heuristic works for many simple sentences, but fails in Ex. : here, the first noun is ""you"", and so this heuristic would lead the LM to incorrectly interpret the sentence as meaning ""Can you repeat what you said? "" The LM could also learn that the subject of the verb is the noun closest to the verb in the linear order of the sentence, in which case it would interpret Ex. as ""Can you repeat what the cats said? "" By contrast, an LM that represents the sentence as hierarchically structured will correctly identify senator as the subject of the embedded clause that contains the verb said. This example demonstrates that a preference for syntactic features over linear features is required for robust linguistic generalization. 
The success of large-scale pre-training across fine-tuning tasks suggests that exposure to natural language may teach models to rely on appropriate syntactic features instead of heuristics (even though models still often rely on heuristics; ). This hypothesis is supported by the finding that, given minimal pairs of grammatical and ungrammatical sentences, the probability distribution over sentences defined by LMs often favors the grammatical sentence. A related line of work has shown that, through pre-training, LMs can under some circumstances acquire syntactic inductive biases which are then applied to fine-tuning tasks, whereas models which have not been pre-trained do not have such inductive biases (). 
When does pre-training endow LMs with a syntactic inductive bias? In this study, we address two specific sub-questions: (1) Which architectural features make a syntactic inductive bias more likely to emerge in a Transformer LM? (2) How is the inductive bias affected by the genre and size of the pre-training corpus? We investigate these questions by evaluating a range of Transformer encoder-decoder models based on T5. We evaluate both existing publicly available models and models that we pre-train ourselves; we explore a variety of model widths (embedding and hidden dimension, feed-forward layer size) and depths (number of layers), and pre-train on corpora of varying genres and sizes. We then evaluate models' inductive biases by observing their out-of-distribution generalization when fine-tuned on syntactic transformations tasks (). We find that depth matters more than width for the acquisition of hierarchical biases (), and that pre-training on simpler language induces hierarchical biases using far less data (and). This last finding suggests that in language acquisition settings in which the training corpus more closely resembles the language that children are exposed to, Transformers may be more sample-efficient than previously thought. 
Our code is available on GitHub."
An Inclusive Notion of Text,2211.05604v2,./img_ACL_2023/2211.05604v2.png,"The same textual document (a) can be seen in many ways (b-d) depending on the assumed notion of text: while a syntax researcher might focus on written language (b), a summarization system can use document structure (c), and multimodal applications might use non-linguistic elements like tables (d). Systematically capturing the differences between the assumed notions of text (top) requires a taxonomy of inclusive approaches to text. Such taxonomy is currently lacking.","Natural language processing (NLP) researchers develop models of grammar, meaning and communication based on written text. Due to task and data differences, what is considered text can vary substantially across studies. A conceptual framework for systematically capturing these differences is lacking. We argue that clarity on the notion of text is crucial for reproducible and generalizable NLP. Towards that goal, we propose common terminology to discuss the production and transformation of textual data, and introduce a two-tier taxonomy of linguistic and non-linguistic elements that are available in textual sources and can be used in NLP modeling. We apply this taxonomy to survey existing work that extends the notion of text beyond the conservative language-centered view. We outline key desiderata and challenges of the emerging inclusive approach to text in NLP, and suggest community-level reporting as a crucial next step to consolidate the discussion.","Text is the core object of analysis in NLP. Annotated textual corpora exemplify NLP tasks and serve for training and evaluation of task-specific models, and massive unlabeled collections of texts enable general language model pre-training. To a large extent, natural language processing today is synonymous to text processing. 
But what belongs to text? More broadly, what information should be captured in NLP corpora and be available to the models during training and inference? Despite its central role, the notion of text in NLP is vague: while earlier work mostly focused on grammatical phenomena and implicitly limited text to written language, the applied NLP of the past years increasingly takes an inclusive approach to text by introducing non-linguistic elements into the analysis. Extensions vary from incorporating emojis to exploiting document structure and cross-document relationships, and apply to all major components of the modern NLP infrastructure, including unlabeled text collections, language models and annotated corpora. The assumption that text in NLP solely refers to written language no longer holds. Yet, as Figure illustrates, a systematic approach to capturing the differences between the assumed notions of text is lacking. 
This is problematic for several reasons. From the reproducibility perspective, machine learning assumes similarity between the source and the target distribution ‚Äì yet lack of consensus on the notion of text might result in undocumented change of the input representation and degraded performance, even if other common variables like domain, language and task remain unchanged. From the modeling perspective, the notion of text has major influence on task and model design, as it both determines the tasks NLP aims to tackle, and implies what information should be used to perform those tasks. The final argument for studying the notion of text in NLP is conceptual: the capabilities of strong pre-trained Transformer models and general-purpose NLP frameworks have led to an explosive growth in NLP beyond traditional, core tasks. The exposure to rich source document types like scientific articles and slides and the growing influence of multimodal processing motivate the use of additional signals beyond written language in NLP. This leads to a general question on the scope of the field: if written language is no longer the sole object of study, what is, and how can it be formally delineated? 
Any empirical discipline relies on operationalization, which casts observed phenomena into abstractions, allowing us to formulate claims and perform measurements to evaluate these claims. For example, operationalizing sentiment (phenomenon) as a binary variable (abstraction) allows us to a build a claim (""this review is positive"") to be evaluated against the ground truth (review rating), and dictates the downstream NLP task design (binary classification). While widely used, this operationalization is limited: alternative notions of sentiment allow making more nuanced claims, fine-grained measurements and precise models. 
The same logic applies to text, which affords a wide range of operationalizations, from a character stream to a rich multimodal graph. Yet, the typology for describing text use in NLP is lacking. While concurrent proposals address other key properties of NLP models and corpora like domain, language, demographics, modality and licensing ‚Äì we lack common terminology and reporting schemata for documenting and formally discussing the assumed notion of text. The growth of the field and the high cost of the retrospective documentation underline the urgent need for a lightweight, semi-structured reporting mechanism to account for text use. To address this need, we contribute the following: 
 
 * A common terminology of text use in NLP (Section); 
 * A taxonomy of text extensions beyond the language-focused approach to text, based on commonly used sources of NLP data and the current state of the art; 
 * Discussion of the challenges brought by the inclusive approach to text (Section); 
 * A new lightweight semi-structured schema for reporting text use in NLP (Section). 
The notion of text is central to NLP, and we expect our discussion to be broadly relevant, with particular merit for the documentation policy, NLP applications, and basic NLP research. The semi-structured reporting as proposed here is a crucial step towards developing formalized documentation schemata for describing text use and general formats to encode non-linguistic information in texts. We encourage the community to adopt our reporting schema, and to contribute to the discussion by suggesting new phenomena to be covered by the taxonomy of inclusive approaches to text."
MIR-GAN: Refining Frame-Level Modality-Invariant Representations with Adversarial Network for Audio-Visual Speech Recognition,2306.10567v1,./img_ACL_2023/2306.10567v1.pdf,Multimodal learning of frame-level modality-invariant and -specific representations.,"Audio-visual speech recognition (AVSR) attracts a surge of research interest recently by leveraging multimodal signals to understand human speech. Mainstream approaches addressing this task have developed sophisticated architectures and techniques for multi-modality fusion and representation learning. However, the natural heterogeneity of different modalities causes distribution gap between their representations, making it challenging to fuse them. In this paper, we aim to learn the shared representations across modalities to bridge their gap. Different from existing similar methods on other multimodal tasks like sentiment analysis, we focus on the temporal contextual dependencies considering the sequence-to-sequence task setting of AVSR. In particular, we propose an adversarial network to refine frame-level modality-invariant representations (MIR-GAN), which captures the commonality across modalities to ease the subsequent multimodal fusion process. Extensive experiments on public benchmarks LRS3 and LRS2 show that our approach outperforms the state-of-the-arts.","Human perception of the world intrinsically comprises multiple modalities, including vision, audio, text, etc. . Audio-visual speech recognition (AVSR) leverages both audio and visual modalities to understand human speech, improving the noise-robustness of audio-only speech recognition with noise-invariant lip movement information. Thanks to recent advances of deep learning techniques, AVSR research has gained a remarkable progress. 
Currently, the mainstream AVSR approaches are centered around developing sophisticated architectures and techniques for multi-modality fusion, including simple feature concatenation, recurrent neural network and cross-modal attention. Despite the advances, these approaches are often challenged by the representation gap persisting between naturally heterogeneous modalities. 
Recently in some other multimodal tasks like sentiment analysis and cross-modal retrieval, there have been research works proposing to learn two distinct representations to benefit multimodal learning. The first representation is modality-invariant, where multiple modalities of a same utterance are mapped to a shared space, indicating the homogeneous semantic meaning from the speaker. In addition, they also learn modality-specific representations that are private to each modality. Given an utterance, each modality contains some unique features with respect to speaker-sensitive information. Combing these two representations provides a holistic view of multimodal data for downstream tasks. However, these methods focus on utterance-level representations that could be easily mapped to either shared or individual modality space using similarity cost functions, which does not apply to AVSR task that requires sequence-to-sequence mapping with temporal contextual dependencies. 
Motivated by above observations, we propose an adversarial network to refine frame-level modality-invariant representations (MIR-GAN) for capturing the commonality across modalities, which bridges their heterogeneous gap to ease the subsequent multimodal fusion. In particular, we first design a MIR generator to learn modality-invariant representations over the shared audio-visual modality space. Meanwhile, a modality discriminator is proposed to strengthen its modality agnosticism via adversarial learning. Moreover, to further enrich its contextual semantic information, we propose a mutual information maximization strategy to align the refined representations to both audio and visual modality sequences. Finally, both modality-invariant and -specific representations are fused for downstream speech recognition. Empirical results demonstrate the effectiveness of our approach. In summary, our main contributions are: 
 
 * We present MIR-GAN, an AVSR approach to refine frame-level modality-invariant representations, which captures the commonality across modalities and thus bridges their heterogeneous gap to ease multimodal fusion. 
 * We first learn modality-invariant representations with a MIR generator, followed by another modality discriminator to strengthen its modality agnosticism via adversarial learning. Furthermore, we propose a mutual information maximization strategy to enrich its contextual semantic information. Finally, both modality-invariant and -specific representations are fused for downstream recognition. 
 * Our proposed MIR-GAN outperforms the state-of-the-arts on LRS3 and LRS2 benchmarks. Extensive experiments also show its superiority on ASR and VSR tasks."
ESCOXLM-R: Multilingual Taxonomy-driven Pre-training for the Job Market Domain,2305.12092v1,./img_ACL_2023/2305.12092v1.pdf,"ESCO Pre-training Objective: From left to right, the figure illustrates the hierarchical structure of the ESCO taxonomy, which consists of occupations, skills, and aliases (OSA). Each OSA includes a definition. For the purposes of this study, we consider aliases of occupations to have the same definition as the occupation itself. In the middle of the figure, we show our pre-training setup. Pre-training instances are uniformly sampled in three ways: randomly, linked, or grouped (this is defined in). The selected instances (can be in different languages) are then fed to the language model, along with its description. We have two pre-training objectives: the regular MLM objective, and a new ESCO relation prediction objective, in which the goal is to predict which group the sampled instances belong to (Random, Linked, or Grouped).","=-1 The increasing number of benchmarks for Natural Language Processing (NLP) tasks in the computational job market domain highlights the demand for methods that can handle job-related tasks such as skill extraction, skill classification, job title classification, and de-identification. While some approaches have been developed that are specific to the job market domain, there is a lack of generalized, multilingual models and benchmarks for these tasks. In this study, we introduce a language model called ESCOXLM-R, based on XLM-Rlarge, which uses domain-adaptive pre-training on the European Skills, Competences, Qualifications and Occupations (ESCO) taxonomy, covering 27 languages. The pre-training objectives for ESCOXLM-R include dynamic masked language modeling and a novel additional objective for inducing multilingual taxonomical ESCO relations. We comprehensively evaluate the performance of ESCOXLM-R on 6 sequence labeling and 3 classification tasks in 4 languages and find that it achieves state-of-the-art results on 6 out of 9 datasets. Our analysis reveals that ESCOXLM-R performs better on short spans and outperforms XLM-Rlarge on entity-level and surface-level span-F1, likely due to ESCO containing short skill and occupation titles, and encoding information on the entity-level.","=-1 The dynamic nature of labor markets, driven by technological changes, migration, and digitization, has resulted in a significant amount of job advertisement data (JAD) being made available on various platforms to attract qualified candidates. This has led to an increase in tasks related to JAD, including skill extraction, skill classification, job title classification, de-identification of entities in job postings, and multilingual skill entity linking. 
=-1 While some previous studies have focused on JAD in non-English languages, their baselines have typically relied on language-specific models, either using domain-adaptive pre-training (DAPT; ) or off-the-shelf models. The lack of comprehensive, open-source JAD data in various languages makes it difficult to fully pre-train a language model (LM) using such data. In this work, we seek external resources that can help improve the multilingual performance on the JAD domain. We use the ESCO taxonomy, which is a standardized system for describing and categorizing the skills, competences, qualifications, and occupations of workers in the European Union. The ESCO taxonomy, which has been curated by humans, covers over 13,000 skills and 3,000 occupations in 27 languages. Therefore, we seek to answer: To what extent can we leverage the ESCO taxonomy to pre-train a domain-specific and language-agnostic model for the computational job market domain? 
=-1 In this work, we release the first multilingual JAD-related model named ESCOXLM-R, a language model based on XLM-Rlarge that incorporates data from the ESCO taxonomy through the use of two pre-training objectives (): Masked Language Modeling (MLM) and a novel ESCO relation prediction task (). We evaluate ESCOXLM-R on 9 JAD-related datasets in 4 different languages covering 2 NLP tasks (). Our results show that ESCOXLM-R outperforms previous state-of-the-art (SOTA) on 6 out of 9 datasets (). In addition, our fine-grained analysis reveals that ESCOXLM-R performs better on short spans compared to XLM-Rlarge, and consistently outperforms XLM-Rlarge on entity-level and surface-level span-F1 (). 
 
Contributions =-1 In this work, we present and release the following: 
 0em 
 * ESCOXLM-R, an XLM-Rlarge-based model, which utilizes domain-adaptive pre-training on the 27 languages from ESCO. 
 * The largest JAD evaluation study to date on 3 job-related tasks, comprising 9 datasets in 4 languages and 4 models. 
 * A fine-grained analysis of ESCOXLM-R's performance on different span lengths, and emerging entities (i.e., recognition of entities in the long tail)."
Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge,2305.02459v2,./img_ACL_2023/2305.02459v2.pdf,"Demonstration of the active learning (AL) loop in general. Our paper examines the three highlighted steps: (i) Bootstrapping with TL model, (ii) Acquisition strategy, and (iii) Model update.","While transformer-based systems have enabled greater accuracies with fewer training examples, data acquisition obstacles still persist for rare-class tasks ‚Äì when the class label is very infrequent (e.g.,<5%of samples). Active learning has in general been proposed to alleviate such challenges, but choice of selection strategy, the criteria by which rare-class examples are chosen, has not been systematically evaluated. Further, transformers enable iterative transfer-learning approaches. We propose and investigate transferand active learning solutions to the rare class problem of dissonance detection through utilizing models trained on closely related tasks and the evaluation of acquisition strategies, including a proposed probability-of-rare-class (PRC) approach. We perform these experiments for a specific rare class problem: collecting language samples of cognitive dissonance from social media. We find that PRC is a simple and effective strategy to guide annotations and ultimately improve model accuracy while transfer-learning in a specific order can improve the cold-start performance of the learner but does not benefit iterations of active learning.","Cognitive dissonance occurs during everyday thinking when one experiences two or more beliefs that are inconsistent in some way. Often expressed in language, dissonance plays a role in many aspects of life, for example affecting health-related behavior such as smoking and contributing to the development of (and exit from) extremism. However, while the phenomenon is common enough to occur on a daily basis, dissonance is still relatively rare among the myriad of other relationships between beliefs that occur across random selections of expressions and thus makes the automatic detection of it a rare-class problem. 
Despite recent advances in modeling sequences of words, rare-class tasks ‚Äì when the class label is very infrequent (e.g.,<5%of samples) ‚Äì remain challenging due to the low rate of positive examples. Not only are more random examples necessary to reach a substantial amount (e.g.,1,000 examples to reach just 50 examples of the rare class) but also it is easy for human annotators to miss the rare instances where dissonance is present. Here, we develop and address the challenges of creating a resource for language-based assessment of dissonance. 
Active learning using large language models presents both new opportunities and challenges. On the one hand, LLMs offer unmatched representations of documentations, able to achieve state-of-the-art language understanding task performance with transfer learning, often only with a few iterations of fine-tuning. On the other hand, representations are high-dimensional, and models trained or fine-tuned with only a small number of examples are prone to overfitting, especially when there is a large class imbalance as in rare-class problems. While LLMs have enabled attempting more and more complex semantic challenges across a growing list of tasks, getting annotated examples for such problems can become a bottleneck due to its timeand labor-intensiveness. Since data-centric improvements for more novel tasks can provide a faster path than model-centric improvements, active learning can be a way forward to be both data-centric and address bottlenecks in label acquisition ‚Äì it aims to reduce annotation costs as well as alleviate the training data deficiency that large language models face. 
However, while active learning has been studied for multiple natural language tasks, little is known about active learning acquisition strategies for LM-based approaches, especially for rare-class problems. High data imbalance coupled with very less training data poses the challenge of ""absolute rarity"", as in our task of dissonance detection. We address this problem by using a novel combination of evaluating the ordering of transfer learning from similar tasks to cold-start the active learning loop, and by acquiring with a relatively simple acquisition strategy focused on probability-of-rare-class (PRC) to increase the rare class samples. 
Our contributions include: (1) finding that bootstrapping AL models with transfer learning on closely related tasks significantly improves rare class detection; (2) a novel systematic comparison of five common acquisition strategies for active learning for a rare class problem; (3) a systematic comparison of two different approaches to handling AL iterations for LLMs ‚Äì cumulative and iterative fine-tuned model updates finding the cumulative approach works best; (4) evaluating annotation costs of a rare-class task, finding that minimum annotation cost does not necessarily lead to better models, especially in realistic scenarios such as absolute rarity; and (5) release of a novel dataset for the task of identifying cognitive dissonance in social media documents."
"Towards Leaving No Indic Language Behind: Building Monolingual Corpora, Benchmark and Models for Indic Languages",2212.05409v3,./img_ACL_2023/2212.05409v3.png,Upsampled data distribution.,"Building Natural Language Understanding (NLU) capabilities for Indic languages, which have a collective speaker base of more than one billion speakers is absolutely crucial. In this work, we aim to improve the NLU capabilities of Indic languages by making contributions along 3 important axes (i) monolingual corpora (ii) NLU testsets (iii) multilingual LLMs focusing on Indic languages. Specifically, we curate the largest monolingual corpora, IndicCorp, with 20.9B tokens covering 24 languages from 4 language families - a 2.3x increase over prior work, while supporting 12 additional languages. Next, we create a human-supervised benchmark, IndicXTREME, consisting of nine diverse NLU tasks covering 20 languages. Across languages and tasks, IndicXTREME contains a total of 105 evaluation sets, of which 52 are new contributions to the literature. To the best of our knowledge, this is the first effort towards creating a standard benchmark for Indic languages that aims to test the multilingual zero-shot capabilities of pretrained language models. Finally, we train IndicBERT v2, a state-of-the-art model supporting all the languages. Averaged across languages and tasks, the model achieves an absolute improvement of 2 points over a strong baseline. The data and models are available at <https: //github. com/AI4Bharat/IndicBERT>.","Recent advances in Natural Language Understanding are largely driven by pretrained multilingual models. One of the advantages of such models is that they can potentially reduce the performance gap between high and low-resource languages through zero-shot knowledge transfer. However, in practice, the benefits of such models are still skewed towards high-resource languages due to 3 main reason as outlined below. 
First, current multilingual models often have a poor representation of low-resource languages. For example, out of the 22 languages listed in the 8^thschedule of the Indian constitution, only 15 languages are supported by the popular XLM-R model. This is mainly due to the non-availability of pretraining data for languages like Bodo, Dogri, Kashmiri, etc. in large multilingual corpora such as CC-100, or mC4. Hence, dedicated efforts towards collecting pretraining data for these languages by discovering and crawling language-specific sources are needed. 
 
 Second, even for low-resource languages supported by existing multilingual models, the size of pretraining data is much smaller than that of English and other resource-rich languages. Due to this disparity, low-resource languages get a very poor share of the model's capacity and vocabulary, and thus the performance on these languages is poor. Indeed, a few recent efforts show that multilingual models trained using pretraining data from a smaller set of related languages leads to better performance on downstream tasks than large scale models which support many languages. Hence, there is a need for training language models only on Indic languages thereby ensuring that the model capacity is not dominated by unrelated high-resource languages. 
 
 The third reason is the poor representation of these languages in existing evaluation benchmarks. For example, in the XTREME-R benchmark, out of the 10 tasks only three contain evaluation data for more than two Indic languages. Further, the maximum number of Indic languages for any task is just seven. In effect, 15 of the 22 constitutionally recognized Indic languages have no representation in XTREME-R for any task. Thus, a human supervised evaluation benchmark tailored for Indic, and other low-resource language families is essential for furthering inclusivity and equity in NLP research. 
 
 In this work, we make contributions toward addressing all the three challenges. We focus on the 22 languages listed in the 8^thschedule of the Indian constitution spanning 4 language families and spoken by over a billion speakers (8 of these languages being amongst the top-20 most spoken languages globally). Some of these languages are also widely spoken and/or are official languages in neighbouring countries viz. , Bangladesh, Nepal and Pakistan. Our first contribution towards serving these languages is to release IndicCorp v2, the largest collection of corpora for languages spanning 4 Indic language families with 20.9 Billion tokens and 1.1 Billion sentences. Table shows a comparison of IndicCorp v2 with existing collections of monolingual corpora. As is clear, IndicCorp not only supports more Indic languages but also improves upon the data for languages supported in existing collections (e.g., √ó2.3 improvement over IndicCorp v1 with 12B new tokens). Our second contribution is IndicBERT v2, a multilingual LM pretrained on IndicCorp v2 and supporting the largest number of Indic languages compared to existing models such as XLM-R, MuRIL, and IndicBERT v1. 
 
 Our third, and perhaps, the most important contribution is IndicXTREME, a human supervised benchmark containing evaluation sets for nine diverse tasks with each task covering 7-18 Indic languages per task. These include five classification tasks, two structure prediction tasks, one QA task, and one text retrieval task. Of the total 105 evaluation sets, summed across languages and tasks, 52 have been newly created as a part of this benchmark. All the newly added evaluation sets have been created manually with the help of in-house language experts with several years of experience in language annotation and translation. The datasets for three tasks, viz. , NER, QA, and paraphrase detection were created from scratch without any translation from English sources. We consciously make an effort to include languages spanning all the classes from the inclusion taxonomy introduced in. According to their classification (Table), nine languages in IndicXTREME are the so-called ""Left-Behinds"", the most ignored, with exceptionally minimal resources. Only three are ""Winners"", the high-resource languages, which have a dominant online presence with industry and government investments. 
 
 Using IndicXTREME, we evaluate IndicBERT and show that it outperforms strong baselines on 7/9 evaluation tasks. We also do a series of ablation tests to show that (i) the translation language modeling (TLM) objective slightly improves zero-shot performance when high-quality parallel data is used, (ii) using noisy parallel data during pretraining leads to sub-optimal zero-shot performance, (iii) using in-language-family development sets allows better model selection, and (iv) zero-shot transfer via Hindi, as opposed to English, leads to better performance. All the datasets, code, and models developed as a part of this work will be open-sourced. All the datasets and models developed as a part of this work are available at <https: //ai4bharat. iitm. ac. in/language-understanding>."
HAUSER: Towards Holistic and Automatic Evaluation of Simile Generation,2306.07554v1,./img_ACL_2023/2306.07554v1.pdf,"An example of Simile Generation (SG) Evaluation. The commonly used automatic metric BLEU deems the second candidate as the most high-quality one among all the generated similes, while our proposed metrics HAUSER deem the first candidate as the best one regarding its quality, creativity and informativeness, which better correlates with human ratings and also provides more criteria for SG evaluation.","Similes play an imperative role in creative writing such as story and dialogue generation. Proper evaluation metrics are like a beacon guiding the research of simile generation (SG). However, it remains under-explored as to what criteria should be considered, how to quantify each criterion into metrics, and whether the metrics are effective for comprehensive, efficient, and reliable SG evaluation. To address the issues, we establish HAUSER, a holistic and automatic evaluation system for the SG task, which consists of five criteria from three perspectives and automatic metrics for each criterion. Through extensive experiments, we verify that our metrics are significantly more correlated with human ratings from each perspective compared with prior automatic metrics. Resources of HAUSER are publicly available at <https: //github. com/Abbey4799/HAUSER>.","Similes play a vital role in human expression, making literal sentences imaginative and graspable. For example, Robert Burns famously wrote ""My Luve is like a red, red rose"" to metaphorically depict the beloved as being beautiful. In this simile, ""Luve"" (a. k. a. topic) is compared with ""red rose"" (a. k. a. vehicle) via the implicit property ""beautiful"" and the event ""is"". Here, topic, vehicle, property, and event are four main simile components. As a figure of speech, similes have been widely used in literature and conversations. 
Simile generation (SG) is a crucial task in natural language processing, with the aim of polishing literal sentences into similes. In Fig. , the literal sentence ""He yelps and howls. "" is polished into a simile by inserting the phrase ""like a wolf"", resulting in ""He yelps and howls like a wolf"". The ability to generate similes can assist various downstream tasks, such as making the generations more imaginative in story or poet generation task and the generated response more human-like in dialogue generation task. 
Automatic evaluation is critical for the SG task since it enables efficient, systematic, and scalable comparisons between models in general. However, existing studies are inadequate for effective SG evaluation. Task-agnostic automatic metrics are widely adopted for SG evaluation, which have several limitations: (1) The simile components should receive more attention than other words during SG evaluation (e.g.,""he"" and ""wolf"" in Fig. ), while there are no automatic metrics that consider the key components. (2) The SG task is open-ended, allowing for multiple plausible generations for the same input (e.g.,the howling man can be compared to ""wolf"", ""buffalo"", or ""tiger"" in Fig. ). Hence, the metrics based on word overlap with a few references are inadequate to accurately measure the overall quality of generated similes. As shown in Fig. , the commonly used metric BLEU deems the second candidate as the highest quality, as it has more overlapped words with the only referenced groundtruth, while human deems the first candidate as the most coherent one. (3) The existing metrics are inadequate to provide fine-grained and comprehensive SG evaluation, considering that the creative generation tasks have distinct criteria for desired generations, such as novelty and complexity for story generation and logical consistency for dialogue generation. 
However, establishing a comprehensive, efficient, and reliable evaluation system for SG is non-trivial, which raises three main concerns: (1) What criteria should be adopted to evaluate the SG task in a comprehensive and non-redundant fashion? (2) How to quantify each criterion into a metric thus enabling efficient and objective SG evaluation, given that the human evaluation of creative generation task is not only time-consuming but also subjective and blurred? (3) Whether the proposed metrics are effective in providing useful scores to guide actual improvements in the real-world application of the SG model? 
In this paper, we establish HAUSER, a Holistic and AUtomatic evaluation system for Simile gEneRation task, consisting of five criteria (Tab. ): (1) The relevance between topic and vehicle, as the foundation of a simile is to compare the two via their shared properties. (2) The logical consistency between the literal sentence and generated simile, since the aim of SG task is to polish the original sentence without altering its semantics. (3) The sentiment consistency between the literal sentence and generated simile, since similes generally transmit certain sentiment polarity. (4,5) The creativity and informativeness of the simile, since novel similes or those with richer content can enhance the literary experience. Overall, these five criteria can be categorized into three perspectives: quality (which considers relevance, logical, and sentiment consistency jointly), creativity, and informativeness. We further quantify each criterion into automatic metrics (Fig. ) and prove their effectiveness through extensive experiments. 
To the best of our knowledge, we are the first to systematically investigate the automatic evaluation of the SG task. To summarize, our contributions are mainly three-fold: (1) We establish a holistic and automatic evaluation system for the SG task, consisting of five criteria based on linguistic theories, facilitating both human and automatic evaluation of this task. (2) We design automatic metrics for each criterion, facilitating efficient and objective comparisons between SG models. (3) We conduct extensive experiments to verify that our metrics are significantly more correlated with human ratings than prior metrics."
Unsupervised Selective Rationalization with Noise Injection,2305.17534v1,./img_ACL_2023/2305.17534v1.pdf,"Example of a rationale selected by BERT-A2R + NI (our model) on the USR Movie Review dataset (our benchmark), which asks models to classify movie reviews as positive or negative.","A major issue with using deep learning models in sensitive applications is that they provide no explanation for their output. To address this problem, unsupervised selective rationalization produces rationales alongside predictions by chaining two jointly-trained components, a rationale generator and a predictor. Although this architecture guarantees that the prediction relies solely on the rationale, it does not ensure that the rationale contains a plausible explanation for the prediction. We introduce a novel training technique that effectively limits generation of implausible rationales by injecting noise between the generator and the predictor. Furthermore, we propose a new benchmark for evaluating unsupervised selective rationalization models using movie reviews from existing datasets. We achieve sizeable improvements in rationale plausibility and task accuracy over the state-of-the-art across a variety of tasks, including our new benchmark, while maintaining or improving model faithfulness.","With the advent of large pre-trained language models like GPT-3, the size and complexity of deep learning models used for natural language processing has dramatically increased. Yet greater performance and complexity can come at the cost of interpretability, masking anything from implementation mistakes to learned bias. 
A model architecture that justifies its output by providing relevant subsets of input text as a rationale is therefore desirable (see example in Figure). The unsupervised selective rationalization architecture as introduced by lei_rationalizing_2016 generates rationales alongside predictions by chaining two jointly-trained components, a rationale-generator and a predictor. The generator extracts a rationale: concatenated short and concise spans of the input text that suffice for prediction. The predictor bases its prediction only on this rationale, which encourages faithfulness, meaning how much the rationale reveals what parts of the input were important to the model 's prediction. In practice, however, the rationale often isn' t plausible, meaning it can 't convince a human of the correct prediction, undermining the architecture' s interpretability. Using a high-capacity generator can further degrade plausibility. 
To prevent this effect, we introduce a novel training strategy that leverages online noise injection, based on word-level unsupervised data augmentation. By definition, if the loss-minimizing generator selects an implausible rationale, then the rationale both (a) offers no plausible connection for a human to the target label and (b) locally improves prediction accuracy. This might include communicating via punctuation or subtle input perturbations. Our new approach is to inject noise into the generated rationale during training by probabilistically replacing lower-importance words with noise - random words from the vocabulary - before passing the rationale to the predictor. We observe that this strategy leads to a significant improvement in plausible rationale generation and prediction accuracy without compromising the faithfulness of the architecture. We also show that powerful generators typically interfere with plausible rationale generation but can be effectively deployed when trained with noise injection. 
To test our approach, we introduce a new benchmark for unsupervised selective rationalization by integrating existing movie review datasets to replace the retracted canonical beer review dataset. We merge a large IMDb movie review dataset for training and validation and a smaller, rationale-annotated movie review dataset for evaluation. We also evaluate our unsupervised approach on the ERASER Movie Review, MultiRC and FEVER tasks. 
Our contributions therefore include: 1) characterizing the issue of implausible rationale generation from the perspective of powerful rationale generators, 2) introducing a novel training strategy that limits implausible rationale generation and enables unsupervised selective rationalization models with powerful generators, 3) proposing a new unsupervised rationalization benchmark by repurposing existing movie review datasets, and 4) achieving more plausible rationale generation, with up to a relative 21%improvement in F1 score and a 7.7 point improvement in IOU-F1 score against the baseline model across a number of tasks."
Socratic Pretraining: Question-Driven Pretraining for Controllable Summarization,2212.10449v3,./img_ACL_2023/2212.10449v3.pdf,Our Socratic pretraining compared to denoising. We mask important sentences in unlabeled input documents and train the model to generate both questions and pseudo-summaries as their answers.,"In long document controllable summarization, where labeled data is scarce, pretrained models struggle to adapt to the task and effectively respond to user queries. In this paper, we introduce Socratic pretraining, a question-driven, unsupervised pretraining objective specifically designed to improve controllability in summarization tasks. By training a model to generate and answer relevant questions in a given context, Socratic pretraining enables the model to more effectively adhere to user-provided queries and identify relevant content to be summarized. We demonstrate the effectiveness of this approach through extensive experimentation on two summarization domains, short stories and dialogue, and multiple control strategies: keywords, questions, and factoid QA pairs. Our pretraining method relies only on unlabeled documents and a question generation system and outperforms pre-finetuning approaches that use additional supervised data. Furthermore, our results show that Socratic pretraining cuts task-specific labeled data requirements in half, is more faithful to user-provided queries, and achieves state-of-the-art performance on QMSum and SQuALITY.","Summarization systems are designed to help users navigate large amounts of information, but often fail to meet the unique needs of different users, especially for long documents. Recent research has explored ways to make summarization systems more controllable by allowing users to input queries or control sequences such as keywords, questions, entity chains, or question-answer pairs. 
A challenge shared by all of the mentioned approaches is the absence of abundant labeled data. Currently available datasets for training these systems are the result of expensive annotation efforts with only hundreds to a few thousand query-document pairs, with the same document often being repeated. This translates into poor adherence of generated summaries to user-provided queries, particularly when these are finegrained plans. Recent work demonstrates the benefits of tailoring the pretraining objective to downstream task characteristics, especially where training data is difficult to obtain in large quantities like factuality-focused and multi-document summarization. In controllable summarization, summaries are grounded by queries, so designing an objective for the task requires introducing realistic queries in unlabeled data in a scalable manner. 
This work introduces Socratic pretraining, an unsupervised pretraining objective for language models that is specifically designed for controllable summarization. It is inspired by the Socratic method and aims to facilitate the identification of relevant content and ensure that the generated summary faithfully responds to the user query. During Socratic pretraining (see) the language model is trained to generate relevant questions based on an input document and then answer them, bringing finegrained controllability to model pretraining which translates to better adherence to user queries. 
Socratic pretraining only relies on unlabeled data and a question generation system and outperforms pre-finetuning approaches relying on additional supervised data. In this work, we demonstrate the effectiveness of the Socratic objective through pretraining adaptation, where a language model is further pretrained with the Socratic objective before finetuning on task-specific labeled data. 
In summary, our contributions are as follows: 
 
 * We introduce the Socratic pretraining objective for controllable summarization to improve adherence to user-specified queries or plans, both high-level and finegrained. 
 * We show that Socratic pretraining performs well across domains, control strategies, and achieves state-of-the-art performance on two datasets. 
 * We perform ablations on our approach showing that Socratic pretraining cuts labeled data requirements in half."
UniSumm and SummZoo: Unified Model and Diverse Benchmark for Few-Shot Summarization,2211.09783v6,./img_ACL_2023/2211.09783v6.pdf,"The few-shot summarization scenario in this paper. We are interested in how to re-use previous datasets (e.g., CNNDM) to improve the few-shot performance on unseen target tasks (e.g., DialogSum).","The high annotation costs and diverse demands of various summarization tasks motivate the development of few-shot summarization. However, despite the emergence of many summarization tasks and datasets, the current training paradigm for few-shot summarization systems ignores potentially shareable knowledge in heterogeneous datasets. To this end, we propose UniSumm, a unified few-shot summarization model pre-trained with multiple summarization tasks and can be prefix-tuned to excel at any few-shot summarization task. Meanwhile, to better evaluate few-shot summarizers, under the principles of diversity and robustness, we assemble and release a new benchmark SummZoo. It consists of","There has been a recent surge of interest in summarizers based on large pre-trained language models (PLMs), where various summarization tasks (the term task later in this paper refers to a specific summarization task, e.g., query-focused meeting summarization, which is usually associated with a corresponding dataset, e.g., QMSum, unless otherwise specified. ) have been proposed to meet different practical demands, such as comprehending different inputs (e.g., news and dialogue) and generating different outputs (e.g., headlines and paragraphs). Because annotating gold summaries for newly-proposed summarization tasks is costly, few-shot summarization, the task of building a model for a specific summarization scenario using very limited ground-truth data, has gained increasing attention from the research community. 
 Recently, prefix-tuning has established strong baselines on many few-shot natural language generation tasks, including summarization. The main idea is to extract knowledge from PLMs by prepending and tuning additional parameters (prefixes) before each layer of the PLM. Work has been done to improve the performance by designing more sophisticated prefixes. Despite being effective, PLMs can have limited summarization knowledge due to the salient gap between pre-training objectives (e.g., language modeling) and summarization objectives. In addition, existing summarization datasets can provide relevant knowledge to newly-proposed summarization tasks, and therefore benefit summarization tasks, especially under the few-shot scenario. However, existing work tends to tune PLMs directly on a new task, without exploiting cross-task knowledge from summarization datasets, which may limit the generalization and adaptation abilities of models. 
We address these issues by proposing a unified few-shot summarization framework, UniSumm. The idea is to combine multi-task pre-training on existing summarization datasets with few-shot prefix-tuning on target tasks. To this end, we first build a multi-task model based on a Transformer-based language model as the backbone and equip it with task-specific prefix vectors, and then pre-train the multi-task model on diverse summarization datasets. In this stage, we optimize the summarization model together with task-specific prefixes and also a universal prefix, using an asymmetrical weight decay strategy. Using prefixes in the multi-task pre-training stage leads to two advantages: First, the mixture of shared summarization parameters and unique task-specific parameters helps to leverage natural benefits across datasets. Second, the pre-trained prefixes can be tuned to serve as a knob for the second stage of prefix-tuning on unseen tasks. When facing an unseen few-shot summarization task, we freeze the multi-task learned backbone model and use the universal prefix as initialization for prefix-tuning. 
A data obstacle for few-shot summarization research is the lack of a benchmark for fair comparison. Previous studies either focus on one type of data, e.g., news text, or train their systems on non-public few-shot samples. However, because few-shot models can be highly sensitive to training data, the selection of different few-shot samples in different papers can lead to ambiguous comparisons (a. k. a. Sample Selection Bias). To address these issues, we assemble and release a new few-shot summarization benchmark, SummZoo, following two principles, namely diversity of tasks and robustness of evaluation. SummZoo collects summarization data from 8 existing datasets, which are diverse in terms of domain (news, academic papers, meetings, etc. ), format (single-document and multi-document), and length on both source and target sides. For more robust evaluation, for each task, SummZoo provides 5 different (randomly sampled) few-shot training sets, and requires all systems to report their averaged results. Finally, SummZoo includes 10-shot and 100-shot settings. 
We compare UniSumm against several strong baselines, including a GPT-3.5 model (text-davinci-002), on SummZoo and conduct thorough analysis. Experimental results of automatic evaluation metrics show that UniSumm outperforms baselines across all sub-stasks and human evaluation shows that UniSumm achieves better performance than baselines of similar sizes and comparable performance compared with text-davinci-002. Additionally, UniSumm is empirically found to be more stable and robust when facing different few-shot samples. Analysis shows that combining multi-task pre-training and few-shot prefix-tuning is essential to the performance of UniSumm and other techniques, such as universal prefix and asymmetrical weight decay strategy, can all improve its generalization ability. We release our code, model and benchmark at <https: //github. com/microsoft/UniSumm>."
An AMR-based Link Prediction Approach for Document-level Event Argument Extraction,2305.19162v1,./img_ACL_2023/2305.19162v1.png,"Treating EAE as a link prediction problem, where the green box means the event trigger and blue circles are non-trigger nodes. The highlighted text means they are captured by the graph. We parse the document to a tailored AMR graph and apply a GNN model to predict edges between the trigger and other nodes. In this example, the model predicts one argument, ""Zabiullah Mujahid"", with the role of ""Communicator"".","Recent works have introduced Abstract Meaning Representation (AMR) for Document-level Event Argument Extraction (Doc-level EAE), since AMR provides a useful interpretation of complex semantic structures and helps to capture long-distance dependency. However, in these works AMR is used only implicitly, for instance, as additional features or training signals. Motivated by the fact that all event structures can be inferred from AMR, this work reformulates EAE as a link prediction problem on AMR graphs. 
Since AMR is a generic structure and does not perfectly suit EAE, we propose a novel graph structure, Tailored AMR Graph (TAG), which compresses less informative subgraphs and edge types, integrates span information, and highlights surrounding events in the same document. With TAG, we further propose a novel method using graph neural networks as a link prediction model to find event arguments. 
Our extensive experiments on WikiEvents and RAMS show that this simpler approach outperforms the state-of-the-art models by 3.63pt and 2.33pt F1, respectively, and do so with reduced 56%inference time. The code is availabel at <https: //github. com/ayyyq/TARA>.","Event Argument Extraction (EAE) is a long-standing information extraction task to extract event structures composed of arguments from unstructured text. Event structures can serve as an intermediate semantic representation and be further used for improving downstream tasks, including machine reading comprehension, question answering, dialog system, and recommendation system. Despite the large performance boost by Pre-trained Language Models (PLMs), extracting complex event structures across sentences is still challenging. 
In real-world text, event structures are usually distributed in multiple sentences. To capture cross-sentence and multi-hop structures, introduces Abstract Meaning Representation (AMR) graphs to assist the model in understanding the document. Their main idea is to take AMR as additional features to enrich span representations. and utilize AMR graphs to provide training signals via self-training and contrastive learning, respectively. These methods exemplify that introducing AMR information facilitates the model's understanding of complex event structures. However, previous works implicitly use AMR information by enriching neural sequential models rather than making explicit use of discrete structures. Intuitively, discrete AMR structures can force the model to better focus on predicate-argument structures and the content most related to EAE, therefore having stronger effect than implicit AMR. 
We aim to exploit the potentials of explicit AMR for improving EAE by formulating EAE as a link prediction task, and Figure illustrates the framework. We parse the input document to a graph structure and adopt a link prediction model to find event arguments. We determine if a node is an argument by whether it is connected to the trigger node or not. The advantages of formulating EAE as a link prediction problem are three-fold: 1) AMR graph is typically more compact than raw text (see Sec-), so processing AMR to find arguments would be simple and efficient. 2) Dependencies among multiple arguments and events are explicitly captured, while previous works have pointed out the importance of these dependencies which are only implicitly considered in the feature space. 3) The simpler model architecture and sparse graphs can lead to improvement over efficiency, as our experiments show (up to 56%inference time saving). 
The proposed method assumes that AMR graphs contain all necessary information for EAE. However, the original AMR graphs generated by off-the-shelf AMR parsers do not meet this assumption. First, they cover only 72.2%event arguments in WikiEvents, impeding the performance of EAE models directly on the parsed AMR graphs. The primary problem is that AMR graphs are defined at word-level, but an event argument could be a text span. Second, the Smatch score of SOTA AMR parsers is around 85, which causes information loss as well. To address the above issue, we propose a novel Tailored AMR Graph (TAG), which compresses information irrelevant to EAE, merges words into text spans via a span proposal module, and highlights the surrounding events in the same document to encourage their communication. Particularly, the number of nodes in TAG equals around 47%of words in WikiEvents, which is a significant reduction. Since too much distracting information is a major challenge of document-level tasks, we also expect performance gains from focusing on TAG, which is evidenced by our experiment results. TAG can cover all EAE samples if the span proposal module adds enough text spans, and we will discuss the trade-off between the recall of spans and model efficiency in Appendix-. 
Although there is a large design space for the link prediction model, we choose a simple architecture that stacks GNN layers on top of pre-trained text encoders. The whole model is called TARA for Tailored AMR-based Argument Extraction. We conduct extensive experiments on latest document-level EAE datasets, WikiEvents and RAMS. TARA achieves 3.63pt and 2.33pt improvements of F1 against the SOTA, respectively. Since interactions in GNN are sparse, the computation cost of our model is also lower, saving up to 56%inference time. 
To our knowledge, we are the first to formulate EAE as a link prediction problem on AMR graphs."
Exploring and Verbalizing Academic Ideas by Concept Co-occurrence,2306.02282v1,./img_ACL_2023/2306.02282v1.pdf,A quintuple with its text attributes. The dashed line and box represent the texts of paper or concept.,"Researchers usually come up with new ideas only after thoroughly comprehending vast quantities of literature. The difficulty of this procedure is exacerbated by the fact that the number of academic publications is growing exponentially. In this study, we devise a framework based on concept co-occurrence for academic idea inspiration, which has been integrated into a research assistant system. From our perspective, the fusion of two concepts that co-occur in an academic paper can be regarded as an important way of the emergence of a new idea. We construct evolving concept graphs according to the co-occurrence relationship of concepts from 20 disciplines or topics. Then we design a temporal link prediction method based on masked language model to explore potential connections between different concepts. To verbalize the newly discovered connections, we also utilize the pretrained language model to generate a description of an idea based on a new data structure called co-occurrence citation quintuple. We evaluate our proposed system using both automatic metrics and human assessment. The results demonstrate that our system has broad prospects and can assist researchers in expediting the process of discovering new ideas.","Academic publications have witnessed the evolution and advancement of human civilization. In modern society, out-of-box and interdisciplinary scientific work can get more attention from science funders, industry, and the public, where a good idea is the cornerstone of academic research. However, for most researchers, it takes a lot of time to put forward new ideas. For one thing, the number of academic publications is increasing exponentially, and it is difficult for an independent researcher to understand these papers thoroughly. Besides, researchers often focus on their specialized but narrow fields, which makes it a challenge to discover underlying connections beyond their familiar areas. In this work, our purpose is to unveil the profound connections between different academic concepts and ignite researchers' exploration of potential academic ideas while expediting the research process. The two primary goals are idea exploration and verbalization. 
For the first goal, we need to understand how new ideas originate. Generally speaking, the emergence of a simple idea is often formed by the interaction between two different concepts rather than from scratch. For example, the combination of convolution and graph neural network contributes to graph convolutional network. This understanding of idea as connection and combination inspires us to model the process of idea exploration as a link prediction task based on the evolving co-occurrence graph of concepts. Such graphs are constructed according to the co-occurrence relationship of concepts in the papers published in different years. It should be highlighted that there exist numerous factors leading to new ideas in the real world. We provide a possible way as a preliminary exploration. 
The second goal, idea verbalization, is carried out after idea exploration to generate fluent and reasonable texts describing an idea, which usually comprises new contents derived from the combination of two different concepts. We retrieve sentences pertaining to concepts from existing publications and then verbalize ideas using the technique of natural language generation. Specifically, We propose a new data structure called co-occurrence citation quintuple (Figure), which stores two concepts, their corresponding sentences of papers, and idea texts. The definition is given in section. The quintuple is an extension of edges in the evolving concept co-occurrence graph and indicates where an idea comes from. We use such quintuples to train a sequence-to-sequence text generation model. 
In our application scenario, there are various types of disciplines. Each of them has distinct characteristics and concepts. Existing methods of link prediction and text generation are mostly trained on one dataset by optimizing a set of parameters. Owing to the fact that different datasets require specific training configurations and hyper-parameters, such models cannot be transferred to other datasets. Particularly, link prediction models need to set the scale of graphs before training, such as the number of nodes. Moreover, in the field of natural language generation, some works tend to construct domain knowledge bases as external information to generate texts. However, building large knowledge bases for each discipline takes tremendous resources, which is unrealistic. To this end, it is preferable to design general and informative models which can be applied to numerous disciplines. 
Thanks to the abundant training corpus of pretrained language models (PLMs) such as BERT, T5, BART, and GPT, PLM can be regarded as an implicit knowledge graph, which has the ability of extrapolation. In this work, we integrate the whole academic information into the same representation space by leveraging the capability of PLM to break through disciplinary barriers. For idea exploration, we devise a PLM-based link prediction method, which only needs to train one set of model parameters. For idea verbalization, we use another sequence-to-sequence-based PLM endowed with academic knowledge from millions of highly-cited papers via unsupervised denoising training. Subsequently, we re-train the denoised PLM with co-occurrence citation quintuples in a supervised way. Our contributions are summarized as follows: 
 
 * New insights: we transform the idea generation into two sequential sub-tasks: temporal link prediction and idea verbalization. The former aims to model and predict potential concept connections, while the latter involves expressing these new connections in natural language. 
 * Publicly-released datasets: we construct 240 evolving concept co-occurrence graphs with 20 high-level disciplines and topics. Each of them includes 23 annual snapshots ranging from 2000 to 2022. For idea verbalization, we propose a new data structure known as the co-occurrence citation quintuple that reveals how ideas appear. We curate nearly 10K high-quality co-occurrence citation quintuples, which originate from 29M papers with high citations. 
 * General system for all disciplines: we design a novel temporal link prediction method and train an idea verbalization model with a large number of academic papers. The two modules are integrated into a system to serve researchers from different fields. Note that the system updates the latest papers to encourage new ideas sustainably. Users are free to enter any academic query. 
 * Systematic experiments: we conduct extensive experiments, including automatic metrics and human assessment, to evaluate the performance of our link prediction method and idea verbalization model. The results show that our system has a promising prospect of helping researchers discover new ideas."
Do Question Answering Modeling Improvements Hold Across Benchmarks?,2102.01065v3,./img_ACL_2023/2102.01065v3.pdf,"Two benchmarks have high concurrence if they rank a set of modeling approaches similarly. Surprisingly, we find that human-constructed benchmarks (e.g., SQuAD, NaturalQuestions) have high concurrence with other human-constructed benchmarks, downsampled human-constructed benchmarks, and even programmatically-generated cloze benchmarks (e.g., the Children's Book Test; CBT). In addition, we are able to construct synthetic benchmarks that have high concurrence with human-constructed benchmarks despite lacking natural language passages or questions.","Do question answering (QA) modeling improvements (e.g., choice of architecture and training procedure) hold consistently across the diverse landscape of QA benchmarks? To study this question, we introduce the notion of concurrence‚Äîtwo benchmarks have high concurrence on a set of modeling approaches if they rank the modeling approaches similarly. We measure the concurrence between 32 QA benchmarks on a set of 20 diverse modeling approaches and find that human-constructed benchmarks have high concurrence amongst themselves, even if their passage and question distributions are very different. Surprisingly, even downsampled human-constructed benchmarks (i.e., collecting less data) and programmatically-generated benchmarks (e.g., cloze-formatted examples) have high concurrence with human-constructed benchmarks. These results indicate that, despite years of intense community focus on a small number of benchmarks, the modeling improvements studied hold broadly.","The NLP community has created a diverse landscape of extractive question answering (QA) benchmarks‚Äîtheir context passages may come from different sources, their questions may focus on different phenomena or be written by different populations, or other aspects of the data collection process may differ. Driven to improve benchmark performance, researchers have proposed a variety of QA modeling approaches. However, not all benchmarks receive equal attention from the community; many QA modeling approaches are developed on a small handful of benchmarks, especially those with popular leaderboards (e.g., SQuAD; ). As a result, it is conceivable that some modeling improvements may not hold because they are (perhaps inadvertently) benchmark-specific, while others (e.g., pre-training on more data) hold more broadly. 
In this work, we evaluate whether improvements from modeling approaches hold (e.g., choices in model architecture or training procedure) ‚Äîif a particular modeling approach improves performance when trained and evaluated on one benchmark, does it also improve performance on others? Although much existing work studies whether systems generalize (i.e., a model with a particular set of parameters; ), research value often comes not from the systems themselves (e.g., model weights), but from the underlying ideas, techniques, and approaches. We study the comparatively under-investigated question of whether such modeling approaches generalize. 
To study whether modeling improvements hold across benchmarks, we introduce the notion of concurrence. We say that two benchmarks have high concurrence on a set of modeling approaches if they rank the modeling approaches similarly. To assess whether modeling improvements hold across the space of QA benchmarks, we measure the concurrence between 32 diverse QA benchmarks on a testbed of 20 representative modeling approaches introduced between 2016 and 2020. 
Overall, we find that benchmarks that differ substantially still often have high concurrence. Human-constructed benchmarks (e.g., SQuAD and MRQA NaturalQuestions) have high concurrence with each other, despite differences in crowdsourcing setups, passage and question distributions, and even linguistic phenomena of focus (). 
How different can a benchmark be, while still maintaining high concurrence with human-constructed benchmarks? In, we investigate the role of training dataset size by measuring concurrence with downsampled training datasets (e.g., using 20K SQuAD training examples rather than the full 88K). We find that downsampled training datasets are sufficient for high concurrence with other human-constructed benchmarks. In, we measure concurrence between human-constructed and programmatically-generated benchmarks (e.g., cloze-formatted or synthetic) to better understand the importance of human-written questions and passages. We find that cloze-formatted benchmarks have high concurrence with human-constructed benchmarks, so human-written questions and passages are not strictly necessary for concurrence. However, programmatically-generated synthetic benchmarks (e.g., the bAbI task suite) have low concurrence. Having found this breaking point of low concurrence, we construct two minimal synthetic benchmarks that achieve high concurrence with human-constructed benchmarks, despite lacking linguistic structure. Intuitively, the benchmarks that concur with human-constructed benchmarks are those that require model capabilities that are also useful for better performance on human-constructed benchmarks (e.g., identifying paraphrase and lexical overlap; -). 
Our results have several implications for the future development of benchmarks and modeling approaches. To summarize: 
 * Human-constructed benchmarks have high concurrence with each other on our testbed of 20 modeling approaches. The modeling approaches studied are not particularly benchmark-specific and that their modeling improvements largely hold across different benchmarks, despite intense community focus on a small number of benchmarks. This is especially true of recent modeling improvements driven by better pre-training, which is largely downstream benchmark-agnostic. 
 * Many benchmarks require reasoning over predicate-argument structure (e.g., SQuAD, NewsQA, NaturalQuestions), and improvements on these benchmarks also transfer to more specialized benchmarks (e.g., HotpotQA or MRQA DROP) because (1) almost all benchmarks involve reasoning over predicate-argument structure and/or (2) better reasoning over predicate-argument structure is correlated with improvements on other phenomena. 
 * Human-constructed benchmarks are not strictly necessary for improving performance on other human-constructed benchmarks. Synthetic benchmarks may be useful tools for isolating, understanding, and improving on particular model capabilities. 
 * Downsampling benchmarks to as few as 10K training examples does not significantly affect concurrence, especially since recent pre-trained modeling approaches have greater sample efficiency. We recommend the community build benchmarks that are smaller but more challenging (e.g., harder/more expensive to label per-example). 
 * Since human-constructed benchmarks have high concurrence amongst themselves, we encourage researchers to seek diversity and build benchmarks that explore qualitatively different modeling capabilities that push research in new directions."
PaCE: Unified Multi-modal Dialogue Pre-training with Progressive and Compositional Experts,2305.14839v2,./img_ACL_2023/2305.14839v2.pdf,"An example of multi-modal dialogue, which involves multiple tasks, including multi-modal intent classification, multi-modal state tracking, multi-modal dialog retrieval and response generation.","Perceiving multi-modal information and fulfilling dialogues with humans is a long-term goal of artificial intelligence. Pre-training is commonly regarded as an effective approach for multi-modal dialogue. However, due to the limited availability of multi-modal dialogue data, there is still scarce research on multi-modal dialogue pre-training. Yet another intriguing challenge emerges from the encompassing nature of multi-modal dialogue, which involves various modalities and tasks. Moreover, new forms of tasks may arise at unpredictable points in the future. Hence, it is essential for designed multi-modal dialogue models to possess sufficient flexibility to adapt to such scenarios. This paper proposes PaCE, a unified, structured, compositional multi-modal dialogue pre-training framework. It utilizes a combination of several fundamental experts to accommodate multiple dialogue-related tasks and can be pre-trained using limited dialogue and extensive non-dialogue multi-modal data. Furthermore, we propose a progressive training method where old experts from the past can assist new experts, facilitating the expansion of their capabilities. Experimental results demonstrate that PaCE achieves state-of-the-art results on eight multi-modal dialog benchmarks.","Enabling seamless communication between humans and machines is a long-standing goal of artificial intelligence research. The recent emergence of chatGPT has increased confidence in the potential for achieving this goal. Beyond the use of textual language as a unique interface between humans and machines, perceiving and utilizing multi-modal information, especially visual information, has become a crucial capability known as multi-modal dialogue. 
To facilitate the research on multi-modal dialogue, plenty of specific tasks and datasets have emerged in the community. However, the overall quantity of data is still limited. Furthermore, multi-modal dialogue presents a greater challenge compared to traditional text-only dialogue track, as it involves the integration of various modalities and more intricate task scenarios. As shown in Figure, the central tasks of multi-modal dialogue include multi-modal intent classification, multi-modal dialogue retrieval, multi-modal dialogue state tracking, and multi-modal response generation. Despite pre-training having become the consensus for multi-task learning in machine learning, the research on pre-training models for multi-modal dialogue is an area that is yet to be fully explored. 
In this paper, we focus on building pre-trained models of multi-modal dialogue. A key challenge is to unify different modalities and task forms, and make the best use of existing multi-modal dialog and non-dialog data. A recent popular trend on textual tasks is to build unified pre-trained foundation models by multi-task learning, e.g., T5. However, it attempts to mix all tasks learned from scratch thus is difficult to control the learning process, which is a completely black box. Although the Mixture-of-Experts (MoE) architecture attempts to select independent experts for each input sample through token-level routing, it lacks specific semantics, i.e., it is entirely unknown what the experts are responsible for. We hope to find a new way to handle many multi-modal dialog tasks simultaneously and combine existing concrete skills to learn new tasks more efficiently. 
To this end, we propose PaCE, a unified multi-modal dialogue pre-training framework with Progressive and Compositional Experts. First, we decompose complicated multi-modal dialogue into fundamental sub-capabilities that could be learned with specific data. Different from traditional MoE, each expert in PaCE is tailored to one specific fundamental sub-capability of multi-modal dialogue, including Caption, Context, Image, Grounding and Generation. Second, we propose a progressive pre-training strategy to evolve the model by controlling the combination of experts in different pre-training phases. Specifically, in stage I, we first train on multi-modal non-dialogue data to obtain Caption, Image, and Grounding experts. In stage II, we train the Context expert, which is guided by the Caption expert on multi-modal dialog data to learn the dependencies in context. Furthermore, a dialogue Generation expert is derived by adding a response generation task based on the previously learned experts. Third, for pre-training PaCE, we collect a multi-modal dialog corpus with 1.4 million dialogs and a multi-modal non-dialog corpus with 4 million samples. Once the pre-training of PaCE is finished, we can flexibly select different capability experts to solve a specific downstream task. 
As illustrated in Figure, PaCE achieves state-of-the-art performance across a broad range of multi-modal dialogue benchmarks spanning four diverse downstream tasks, i.e., multi-modal intent classification, multi-modal dialogue retrieval, multi-modal state tracking, and multi-modal response generation This demonstrates that PaCE not only possesses a flexible model architecture but also exhibits adaptable training methodologies, resulting in remarkable performance."
Topic-Guided Sampling For Data-Efficient Multi-Domain Stance Detection,2306.00765v1,./img_ACL_2023/2306.00765v1.pdf,The two components of TESTED: Topic Guided Sampling (top) and training with contrastive objective (bottom).,"Stance Detection is concerned with identifying the attitudes expressed by an author towards a target of interest. This task spans a variety of domains ranging from social media opinion identification to detecting the stance for a legal claim. However, the framing of the task varies within these domains, in terms of the data collection protocol, the label dictionary and the number of available annotations. Furthermore, these stance annotations are significantly imbalanced on a per-topic and inter-topic basis. These make multi-domain stance detection a challenging task, requiring standardization and domain adaptation. To overcome this challenge, we propose Topic Efficient StancE Detection (TESTED), consisting of a topic-guided diversity sampling technique and a contrastive objective that is used for fine-tuning a stance classifier. We evaluate the method on an existing benchmark of",
Uncovering and Categorizing Social Biases in Text-to-SQL,2305.16253v2,./img_ACL_2023/2305.16253v2.pdf,Two main categories of social biases existed in prevalent Text-to-SQL models.,"Content Warning: This work contains examples that potentially implicate stereotypes, associations, and other harms that could be offensive to individuals in certain social groups. 
Large pre-trained language models are acknowledged to carry social biases towards different demographics, which can further amplify existing stereotypes in our society and cause even more harm. Text-to-SQL is an important task, models of which are mainly adopted by administrative industries, where unfair decisions may lead to catastrophic consequences. However, existing Text-to-SQL models are trained on clean, neutral datasets, such as Spider and WikiSQL. This, to some extent, cover up social bias in models under ideal conditions, which nevertheless may emerge in real application scenarios. In this work, we aim to uncover and categorize social biases in Text-to-SQL models. We summarize the categories of social biases that may occur in structured data for Text-to-SQL models. We build test benchmarks and reveal that models with similar task accuracy can contain social biases at very different rates. We show how to take advantage of our methodology to uncover and assess social biases in the downstream Text-to-SQL task.","Automated systems are increasingly being used for numerous real-world applications, such as filtering job applications, determining credit eligibility, making hiring decisions, etc. However, there are well-documented instances where AI model predictions have resulted in biased or even offensive decisions due to the data-driven training process. The relational database stores a vast of information and in turn support applications in vast areas. With the development of benchmark datasets, such as WikiSQL and Spider, many Text-to-SQL models have been proposed to map natural language utterances to executable SQL queries. 
Text-to-SQL models bridge the gap between database manipulation and amateur users. In real-world applications, Text-to-SQL models are mainly applied by administrative industries, such as banks, schools, and governments. Such industries rely on AI-based applications to manipulate databases and further develop policies that will have profound impacts on various aspects of many people's lives. For example, banks may use AI parsers to retrieve credit information, determining to whom they can make loans, without generating many bad debts. If there are unwanted prejudices against specific demographics in applied Text-to-SQL models, these stereotypes can be significantly amplified since their retrieval results are adopted by administrative industries to draft policies. Unfortunately, large pre-trained language models (PLMs) are actually acknowledged to contain social biases towards different demographics, and these wicked biases are observed to be inherited by downstream tasks. Some may suppose that these harmful biases could be forgotten or mitigated when fine-tuned on downstream neutral data that does not contain any toxic words, specific demographic keywords, or any judgemental expressions. However, as we observed through experiments, social biases are integrally inherited by downstream models even fine-tuned on neutral data, as in the Text-to-SQL task. 
As shown in Figure, we notice that there are mainly two categories of social biases in the Text-to-SQL task. One category of social bias is that Text-to-SQL models based on large pre-trained language models would build stereotypical correlations between judgemental expressions with different demographics. The other category of social bias is that PLM-based Text-to-SQL models tend to make wrong comparisons, such as viewing some people as worse or better than others because of their exam results, income, or even ethnicity, or religion. To better quantify social biases in Text-to-SQL models, we propose a new social bias benchmark for the Text-to-SQL task, which we dub as BiaSpider. We curate BiaSpider by proposing a new paradigm to alter the Text-to-SQL dataset, Spider. For biases induced by judgmental expressions in the Text-to-SQL task, we analyze three scenarios: negative biases for demographics, positive biases for demographics, biases between different demographics under one demographic dimension. 
Main contributions of this work include: 
 
 * To the best of our knowledge, we are the first to uncover the social bias problem for the Text-to-SQL task. We formalize the definitions and principles to facilitate future research of this important problem. 
 * We analyze and categorize different kinds of social biases in the Text-to-SQL task. 
 * We propose a novel prompt paradigm for structured data, while previous works only focus on biases in unstructured data. 
 * We develop a new benchmark that can later be used for the evaluation of social biases in the Text-to-SQL models."
Training Trajectories of Language Models Across Scales,2212.09803v3,./img_ACL_2023/2212.09803v3.pdf,"Validation perplexity (PPL) of OPT models against training FLOPs. Our work suggests that models with comparable perplexity levels during training exhibit similar predictions, regardless of their scales.",,
Measuring Consistency in Text-based Financial Forecasting Models,2305.08524v2,./img_ACL_2023/2305.08524v2.pdf,Examples of four consistency transformations used in FinTrust.,"Financial forecasting has been an important and active area of machine learning research, as even the most modest advantage in predictive accuracy can be parlayed into significant financial gains. Recent advances in natural language processing (NLP) bring the opportunity to leverage textual data, such as earnings reports of publicly traded companies, to predict the return rate for an asset. However, when dealing with such a sensitive task, the consistency of models ‚Äì their invariance under meaning-preserving alternations in input ‚Äì is a crucial property for building user trust. Despite this, current financial forecasting methods do not consider consistency. To address this problem, we propose FinTrust, an evaluation tool that assesses logical consistency in financial text. Using FinTrust, we show that the consistency of state-of-the-art NLP models for financial forecasting is poor. Our analysis of the performance degradation caused by meaning-preserving alternations suggests that current text-based methods are not suitable for robustly predicting market information. All resources are available at <https: //github. com/yingpengma/FinTrust>.","NLP techniques have been used in various financial forecasting tasks, including stock return prediction, volatility forecasting, portfolio management, and more. Despite the increased performance of NLP models on financial applications, there has been pushback questioning their trustworthiness, and robustness. Recently, the causal explanation has been viewed as one of the promising directions for measuring the robustness and thus improving the transparency of models. Among them, consistency has been viewed as a crucial feature, reflecting the systematic ability to generalize in semantically equivalent contexts and receiving increasing attention in tasks such as text classification and entailment. 
Previous text-based financial forecasting methods have mostly considered stock movement prediction based on various sources of data, including financial news, analyst reports, and earnings conference calls. While most work evaluates their methods using accuracy and profit gains based on the final outcome in the market, consistency evaluation remains largely unexplored. The only exception focuses on evaluating the implicit preferences in Pre-trained Language Models (PLMs) but not the consistency in predictive models. The lack of evaluation in behavior consistency, an important characteristic of human decisions, hinders the deployment of financial forecasting models in real-world scenarios. 
The main objective of this work is to explore a wholistic measure for stock movement prediction, integrating consistency as a criterion of trustworthiness. To this end, we define behavior consistency of text-based models in the financial domain. Regarding the intrinsic characteristics of financial text data, we consider four types of logical consistency tests. As shown in Figure, these transformations include Negation Consistency, Symmetric Consistency, Additive Consistency, and Transitive Consistency. Taking negation consistency as an example, given an input ""the cost of raw materials has been greatly decreased"", if the token ""decreased"" is changed to ""increased"", the model prediction is expected to be flipped accordingly. 
Based on the above logical transformations, we introduce FinTrust, a new evaluation tool that enables researchers to measure consistency in PLMs and text-based financial forecasting models. Using FinTrust, we design three tasks to investigate the influence of these logical transformations. First, we assess implicit preference in PLMs such as BERT and FinBERT, especially for economic words. Second, we measure the accuracy of stock movement prediction on a real-world earnings call dataset after the meaning-preserving modifications. Finally, we propose a realistic trading simulation to see if simple meaning-preserving modifications can wipe out positive returns. 
Experiments on several baseline models, including previous best-performing architectures and the machine learning classifier show that all current methods exhibit a significant decline in the performance of stock movement predictions when evaluating on FinTrustcompared to their original results. Notably, some models demonstrate a level of accuracy that is even lower than that of a random guess after undergoing logical consistency transformation, and most methods fail to surpass the performance of the simplest Buy-all strategy in the trading simulation. These results suggest that existing text-based financial models have robustness and trustworthiness issues, which can limit their use in practical settings. 
To our knowledge, FinTrustis the first evaluation tool for probing if the relatively accurate stock movement prediction is based on the right logical behavior. We release our tool and dataset at Github, which can assist future research in developing trustworthy FinNLP methods."
Mitigating Label Biases for In-context Learning,2305.19148v3,./img_ACL_2023/2305.19148v3.png,"Illustration of domain-label bias. (a) On a Twitter hate speech detection dataset (TweetEval-hate; ), the model is severely biased toward predicting label hate when random in-domain (i. d. ) words from the dataset are provided as input. Random English (Eng. ) words show no such preference. (b) On a movie review dataset, SST-2, no such a bias is observed.","Various design settings for in-context learning (ICL), such as the choice and order of the in-context examples, can bias a model toward a particular prediction without being reflective of an understanding of the task. While many studies discuss these design choices, there have been few systematic investigations into categorizing them and mitigating their impact. In this work, we define a typology for three types of label biases in ICL for text classification: vanilla-label bias, context-label bias, and domain-label bias (which we conceptualize and detect for the first time). 
Our analysis demonstrates that prior label bias calibration methods fall short of addressing all three types of biases. Specifically, domain-label bias restricts LLMs to random-level performance on many tasks regardless of the choice of in-context examples. To mitigate the effect of these biases, we propose a simple bias calibration method that estimates a language model's label bias using random in-domain words from the task corpus. After controlling for this estimated bias when making predictions, our novel domain-context calibration significantly improves the ICL performance of GPT-J and GPT-3 on a wide range of tasks. The gain is substantial on tasks with large domain-label bias (up to 37%in Macro-F1). Furthermore, our results generalize to models with different scales, pretraining methods, and manually-designed task instructions, showing the prevalence of label biases in ICL. Our codes are available at <https: //github. com/fywalter/label-bias>.","Large language models (LLMs) can perform unseen tasks by conditioning on a context prompt that consists of a few training example-label pairs. However, such in-context learning ability is highly sensitive to various design settings, such as the choice and order of the in-context samples. Recently, showed that the instability of ICL largely arises from the fact that these design settings bias the model toward predicting certain answers (e.g., LLMs often predict the label of the last in-context example). As a result, the sensitivity of the results in ICL studies calls for a systematic discussion of biases in ICL and new methods to properly categorize, detect, and comprehensively mitigate various types of biases. 
In this work, we conduct a thorough investigation of biases in ICL for text classification. We start by defining a typology of three types of label biases (the model 's undesirable preference toward certain label names): vanilla label bias, context-label bias, and domain-label bias. What we term vanilla label bias captures the model' s non-contextualized preference for the label names (e.g., the common token bias mentioned by caused by different frequencies of label names in the pretraining corpus). Context-label bias summarizes the effects of the context prompt (e.g., LLMs tend to prefer the majority and last label of the in-context examples). Finally, domain-label bias captures the effects of the task corpus on the model 's predictions. 
We show that domain-label biases significantly affect a model' s prediction in ICL. For example, on a hate detection task with two nearly balanced classes, we observe that when random words are sampled from the dataset and provided as input, the model is severely biased towards predicting the label hate (Fig. (a) ). When random English words are provided as part of the input, no such effect is observed. More importantly, on many tasks with large domain-label bias, LLMs achieve no better than random performance, regardless of the choice of in-context examples (Fig. ). Moreover, we find that existing bias mitigation methods, such as Contextual Calibration (CC; ) do not combat this effect. 
To this end, we propose Domain-context Calibration (DC) to mitigate label biases in ICL. DC first estimates the effects of different label biases holistically using random words sampled from the task corpus. Specifically, we compute the probabilities assigned by the model to each label using random in-domain words as the task input (with optional real in-context learning examples prepended). Using random words limits the semantic meaning of the input, allowing us to estimate the vanilla-label and context-label biases while using in-domain words accounts for the effect of the task corpus. Then, at inference time, we use this label bias estimate to calibrate the model's output probabilities. 
We evaluate the impact of DC on 24 classification datasets, showing that DC improves the average ICL performance of GPT-J and GPT-3 by 20%and 18%. We observe substantial gains on tasks with large domain-label bias (up to 37%in Macro-F1). DC also benefits models with different scales, instruction-tuning (e.g., Instruct-GPT, ), and provided with task instructions. Finally, we show that DC improves the zero-shot prompting performance of smaller models like RoBERTa, demonstrating that label bias can be mitigated in prompt-based frameworks beyond ICL. 
Overall, our work proposes a new typology of label biases in prompt-based methods and a simple method for mitigating them. When studying ICL on a diverse collection of datasets, the results on datasets with severe label bias can obfuscate the actual behaviors of the model. Thus, rigorous design for dataset selection (that accounts for confounders) and fine-grained analysis of individual datasets are essential for effectively studying ICL."
Explicit Syntactic Guidance for Neural Text Generation,2306.11485v2,./img_ACL_2023/2306.11485v2.pdf,"Syntax-guided generation: searching the hypotheses hierarchically throughout the syntax tree in a top-down direction, starting from the root node "" <T> "". The green blocks denote the possible syntax structures at different tree depths, the blue one denotes the external modification, whereas the gray ones denote the finalized hypotheses, marking the end of search paths.","Most existing text generation models follow the sequence-to-sequence paradigm. Generative Grammar suggests that humans generate natural language texts by learning language grammar. We propose a syntax-guided generation schema, which generates the sequence guided by a constituency parse tree in a top-down direction. The decoding process can be decomposed into two parts: (1) predicting the infilling texts for each constituent in the lexicalized syntax context given the source sentence; (2) mapping and expanding each constituent to construct the next-level syntax context. Accordingly, we propose a structural beam search method to find possible syntax structures hierarchically. Experiments on paraphrase generation and machine translation show that the proposed method outperforms autoregressive baselines, while also demonstrating effectiveness in terms of interpretability, controllability, and diversity.","Natural language generation (NLG), such as paraphrase generation, text summarization, machine translation, and language models, have shown remarkable progress in the past few years. Most of the highest-performing NLG models train the model based on source-target correspondence and conduct autoregressive inference, which achieves competitive empirical performances yet deviates from a range of desirable attributes of human language generation, e.g., lack of interpretability. 
It has been shown that humans generate language by learning and manipulating language grammar, which generative grammar considers as a finite rule set that combines words to form grammatical sentences, thereby avoiding enumeration of surface sequences, which can significantly increase data sparsity and reduce learning efficiency. In this process, syntax plays a crucial role, imposing constraints on how to construct sentences. Syntax knowledge has been found implicitly contained by deep neural models and also useful for NLG tasks. However, relatively little recent work has considered explict syntax in NLG. 
Inspired by the above psycholinguistic observation, we propose a syntax-guided generation scheme, which generates text by following a well-defined grammar. As shown in Figure, instead of sequential generation, the model generates the sentence in a hierarchically top-down manner guided by the constituency parse tree, starting with the root node <T>. Syntactic categories such as noun phrases <NP> and verb phrases <VP> are integrated with tokens in the generation process, and the model simultaneously considers multiple syntax structures at each tree depth, hierarchically exploring the syntax tree for reasonable hypotheses. 
Intuitively, such a generation paradigm has the following advantages compared with autoregressive generation. First, akin to the language learning process of human beings, grammar learning breaks down non-enumerable surface sequences into finite pieces, acting as a training curriculum. Second, it provides an effective and interpretable pathway to probe into the generation process. Consequently, generation errors can be traced back to specific constituent expansion at the respective tree depth. Third, one can manipulate the generation process by exerting versatile control at arbitrary depths, e.g., modifying the translation of a verb phrase and constraining the paraphrase style with syntax templates. Forth, diverse sequences can be generated by exploring various syntax structures hierarchically throughout the syntax tree. 
We implement the above process on Transformer. As shown in Figure, the generation process proceeds under the guidance of syntactic grammar. Starting from the root node "" <T> "", the model recursively generates the infilling texts (e.g., ""he"" and ""seems <S> "") for each constituent in the current lexicalized syntax context (e. g, "" <NP> <VP>. "". ), and infills each one accordingly to construct the next-level lexicalized syntax context (e.g., ""he seems <S>. ""). The generation proceeds until there is no remaining constituent. The infilling texts are predicted by a Transformer-based model, which is trained by maximizing the likelihood of infilling texts for each constituent in the syntax context based on the source input. To explore more syntactically diverse and reasonable hypotheses during inference, we propose structural beam search, which searches promising syntax structures over the entire syntax tree in a top-down manner, as shown in Figure. 
To isolate the effect of syntax and avoid the influence of other transformation factors, we conduct experiments on two sequence-to-sequence (seq2seq) tasks with semantic equivalence between the source and target sequences: paraphrase generation and machine translation. Empirical results demonstrate that our method can generate sequences with higher quality than the seq2seq baselines. Quantitative analysis demonstrates that the generation process can be interpreted effectively. In addition, our method demonstrates the capability of executing control from both syntax templates and fine-grained manual modifications. Finally, we show the diversity advantage through both automatic evaluation and human evaluation. We release the code on <https: //github. com/yafuly/SyntacticGen>."
Weaker Than You Think: A Critical Look at Weakly Supervised Learning,2305.17442v3,./img_ACL_2023/2305.17442v3.pdf,"Performance improvement over weak labels on the test sets. Each point represents the average performance improvement of one approach over five runs. On various NLP datasets, weakly supervised methods (dots) outperform weak labels (blue line) on the test sets. However, simply fine-tuning on the available clean validation data (light green crosses) outperforms all sophisticated weakly supervised methods in almost all cases. See Appendix for experimental details.","Weakly supervised learning is a popular approach for training machine learning models in low-resource settings. Instead of requesting high-quality yet costly human annotations, it allows training models with noisy annotations obtained from various weak sources. Recently, many sophisticated approaches have been proposed for robust training under label noise, reporting impressive results. In this paper, we revisit the setup of these approaches and find that the benefits brought by these approaches are significantly overestimated. Specifically, we find that the success of existing weakly supervised learning approaches heavily relies on the availability of clean validation samples which, as we show, can be leveraged much more efficiently by simply training on them. After using these clean labels in training, the advantages of using these sophisticated approaches are mostly wiped out. This remains true even when reducing the size of the available clean data to just five samples per class, making these approaches impractical. To understand the true value of weakly supervised learning, we thoroughly analyze diverse NLP datasets and tasks to ascertain when and why weakly supervised approaches work. Based on our findings, we provide recommendations for future research.","Weakly supervised learning (WSL) is one of the most popular approaches for alleviating the annotation bottleneck in machine learning. Instead of collecting expensive clean annotations, it leverages weak labels from various weak labeling sources such as heuristic rules, knowledge bases or lower-quality crowdsourcing. These weak labels are inexpensive to obtain, but are often noisy and inherit biases from their sources. Deep learning models trained on such noisy data without regularization can easily overfit to the noisy labels. Many advanced WSL techniques have recently been proposed to combat the noise in weak labels, and significant progress has been reported. On certain datasets, they even manage to match the performance of fully-supervised models. 
In this paper, we take a close look at the claimed advances of these WSL approaches and find that the benefits of using them are significantly overestimated. Although they appear to require only weak labels during training, a substantial number of clean validation samples are used for various purposes such as early-stopping and meta-learning. We cast doubt on this practice: in real-world applications, these clean validation samples could have instead been used for training. To address our concern, we explore fine-tuning models directly on the validation splits of eight datasets provided by the WRENCH benchmark and compare it to recent WSL algorithms. The results are shown in Figure. Interestingly, although all WSL models generalize better than the weak labels, simply fine-tuning on the validation splits outperforms all WSL methods in almost all cases, sometimes even by a large margin. This suggests that existing WSL approaches are not evaluated in a realistic setting and the claimed advances of these approaches may be overoptimistic. In order to determine the true benefits of WSL approaches in a realistic setting, we conduct extensive experiments to investigate the role of clean validation data in WSL. Our findings can be summarized as follows: 
 
 * Without access to any clean validation samples, all WSL approaches considered in this paper fail to work, performing similarly to or worse than the weak labels (). 
 * Although increasing the amount of clean validation samples improves WSL performance (), these validation samples can be more efficiently leveraged by directly training on them, which can outperform WSL approaches when there are more than 10 samples per class for most datasets (). 
 * Even when enabling WSL models to continue training on clean validation samples, they can barely beat an embarrassingly simple baseline which directly fine-tunes on weak labels followed by fine-tuning on clean samples. This stays true with as few as 5 samples per class (). 
 * The knowledge encoded in pre-trained language models biases them to seek linguistic correlations rather than shallow rules from the weak labels; further fine-tuning the pre-trained language models with contradicting examples helps reduce biases from weak labels (). 
Altogether, we show that existing WSL approaches significantly overestimate their benefits in a realistic setting. We suggest future work to (1) fully leverage the available clean samples instead of only using them for validation and (2) consider the simple baselines discussed in this work when comparing WSL approaches to better understand WSL's true benefits."
"Prompt Tuning Pushes Farther, Contrastive Learning Pulls Closer: A Two-Stage Approach to Mitigate Social Biases",2307.01595v1,./img_ACL_2023/2307.01595v1.pdf,"The motivation of CCPA. For the new input samples, the PLM's performance of the augmented sample training is stronger than that of the unaugmented sample training.","As the representation capability of Pre-trained Language Models (PLMs) improve, there is growing concern that they will inherit social biases from unprocessed corpora. Most previous debiasing techniques used Counterfactual Data Augmentation (CDA) to balance the training corpus. However, CDA slightly modifies the original corpus, limiting the representation distance between different demographic groups to a narrow range. As a result, the debiasing model easily fits the differences between counterfactual pairs, which affects its debiasing performance with limited text resources. In this paper, we propose an adversarial training-inspired two-stage debiasing model using Contrastive learning with Continuous Prompt Augmentation (named CCPA) to mitigate social biases in PLMs 'encoding. In the first stage, we propose a data augmentation method based on continuous prompt tuning to push farther the representation distance between sample pairs along different demographic groups. In the second stage, we utilize contrastive learning to pull closer the representation distance between the augmented sample pairs and then fine-tune PLMs' parameters to get debiased encoding. Our approach guides the model to achieve stronger debiasing performance by adding difficulty to the training process. Extensive experiments show that CCPA outperforms baselines in terms of debiasing performance. Meanwhile, experimental results on the GLUE benchmark show that CCPA retains the language modeling capability of PLMs.","Pre-trained Language Models (PLMs) have demonstrated outstanding performance in recent years and have been widely used in natural language understanding tasks. However, the powerful language modeling capability enables PLMs to learn good representations from large-scale training corpora while capturing human-like social biases. Recent studies have demonstrated that the representations encoded by PLMs learn social biases specific to demographic groups (e.g., gender, race, religion) and can be amplified to downstream tasks, leading to unfair outcomes and adverse social effects. As a result, mitigating social biases in PLMs 'encoding can improve the fairness of NLP systems significantly. 
Most existing debiasing techniques first need to construct sample pairs using Counterfactual Data Augmentation (CDA) to balance the training corpora. The general approach of CDA is to replace the original corpus with attribute words (e.g., he/she, man/woman) specific to different demographic groups. For example, RCDA uses a generator to generate a large number of antisense sentences and then uses a discriminator to evaluate the quality of the original and antisense samples jointly. FairFil obtains a pair of positive sample sentences by replacing the attribute words in the training corpora with the antonyms and then uses contrastive learning to train a filter for debiasing. Auto-Debias uses pairs of attribute words as training corpora, amplifies the bias between sample pairs by searching biased prompt texts in the Wikipedia vocabulary, and then performs semantic alignment using Jensen-Shannon divergence. These methods aim to mitigate social biases between different demographic groups by narrowing the representation distance between sample pairs. However, CDA slightly modifies the original corpus, limiting the representation distance between different demographic groups to a narrow range. As a result, the debiasing model is easy to overfit the difference between counterfactual pairs, which affects its learning ability with limited text resources. As shown in Figure, it is difficult for PLMs to achieve the ideal debiasing performance for newly input samples with greater difficulty. 
In this work, we propose a two-stage debiasing method using Contrastive learning with Continuous Prompt Augmentation (named CCPA) to mitigate social biases in PLMs' encoding. Inspired by adversarial training, our approach improves the debiasing ability of PLMs by first amplifying and then attenuating the bias between different demographic groups. Specifically, we first use CDA to replace attribute words in the original training corpus to construct counterfactual pairs corresponding to different demographic groups. In the first stage, we augment the positive sample pairs with continuous prompt tuning to increase the distance between them to amplify the biases between different demographic groups. In the second stage, we utilize contrastive learning to pull the distance between the positive sample pairs to attenuate the biases between different demographic groups. CCPA increases the difficulty of model fitting by expanding the representation space between sample pairs. We believe that difficult learning experiences make the model more powerful, thus improving the debiasing ability of PLMs training in corpora with limited resources. Our main contributions are as follows: 
 
 * We propose the CCPA debiasing framework that combines prompt tuning and contrastive learning to learn a debiased PLM representation. The PLM 's parameters are fixed in the first stage, and a generator encoding continuous prompts is trained. In the second stage, the prompts are fixed, and the PLM' s parameters are fine-tuned using contrastive learning. 
 * We propose data augmentation using continuous prompts to achieve excellent debiasing performance using small training data rather than relying on a large external corpus. Given that continuous prompts may cause the representation distance between sample pairs to be too far apart, causing the semantic space to degrade, we propose constraining the prompt tuning using the Mahalanobis Distance to keep the semantic space as stable as possible. 
 * We train CCPA on several real-world corpora and mitigate bias on the most common gender bias. The results on BERT and DistilBERT show that CCPA is superior to state-of-the-art models. In addition, we test the downstream tasks on the GLUE benchmark, and show that CCPA retains the language modeling capability while improving the PLMs' fairness. 
The overall architecture of CCPA. In the first stage, the parameters of PLM encoder E (¬∑) are fixed and a prompt generator G (¬∑) encoding the continuous prompts is trained, where the goal is to enlarge the bias of sentence pairs. In the second stage, the parameters of the prompt generator G (¬∑) are fixed and the parameters of PLM encoder E (¬∑) are fine-tuned using contrastive loss. Ultimately, we can obtain the debiased PLM encoder E (¬∑)."
Towards Understanding Omission in Dialogue Summarization,2211.07145v2,./img_ACL_2023/2211.07145v2.pdf,The percentage of candidate summaries with omission errors. We report the results of six adopted models on the test set of each dialogue domain.,"Dialogue summarization aims to condense the lengthy dialogue into a concise summary, and has recently achieved significant progress. However, the result of existing methods is still far from satisfactory. Previous works indicated that omission is a major factor in affecting the quality of summarization, but few of them have further explored the omission problem, such as how omission affects summarization results and how to detect omission, which is critical for reducing omission and improving summarization quality. Moreover, analyzing and detecting omission relies on summarization datasets with omission labels (i.e., which dialogue utterances are omitted in the summarization), which are not available in the current literature. In this paper, we propose the Olds dataset, which provides high-quality Omission Labels for Dialogue Summarization. By analyzing this dataset, we find that a large improvement in summarization quality can be achieved by providing ground-truth omission labels for the summarization model to recover omission information, which demonstrates the importance of omission detection for omission mitigation in dialogue summarization. Therefore, we formulate an omission detection task and demonstrate our proposed dataset can support the training and evaluation of this task well. We also call for research action on omission detection based on our proposed datasets. Our dataset and codes are publicly available.","With the exponential increase in the volume of conversational messages from daily life, there is a growing demand for dialogue summarization, which compresses lengthy interactions into a more concise and structured piece of text while preserving the most important and relevant information. Recent years have witnessed significant progress in abstractive dialogue summarization, especially using large-scale pre-trained language models. Despite the advances in a high level of fluency and coherence, existing models are still prone to generate defective summaries that limit their practical usage. Previous works have investigated the taxonomy of errors involved in output summaries, and human evaluations revealed that the majority of errors fall into the category of omission, which often leads to incomplete summaries where critical facts are lost. However, few of these works have further analyzed the omission problem, let alone addressing this problem. 
To reduce omission rate and improve summarization quality, a comprehensive analysis on omission problem (e.g., how omission affects summary results) and a precise omission detection (i.e., to locate which dialogue utterances are omitted in the summarization) is important. However, there are no omission related datasets in dialogue summarization literature to support such analysis and detection. Hence, in this work, we construct the Olds dataset, which provides high-quality Omission Labels for Dialogue Summarization. Our dataset is built upon five existing benchmarks covering different domains. For each dialogue, we use different abstractive models to generate diverse candidates and propose a reference-based strategy to automatically label omissions for these candidates. The human evaluation indicates that our Olds dataset presents a high quality of omission labels. 
Based on the curated Olds dataset, we comprehensively investigate the omission problem in dialogue summarization from multiple aspects. First, we analyze the proportion of candidates with omission errors and the position distribution of omitted information in dialogues. The results reveal that omission is a severe problem that frequently occurs in dialogue summarization. Second, we measure the correlation between the omission rate and multiple reference-based metrics (e.g., ROUGE and BERTScore), discovering that omission is one of the decisive factors influencing the summary evaluation results. Third, we explore the potential performance improvement brought by utilizing the omission information in a post-editing manner. The analyses probe that candidate summaries could be effectively improved as long as the model is provided with the omitted dialogue utterances. Hence, how to accurately locate omission information in dialogue naturally becomes a critical question. 
To pave the way to omission mitigation and summary improvement, we formulate the task of omission detection, which aims to identify the omitted utterance given the whole dialogue utterances and the generated summary with potential omission. In addition, we present three different frameworks as baselines for the omission detection task, including pair-wise classification, sequence labeling, and pointer network extraction. Experimental analyses on the Olds dataset reveal that omission detection, as a promising direction to assessment and improvement for dialogue summarization, poses significant values and challenges. 
The contributions of our paper are as follows: 
 
 * We propose Olds, a dataset with high-quality omission labels for dialogue summarization, to facilitate the research on the omission problem. 
 * Based on Olds, we systematically analyze the omission problem and demonstrate the significance of omission in dialogue summarization. 
 * We introduce the omission detection task that paves the way to omission mitigation and summary improvement. We design 3 frameworks as baselines and conduct comprehensive analyses to provide possible directions for solving this task."
Vision Language Pre-training by Contrastive Learning with Cross-Modal Similarity Regulation,2305.04474v3,./img_ACL_2023/2305.04474v3.pdf,"Conceptual illustration of cross-modal similarity regularization. Sub-figure (a) demonstrates that (partial) false negatives commonly exist in cross-modal training data. In sub-figure (b), negative samples will be equally pushed away from the anchor in conventional cross-modal contrastive learning, leading to data deficiency given these false ones. Instead, we take the first step to contrast negatives according to cross-modal similarity (represented by a set of concentric circles in sub-figure (c) ), keeping good while removing harmful effects of (partial) false negative samples.","Cross-modal contrastive learning in vision language pretraining (VLP) faces the challenge of (partial) false negatives. In this paper, we study this problem from the perspective of Mutual Information (MI) optimization. It is common sense that InfoNCE loss used in contrastive learning will maximize the lower bound of MI between anchors and their positives, while we theoretically prove that MI involving negatives also matters when noises commonly exist. Guided by a more general lower bound form for optimization, we propose a contrastive learning strategy regulated by progressively refined cross-modal similarity, to more accurately optimize MI between an image/text anchor and its negative texts/images instead of improperly minimizing it. Our method performs competitively on four downstream cross-modal tasks and systematically balances the beneficial and harmful effects of (partial) false negative samples under theoretical guidance.","Large-scale pre-trained vision-language models have recently achieved tremendous success on a wide range of cross-modal tasks. Self-supervised learning (SSL) have impressively contributed to vision-language pre-training (VLP) due to its capability of leveraging large-scale image-text pairs without annotations. More recently, Self-supervised Multi-modal Contrastive Learning (SMCL) triggered great progress by conducting cross-modal alignment. SMCL consists of image-to-text and text-to-image contrastive learning, e.g., with the InfoNCE loss. Taking the text-to-image one as an example, given a text-image pair (T, I), I will be treated as the positive sample for the anchor T, and other images in a mini-batch of text-image pairs will be regarded as negatives. The training objective is to attract the positive to the anchor while repelling all the negative samples. 
However, this contrasting strategy can be problematic given the many-to-many correspondences between images and texts. As shown in Figure (a), a text can be semantically paired with multiple images. In this scenario, though images 
 I_4andI_5are treated as negatives, they are actually semantically consistent (or partially consistent) with the text anchor ""A bird in the tree. "" The (partial) false negatives likeI_4andI_5will inevitably hinder the contrasting effect, yielding sub-optimal cross-modal representations. 
 
 Some pioneering efforts have addressed the noisy image-text pairing problem in VLP pre-training datasets, by feeding the contrastive loss with soft labels in a self-distillation manner. Though these methods can address the problem of false negatives to some extent, the specific harmful effect of false negatives remains far from being systematically studied. For example, based on these methods (e.g., ALBEF), we can easily improve the performances of downstream tasks by simply filtering false negatives, as shown in Table. 
 
 In this paper, we investigate the problem of false negatives from the perspective of Mutual Information (MI) optimization. The InfoNCE loss used in contrastive learning has been proved to maximize the lower bound of MI between anchors and their positives. We revisit the theoretical proof in the presence of non-negligible false negatives. Defining the MI between anchors and positives as MI-P, and the counterpart between anchors and negatives as MI-N, we derive a more general conclusion (see the appendix) that optimizing InfoNCE is equivalent to maximizing the lower bound of (MI-P-MI-N). The finding suggests that MI-N will be minimized (e.g., as close to zero as possible), even though some negatives may semantically match the anchor. The theoretical analyses explain the deficiency of the vanilla contrasting strategy on the one hand, and inspire us with another derivation (appendix) that guarantees proper MI optimization for negative samples on the other hand. 
 
 Guided by these theoretical analyses, we propose a novel contrasting strategy regulated by cross-modal similarity. We hypothesize that the MI between an image and text positively correlates with their semantic similarity. Therefore, we introduce a contrastive weight, which is derived based on cross-modal similarity and progressively refined with training, for each negative sample as a contrasting regulator. This regulator will guide the model to optimize MI-N properly, keeping it from being unexpectedly minimized and thus yielding a more semantically structural representation space. We equip our proposed contrasting strategy on ALBEF framework and evaluate it on various representative vision-language downstream tasks, including Visual Question Answering (VQA), Cross-modal Retrieval, Zero-shot Cross-modal Retrieval, and Natural Language for Visual Reasoning (NLVR). The experimental results show that our adjusted contrastive learning significantly improves their performances. 
 
 In summary, our contributions are: 
 
 
 * We investigate the issue of false negatives in cross-modal contrastive learning from the perspective of Mutual Information (MI) optimization. We deduce a more general form of MI's lower bound for InfoNCE loss in the presence of non-negligible false negatives, revealing that the MI between (partial) false negatives and anchors is improperly minimized. 
 
 
 * Based on a theoretical derivation that guarantees appropriate MI optimization for negative samples, we propose a novel contrasting strategy by attaching each negative sample with a progressively refined contrastive weight based on cross-modal similarity. 
 
 
 * Applying the contrasting strategy to VLP methods yields impressive performance improvement on various downstream tasks, and demonstrates our contrasting strategy systematically balances the positive and negative impacts of false negatives."
Towards Benchmarking and Improving the Temporal Reasoning Capability of Large Language Models,2306.08952v2,./img_ACL_2023/2306.08952v2.pdf,Illustration of three levels of understanding towards time.,"Reasoning about time is of fundamental importance. Many facts are time-dependent. For example, athletes change teams from time to time, and different government officials are elected periodically. Previous time-dependent question answering (QA) datasets tend to be biased in either their coverage of time spans or question types. In this paper, we introduce a comprehensive probing dataset Temp¬≠ Reasonto evaluate the temporal reasoning capability of large language models. Our dataset includes questions of three temporal reasoning levels. In addition, we also propose a novel learning framework to improve the temporal reasoning capability of large language models, based on temporal span extraction and time-sensitive reinforcement learning. We conducted experiments in closed book QA, open book QA, and reasoning QA settings and demonstrated the effectiveness of our approach.","In recent years, large language models (LLMs) have achieved significant success in many natural language processing (NLP) tasks, such as natural language understanding (NLU), information extraction (IE), and question answering (QA). Many facts and answers are dependent on their related time scopes, such as 'What soccer club was Lionel Messi playing for? '. has pointed out around 48%of the qualifiers in the widely-used knowledge base Wikidata are time-related. That is, a significant number of the knowledge triples in the Wikidata KB have their expiry dates. Correct understanding of temporal concepts is crucial for language models to be successful in real-world applications. To examine the temporal reasoning capabilities of LLMs, the Time-Sensitive Question Answering (TSQA) task has been proposed and several evaluation datasets were published for research purposes. The Time-sensitive QA dataset and the Temp¬≠ LAMAdataset were constructed based on the Wikidata temporal KB. StreamingQA was constructed by news article collections in English WMT challenges from 2007 to 2020. One consensus of prior work is that time-sensitive QA is a challenging task and its performance is still far below human performance. However, they did not provide a systematic analysis of LM 's temporal reasoning capability. In this paper, we aim to systematically analyze such capability and identify the strengths and weaknesses of LMs on temporal reasoning. 
As shown in Figure, humans' understanding of temporal reasoning could be broken down into three levels: time-time (L1) relation, time-event (L2) relation, and event-event (L3) relation. For the understanding of time-time relations, humans can easily determine the relation between two timestamps 
 t_1andt_2on the time axis. For example, when humans are asked 'What is the year after 2020? ', they are able to answer this question without any external information. This level of temporal understanding could be regarded as a set of logic rules and is highly generalizable across different times, while this type of reasoning was overlooked in prior TSQA research (; ; ). For time-event relations, the reasoning process requires grounding events to their specific time ranges. In this paper, the concept of events includes time-dependent facts. Humans either memorize a large number of time-event pairs or need to rely on relevant contexts to deduce such relations. An example question is 'What soccer club was Lionel Messi playing for in Dec 2010? ', where a time is specified in the question, and the answer changes based on the given time. If this question is posed to a person who is unfamiliar with sports, this person also needs external information to provide the answer. Answering this type of questions requires information retrieval and temporal grounding. For event-event relations, there are multiple reasoning paths to determine such relations. One possible path is to first identify the timestamps of different events and perform time-time reasoning. Another path is to search for the textual cues of relative relation, such as 'before', 'after', 'during', and 'simultaneous'. 
 
 We first conducted a simple preliminary experiment for probing LLM 's L1 temporal reasoning capability. We found that not only do LMs perform poorly on the time-time relation task, but they are also heavily biased in favor of contemporary years (2000 - 2020). This may be due to the imbalanced term frequencies in the pre-training corpora. Most LLMs (such as BERT, GPT, and T5) are pre-trained on raw texts from a snapshot at a specific timestamp, typically around 2018 to 2020. Therefore, the time expression vocabulary is highly dependent on term frequencies in the pre-training corpora. Typically, year tokens that occur frequently will have a smaller index in the vocabulary and the uncommon years generally have larger indices or will be split into subtokens. Take the T5 tokenizer as an example, the year' 2014 'is tokenized as' 2014 ', however, the year' 2021 'is tokenized as' 20 'and' 21 '. This means that language models only learn the co-occurrences of time expressions and their context. 
 
 Given such findings, we found that the recently proposed TSQA Temp¬≠ LAMA dataset has several main drawbacks. Firstly, the time span of the dataset is only from 2010 to 2020, which is a highly biased distribution in favor of LM. Secondly, it only focused on the questions of time-event relations. To overcome these shortcomings, we created a more comprehensive TSQA benchmark Temp¬≠ Reason, which spans a longer time range and all three types of temporal understanding. We conducted comprehensive experiments in closed book QA, open book QA, and reasoning QA settings. We found that the temporal reasoning capabilities of LLMs are highly variable with respect to the reference time in the question. LLMs perform well on the contemporary years and poorly on low-resource years. 
 
 Moreover, we proposed a novel temporal learning framework based on temporal span extraction and time-sensitive reinforcement learning. Our proposed framework encourages LMs to generate temporally correct answers while penalizing predictions that do not satisfy the temporal constraints. Experimental results showed that our proposed benchmark Temp¬≠ Reason provides a more comprehensive evaluation for LM' s temporal reasoning capability and our model consistently outperforms strong baselines."
PESCO: Prompt-enhanced Self Contrastive Learning for Zero-shot Text Classification,2305.14963v1,./img_ACL_2023/2305.14963v1.png,"In this example, there are three classes, whose label descriptions are ""sports"", ""business"", and ""world"" respectively. We convert the descriptions into label-prompts by placing them into a template. The model predicts a label whose label-prompt embedding is the most similar to the document embedding.","We present PESCO, a novel contrastive learning framework that substantially improves the performance of zero-shot text classification. We formulate text classification as a neural text matching problem where each document is treated as a query, and the system learns the mapping from each query to the relevant class labels by (1) adding prompts to enhance label matching, and (2) using retrieved labels to enrich the training set in a self-training loop of contrastive learning. PESCO achieves state-of-the-art performance on four benchmark text classification datasets. On DBpedia, we achieve 98.5%accuracy without any labeled data, which is close to the fully-supervised result. Extensive experiments and analyses show all the components of PESCO are necessary for improving the performance of zero-shot text classification.","Text classification is the task of assigning relevant category labels to each input document. It is an important problem in machine learning research with a wide spectrum of applications, including sentiment analysis, question answering, and intent classification, etc. Recently, deep neural networks have obtained remarkable improvements in text classification, including CNNs, RNNs, Transformers, and more, thanks to the successful modeling of contextualized representations. 
Despite the remarkable progress, training well-performing neural classifiers still requires a large amount of human-labeled documents, which is costly and time-consuming, especially for new application domains. This stimulates the recent trend of exploring self-supervised pre-training neural models on text classification tasks. In particular, pre-trained language models (PTLMs) clearly stand out from other methods owing to the pre-training on large-scale unlabeled data. Nevertheless, how to adapt PTLMs to downstream tasks with less supervision remains an open question for the research community, inviting new ideas to explore. 
Prompt-based learning has been actively studied to better adapt PTLMs to downstream tasks with the goal of reducing human annotation effort. For example, PET is a prompt-based method for few-shot text classification. It formulates the task as a Cloze Test, where a PTLM is used to predict the output label (s) by completing a prompt concatenated right after an input document. For example, the sentiment of a product review is highly likely to be positive if a PTLM fills the word ""good"" into the following input: [Review] |It is a _ product.
 
 This example shows that prompt-based learning could unleash the potential power of a PTLM by constructing the input format of a downstream task in a way that closely resembles the PTLM pre-training objective, which is masked language modeling (MLM) in this case. 
 
 Motivated by the recent success of prompt-based learning, we propose PESCO, a novel self-training framework for zero-shot classification that uses prompts to enhance performance. The self-training consists of two iterative steps, pseudo-label prediction and model update. To make label descriptions more informative, we first put label descriptions into some predefined prompts and call the enhanced descriptions label-prompts. As depicted in Figure, to predict the pseudo-label of a document, PESCO formulates text classification as a neural matching task. A pre-trained text encoder maps both documents and label-prompts into a shared embedding space. A label whose embedding is closest to the document is predicted as the pseudo-label. 
 
 To effectively update the text encoder with pseudo-labels, we propose the Prompt-enhanced Label-aware Cloze Test (PLCT), a contrastive learning framework for self-training. The text encoder is trained to match a document and the text relevant to its pseudo-label. The relevant texts include pseudo-label prompts and the key sentences from the documents assigned to the same pseudo-label. The key sentence of each document is the sentence most related to its pseudo-label. 
 
 In our experiments, we show that the iterative self-training consistently improves the classification performance compared to the same model without self-training and that our proposed approach substantially outperforms other strong zero-shot classification baselines. On some datasets, the zero-shot results are even on par with a fully supervised baseline. On the Dbpedia dataset, in particular, PESCO achieves 98.5%accuracy without any labeled data. 
 
 In summary, the contributions of this paper are twofold: 
 
 
 * We explore text classification in a neural matching formulation enhanced by prompts. We demonstrate that even without any finetuning on the text encoder, this straightforward formulation is an effective method for zero-shot text classification. 
 
 
 * The potential of contrastive learning for self-training has not been explored. We show that this is a promising direction for self-training and can achieve state-of-the-art performance on zero-shot text classification."
Exploring Large Language Models for Classical Philology,2305.13698v1,./img_ACL_2023/2305.13698v1.pdf,"Example sentence (Hom. Il. 1.148) with corresponding dependency, lemma, UPoS, and XPoS labels. Translation: Then watching him grimly from under his brows, swift-footed Achilles spoke to him.","Recent advances in NLP have led to the creation of powerful language models for many languages including Ancient Greek and Latin. While prior work on Classical lan¬≠ gua¬≠ ges unanimously uses BERT, in this work we create four language models for Ancient Greek that vary along two dimensions to study their versatility for tasks of interest for Classical languages: we explore (i) encoder-only and encoder-decoder architectures using RoBERTaand T5as strong model types, and create for each of them (ii) a monolingual Ancient Greek and a multi¬≠ lin¬≠ gu¬≠ al instance that includes Latin and English. We evaluate all models on morphological and syntactic tasks, including lemmatization, which demonstrates the added value of T5's decoding abilities. We further define two probing tasks to investigate the knowledge acquired by models pre-trained on Classical texts. Our experiments provide the first benchmarking analysis of existing models of Ancient Greek. Results show that our models provide significant improvements over the SoTA. The systematic analysis of model types can inform future research in designing language models for Classical languages, including the development of novel generative tasks. We make all our models available as community resources, along with a large curated pre-training corpus for Ancient Greek, to support the creation of a larger, comparable model zoo for Classical Philology. Our models and resources are available at <https: //github. com/Heidelberg-NLP/ancient-language-models>.","Since the beginning of the creation of the Index Thomisticus in 1946 and the publication of the Concordance to Livy, Classical Philology has been revitalized by the digital revolution. Today, numerous efforts have been undertaken to make Classical texts digitally available, annotate, and automatically process them. E.g., the Classical Language Toolkit (CLTK, ) offers various tools to process pre-modern languages, in particular Latin and pre-modern Greek. 
Recently, we see a surge of the first pre-trained contextualized language models (PLMs) for Classical languages: Latin BERT has been proposed by, Ancient Greek (AG) BERT by. Lately, a second AG BERT has been proposed by. However, both AG BERT models have been pre-trained on a comparatively small pre-training dataset. Moreover, they have been initialized from Modern Greek BERT, which limits them to the modern Greek alphabet, ignoring the diacritics of Ancient Greek. 
Although numerous richly annotated treebanks are available for Latin and AG, systems have, by now, not been evaluated on a shared benchmark. Given that two popular treebanks for AG have been integrated into Universal Dependencies, it is surprising that researchers working on AG do not compare to benchmarking results of, e.g., . Hence, a thorough assessment of the performance of the existing models is necessary in order to compare and evaluate their effectiveness for this underexplored language. 
While BERT models are known to achieve high performance on a wide range of tasks, encoder-decoder models or multilingual models may often be a better choice, depending on the task. In this work, we explore a variety of language models for Classics in general and Ancient Greek in particular: We introduce GrŒµTa, GrŒµBERTa, PhilBERTa, and PhilTa, four PLMs for Classics. GrŒµBERTaand GrŒµTaare RoBERTa and T5 models trained on Ancient Greek texts, respectively. PhilBERTaand PhilTaare their trilingual counterparts pre-trained on Greek as well as Latin and English data. 
We explore the advantages of [label= () ] * the two model architectures in 
 * monoand multilingual pre-training for the mid-resource language Ancient Greek on a variety of morphological, syntactic, and semantic tasks, helping to answer questions, such as: When to choose one architecture over the other? or: How does multilinguality affect a language model? 
Moreover, we publish the first wide-ranging benchmark results to compare our models for AG and Latin to the relevant prior work, establishing new SoTA results for both languages. 
In summary, we aim to unify and push forward the current research landscape at the intersection of Classics and NLP with the following contributions: 
 * We introduce four pre-trained language models for Classics: GrŒµ (BERT|T) aand Phil (BERT|T) a. To our know¬≠ ledge, we are the first to develop encoder-de¬≠ co¬≠ der models for Classics, and multi¬≠ lin¬≠ gu¬≠ al models tailored to both Latin and Greek. 
 * We evaluate the already existing and our proposed models on several tasks, making many of them comparable for the first time. Furthermore, we outperform the existing Ancient Greek BERTmodels by a notable margin. 
 * Our evaluation sheds light on the differences between encoders like RoBERTaand encoders of encoder-decoder models like T5as well as on the influence of multilinguality on the mid-resource language Ancient Greek. By offering novel model types for AG, we aim to inspire new research and application tasks. 
 * We develop and publish a large-scale, high-quality pre-training corpus for AG as a contribution to the community."
Hearing Lips in Noise: Universal Viseme-Phoneme Mapping and Transfer for Robust Audio-Visual Speech Recognition,2306.10563v1,./img_ACL_2023/2306.10563v1.pdf,"Illustration of noisy audio-visual speech recognition. (a) Mainstream AVSR approaches with noise adaptation. (b) Our framework constructs viseme-phoneme mapping for modality transfer, which restores clean audio from visual signals to enable speech recognition under any noisy conditions.","Audio-visual speech recognition (AVSR) provides a promising solution to ameliorate the noise-robustness of audio-only speech recognition with visual information. However, most existing efforts still focus on audio modality to improve robustness considering its dominance in AVSR task, with noise adaptation techniques such as front-end denoise processing. Though effective, these methods are usually faced with two practical challenges: 1) lack of sufficient labeled noisy audio-visual training data in some real-world scenarios and 2) less optimal model generality to unseen testing noises. In this work, we investigate the noise-invariant visual modality to strengthen robustness of AVSR, which can adapt to any testing noises while without dependence on noisy training data, a. k. a. , unsupervised noise adaptation. Inspired by human perception mechanism, we propose a universal viseme-phoneme mapping (UniVPM) approach to implement modality transfer, which can restore clean audio from visual signals to enable speech recognition under any noisy conditions. Extensive experiments on public benchmarks LRS3 and LRS2 show that our approach achieves the state-of-the-art under various noisy as well as clean conditions. In addition, we also outperform previous state-of-the-arts on visual speech recognition task.","The world surrounding us involves multiple modalities, including vision, audio, text, etc. , which complement each other and jointly comprise human perception. Audio-visual speech recognition (AVSR) leverages both audio and visual modalities to understand human speech, which provides a promising solution to ameliorate the noise-robustness of audio-only speech recognition with noise-invariant lip movement information. 
However, most existing efforts still focus on audio modality to improve noise-robustness considering its dominance in AVSR, where audio modality contains much richer information to represent speech content than visual modality. Current mainstream approaches introduce noise adaptation techniques to improve robustness, inspired by robust speech recognition. Most of them leverage noise-corrupted training data to strengthen robustness, and recent works extend it to self-supervised learning scheme. Based on that, latest works introduce speech enhancement as front-end to denoise before recognition. Despite the effectiveness, these methods are usually faced with two practical challenges. First, they require abundant labeled noisy audio-visual data for network training, which is not always available in some real-world scenarios. Second, the well-trained model may not adapt to new-coming noise scenes in practical applications, resulting in less optimal model generality. Therefore, our research idea in this paper is leveraging visual modality to develop a general noise-robust AVSR system while without dependence on noisy training data. 
We may gain some inspirations from human perception mechanism of noisy audio-visual speech. Neuroscience studies find that human brain will unconsciously rely more on the lip movement to understand speech under noisy conditions (a. k. a. , McGurk Effect, ). During this process, instead of directly recognizing lip movement, human brain will first transfer it to speech signal in auditory cortex for further understanding. With prior knowledge of lip-audio mapping, human brain can restore informative clean audio from lip movement under any noisy conditions to aid in speech understanding. 
Motivated by above observations, we propose a universal viseme-phoneme mapping approach (UniVPM) to implement modality transfer, which can restore clean audio from lip movement to enable speech recognition under any noisy conditions. We first build two universal memory banks to model all the visemes and phonemes via online balanced clustering. Based on that, an adversarial mutual information estimator is proposed to construct strong viseme-phoneme mapping, which enables final lip-to-audio modality transfer via retrieval. As a result, our system can adapt well to any testing noises while without noisy training data. Empirical results show the effectiveness of our approach. Our contributions are summarized as: 
 
 * We present UniVPM, a general noise-robust AVSR approach investigated on visual modality, which can adapt to any testing noises while without dependence on noisy training data, a. k. a. , unsupervised noise adaptation. 
 * We build two universal banks to model all the visemes and phonemes via online balanced clustering, followed by an adversarial mutual information estimator to construct strong mapping between them, which enables modality transfer to restore clean audio from lip movement for speech recognition under any noises. 
 * Our UniVPM outperforms previous state-of-the-arts on LRS3 and LRS2 benchmarks. Extensive experiments also show its superiority on visual speech recognition (VSR) task."
An Extensible Plug-and-Play Method for Multi-Aspect Controllable Text Generation,2212.09387v2,./img_ACL_2023/2212.09387v2.pdf,"Overview of our proposed extensible plug-and-play method for multi-aspect controllable text generation. First, plugins are trained on single-aspect labeled data separately (left). Then, arbitrary plugins can be combined by simply concatenation and plugged into the pretrained model to satisfy corresponding combinations of constraints (right). Due to the separate training of different plugins, the cost of extending a new constraint is relatively low. Besides, our approach restrains the accumulation of mutual interference, alleviating the degeneration of constraints.",,"Multi-aspect controllable text generation (MCTG), which aims at generating fluent text while satisfying multiple aspects of constraints simultaneously, has attracted increasing attention in recent years. To effectively control diverse aspects such as sentiment, topic, and detoxification, extensive efforts have been devoted to the task, including methods based on conditional generative model, decoding-time regulation, and parameter efficient tuning. 
Despite their effectiveness, existing methods still suffer from low extensibility. Ideally, suppose a multi-aspect controllable text generation system has learned how to control sentiment, topic and keywords separately, it should be extensible to any combinations of the three aspects, e.g., generating a sports-themed sentence with negative sentiment containing keywords ""New York"" (see Figure). Moreover, an extensible system should also be easily extended to control new aspects in a plug-and-play way. However, it is non-trivial for existing methods to achieve this goal. Specifically, the dedicated conditional generative models mostly need to be trained from scratch or finetuned when facing new aspect combinations. The decoding-time regulation based methods intervene in the probabilities of sentences by light-weight attribute classifiers or language models during inference, which significantly impairs text fluency when multiple distributions are interpolated. The parameter efficient tuning based methods control aspects by inserting trainable prompts or prefixes into the model, referred to as plugins. By leveraging one plugin for each aspect, these methods can naturally work in a plug-and-play way, showing better potential to achieve high extensibility. 
However, existing studies show that directly combining multiple plugins results in significantly lower controllability of the corresponding aspects than before combining (i.e., attribute degeneration). argue that mutual interference of the plugins is the major reason for attribute degeneration, which is further justified by our theoretical and empirical analyses. Previous works alleviate the problem by introducing connectors to connect multiple plugins, latent variables to represent the unsupervised aspects, or objectives to narrow the discrepancy of aspects. However, these methods require joint training of plugins and are designed for pre-defined closed-set constraints. In consequence, their extensibility is limited. 
In this paper, we propose an extensible plug-and-play method, Prompt Gating, for multi-aspect controllable text generation. We derive a theoretical lower bound for the interference of plugins and reveal that it accumulates with the increasing number of layers where prefixes are inserted. Based on these findings, we propose attaching trainable gates to the plugins, which normalize the interventions of plugins. As a result, the mutual interference has been significantly reduced such that the control of arbitrary combinations of aspects can be realized by simply concatenating the corresponding plugins. Thus, our method is both extensible and plug-and-play. Moreover, we represent the constraints of the aspects in textual form, which makes our method applicable not only to categorical aspects (e.g., sentiment) but also to free-form aspects (e.g., lexical constraint). 
Our contributions are three-fold: 
 
 * We propose an extensible plug-and-play method, Prompt Gating, for multi-aspect controllable text generation, which is able to control training-time unseen aspect combinations by simply concatenating plugins. 
 * We provide a theoretical lower bound along with empirical analyses for the mutual interference problem, which we believe will facilitate future research. 
 * Experiments show that our approach has lower mutual interference, leading to superiority over strong baselines on text quality, constraint accuracy, and extensibility."
Can Large Language Models Be an Alternative to Human Evaluations?,2305.01937v1,./img_ACL_2023/2305.01937v1.pdf,"Illustration of the core idea of the paper using open-ended story generation as the example task. The left part shows the instruction, story fragments, and questions used in human evaluation. The human experts are asked to rate the quality of the story fragments using a 5-point Likert scale, shown on the upper right. The lower right part shows the process of LLM evaluation, where we feed the LLMs the same instruction, story fragments, and questions and parse the LLM-generated output to get the rating.","Human evaluation is indispensable and inevitable for assessing the quality of texts generated by machine learning models or written by humans. However, human evaluation is very difficult to reproduce and its quality is notoriously unstable, hindering fair comparisons among different natural language processing (NLP) models and algorithms. Recently, large language models (LLMs) have demonstrated exceptional performance on unseen tasks when only the task instructions are provided. In this paper, we explore if such an ability of the LLMs can be used as an alternative to human evaluation. We present the LLMs with the exact same instructions, samples to be evaluated, and questions used to conduct human evaluation, and then ask the LLMs to generate responses to those questions; we dub this LLM evaluation. We use human evaluation and LLM evaluation to evaluate the texts in two NLP tasks: open-ended story generation and adversarial attacks. We show that the result of LLM evaluation is consistent with the results obtained by expert human evaluation: the texts rated higher by human experts are also rated higher by the LLMs. We also find that the results of LLM evaluation are stable over different formatting of the task instructions and the sampling algorithm used to generate the answer. We are the first to show the potential of using LLMs to assess the quality of texts and discuss the limitations and ethical considerations of LLM evaluation.","Human evaluation is an important method to understand the performance of an NLP model or algorithm. We rely on human evaluation because there are certain aspects of texts that are hard to evaluate using automatic evaluation metrics; thus, researchers resort to humans to rate the quality of the output of NLP models. While human evaluation is prevalent and indispensable in NLP, it is notoriously unstable. has shown that low-quality workforces in human evaluation can have a detrimental effect on the evaluation result, making it impossible to compare the performance among different systems. Reproducibility is another issue in human evaluation since it is hard to recruit the same human evaluators and rerun the same evaluation. Even if the same workers are recruited, the workers that have seen the task before are likely to produce a different evaluation result the next time because they have already done the task. While human evaluation is used to better assess NLP systems and has some advantages over automatic evaluation metrics, the drawbacks of human evaluation somewhat make it difficult to reliably evaluate NLP systems. 
To resolve some of the drawbacks, we take advantage of large language models (LLMs). LLMs are large models that are trained to model human languages using self-supervised learning and further using special training procedures to improve the performance on unseen tasks and better follow natural language instructions. The ability to perform a task just given the task instructions motivates us to ask if these LLMs can perform what humans do in human evaluation. To answer this question, we feed in the LLM with the same instruction, sample, and question used in human evaluation, and take the sequences generated by the LLM as the LLM's answer to the question. This process is shown in Figure, and we call this process LLM evaluation. 
To test if LLM evaluation yields meaningful results, we conduct LLM evaluation on two different NLP tasks: evaluating the quality of stories in open-ended story generation and the quality of sentences generated by adversarial attacks. We summarize our findings and contribution as follows: 
 
 * We show that LLM evaluation produces results similar to expert human evaluation, verifying the effectiveness of LLM evaluation (and). This paper is the first to propose using LLMs as an alternative to human evaluation and show their effectiveness. 
 * We show that LLM evaluation results only slightly vary due to different task instructions and the hyperparameters of the sampling algorithm used to generate the answer. (and) * We carefully discuss the pros and cons of using LLM evaluation and discuss the ethical considerations of LLM evaluation. () "
Plug-and-Play Document Modules for Pre-trained Models,2305.17660v1,./img_ACL_2023/2305.17660v1.pdf,"Illustration of plug-and-play document modules. Document encoding is decoupled from concrete tasks. By plugging document plugins into task-specific models, we can handle multiple tasks such as question answering, fact verification, and slot filling.","Large-scale pre-trained models (PTMs) have been widely used in document-oriented NLP tasks, such as question answering. However, the encoding-task coupling requirement results in the repeated encoding of the same documents for different tasks and queries, which is highly computationally inefficient. To this end, we target to decouple document encoding from downstream tasks, and propose to represent each document as a plug-and-play document module, i.e., a document plugin, for PTMs (PlugD). By inserting document plugins into the backbone PTM for downstream tasks, we can encode a document one time to handle multiple tasks, which is more efficient than conventional encoding-task coupling methods that simultaneously encode documents and input queries using task-specific encoders. Extensive experiments on 8 datasets of 4 typical NLP tasks show that PlugD enables models to encode documents once and for all across different scenarios. Especially, PlugD can save","In recent years, large-scale pre-trained models (PTMs) have been widely adopted and achieved breakthrough performance for document-oriented NLP tasks, such as question answering. However, due to the tight coupling of document encoding and concrete tasks, PTMs have to dynamically generate document representations according to specific tasks and queries, leading to the repeated encoding of the same documents in different applications. For example, Wikipedia documents are commonly used in various knowledge-intensive tasks such as question answering, fact verification, and dialogue generation. In this case, existing methods separately encode one document for each task or even for each input query (e.g., a question for question answering, a claim for fact verification), making them highly computationally inefficient. To this end, it raises a natural question: can we decouple document encoding from concrete tasks, encoding documents only once and with guaranteed transferability across multiple tasks? 
For this question, we propose a novel framework based on PTMs to decouple document encoding from tasks, named PlugD. Specifically, PlugD incorporates plug-and-play modules to store document information and utilizes a PTM backbone to capture information from plugins for task reasoning. As shown in Figure, documents are encoded into pluggable plugins once and for all before task adaptation. The semantics and knowledge of documents can be injected into task-specific models by plugging document plugins. During task reasoning, the task-specific models can activate the information encoded in the document plugins to handle the input queries. In this way, PlugD can decouple the document encoding from downstream task reasoning and reduce the computation costs. 
For representing documents as pluggable modules, there are two main challenges: (1) Plugin learning: The document plugins must be effective for various downstream tasks, requiring them to contain sufficient semantics and knowledge. (2) Plugin utilization: Once the document plugins are ready, it is important for task-specific models to capture relevant information from them effectively for task reasoning. 
As for plugin learning, we adopt a self-supervised method, which requires document plugins to provide sufficient knowledge for the PTM to make predictions. Specifically, for each document, we first randomly select parts of sentences as a query and use the remaining sentences as context to learn plugins. Then, after encoding the context into plugins, the model is required to recover the masked recurring spans or generate the next sentences for the query based on the plugin knowledge. 
As for plugin utilization, we propose two strategies to utilize document plugins for downstream tasks: plugging during tuning and plugging after tuning. For plugging during tuning, document plugins are utilized in both tuning and inference stages. Task data and document plugins are combined together to train task-specific models. For plugging after tuning, document plugins are only utilized in the inference stage to provide external knowledge. Document plugins are adopted as a post-processing way to inject knowledge into task-specific models without additional training. 
To verify the effectiveness of our plug-and-play framework, we adopt Wikipedia as our document collection and conduct experiments on 
 8datasets of4typical knowledge-intensive NLP tasks. The results show that we can generate document plugins once and successfully adapt plugins to various downstream tasks. Compared to competitive baselines that encode documents and task-specific inputs simultaneously, our plugin-based method can save69%computational costs with comparable performance. Besides, utilizing document plugins works as an effective post-processing approach to introducing the knowledge of documents into task-specific models and achieving performance improvements without model training. We argue that with the current trend of increasing the model size of PTMs, decoupling document encoding from concrete tasks like PlugD can be a promising direction that enables large PTMs to effectively and efficiently serve diverse downstream tasks."
Human-in-the-loop Evaluation for Early Misinformation Detection: A Case Study of COVID-19 Treatments,2212.09683v4,./img_ACL_2023/2212.09683v4.pdf,"Overview of our human-in-the-loop evaluation framework for early misinformation detection. In stage one (left), a system extracts check-worthy claims directly from raw tweets in the wild (rather than retrieving relevant tweets based on provided claims), then aggregates trending claims to be validated by human experts. In stage two (right), the system classifies authors' stances toward false claims and flags tweets for further manual inspection.","We present a human-in-the-loop evaluation framework for fact-checking novel misinformation claims and identifying social media messages that support them. Our approach extracts check-worthy claims, which are aggregated and ranked for review. Stance classifiers are then used to identify tweets supporting novel misinformation claims, which are further reviewed to determine whether they violate relevant policies. To demonstrate the feasibility of our approach, we develop a baseline system based on modern NLP methods for human-in-the-loop fact-checking in the domain of COVID-19 treatments. We make our data and detailed annotation guidelines available to support the evaluation of human-in-the-loop systems that identify novel misinformation directly from raw user-generated content.","As many people now get information from social networking websites such as Facebook and Twitter, misinformation has become a serious societal problem. To address this, social media companies have spent billions on content moderation. Prior work on developing natural language processing systems to combat misinformation has mainly focused on various sub-tasks, including claim detection, evidence retrieval, fact verification, stance classification, and fallacy recognition. Researchers have also attempted to perform early detection of novel misinformation claims, as it is crucial for supporting early interventions such as pre-bunking. However, evaluations are often set up automatically using datasets that were retrospectively constructed based on a predefined set of debunked claims. 
Recent work by presented convincing evidence that existing NLP fact-checking pipelines are unsuitable for detecting novel real-world misinformation. They show these systems rely on leaked counter-evidence from news sources that have already fact-checked the claim. In general, it is unrealistic to assume this type of evidence will be available for new claims that have not yet been widely spread. 
In this paper, we address this challenge by presenting a more realistic human-in-the-loop detection and evaluation framework that can measure a system 's capabilities for detecting novel check-worthy claims in the wild (see Figure). We focus on discovering new, domain-specific claims from raw tweets which are then verified by humans, rather than relying on a pre-defined list of claims that have already been fact-checked for evaluation. More importantly, we consider not only the accuracy but also the volume, relevance, and timeliness of misinformation claims automatically identified by a system, given a collection of raw tweets. We argue this approach provides more realistic experimental conditions because (1) it does not rely on leaked counter-evidence from claims that have already been fact-checked, (2) human expertise is vital in verifying the truthfulness of claims and (3) it is more effective for humans to check aggregated claims within a specific domain (e.g., claims about the efficacy of COVID-19 treatments), before proceeding to individual social media messages to determine if they violate specific misinformation policies. 
We validate our methodology for end-to-end misinformation detection in the domain of COVID-19 treatments. COVID-19 treatments make an ideal testbed for human-in-the-loop misinformation extraction because Twitter has provided clearly defined policies in this area, which we use as guidelines in a realistic human evaluation of a system' s output. We evaluate our baseline system with our four defined metrics and find that 
 18%of the top-50trending claims were actually misleading (relevance),50%of new misleading claims (unapproved COVID-19 treatments) are detected before they are debunked by journalists in a news article (timeliness),65%of tweets flagged constitute policy-violations (accuracy), and an average of124policy violations can be confirmed by a human-annotator per hour (volume) when using our system. 
 
 Our work fills an important gap in the literature, by showing that it is possible to construct a realistic end-to-end evaluation that supports the early detection of novel rumors directly from raw data. Instead of classifying individual tweets as rumorous or not, we extract phrase-level claims that can be aggregated and ranked across a large amount of data and thus can be reviewed more time-efficiently by fact-checkers for human evaluation and for real-world applications. Tweets that are automatically classified as supporting misinformation claims can then be reviewed to determine whether they violate relevant policies."
Crosslingual Generalization through Multitask Finetuning,2211.01786v2,./img_ACL_2023/2211.01786v2.pdf,An overview of datasets in xP3. Datasets added to P3 in this work are marked bold. Yellow datasets are trained on. Green datasets are held out for evaluation.,"Multitask prompted finetuning (MTF) has been shown to help large language models generalize to new tasks in a zero-shot setting, but so far explorations of MTF have focused on English data and models. We apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned variants called BLOOMZ and mT0. We find finetuning large multilingual language models on English tasks with English prompts allows for task generalization to non-English languages that appear only in the pretraining corpus. Finetuning on multilingual tasks with English prompts further improves performance on English and non-English tasks leading to various state-of-the-art zero-shot results. We also investigate finetuning on multilingual tasks with prompts that have been machine-translated from English to match the language of each dataset. We find training on these machine-translated prompts leads to better performance on human-written prompts in the respective languages. Surprisingly, we find models are capable of zero-shot generalization to tasks in languages they have never intentionally seen. We conjecture that the models are learning higher-level capabilities that are both taskand language-agnostic. In addition, we introduce xP3, a composite of supervised datasets in 46 languages with English and machine-translated prompts. Our code, datasets and models are freely available at <https: //github. com/bigscience-workshop/xmtf>.","Large language models pretrained on vast amounts of text show some capability of solving tasks expressed in natural language, even without explicit training on these tasks. Finetuning on groups of language tasks has been shown to significantly boost this zero-shot task generalization of language models. For example, finetune on tasks like summarization and question answering leading to better performance on unseen tasks like natural language inference. Previous work has focused on multitask finetuning in the context of large English language models and tasks. 
Multilingual large language models show the same zero-shot learning capabilities for both monolingual and crosslingual tasks. However, zero-shot performance tends to be significantly lower than finetuned performance. Thus, task-specific or language-specific transfer learning via finetuning remains the predominant practice. This is particularly challenging for low-resource languages or tasks with limited data available, such as writing a fable that teaches a specified moral. In the spirit of multitask finetuning, it would be desirable to improve the zero-shot task generalization of multilingual models to make them usable on tasks from low-resource languages without requiring further finetuning. 
To address this goal, we focus on crosslingual multitask finetuning. Due to the difficulty of collecting supervised task data in low-resource languages, previous work typically aims to transfer capabilities learned from finetuning on English data, which can improve performance on non-English language tasks. We investigate whether English-only multitask finetuning also improves performance on non-English held-out tasks using the multilingual BLOOM and mT5 models. We find that after finetuning on the English-only multitask mixture used for T0 (P3), performance on a diverse set of non-English held-out tasks increases. 
To investigate whether multilingual task data can further improve performance, we extend P3 to xP3 by adding datasets from 46 different languages that cover tasks previously not present in P3 (such as translation and program synthesis). Finetuning on xP3 leads to even better zero-shot task generalization in both English and non-English compared to the P3-trained baseline. Models finetuned on xP3 perform best on English prompts, even for non-English samples. Hypothesizing that better performance could be attained by training on non-English prompts, we construct a variant of xP3 with machine-translated prompts called xP3mt. We find that finetuning on machine-translated prompts is enough to significantly increase performance on held-out tasks with non-English human-written prompts. However, reducing the number of English prompts in the finetuning also worsens English prompt performance on multilingual tasks. 
Notably, we also find that models finetuned on xP3 generalize to held-out tasks in languages never intentionally seen during pretraining nor finetuning. We conduct a contamination analysis and find that only small amounts of these languages were included in the pretraining corpus. Thus, we hypothesize the models learn some languageand task-agnostic capabilities. 
We publicly release all our datasets and models (URLs in Appendix)."
"Facilitating Fine-grained Detection of Chinese Toxic Language: Hierarchical Taxonomy, Resources, and Benchmarks",2305.04446v1,./img_ACL_2023/2305.04446v1.pdf,"Monitor Toxic Frame illustration. The framework introduces four questions to determine whether a comment is general offensive language or hate speech, and further analyzes the attacked group and expression type.","Disclaimer: The samples presented by this paper may be considered offensive or vulgar. 
The widespread dissemination of toxic online posts is increasingly damaging to society. However, research on detecting toxic language in Chinese has lagged significantly. Existing datasets lack fine-grained annotation of toxic types and expressions, and ignore the samples with indirect toxicity. In addition, it is crucial to introduce lexical knowledge to detect the toxicity of posts, which has been a challenge for researchers. In this paper, we facilitate the fine-grained detection of Chinese toxic language. First, we built Monitor Toxic Frame, a hierarchical taxonomy to analyze toxic types and expressions. Then, a fine-grained dataset ToxiCN is presented, including both direct and indirect toxic samples. We also build an insult lexicon containing implicit profanity and propose Toxic Knowledge Enhancement (TKE) as a benchmark, incorporating the lexical feature to detect toxic language. In the experimental stage, we demonstrate the effectiveness of TKE. After that, a systematic quantitative and qualitative analysis of the findings is given.","More and more people have acquired information from social media platforms where posts containing toxic language are also rampant. Toxic language is viewed as a rude, disrespectful, or unreasonable comment that is likely to make someone leave a discussion. Due to its negative impact on individuals and society, toxic language has been rapidly recognized as an increasing concern. Recently, researchers have tackled the problem of toxic language detection using the techniques of natural language processing, making great progress in many languages. . 
In contrast, the relevant research on Chinese toxic language detection has lagged significantly. There are two key issues that have been overlooked. First, existing studies lack a fine-grained annotation of textual toxic types, resulting in hate speech being conflated with general offensive language. Compared to hate speech, general offensive language does not attack groups with special social attributes, and it is just used to emphasize emotions in many cases. Like Exp. 1 in Table, the insult ""fuck"" can be considered as a modal particle to express surprise. Since there is no equivalence between general offensive language and hate speech, it is crucial to determine their boundary conditions. 
In addition, most studies on toxic Chinese language only concentrate on detecting direct and explicit bias and offense. And they lose sight of indirect expressions including implicit hatred (e.g., stereotype and irony) and reporting experiences of discrimination. Due to the absence of direct swear words, these indirect toxic samples are obviously harder to be filtered. To further illustrate the distinction of several expressions, a few examples are listed in Table. Meanwhile, compared to English, Chinese has richer variants of profanity with implicit toxic meaning, which brings challenge to research on toxic language detection. However, the existing insult lexicon fails to cover these terms. An example is ""fairy"" in Exp. 3, which itself is a positive word and is used here to implicitly attack women. Due to the significance of lexical knowledge to detect toxic language, it is important to construct an insult lexicon containing implicit toxic terms. 
To fill these gaps, we facilitate fine-grained detection of Chinese toxic language. To distinguish hate speech from general offensive language and analyze the expressions of samples, we first introduce Monitor Toxic Frame, a hierarchical taxonomy. Based on the taxonomy, the posts are progressively divided into diverse granularities as follows: (I) Whether Toxic, (II) Toxic Type (general offensive language or hate speech), (III) Targeted Group, (IV) Expression Category (explicitness, implicitness, or reporting). After taking several measures to alleviate the bias of annotators, we then conduct a fine-grained annotation of posts, including both direct and indirect toxic samples. And ToxiCN dataset is presented, which has 12k comments containing sexism, racism, regional bias, and anti-LGBTQ. 
For the convenient detection of toxic language, we construct an insult lexicon against different targeted groups. It contains not only explicit profanities but also implicit words with toxic meanings, such as ironic metaphors (e.g., ""fairy""). To exploit the lexical feature, we further present a migratable benchmark of Toxic Knowledge Enhancement (TKE), enriching the text representation. In the evaluation phase, several benchmarks with TKE are utilized to detect toxic language, demonstrating its effectiveness. After that, we analyze the experimental results in detail and offer our suggestions for identifying toxic language. The main contributions of this work are summarized as follows: 
 
 * We present a hierarchical taxonomy, Monitor Toxic Frame, to progressively explore the toxic types and expressions of samples from diverse granularities. 
 * Based on the taxonomy, we propose ToxiCN, a fine-grained dataset of Chinese toxic language. It divides hate speech from offensive language, including samples with not only direct offense but also indirect expressions. 
 * We present an insult lexicon, and a Toxic Knowledge Enhancement benchmark to incorporate the lexical feature. We evaluate its performance at different levels and conduct an exhaustive analysis."
SpeechMatrix: A Large-Scale Mined Corpus of Multilingual Speech-to-Speech Translations,2211.04508v1,./img_ACL_2023/2211.04508v1.png,Architecture of speech encoders training.,"We present, a large-scale multilingual corpus of speech-to-speech translations mined from real speech of European Parliament recordings. It contains speech alignments in 136 language pairs with a total of 418 thousand hours of speech. To evaluate the quality of this parallel speech, we train bilingual speech-to-speech translation models on mined data only and establish extensive baseline results on Europarl-ST, and FLEURS test sets. Enabled by the multilinguality of, we also explore multilingual speech-to-speech translation, a topic which was addressed by few other works. We also demonstrate that model pre-training and sparse scaling using Mixture-of-Experts bring large gains to translation performance. The mined data and models are freely available.","Research has progressed in the area of speech-to-speech translation (S2ST) with the goal of seamless communication among people who speak different languages. Direct S2ST models attract increasing research interest, e.g.,. Compared to conventional cascaded models, direct models do not rely on intermediate text representations which make them applicable to the translation of languages without a well-defined writing script. Moreover, direct S2ST have the advantage of higher training and inference efficiency.
Despite the benefits of direct approaches, model training is faced with the major issue of data scarcity. Human labeled speech data is expensive to create, there are very few data resources providing parallel speech, and the data amount is quite limited. To mitigate the data scarcity, some works have leveraged multitask learning, data augmentation with speech variation, or with synthesized speech, and knowledge from pre-trained models such as HuBERT, wav2vec 2.0 and mBART. 
Recently, the multilingual speech/text sentence embedding space from enabled the first speech mining results, aligning speech and text in different languages. Using this mined data to train direct speech-to-text and speech-to-speech translation systems can improve the performance of such models. Finally, showed that such multilingual and multimodal sentence embeddings could be decoded into different languages and/or modalities in a zero-shot way, which suggests that multilingual speech content is well encoded in these fixed-size representations. 
In this work, we trained speech encoders for 17 languages and mined speech-to-speech alignments for all possible language pairs. To the best of our knowledge, is by far the largest freely available speech-to-speech translation corpus, with 136 language directions and an average of 1,537 hours of source speech in each direction for a total of 418 thousand hours. We demonstrate that strong S2ST models can be trained with these mined data and validate the good quality of the speech alignments across languages. We are open-sourcing the mined data, the speech encoders used for mining, multilingual HuBERT models in four language families for target unit generation, language-specific vocoders for speech synthesis from discrete units, and S2S models trained and presented in this work."
FSUIE: A Novel Fuzzy Span Mechanism for Universal Information Extraction,2306.14913v1,./img_ACL_2023/2306.14913v1.pdf,"An example of annotation ambiguity for ""vehicle"" entity in sentence ""On the 3rd September evening, I saw a yellow sports car drive past my house"".","Universal Information Extraction (UIE) has been introduced as a unified framework for various Information Extraction (IE) tasks and has achieved widespread success. Despite this, UIE models have limitations. For example, they rely heavily on span boundaries in the data during training, which does not reflect the reality of span annotation challenges. Slight adjustments to positions can also meet requirements. Additionally, UIE models lack attention to the limited span length feature in IE. To address these deficiencies, we propose the Fuzzy Span Universal Information Extraction (FSUIE) framework. Specifically, our contribution consists of two concepts: fuzzy span loss and fuzzy span attention. Our experimental results on a series of main IE tasks show significant improvement compared to the baseline, especially in terms of fast convergence and strong performance with small amounts of data and training epochs. These results demonstrate the effectiveness and generalization of FSUIE in different tasks, settings, and scenarios.","Information Extraction (IE) is focused on extracting predefined types of information from unstructured text sources, such as Named Entity Recognition (NER), Relationship Extraction (RE), and Sentiment Extraction (SE). To uniformly model the various IE tasks under a unified framework, a generative Universal Information Extraction (UIE) was proposed in and has achieved widespread success on various IE datasets and benchmarks. Due to the necessity of a powerful generative pre-training model for the generative UIE, the time overhead is extensive and the efficiency is not satisfactory. For this reason, this paper examines span-based UIE to unify various IE tasks, conceptualizing IE tasks as predictions of spans. 
However, UIE models still have some limitations. First, as it is the process of training machine learning models to extract specific information from unstructured text sources, IE relies heavily on human annotation which involves labeling the data by identifying the specific information to be extracted and marking the corresponding span boundaries in the text manually. However, due to the complexity of natural language, determining the correct span boundaries can be challenging, leading to the phenomenon of annotation ambiguity. As shown in Figure, different annotated spans can be considered reasonable. In the span learning of UIE models, the method of teacher forcing is commonly used for loss calculation, making the model dependent on the precise span boundaries given in the training data. This can cause performance bottlenecks due to annotation ambiguity. 
When the model structure in UIE places too much emphasis on the exact boundaries of IE tasks, it leads to insufficient utilization of supervision information. In order to predict span boundaries, positions closer to the ground-truth should be more accurate than those relatively farther away, as shown in Figure. For example, words close to the target ""car"" are more likely to be correct than the word ""evening"" which is farther away from the target. Under the premise of positioning to the span where ""car"" is located, both the ""yellow car"" and the ""yellow sports car"" can be regarded as vehicle entities. This means that the span model learned should be fuzzy rather than precise. 
In addition, the use of pre-trained Transformer in UIE to extract the start and end position representations also poses a problem. The Transformer model is designed to focus on the global representation of the input text, while UIE requires focusing on specific parts of the text to determine the span boundaries. This mismatch between the Transformer 's focus on global representation and UIE' s focus on specific parts of the text can negatively impact the performance of the model. 
When there is a mismatch between the Transformer architecture and the span representation learning, the model may not make good use of prior knowledge in IE. Specifically, given the start boundary (end boundary) of the label span, the corresponding end boundary (start boundary) is more likely to be found within a certain range before and after, rather than throughout the entire sequence. This is a prior hypotheses that span has limited length, which is ignored in the vanilla UIE model. To address this, a fuzzy span attention mechanism, rather than fixed attention, should be applied. 
In this paper, we propose the Fuzzy Span Universal Information Extraction (FSUIE) framework that addresses the limitations of UIE models by applying the fuzzy span feature, reducing over-reliance on label span boundaries and adaptively adjusting attention span length. Specifically, to solve the issue of fuzzy boundaries, we design the fuzzy span loss that quantitatively represents the correctness information distributed on fuzzy span. At the same time, we introduce fuzzy span attention that sets the scope of attention to a fuzzy range and adaptively adjusts the length of span according to the encoding. We conduct experiments on various main IE tasks (NER, RE, and ASTE). The results show that our FSUIE has a significant improvement compared to the strong UIE baseline in different settings. Additionally, it achieves new state-of-the-art performance on some NER, RE, and ASTE benchmarks with only bert-base architecture, outperforming models with stronger pre-trained language models and complex neural designs. Furthermore, our model shows extremely fast convergence, and good generalization on low-resource settings. These experiments demonstrate the effectiveness and generalization of FSUIE in different tasks, settings, and scenarios."
